# Test Design: Story 3.1

Date: 2025-10-29
Designer: Quinn (Test Architect)

## Test Strategy Overview

- Total test scenarios: 7
- Unit tests: 3 (43%)
- Integration tests: 3 (43%)
- E2E tests: 1 (14%)
- Priority distribution: P0: 3, P1: 2, P2: 2

## Test Scenarios by Acceptance Criteria

### AC1: Dynamic batch sizing avoids 12 GB OOM & logs leasing events

| ID           | Level       | Priority | Test                                                                                                                    | Justification                                             |
| ------------ | ----------- | -------- | ----------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| 3.1-UNIT-001 | unit        | P1       | Verify `_calculate_optimal_batch_size` uses GPU config + safety factor                                                  | Direct logic coverage without GPU dependency              |
| 3.1-INT-001  | integration | P0       | Run executor through `BatchRunner` with mocked lease to ensure telemetry logs acquire/release including batch rationale | Validates orchestration wiring rather than isolated class |
| 3.1-E2E-001  | e2e         | P0       | CLI search triggering rerank on GPU; assert telemetry and rerank payload exported                                       | Guarantees end-user flow exercises executor               |

### AC2: Unit tests simulate multi-GPU leases and OOM recovery

| ID           | Level       | Priority | Test                                                                 | Justification                                    |
| ------------ | ----------- | -------- | -------------------------------------------------------------------- | ------------------------------------------------ |
| 3.1-UNIT-002 | unit        | P0       | Simulate OutOfMemoryError to confirm batch halving and retry counter | Core resilience logic                            |
| 3.1-INT-002  | integration | P0       | BatchRunner scenario forcing OOM fallback via mocked CrossEncoder    | Ensures pipeline surfaces OOM handling telemetry |

### AC3: Executor returns ranked results plus telemetry payload

| ID           | Level       | Priority | Test                                                                                 | Justification                                      |
| ------------ | ----------- | -------- | ------------------------------------------------------------------------------------ | -------------------------------------------------- |
| 3.1-UNIT-003 | unit        | P2       | Validate `CrossEncoderRerankRun.to_dict` and query truncation                        | Low-cost validation of serialization & privacy     |
| 3.1-INT-003  | integration | P1       | Combine executor with telemetry export runtime to ensure payload appended to summary | Connects executor output with downstream consumers |
| 3.1-INT-004  | integration | P2       | CPU-only fallback scenario verifying zero GPU telemetry but valid scores             | Ensures maintainability for non-GPU deployments    |

## Risk Coverage

- TECH-001 mapped to 3.1-INT-001, 3.1-E2E-001.
- OPS-001 mapped to 3.1-INT-002 and 3.1-INT-003.
- PERF-001 mapped to 3.1-UNIT-001 and instrumentation added in 3.1-E2E-001.

## Recommended Execution Order

1. P0 unit tests (3.1-UNIT-002) to guarantee OOM resilience.
2. P0 integration tests (3.1-INT-001, 3.1-INT-002) establishing wiring + telemetry.
3. P0/P1 E2E flow (3.1-E2E-001) to confirm user-facing behaviour.
4. Remaining P1/P2 scenarios to solidify serialization and CPU fallback confidence.

Test design matrix: docs/qa/assessments/3.1-test-design-20251029.md
P0 tests identified: 3
