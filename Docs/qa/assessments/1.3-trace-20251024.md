# Requirements Traceability Matrix

## Story: 1.3 - Telemetry & Monitoring Baseline Updates

**Generated:** 2025-10-24  
**Story Status:** Done  
**Task 1 Status:** Complete (OTel span attributes added)  
**Remaining Tasks:** 2 (Prometheus metrics), 3 (Documentation), 4 (Smoke tests) - Incomplete

---

## Coverage Summary

- **Total Requirements:** 7 (3 ACs √ó multiple aspects + 4 task groups)
- **Fully Covered:** 2 (29%)
- **Partially Covered:** 2 (29%)
- **Not Covered:** 3 (43%)

**Overall Assessment:** üî¥ **HIGH RISK** - Critical gaps in Prometheus metrics, documentation, and smoke test validation

---

## Requirement Mappings

### AC1: Telemetry emits stage indicators when rerank/sparse are active, capturing GPU leasing metadata per stage

**Coverage: PARTIAL** ‚ö†Ô∏è

#### Requirement Breakdown

##### AC1.1: OpenTelemetry Span Emission

**Coverage: FULL** ‚úÖ

**Unit Tests:**

- **Test:** `test_build_processing_summary_includes_stage_sections`
  - **File:** `tests/test_processing_summary.py:19`
  - **Given:** Feature toggles with rerank/sparse enabled, built stage summaries with telemetry
  - **When:** `build_processing_summary()` called with rerank and sparse stages active
  - **Then:** Summary includes `rerank_run`, `sparse_run`, and `telemetry.spans` with `rag.rerank` and `rag.sparse` span events with status "active"
  - **Coverage Level:** Unit - validates summary structure

- **Test:** `test_write_processing_summary_generates_default_sections`
  - **File:** `tests/test_processing_summary.py:253`
  - **Given:** Full embedder pipeline with toggles enabled
  - **When:** Pipeline executes and writes processing summary
  - **Then:** Summary JSON contains `telemetry.spans["rag.rerank"]` and `telemetry.spans["rag.sparse"]` with status in `{"active", "skipped"}`
  - **Coverage Level:** Integration - validates end-to-end span creation

**Implementation:**

- **Module:** `processor/ultimate_embedder/telemetry.py:161`
  - `record_span_presence()` method accepts OTel-style attributes: `latency_ms`, `batch_size`, `gpu_peak_gb`, `candidate_count`, `fallback_used`, `**kwargs`
  
- **Module:** `processor/ultimate_embedder/core.py:1539-1546`
  - Rerank stage: Emits `rag.rerank` span with `candidate_count`, `fallback_used` attributes
  
- **Module:** `processor/ultimate_embedder/core.py:1596-1604`
  - Sparse stage: Emits `rag.sparse` span with `batch_size`, `coverage_ratio`, `fallback_used` attributes
  
- **Module:** `processor/ultimate_embedder/batch_runner.py:638-648`
  - Dense stage: Emits `rag.dense` span with `latency_ms`, `batch_size`, `gpu_peak_gb`, `models_executed`, `chunks_per_second` attributes
  
- **Module:** `processor/ultimate_embedder/export_runtime.py:130-139`
  - Export stage: Emits `rag.export` span with `latency_ms`, `batch_size`, `file_count`, `total_size_mb`, export format flags

##### AC1.2: GPU Leasing Metadata Capture

**Coverage: FULL** ‚úÖ

**Tests:**

- **Test:** `test_processing_summary_includes_performance_baseline`
  - **File:** `tests/test_processing_summary.py:602`
  - **Given:** Embedder with GPU memory snapshots recorded via `telemetry.record_gpu_snapshot()`
  - **When:** Performance baseline created in summary
  - **Then:** Summary contains `performance_baseline.gpu` with `peak_memory_used_gb`, `soft_limit_gb`, `soft_limit_exceeded` flag
  - **Coverage Level:** Integration

- **Test:** `test_performance_baseline_flags_soft_limit_exceedance`
  - **File:** `tests/test_processing_summary.py:744`
  - **Given:** GPU snapshot exceeding soft limit (11.5GB allocated, 10GB limit)
  - **When:** Summary generated with soft limit validation
  - **Then:** `performance_baseline.gpu.soft_limit_exceeded = True`, `status = "exceeded_soft_limit"`, `soft_limit_devices = [0]`
  - **Coverage Level:** Unit - validates GPU limit detection

**Implementation:**

- **Module:** `processor/ultimate_embedder/telemetry.py:271-313`
  - `record_gpu_snapshot()` captures per-device GPU memory with soft limit validation
  - `record_gpu_lease_event()` logs lease lifecycle (acquire/release) with VRAM snapshots

##### AC1.3: Prometheus Metrics Emission

**Coverage: NONE** ‚ùå **CRITICAL GAP**

**Gap:** No actual Prometheus metric emission implementation found. Only recording of metric **status** exists.

**Evidence:**

- **Module:** `processor/ultimate_embedder/core.py:500-530`
  - `_emit_metrics_for_stage()` exists but only calls `telemetry.record_metrics_status()` - does NOT emit to Prometheus
  - Metric registry defines: `rag_rerank_latency_seconds`, `rag_sparse_latency_seconds`, `rag_gpu_peak_bytes`, `rag_dense_latency_seconds`, `rag_export_latency_seconds`
  - No Prometheus client library usage found (no `prometheus_client` imports)

**Uncovered Requirements:**

- Actual histogram emission for `rag_rerank_latency_seconds`
- Actual gauge emission for `rag_gpu_peak_bytes{stage=...}`
- Prometheus client integration
- Metric scrape endpoint exposure

**Risk:** HIGH - Operators cannot monitor production systems without actual metrics

---

### AC2: Dashboards or runbooks document new metrics and default expectations

**Coverage: NONE** ‚ùå **CRITICAL GAP**

**Gap:** No runbook or dashboard documentation exists.

**Expected Artifact:** `docs/telemetry/rerank_sparse_signals.md` (per story Dev Notes and architecture)

**File Search Result:** No files found in `docs/telemetry/`

**Uncovered Requirements:**

- Metric definitions (what each metric measures)
- Expected value ranges for default configurations
- Alert threshold recommendations
- Troubleshooting playbooks for common failure modes
- Dashboard JSON/configuration examples

**Risk:** HIGH - Operators lack guidance for interpreting telemetry signals

---

### AC3: Smoke test run verifies telemetry coverage with rerank/sparse enabled and disabled

**Coverage: PARTIAL** ‚ö†Ô∏è

#### AC3.1: Toggle-State Span Validation

**Coverage: FULL** ‚úÖ

**Tests:**

- **Test:** `test_build_processing_summary_includes_stage_sections`
  - **File:** `tests/test_processing_summary.py:19`
  - **Given:** Toggles enabled
  - **When:** Summary built
  - **Then:** Spans present with status "active"

- **Test:** `test_build_processing_summary_omits_disabled_stages`
  - **File:** `tests/test_processing_summary.py:110`
  - **Given:** Toggles disabled (via CLI/env overrides)
  - **When:** Summary built
  - **Then:** `rerank_run` and `sparse_run` omitted, `telemetry.spans = {}`, `telemetry.metrics = {}`

- **Test:** `test_write_processing_summary_omits_disabled_sections`
  - **File:** `tests/test_processing_summary.py:365`
  - **Given:** Full pipeline with toggles disabled
  - **When:** Pipeline executes and writes summary
  - **Then:** Summary JSON excludes `rerank_run`, `sparse_run` sections; provenance shows "override" events

#### AC3.2: Metrics Emission Status Validation

**Coverage: PARTIAL** ‚ö†Ô∏è

**Tests:**

- **Test:** `test_write_processing_summary_generates_default_sections`
  - **File:** `tests/test_processing_summary.py:253`
  - **Given:** Pipeline with toggles enabled
  - **When:** Summary generated
  - **Then:** `telemetry.metrics["rerank"]["status"] in {"emitted", "skipped"}`
  - **Coverage Level:** Integration - validates metrics status recording

**Gap:** No validation that metrics status = "emitted" corresponds to **actual** Prometheus emission (because emission doesn't exist yet)

#### AC3.3: Manifest Section Alignment

**Coverage: NONE** ‚ùå

**Gap:** No smoke tests asserting `processing_summary.json` sections (`rerank_run`, `sparse_run`) align with toggle states in **actual pipeline runs**.

**Current Test Limitation:** `test_write_processing_summary_generates_default_sections` uses dummy models via monkeypatching - not a true end-to-end smoke test.

**Uncovered Requirements:**

- Smoke test with real embedding models (or realistic mocks)
- Assert `CrossEncoderRerankRun` fields populated when `enable_rerank=True`
- Assert `SparseInferenceRun` fields populated when `enable_sparse=True`
- Assert sections absent when toggles disabled
- CLI summary footer assertions for metrics emission status

#### AC3.4: CLI Summary Assertions

**Coverage: NONE** ‚ùå

**Gap:** No automated assertions on CLI stdout/stderr output validating metrics emission status reporting.

**Expected Behavior (per story):** CLI footer should report:
```
Metrics emission summary:
  rag.rerank: emitted (rag_rerank_latency_seconds)
  rag.sparse: emitted (rag_sparse_latency_seconds)
  gpu_soft_limit: within_limit
```

**Uncovered Requirements:**

- Parse CLI output in smoke test
- Assert emission status lines present
- Assert skip reasons printed when stages disabled

---

## Task-Level Coverage Analysis

### Task 1: Extend OpenTelemetry spans with GPU leasing metadata

**Status:** ‚úÖ **COMPLETE**

**Subtasks:**

1.1. Update `telemetry.py` span helpers ‚úÖ
   - Implemented: `record_span_presence()` with `latency_ms`, `batch_size`, `gpu_peak_gb`, `candidate_count`, `fallback_used`, `**kwargs`
   - Test Coverage: Full

1.2. Ensure `BatchRunner` emits spans ‚úÖ
   - Implemented: `rag.dense`, `rag.sparse`, `rag.rerank`, `rag.export` spans in lifecycle hooks
   - Test Coverage: Full

**Change Log Evidence:** Task 1 marked complete on 2025-01-28 with detailed completion notes

---

### Task 2: Wire Prometheus metrics with CLI summaries

**Status:** ‚ùå **NOT STARTED**

**Subtasks:**

2.1. Wire Prometheus metrics into runtime ‚ùå
   - Gap: No Prometheus client integration
   - Gap: No actual metric emission (only status recording)
   - Gap: No scrape endpoint

2.2. Add emission guards for disabled stages ‚ö†Ô∏è PARTIAL
   - Implemented: `_emit_metrics_for_stage()` calls `record_metrics_status()` with skip reasons
   - Gap: Guards work for status recording but no actual metrics to guard

2.3. Persist metric emission outcomes into `processing_summary.json` ‚úÖ
   - Implemented: `telemetry.metrics` section in summary
   - Test Coverage: `test_write_processing_summary_generates_default_sections`

**Risk:** HIGH - Core requirement of AC1 unfulfilled

---

### Task 3: Publish telemetry dashboard/runbook updates

**Status:** ‚ùå **NOT STARTED**

**Subtasks:**

3.1. Create `docs/telemetry/rerank_sparse_signals.md` ‚ùå
   - Gap: File does not exist
   - Required Content: Metric definitions, expected ranges, default-on posture

3.2. Cross-link CLI/operator documentation ‚ùå
   - Gap: No telemetry guide to link
   - Required: Update user-facing docs with runbook references

**Risk:** HIGH - Operators lack actionable guidance

---

### Task 4: Implement smoke tests for telemetry coverage

**Status:** ‚ùå **NOT STARTED**

**Subtasks:**

4.1. Extend integration fixtures to validate rerank/sparse telemetry fields ‚ö†Ô∏è PARTIAL
   - Partial Coverage: `test_write_processing_summary_generates_default_sections` validates summary structure
   - Gap: No fixtures specifically for `CrossEncoderRerankRun` and `SparseInferenceRun` data models
   - Gap: Not true smoke tests (uses dummy models)

4.2. Automate CLI summary assertions ‚ùå
   - Gap: No tests asserting CLI stdout/stderr content
   - Required: Parse metrics footer and validate emission status lines

**Risk:** MEDIUM - Cannot verify telemetry coverage in real pipeline execution

---

## Critical Gaps Summary

### 1. Prometheus Metrics Emission (AC1, Task 2) - **P0**

**Severity:** CRITICAL  
**Impact:** Operators cannot monitor production systems  
**Probability:** 100% (confirmed absent)

**Missing Components:**

- Prometheus client library integration (`prometheus_client`)
- Histogram emission for latency metrics
- Gauge emission for GPU metrics
- Label support for `{stage=...}` attributes
- Metrics scrape endpoint (e.g., `/metrics` HTTP server)

**Suggested Implementation:**

```python
# In processor/ultimate_embedder/metrics.py (new file)
from prometheus_client import Histogram, Gauge, CollectorRegistry

RERANK_LATENCY = Histogram(
    'rag_rerank_latency_seconds',
    'Rerank stage latency',
    buckets=[0.1, 0.5, 1.0, 2.5, 5.0, 10.0]
)

GPU_PEAK_BYTES = Gauge(
    'rag_gpu_peak_bytes',
    'Peak GPU memory usage',
    ['stage']
)

# In _emit_metrics_for_stage():
if active and self.metrics_enabled:
    if stage == "rerank" and "latency_seconds" in details:
        RERANK_LATENCY.observe(details["latency_seconds"])
    if "gpu_peak_gb" in details:
        GPU_PEAK_BYTES.labels(stage=stage).set(details["gpu_peak_gb"] * 1e9)
```

**Test Scenario:**

```python
# tests/test_metrics_emission.py
def test_prometheus_metrics_emitted_for_rerank(monkeypatch):
    # Given: Rerank enabled, pipeline executes
    # When: generate_embeddings_kaggle_optimized() completes
    # Then: RERANK_LATENCY._sum._value > 0
    #       GPU_PEAK_BYTES.labels(stage="rerank")._value > 0
```

---

### 2. Telemetry Runbook Documentation (AC2, Task 3) - **P0**

**Severity:** HIGH  
**Impact:** Operators cannot interpret signals or troubleshoot failures  
**Probability:** 100% (confirmed absent)

**Required Content:**

```markdown
# docs/telemetry/rerank_sparse_signals.md

## Metrics Catalog

### rag_rerank_latency_seconds
- **Type:** Histogram
- **Description:** End-to-end rerank stage latency
- **Expected Range (default):** 0.5-5.0s for 100 candidates on CPU
- **Alert Threshold:** >10s indicates capacity issue

### rag_gpu_peak_bytes
- **Type:** Gauge (labeled by stage)
- **Description:** Peak GPU memory allocated during stage
- **Expected Range:** 6-10GB with 12GB soft limit
- **Alert Threshold:** >11.5GB triggers soft limit warning

## Default Expectations

- **Rerank Default:** Enabled, runs after fusion on CPU (GPU lease pending)
- **Sparse Default:** Enabled, metadata fallback if live inference unavailable
- **Telemetry Emission:** Always-on for all active stages

## Troubleshooting

### Rerank latency spike
1. Check `rag.rerank` span attributes for fallback_used=true
2. Validate top_k_candidates not excessive (default: 100)
3. Review GPU lease events for contention

### Sparse coverage drop
1. Check `rag.sparse` span coverage_ratio < 1.0
2. Verify sparse_models toggle not empty
3. Inspect fallback_used flag and fallback_reason
```

---

### 3. End-to-End Smoke Tests (AC3, Task 4) - **P1**

**Severity:** MEDIUM  
**Impact:** Cannot verify telemetry integrity in realistic pipeline runs  
**Probability:** 100% (confirmed absent)

**Suggested Implementation:**

```python
# tests/test_telemetry_smoke.py
@pytest.mark.integration
def test_telemetry_smoke_rerank_enabled(tmp_path):
    """AC3: Verify telemetry coverage with rerank enabled"""
    # Given: Real embedder config with enable_rerank=True
    # When: Pipeline processes 10 chunks
    # Then:
    #   - processing_summary.json contains rerank_run section
    #   - rerank_run.status in {"staged", "executed"}
    #   - telemetry.spans["rag.rerank"].status == "active"
    #   - telemetry.metrics["rerank"].status == "emitted"
    #   - CLI output contains "rag.rerank: emitted"

@pytest.mark.integration
def test_telemetry_smoke_stages_disabled(tmp_path):
    """AC3: Verify telemetry coverage with rerank/sparse disabled"""
    # Given: Embedder with enable_rerank=False, enable_sparse=False
    # When: Pipeline executes
    # Then:
    #   - processing_summary.json excludes rerank_run, sparse_run
    #   - telemetry.spans empty
    #   - CLI output contains skip reasons
```

---

### 4. CLI Summary Output Validation (AC3.4) - **P2**

**Severity:** LOW  
**Impact:** Operators lack immediate feedback, must inspect JSON  
**Probability:** 100% (confirmed absent)

**Suggested Test:**

```python
def test_cli_metrics_footer_with_emission(capsys):
    # Given: Pipeline with metrics_enabled=True
    # When: main() executes
    # Then: stdout contains "Metrics emission summary:"
    #       stdout contains "rag.rerank: emitted"
```

---

## Test Design Recommendations

### Priority 1: Prometheus Integration Tests

**Type:** Integration  
**Scope:** Validate actual Prometheus metric emission

```python
def test_prometheus_metrics_lifecycle():
    """Validate metrics emitted, scraped, and reset correctly"""
    # Setup: Start Prometheus scrape endpoint
    # Execute: Run embedding pipeline
    # Verify: HTTP GET /metrics returns rag_rerank_latency_seconds histogram
    # Verify: Histogram sample count > 0
    # Cleanup: Stop scrape endpoint
```

### Priority 2: Smoke Test Suite

**Type:** Integration/E2E  
**Scope:** Real pipeline runs validating AC3

```python
@pytest.mark.slow
def test_smoke_toggle_matrix():
    """Test all combinations: (rerank, sparse) √ó (enabled, disabled)"""
    for enable_rerank in [True, False]:
        for enable_sparse in [True, False]:
            # Run pipeline with toggle combination
            # Assert summary sections align with toggles
            # Assert CLI output matches expectations
```

### Priority 3: CLI Output Assertions

**Type:** Integration  
**Scope:** Validate operator-facing CLI summaries

```python
def test_cli_summary_skip_reasons(capsys, monkeypatch):
    """Ensure skip reasons printed when stages disabled"""
    # Given: enable_rerank=False (via CLI arg)
    # When: Pipeline executes
    # Then: stdout contains "rag.rerank: skipped (disabled via cli:--disable-rerank)"
```

---

## Risk Assessment

### High-Risk Requirements (Probability √ó Impact)

| Requirement | Probability | Impact | Risk Score | Mitigation |
|-------------|-------------|--------|------------|------------|
| **Prometheus Metrics (AC1.3)** | 100% (absent) | CRITICAL | **üî¥ 10/10** | Implement P0 - blocks production monitoring |
| **Runbook Documentation (AC2)** | 100% (absent) | HIGH | **üî¥ 9/10** | Implement P0 - blocks operator training |
| **Smoke Tests (AC3.3/AC3.4)** | 100% (absent) | MEDIUM | **üü° 6/10** | Implement P1 - limits validation confidence |
| **Metrics Scrape Endpoint** | 100% (implied) | HIGH | **üî¥ 9/10** | Design required - how to expose metrics? |

### Medium-Risk Requirements

| Requirement | Coverage | Gap |
|-------------|----------|-----|
| **GPU Leasing Metadata** | FULL | None - implemented and tested |
| **Span Emission** | FULL | None - OTel attributes complete |
| **Toggle Guards** | PARTIAL | Metrics status recorded but no actual metrics to guard |

### Low-Risk Requirements

| Requirement | Coverage | Gap |
|-------------|----------|-----|
| **Processing Summary Schema** | FULL | None - v4.1 schema validated |
| **Provenance Tracking** | FULL | None - tested in multiple scenarios |

---

## Traceability Matrix

| AC | Requirement | Implementation | Tests | Coverage | Risk |
|----|-------------|----------------|-------|----------|------|
| AC1.1 | OTel Span Emission | `telemetry.py:161`, `core.py:1539,1596`, `batch_runner.py:638`, `export_runtime.py:130` | `test_build_processing_summary_includes_stage_sections`, `test_write_processing_summary_generates_default_sections` | FULL | ‚úÖ LOW |
| AC1.2 | GPU Leasing Metadata | `telemetry.py:271-313` | `test_processing_summary_includes_performance_baseline`, `test_performance_baseline_flags_soft_limit_exceedance` | FULL | ‚úÖ LOW |
| AC1.3 | Prometheus Metrics | `core.py:500` (stub only) | ‚ùå NONE | NONE | üî¥ CRITICAL |
| AC2.1 | Runbook Creation | ‚ùå NONE | ‚ùå NONE | NONE | üî¥ HIGH |
| AC2.2 | Doc Cross-linking | ‚ùå NONE | ‚ùå NONE | NONE | üî¥ HIGH |
| AC3.1 | Toggle Span Validation | `summary.py` | `test_build_processing_summary_omits_disabled_stages` | FULL | ‚úÖ LOW |
| AC3.2 | Metrics Status Validation | `core.py:500` | `test_write_processing_summary_generates_default_sections` | PARTIAL | üü° MEDIUM |
| AC3.3 | Manifest Alignment | ‚ö†Ô∏è Partial (dummy models) | `test_write_processing_summary_generates_default_sections` | PARTIAL | üü° MEDIUM |
| AC3.4 | CLI Summary Assertions | ‚ùå NONE | ‚ùå NONE | NONE | üü° MEDIUM |

---

## Quality Indicators

### ‚úÖ Strengths

1. **Comprehensive OTel Span Instrumentation**: All 4 pipeline stages (`rag.dense`, `rag.sparse`, `rag.rerank`, `rag.export`) emit structured spans with rich attributes
2. **Robust Test Coverage for Spans**: Unit and integration tests validate span presence, attributes, and toggle-driven behavior
3. **GPU Soft Limit Monitoring**: Telemetry tracks GPU usage against configurable soft limits with automated alerting
4. **Provenance Tracking**: Toggle resolution events provide audit trail for debugging configuration issues

### ‚ö†Ô∏è Concerns

1. **Metrics Emission is Telemetry Theater**: `_emit_metrics_for_stage()` records **intent to emit** but doesn't actually emit Prometheus metrics
2. **No Operator Documentation**: Missing runbook leaves operators unable to interpret signals or set alert thresholds
3. **Smoke Tests Not Truly End-to-End**: Current tests use dummy models and don't validate CLI output

### üî¥ Critical Deficiencies

1. **Zero Production Monitoring Capability**: Without Prometheus metrics, operators are blind to pipeline health
2. **Metrics Scrape Strategy Undefined**: No HTTP endpoint, sidecar pattern, or push gateway implementation
3. **No Validation Path**: Cannot prove telemetry works in realistic scenarios without smoke tests

---

## Given-When-Then Best Practices Applied

This traceability matrix uses Given-When-Then to **document what tests validate**, not how they're coded:

- **Given** describes test preconditions and setup
- **When** describes the action or method invoked
- **Then** describes assertions validating requirements

Tests maintain idiomatic Python/pytest structure (`def test_*`, `assert`) while Given-When-Then provides requirement mapping clarity.

---

## Next Steps

### Immediate Actions (Blocking Story Completion)

1. **Implement Prometheus Metrics** (Task 2.1)
   - Install `prometheus_client` dependency
   - Create `processor/ultimate_embedder/metrics.py`
   - Wire histograms/gauges into `_emit_metrics_for_stage()`
   - Design scrape endpoint strategy (HTTP server vs push gateway)

2. **Create Telemetry Runbook** (Task 3.1)
   - Draft `docs/telemetry/rerank_sparse_signals.md`
   - Document metric definitions, thresholds, troubleshooting
   - Cross-link from operator documentation

3. **Build Smoke Test Suite** (Task 4)
   - Implement `test_telemetry_smoke_rerank_enabled()`
   - Implement `test_telemetry_smoke_stages_disabled()`
   - Add CLI output assertions

### Follow-Up Actions

4. **Performance Baseline Validation**
   - Add load tests with realistic chunk volumes (1000+ chunks)
   - Validate 12GB GPU ceiling under rerank/sparse load
   - Benchmark metrics scrape overhead

5. **Dashboard Templates**
   - Create Grafana dashboard JSON for standard metrics
   - Document alerting rules for common failure modes

---

## Conclusion

**Story 1.3 Status:** üü° **PARTIALLY COMPLETE** (Task 1 done, Tasks 2-4 incomplete)

**Quality Gate Recommendation:** **CONCERNS** ‚Üí requires Prometheus implementation and documentation before production readiness

**Test Coverage Quality:** Good for implemented features (OTel spans), critical gaps for unimplemented features (Prometheus metrics)

**Risk Posture:** HIGH - Core monitoring capability missing despite strong telemetry instrumentation foundation
