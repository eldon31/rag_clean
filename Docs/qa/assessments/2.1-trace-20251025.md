# Requirements Traceability Matrix

## Story: 2.1 - Implement Sparse Generator Module

### Coverage Summary
- Total Requirements: 5
- Fully Covered: 5 (100%)
- Partially Covered: 0 (0%)
- Not Covered: 0 (0%)

### Requirement Mappings

#### AC1: Sparse generator produces vectors per chunk with fallback metadata

**Coverage:** FULL

Given-When-Then Mappings:
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_cpu_success`
  - Given a loaded sparse model and sample chunk records
  - When `SparseVectorGenerator.generate` runs on CPU
  - Then it returns a `SparseInferenceResult` with per-chunk vectors, fallback indices, and telemetry
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_large_batch_cpu`
  - Given 100 chunk records with sparse metadata
  - When generation runs on CPU
  - Then every chunk yields a sparse vector and latency is recorded for downstream reporting

#### AC2: Fallback handling recovers from inference failures and tracks metadata usage

**Coverage:** FULL

Given-When-Then Mappings:
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_model_not_loaded`
  - Given the requested sparse model is absent
  - When generation is invoked
  - Then the result flags failure, every chunk uses metadata fallback, and fallback indices cover the batch
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_cpu_encoding_failure`
  - Given encoding raises an exception
  - When generation runs on CPU
  - Then all chunks fall back to metadata vectors and the device remains `cpu`
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_gpu_lease_exception`
  - Given GPU leasing raises `RuntimeError`
  - When generation attempts GPU execution
  - Then execution falls back to CPU metadata with `fallback_count` equal to `chunk_count`

#### AC3: GPU leasing mirrors dense passes and emits telemetry with device usage

**Coverage:** FULL

Given-When-Then Mappings:
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_gpu_success`
  - Given GPU leasing succeeds and model hydration works
  - When generation executes with `use_gpu=True`
  - Then vectors are produced on `cuda:0`, leases hydrate to GPU, and the model is staged back to CPU
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_generate_gpu_hydration_failure`
  - Given GPU hydration returns `None`
  - When generation runs with `use_gpu=True`
  - Then execution falls back to CPU inference while still returning vectors
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_record_telemetry`
  - Given a successful `SparseInferenceResult`
  - When telemetry is recorded
  - Then span and metrics calls capture device, `latency_ms`, and fallback counts for reporting
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_record_telemetry_with_failure`
  - Given a failed inference result
  - When telemetry is recorded
  - Then span presence logs `active=False` with the failure reason, matching AC instrumentation needs
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_gpu_inference_with_vram_enforcement`
  - Given GPU execution with VRAM enforcement hooks
  - When generation runs on a leased GPU
  - Then telemetry records VRAM usage metrics and the generator keeps device staging healthy

#### Task: Persist sparse outputs and metadata into `SparseInferenceRun` artifacts

**Coverage:** FULL

Given-When-Then Mappings:
- Integration Test: `tests/test_processing_summary.py::test_sparse_generator_end_to_end_persistence`
  - Given `SparseVectorGenerator` produces live vectors for two chunks
  - When the sparse stage summary is built and serialized to `processing_summary.json`
  - Then the persisted artifact captures vector counts, fallback metadata, and schema version `v4.1`
- Integration Test: `tests/test_processing_summary.py::test_build_processing_summary_includes_stage_sections`
  - Given sparse stage details with coverage, device map, and fallback flags
  - When `build_processing_summary` assembles the manifest
  - Then the resulting payload exposes `sparse_run` coverage ratios and fallback metadata
- Integration Test: `tests/test_processing_summary.py::test_write_processing_summary_generates_default_sections`
  - Given a pipeline run writing `processing_summary.json`
  - When the embedder exports the summary
  - Then the serialized `sparse_run` section includes vector counts, fallback indicators, and activation provenance

#### Security: Telemetry and artifacts avoid storing full query strings

**Coverage:** FULL

Given-When-Then Mappings:
- Unit Test: `tests/test_sparse_generator.py::TestSparseVectorGenerator::test_telemetry_sanitization`
  - Given chunks containing sensitive terms
  - When telemetry is recorded for sparse inference
  - Then span attributes and metric details omit the sensitive substrings, demonstrating sanitization

### Residual Considerations

- Keep GPU load testing in pre-production rotations to ensure adaptive batching heuristics remain tuned as corpora sizes grow.
- Maintain telemetry snapshot reviews to guard against future schema changes reintroducing sensitive content.

