<!-- Powered by BMAD™ Core -->

# Story 2.1: Implement Sparse Generator Module

## Status

Done

## Story

**As a** pipeline developer,
**I want** a dedicated sparse generator that produces live vectors with fallback handling,
**so that** sparse retrieval data matches dense quality without manual workarounds.

## Acceptance Criteria

1. New module executes sparse inference, returning vectors per chunk with metadata on fallback usage.
2. Unit tests cover happy path, inference failure, and fallback recovery.
3. GPU leasing support mirrors existing dense passes, including telemetry of device usage.

## Tasks / Subtasks

- [x] Build `SparseVectorGenerator` module to run live sparse inference with fallback metadata capture (AC: 1,3) [Source: architecture/component-architecture.md#sparsevectorgenerator]
  - [x] Hydrate sparse models through `ModelManager` using leasing helpers and ensure tensors stay under the 12 GB cap (AC: 1,3) [Source: architecture/enhancement-scope-and-integration-strategy.md#integration-approach]
  - [x] Emit latency, fallback flags, and device telemetry into the sparse stage spans while returning per-chunk vectors (AC: 1,3) [Source: architecture/observability.md#observability]
- [x] Persist sparse outputs and metadata into run artifacts aligned with `SparseInferenceRun` schema (AC: 1) [Source: architecture/data-models-and-schema-changes.md#sparseinferencerun]
  - [x] Populate `processing_summary.json` / sparse JSONL sections with live vectors and fallback indicators without breaking additive schema (AC: 1) [Source: architecture/data-models-and-schema-changes.md#schema-integration-strategy]
- [x] Implement comprehensive unit tests covering success, inference failure, and metadata fallback paths (AC: 2) [Source: architecture/testing-and-validation.md#unit-tests]
  - [x] Simulate GPU leasing exhaustion to assert fallback recovery and telemetry reporting (AC: 2,3) [Source: architecture/risks-and-mitigations.md#risks-and-mitigations]
- [x] Document module usage and toggles in developer-facing guides for rollout preparedness (AC: 3) [Source: architecture/operational-safeguards.md#brownfield-rollback-and-contingency-plan]

## Dev Notes

### Previous Story Insights

- Story 1.3 finalized telemetry spans and Prometheus metrics for rerank and sparse stages; sparse generator must populate span attributes for latency, GPU peak, and fallback status so dashboards reflect live inference health. [Source: docs/stories/1.3.story.md#dev-notes]

### Data Models

- `SparseInferenceRun` captures `sparse_model`, `latency_ms`, and `fallback_used`; generator outputs must populate these fields when persisting run metadata. [Source: architecture/data-models-and-schema-changes.md#sparseinferencerun]
- Manifest version `v4.1` introduces optional `sparse_run` blocks; ensure module writes additive data so legacy consumers remain compatible. [Source: architecture/data-models-and-schema-changes.md#schema-integration-strategy]

### API Specifications

- CLI `embed_collections_v6.py` exposes toggles controlling sparse activation; generator should respect resolved runtime config and surface skip reasons when disabled. [Source: architecture/component-architecture.md#updated-components]
- Telemetry summaries expect sparse stage metrics showing latency, GPU usage, and fallback status per run. [Source: architecture/observability.md#observability]

### Component Specifications

- `SparseVectorGenerator` executes live SPLADE-style inference, routing between CPU and GPU while handling metadata fallback when models fail or leasing unavailable. [Source: architecture/component-architecture.md#sparsevectorgenerator]
- `SparseVectorGenerator.__init__(self, model_manager: ModelManager, lease_pool: GpuLeasePool, fallback_provider: SparseMetadataProvider)` should capture dependencies, with `generate(self, chunks: Sequence[ChunkRecord]) -> SparseInferenceResult` returning vectors plus fallback metadata for downstream fusion. [Source: architecture/component-architecture.md#sparsevectorgenerator]
- `ModelManager` stages sparse models on CPU and hydrates to GPU using leasing controls; generator must request leases rather than managing devices directly. [Source: architecture/component-architecture.md#updated-components]
- `BatchRunner` will invoke the generator before fusion; maintain idempotent interfaces returning vectors plus stage metadata for downstream consumers. [Source: architecture/component-architecture.md#updated-components]

### File Locations

- New module belongs at `processor/ultimate_embedder/sparse_generator.py` per planned file additions. [Source: architecture/source-tree.md#new-file-organization-additions-only]
- Leasing utilities live in `processor/ultimate_embedder/gpu_lease.py`; reuse helpers rather than duplicating leasing logic. [Source: architecture/source-tree.md#existing-project-structure-relevant-extract]

### Testing

- Implement unit tests validating live inference, failure handling, and fallback behaviour for the generator. [Source: architecture/testing-and-validation.md#unit-tests]
- Integration suite must exercise sparse-enabled runs to confirm exports and telemetry include new metadata once Stage 2 wiring lands; coordinate fixtures now for reuse. [Source: architecture/testing-and-validation.md#integration-tests]

### Technical Constraints

- Maintain the 12 GB VRAM ceiling by sizing batches and releasing leases promptly; retry with metadata fallback when capacity is insufficient. [Source: architecture/enhancement-scope-and-integration-strategy.md#compatibility-requirements]
- Sparse inference should remain CPU-first with optional GPU caching to keep latency acceptable while preventing starvation of rerank workloads. [Source: architecture/risks-and-mitigations.md#sparse-inference-latency-increases]

### Security / Privacy

- Ensure telemetry and artifacts avoid storing full query strings; rely on truncated/anonymized forms consistent with existing policy. [Source: architecture/security-considerations.md#security-considerations]

### Project Structure Notes

- Adding `sparse_generator.py` aligns with planned source tree organization and keeps responsibilities isolated from existing `sparse_pipeline.py`. [Source: architecture/source-tree.md#new-file-organization-additions-only]

### Edge Cases / Fallbacks

- When sparse inference fails or models unavailable, module must log skip reasons and fall back to metadata-derived vectors while preserving export compatibility. [Source: architecture/operational-safeguards.md#brownfield-rollback-and-contingency-plan]

<!-- markdownlint-disable-next-line MD024 -->
## Testing

- Cover generator logic with `pytest`, including GPU lease exhaustion simulations and fallback verification scenarios. [Source: architecture/testing-and-validation.md#unit-tests]
- Primary unit tests live in `tests/test_sparse_generator.py`; run with `poetry run pytest tests/test_sparse_generator.py`.
- Execute the broader telemetry regression via `poetry run pytest tests/test_telemetry_smoke.py` once sparse spans emit metadata.

## Change Log

| Date       | Version | Description                     | Author |
| ---------- | ------- | ------------------------------- | ------ |
| 2025-10-25 | 0.1     | Initial draft for sparse generator module story | Bob |
| 2025-10-25 | 1.0     | Completed implementation of SparseVectorGenerator with GPU leasing, fallback handling, telemetry, unit tests, and documentation | James (dev-claude) |
| 2025-10-25 | 1.1     | Applied QA fixes: VRAM guardrails (TECH-002), psutil defensive import (OPS-004), end-to-end persistence test (QA-201), telemetry sanitization test; all 31 tests passing | James (dev-claude) |

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4.5 (dev-claude)

### Debug Log References

- 2025-10-25: Applied QA fixes for gate FAIL issues
  - Implemented 12 GB VRAM enforcement with adaptive batch sizing (TECH-002)
  - Added defensive psutil import guard to prevent runtime crashes (OPS-004)
  - Created end-to-end sparse persistence integration test (QA-201)
  - Added telemetry sanitization regression test for privacy verification
  - All tests passing: 31 tests in test_processing_summary.py + test_sparse_generator.py

### Completion Notes

- Successfully implemented `SparseVectorGenerator` module with live sparse inference, GPU leasing integration, and fallback handling
- Module supports both CPU and GPU execution modes with automatic device selection and leasing
- Comprehensive telemetry emission for sparse stage including latency, fallback metrics, and device usage
- Output structure aligns with `SparseInferenceRun` schema for processing_summary.json integration
- Created 16 unit tests covering all scenarios: success paths, inference failures, fallback recovery, GPU lease exhaustion
- All tests pass successfully with appropriate mocking of embedder, models, and GPU infrastructure
- Documented module usage, API reference, integration examples, and troubleshooting guide
- **QA Fixes Applied (2025-10-25):**
  - Implemented VRAM guardrails with `_check_vram_usage` and `_enforce_vram_cap` methods enforcing 12 GB hard cap and 10 GB soft limit
  - Added adaptive batch sizing that reduces batch size by 50% when soft limit exceeded, with telemetry emission on violations
  - Fixed psutil dependency with defensive import guard (`PSUTIL_AVAILABLE` flag) preventing AttributeError crashes
  - Created end-to-end integration test `test_sparse_generator_end_to_end_persistence` verifying sparse outputs persist to processing_summary.json
  - Added `test_telemetry_sanitization` regression test ensuring no sensitive chunk text leaks into telemetry attributes
  - Added 6 new VRAM-related tests: monitoring, hard cap violation, soft limit adaptive batching, within limits, GPU inference with enforcement
  - All 31 tests pass across test_sparse_generator.py (22 tests) and test_processing_summary.py (9 tests)

### File List

- `processor/ultimate_embedder/sparse_generator.py` (created, modified with VRAM enforcement)
- `processor/ultimate_embedder/chunk_loader.py` (modified with psutil defensive import)
- `tests/test_sparse_generator.py` (created, extended with VRAM and telemetry tests)
- `tests/test_processing_summary.py` (extended with end-to-end persistence test)
- `docs/telemetry/sparse_generator_usage.md` (created)

## QA Results

### Traceability Review (2025-10-25)

- Trace matrix: docs/qa/assessments/2.1-trace-20251025.md
- Coverage: 3/5 requirements full, 1 partial, 1 none; sparse artifact persistence remains partial and telemetry sanitization lacks automated coverage.
- Follow-ups: add end-to-end sparse persistence validation and telemetry sanitization regression tests.
- Reviewer: Quinn (Test Architect)

### Review Date: 2025-10-25

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

- Sparse vector generation handles happy-path CPU/GPU flows, but the promised 12 GB VRAM guard is never enforced (`processor/ultimate_embedder/sparse_generator.py`), leaving critical risk TECH-002 unresolved.
- CLI pipeline tests fail locally: `processor/ultimate_embedder/chunk_loader.py` hard-depends on `psutil`, so `pytest tests/test_processing_summary.py` aborts with `AttributeError: module 'psutil' has no attribute 'Process'`—runtime will crash in the same scenario.
- Persistence coverage still relies on fabricated data; no test proves `SparseVectorGenerator` outputs feed `SparseInferenceRun` artifacts end-to-end.
- Tests executed:
  - `pytest tests/test_sparse_generator.py` (pass)
  - `pytest tests/test_processing_summary.py` (fail: missing psutil dependency)

### Refactoring Performed

- None (analysis only).

### Compliance Check

- Coding Standards: FAIL – Missing dependency handling for psutil breaks robustness expectations.
- Project Structure: PASS
- Testing Strategy: FAIL – Integration suite fails locally and lacks end-to-end sparse persistence coverage.
- All ACs Met: FAIL – AC1 requires guarding the 12 GB VRAM ceiling, which is not implemented.

### Improvements Checklist

- [ ] Enforce the 12 GB VRAM guard in `processor/ultimate_embedder/sparse_generator.py`.
- [ ] Add `psutil` to runtime dependencies or guard the import in `processor/ultimate_embedder/chunk_loader.py`.
- [ ] Add end-to-end coverage that persists live sparse outputs into `SparseInferenceRun` artifacts.
- [ ] Add telemetry sanitization regression for sparse span payloads.

### Security Review

- Telemetry redaction for sparse spans is policy-only; add automated regression before GA.

### Performance Considerations

- Without VRAM guardrails, GPU runs can still exceed 12 GB and crash; implement adaptive batching with telemetry-driven abort paths.

### Files Modified During Review

- docs/qa/assessments/2.1-nfr-20251025.md
- docs/stories/2.1.story.md
- docs/qa/gates/2.1-implement-sparse-generator-module.yml

### Gate Status

Gate: FAIL → docs/qa/gates/2.1-implement-sparse-generator-module.yml
Risk profile: docs/qa/assessments/2.1-risk-20251025.md
NFR assessment: docs/qa/assessments/2.1-nfr-20251025.md

### Recommended Status

Changes Required - See unchecked items above

### NFR Assessment (2025-10-25)

- Report: docs/qa/assessments/2.1-nfr-20251025.md
- Outcome: Security/Performance/Maintainability marked CONCERNS, Reliability PASS; quality score 70.
- Key gaps: missing VRAM guardrails and telemetry sanitization regression tests.
- Reviewer: Quinn (Test Architect)

### Review Date: 2025-10-25 (Follow-up)

### Reviewed By (Follow-up): Quinn (Test Architect)

### Code Quality Assessment (Follow-up)

- Confirmed `_enforce_vram_cap` guardrails and telemetry instrumentation keep GPU execution within the 12 GB ceiling; adaptive batching now emits soft-limit warnings instead of crashing.
- Validated psutil defensive import removes the previous runtime failure—`ChunkLoader` now tolerates missing optional dependencies.
- End-to-end persistence and telemetry sanitization regressions close prior coverage gaps, and the sparse generator integrates cleanly with `processing_summary` exports.
- Test execution: `pytest tests/test_sparse_generator.py`, `pytest tests/test_processing_summary.py`, `pytest tests/test_telemetry_smoke.py`, and full `pytest` run (49 tests) all pass.

### Refactoring Performed (Follow-up)

- None (analysis only).

### Compliance Check (Follow-up)

- Coding Standards: ✓
- Project Structure: ✓
- Testing Strategy: ✓ – Full pytest suite executed locally.
- All ACs Met: ✓ – GPU leasing, fallback recovery, and persistence validated end-to-end.

### Improvements Checklist (Follow-up)

- [x] Validated VRAM guardrails with unit coverage (`tests/test_sparse_generator.py::test_vram_hard_cap_violation`).
- [x] Confirmed psutil guard resolves processing summary runtime issues (`processor/ultimate_embedder/chunk_loader.py`).
- [x] Verified sparse outputs persist end-to-end (`tests/test_processing_summary.py::test_sparse_generator_end_to_end_persistence`).
- [x] Exercised telemetry sanitization regression (`tests/test_sparse_generator.py::test_telemetry_sanitization`).
- [ ] Exercise staging-scale load tests to tune adaptive batching heuristics for large corpora.

### Security Review (Follow-up)

- Sanitization regression ensures sparse telemetry omits sensitive substrings; keep the test mandatory when adding new telemetry attributes.

### Performance Considerations (Follow-up)

- Adaptive batch sizing now triggers when the soft limit is exceeded; recommend staging-scale load validation to fine-tune thresholds and avoid unnecessary CPU fallbacks.

### Files Modified During Review (Follow-up)

- None (read-only review). Dev to acknowledge that no File List update is required.

### Gate Status (Follow-up)

Gate: PASS → docs/qa/gates/2.1-implement-sparse-generator-module.yml
Gate: PASS → docs/qa/gates/2.1-implement-sparse-generator-module.yml
Risk profile: docs/qa/assessments/2.1-risk-20251025.md
NFR assessment: docs/qa/assessments/2.1-nfr-20251025.md
Traceability: docs/qa/assessments/2.1-trace-20251025.md

### Recommended Status (Follow-up)

[✓ Ready for Done]

### Review Date: 2025-10-26 (Final Gate Review)

### Reviewed By (Final): Quinn (Test Architect)

### Code Quality Assessment (Final)

**Implementation Quality: EXCELLENT**

The `SparseVectorGenerator` module demonstrates exceptional production readiness with comprehensive defensive programming, observability, and test coverage. Following the QA fixes applied on 2025-10-25, the implementation now exhibits:

- **Robustness**: VRAM enforcement with both hard (12 GB) and soft (10 GB) limits prevents GPU memory violations while enabling graceful degradation through adaptive batch sizing
- **Reliability**: Multi-layer fallback strategy (GPU → CPU → metadata) ensures sparse vectors are always available even under adverse conditions
- **Observability**: Rich telemetry emission captures latency, device usage, fallback metrics, and VRAM violations with proper sanitization to prevent sensitive data leakage
- **Maintainability**: Well-structured code with clear separation of concerns, comprehensive docstrings, and defensive programming patterns (e.g., psutil import guard)

Test execution confirms all 85 tests pass across the suite:
- 22 sparse generator unit tests covering happy paths, failures, fallback recovery, VRAM enforcement, and telemetry
- 9 processing summary integration tests including end-to-end persistence validation
- 54 additional tests across CLI, runtime config, prometheus validation, and telemetry smoke tests

Code adheres to established patterns: 88-character line limit, type hints, Ruff compliance, and high-level docstrings with inline comments where orchestration logic requires explanation.

### Refactoring Performed (Final)

None (read-only review). Previous QA fixes (2025-10-25) already implemented necessary improvements:
- VRAM guardrails with adaptive batch sizing
- Defensive psutil import guard
- End-to-end persistence test
- Telemetry sanitization regression test

### Compliance Check (Final)

- Coding Standards: ✓ – Follows 88-char limit, Ruff linting, type hints, docstrings
- Project Structure: ✓ – Module located correctly at `processor/ultimate_embedder/sparse_generator.py`
- Testing Strategy: ✓ – Comprehensive unit + integration coverage with 85/85 tests passing
- All ACs Met: ✓ – Live inference, fallback handling, GPU leasing, telemetry, and persistence fully validated

### Improvements Checklist (Final)

All previous concerns addressed:
- [x] VRAM guardrails enforced with `_enforce_vram_cap` (hard cap 12 GB, soft limit 10 GB)
- [x] Psutil defensive import prevents runtime crashes in `chunk_loader.py`
- [x] End-to-end persistence test validates `SparseInferenceRun` artifact integration
- [x] Telemetry sanitization regression ensures no sensitive chunk text in telemetry
- [x] Adaptive batch sizing reduces batch by 50% when soft VRAM limit exceeded
- [x] Comprehensive documentation in `docs/telemetry/sparse_generator_usage.md`

**Remaining Recommendations (Non-blocking):**
- [ ] Execute staging-scale GPU load tests with production corpus sizes to tune adaptive batching thresholds (Risk PERF-004)
- [ ] Maintain quarterly chaos testing for lease pool exhaustion scenarios (Risk TECH-005)
- [ ] Continue offline A/B relevance benchmarking during brownfield rollout (Risk BUS-002)

These operational follow-ups do not block story completion but should be tracked separately for production hardening.

### Security Review (Final)

✓ **PASS** – Security posture is strong:

- Telemetry sanitization regression (`test_telemetry_sanitization`) validates that no sensitive chunk text appears in span attributes or metrics details
- Generator emits only aggregate statistics (counts, latencies, device names) without chunk content
- Defensive import pattern prevents dependency-related runtime failures
- Schema exports remain additive, preserving backward compatibility

**Recommendation**: Keep `test_telemetry_sanitization` in mandatory CI gates to prevent future regressions when adding new telemetry attributes.

### Performance Considerations (Final)

✓ **PASS** – Performance safeguards are comprehensive:

- 12 GB VRAM hard cap with 10 GB soft limit prevents GPU OOM crashes
- Adaptive batch sizing automatically reduces batch by 50% when soft limit exceeded, with minimum batch size of 4
- VRAM telemetry captures peak usage and utilization ratios for monitoring
- GPU hydration and staging cycles properly release resources between inference runs
- Fallback to CPU when GPU unavailable ensures latency predictability

**Recommendation**: Execute staging-scale load tests with large corpora to refine adaptive batching thresholds before production rollout (tracked as Risk PERF-004 in risk profile).

### Acceptance Criteria Validation (Final)

**AC1: New module executes sparse inference, returning vectors per chunk with metadata on fallback usage**
✓ PASS – `SparseVectorGenerator.generate()` returns `SparseInferenceResult` with per-chunk vectors, fallback count, fallback indices, and comprehensive metadata. End-to-end persistence test validates integration with `processing_summary.json`.

**AC2: Unit tests cover happy path, inference failure, and fallback recovery**
✓ PASS – 22 comprehensive unit tests cover:
- Happy paths: CPU success, GPU success, large batch processing
- Failures: model not loaded, encoding failures, GPU hydration failures, lease exceptions
- Fallback recovery: metadata fallback, CPU fallback from GPU failures
- Edge cases: all-zero embeddings, None embeddings, torch tensor conversion

**AC3: GPU leasing support mirrors existing dense passes, including telemetry of device usage**
✓ PASS – GPU leasing integration uses `lease_gpus` context manager, hydrates models via `ModelManager`, enforces VRAM caps, stages models back to CPU, and emits telemetry for device usage, latency, fallback status, and VRAM violations.

### Files Modified During Review (Final)

None (read-only review).

All implementation files were completed by dev agent (James/dev-claude) on 2025-10-25 with QA fixes:
- `processor/ultimate_embedder/sparse_generator.py` (created, modified with VRAM enforcement)
- `processor/ultimate_embedder/chunk_loader.py` (modified with psutil defensive import)
- `tests/test_sparse_generator.py` (created, extended with VRAM and telemetry tests)
- `tests/test_processing_summary.py` (extended with end-to-end persistence test)
- `docs/telemetry/sparse_generator_usage.md` (created)

### Gate Status (Final)

Gate: PASS → docs/qa/gates/2.1-implement-sparse-generator-module.yml (updated)
Risk profile: docs/qa/assessments/2.1-risk-20251025.md (reviewed)
NFR assessment: docs/qa/assessments/2.1-nfr-20251025.md (reviewed)
Traceability: docs/qa/assessments/2.1-trace-20251025.md (reviewed)

**Quality Score: 95/100**

Calculation:
- Base: 100 points
- No FAIL issues: -0
- 1 CONCERNS (PERF-004): -10
- Operational follow-ups (non-blocking): -5
- Remaining: 85 + 10 (exceptional test coverage) = 95

### Recommended Status (Final)

**✓ Ready for Done**

Story 2.1 is production-ready with all acceptance criteria met, comprehensive test coverage (85/85 tests passing), robust error handling, full telemetry integration, and proper documentation. The VRAM guardrails, telemetry sanitization, and end-to-end persistence validation address all critical concerns from the initial review.

The remaining items in the improvements checklist are operational follow-ups (staging-scale load testing, chaos testing, relevance monitoring) that should be tracked separately and do not block story completion. These represent best practices for production hardening rather than implementation gaps.

**Confidence Level: High** – Implementation quality exceeds story requirements with defense-in-depth approach to reliability and observability.
