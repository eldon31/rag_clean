<!-- Powered by BMADâ„¢ Core -->

# Story 3.3: Telemetry and Observability Enhancements

## Status

Done

## Story

**As a** telemetry consumer,
**I want** rerank-specific metrics (latency, GPU peak, fallback counts),
**so that** monitoring can detect issues with the new stage quickly.

## Acceptance Criteria

1. Telemetry/metrics emit rerank-specific spans, Prometheus counters, and fallback counts.
2. Runbook entries document interpreting rerank metrics, thresholds, and fallback signals.
3. Alert thresholds and dashboards highlight rerank performance, fallback counts, and toggle provenance.

## Tasks / Subtasks

- [x] Validate rerank span and metrics emission (AC: 1)
  - [x] Confirm `rag.rerank` spans persist `status`, `latency_ms`, `batch_size`, `gpu_peak_gb`, and `fallback_count` in `processing_summary.json` via telemetry helpers (AC: 1) [Source: architecture/observability.md#opentelemetry-spans]
  - [x] Ensure Prometheus metrics emit `rag_rerank_latency_seconds` and stage-labelled `rag_gpu_peak_bytes{stage="rerank"}` with rerank-specific attributes (AC: 1) [Source: architecture/observability.md#prometheus-metrics]
  - [x] Persist rerank metrics status (`emitted`, `skipped`, `reason`) inside summary telemetry blocks for CLI consumption (AC: 1) [Source: architecture/observability.md#cli-summary-output]
- [x] Instrument rerank fallback counters and provenance (AC: 1)
  - [x] Emit rerank fallback counter metrics (e.g., `rerank_fallback_total`) using `PrometheusMetricsEmitter.emit_counter` with labels for reason and toggle source (AC: 1) [Source: architecture/observability.md#prometheus-metrics]
  - [x] Extend `TelemetryTracker.record_span_presence` attributes so `rag.rerank` spans include `fallback_count` and `fallback_reason` when rerank skips or errors (AC: 1) [Source: docs/prd/epic-3-crossencoder-rerank-execution-telemetry.md#story-3-3-telemetry-and-observability-enhancements]
  - [x] Surface rerank fallback counts in `build_processing_summary` and CLI logs to keep operators informed (AC: 1, 3) [Source: processor/ultimate_embedder/summary.py]
- [x] Extend telemetry runbook coverage (AC: 2)
  - [x] Update rerank telemetry runbook guidance to document interpreting latency, GPU thresholds, fallback counters, and toggle provenance (AC: 2) [Source: telemetry/rerank_sparse_signals.md#rerank-stage-metrics]
  - [x] Cross-link fallback counter response steps in runbook alert procedures so operators understand mitigation options (AC: 2) [Source: telemetry/rerank_sparse_signals.md#gpu-alert-response]
- [x] Align alerts and dashboards with rerank metrics (AC: 3)
  - [x] Refresh Prometheus alert configuration so rerank fallback counter spikes trigger WARN/CRIT annotations alongside GPU thresholds (AC: 3) [Source: architecture/observability.md#gpu-alert-configuration]
  - [x] Enumerate affected Grafana panels (Stage Latency Timeline, GPU Peak Memory, Feature Toggle Status, and add a new Rerank Fallback Counter panel) in documentation and ensure dashboard JSON reflects the additions (AC: 3) [Source: architecture/observability.md#dashboards]
- [x] Regression coverage and validation (AC: 1, 3)
  - [x] Add or update telemetry smoke tests to assert rerank span, fallback counters, and Prometheus metrics emission across toggle combinations (AC: 1, 3) [Source: architecture/testing-and-validation.md#integration-tests]
  - [x] Extend CLI acceptance test to verify rerank fallback counter and reason appear in completion logs (AC: 1, 3) [Source: scripts/embed_collections_v6.py]
  - [x] Capture CLI output evidence showing rerank latency, GPU peak, candidate counts, and fallback counters rendered with telemetry status (AC: 1, 3) [Source: architecture/observability.md#cli-summary-output]

## Dev Notes

### Previous Story Insights

- Rerank stage already executes after dense+sparse fusion and persists fused candidates, rerank run metadata, and telemetry attributes required for observability hooks. [Source: docs/stories/3.2.story.md#dev-notes]
- CLI summary currently reports rerank latency, GPU peak, batch size, and candidate counts once metrics are supplied; ensure telemetry payloads stay aligned with that contract. [Source: docs/stories/3.2.story.md#dev-notes]

### Current Gaps

- Rerank telemetry does not expose fallback counters or reasons today, leaving AC1 unmet until instrumentation covers span attributes, summary payloads, metrics, and CLI surfaces. [Source: docs/prd/epic-3-crossencoder-rerank-execution-telemetry.md#story-3-3-telemetry-and-observability-enhancements]

### Blockers

- Do not start implementation until the rerank fallback metrics scope is approved; the story remains blocked pending confirmation that counter instrumentation and dashboard updates are in scope.

### Data Models

- `CrossEncoderRerankRun` provides `batch_size`, `latency_ms`, `gpu_peak_gb`, and `candidate_ids`; these values must populate rerank telemetry fields to avoid recomputing metrics and to support fallback counters. [Source: architecture/data-models-and-schema-changes.md#crossencoderrerankrun]

### API Specifications

- `BatchRunner` sequences dense, sparse, fusion, rerank, and export stages; rerank telemetry instrumentation must hook into this orchestration without altering execution order. [Source: architecture/component-architecture.md#updated-components]
- CLI entry `scripts/embed_collections_v6.py` exposes rerank toggle flags and renders per-stage telemetry; rerank fallback counters must integrate with existing CLI rendering. [Source: architecture/component-architecture.md#updated-components]

### Component Specifications

- `CrossEncoderBatchExecutor` already captures rerank telemetry payloads; extend emission paths or downstream telemetry helpers to publish fallback counters alongside latency and GPU metrics. [Source: architecture/component-architecture.md#crossencoderbatchexecutor]
- Export runtime appends rerank data to manifests; update telemetry blocks so fallback counters align with schema additions. [Source: architecture/component-architecture.md#updated-components]

### Instrumentation Targets

- Implement rerank fallback counter recording in `processor/ultimate_embedder/telemetry.py`, `processor/ultimate_embedder/prometheus_metrics.py`, and `processor/ultimate_embedder/summary.py` so spans, metrics, and manifests stay aligned. [Source: architecture/source-tree.md#existing-project-structure-relevant-extract]
- Update CLI summary rendering in `scripts/embed_collections_v6.py` to display rerank fallback counts and reasons next to latency metrics. [Source: architecture/component-architecture.md#updated-components]
- Ensure `build_processing_summary` and downstream exporters persist rerank fallback counters alongside latency/GPU stats for run artifacts. [Source: processor/ultimate_embedder/summary.py]

### File Locations

- Rerank orchestration and telemetry helpers reside in `processor/ultimate_embedder/batch_runner.py`, `telemetry.py`, and `core.py`; update these modules when wiring telemetry. [Source: architecture/source-tree.md#existing-project-structure-relevant-extract]
- CLI reporting updates live in `scripts/embed_collections_v6.py` for operator-facing metrics summaries. [Source: architecture/source-tree.md#existing-project-structure-relevant-extract]
- Telemetry runbook documentation belongs in `docs/telemetry/rerank_sparse_signals.md`; reference dashboard integrations and alert responses there. [Source: architecture/source-tree.md#new-file-organization-additions-only]

### Testing Requirements

- Integration tests should exercise rerank-enabled pipeline runs, validating span emission, fallback counters, metrics status, and CLI outputs using `pytest` suites. [Source: architecture/testing-and-validation.md#integration-tests]
- Performance tests must confirm rerank latency and GPU peaks remain within the documented baselines (P95 < 5000 ms, GPU < 6 GB) even when fallback counters increment. [Source: architecture/observability.md#performance-baselines]

### Technical Constraints

- Maintain GPU usage under the 12 GB ceiling with warning at 11.5 GB and soft limit at 10 GB while emitting telemetry; alert thresholds depend on these limits. [Source: architecture/observability.md#gpu-alert-configuration]
- Reuse existing OpenTelemetry and Prometheus integrations without introducing new dependencies, aligning with the defined tech stack. [Source: architecture/tech-stack.md#new-technology-additions]

### Security / Privacy

- Ensure rerank spans and metrics continue truncating queries and redact sensitive data per telemetry privacy guidance. [Source: architecture/security-considerations.md#security-considerations]

### Project Structure Notes

- Telemetry adjustments must remain within existing modules under `processor/ultimate_embedder` and the CLI script to preserve the documented source tree layout. [Source: architecture/source-tree.md#existing-project-structure-relevant-extract]
- Unified project structure guide not present; adhere to the source tree mapping above when adding or modifying files. [Source: architecture/source-tree.md#existing-project-structure-relevant-extract]

### Edge Cases / Fallbacks

- Respect rerank toggle overrides so telemetry records `status="skipped"` with `reason="feature_disabled"` when rerank is off, matching rollback expectations. [Source: architecture/observability.md#opentelemetry-spans]
- Maintain skip/failure documentation in telemetry so operators can distinguish disabled runs from errors per the runbook. [Source: telemetry/rerank_sparse_signals.md#default-on-posture]

### Telemetry Monitoring

- Prometheus metrics `rag_rerank_latency_seconds` and `rag_gpu_peak_bytes` must include rerank-specific labels for dashboard panels, and new fallback counters should use consistent naming. [Source: architecture/observability.md#prometheus-metrics]
- Grafana dashboards should surface rerank latency timelines, GPU peak memory, feature toggle status, and newly added fallback counter visualizations. [Source: architecture/observability.md#dashboards]

### Dev Notes - Testing

- Follow repository testing patterns with `pytest`, focusing on telemetry smoke suites (e.g., `tests/test_telemetry_smoke.py`) and CLI acceptance checks to prove rerank metrics and fallback counters surface correctly. [Source: architecture/testing-and-validation.md#integration-tests]

## Testing

- `poetry run pytest tests/test_telemetry_smoke.py::TestTelemetrySpansWithMetrics::test_rerank_span_active_includes_metrics_status` [Source: architecture/testing-and-validation.md#integration-tests]
- `poetry run pytest tests/test_telemetry_smoke.py::TestPrometheusMetricsEmitter::test_emitter_enabled_emits_latency_metric` (extend to cover fallback counters) [Source: architecture/testing-and-validation.md#integration-tests]
- `poetry run pytest tests/test_telemetry_smoke.py::TestProcessingSummaryWithMetrics::test_summary_includes_rerank_and_sparse_metrics_when_enabled` [Source: architecture/testing-and-validation.md#integration-tests]
- Add a new regression test validating rerank fallback counter persistence across summary, telemetry, and CLI outputs. [Source: docs/prd/epic-3-crossencoder-rerank-execution-telemetry.md#story-3-3-telemetry-and-observability-enhancements]

## Change Log

| Date       | Version | Description                                                               | Author |
| ---------- | ------- | ------------------------------------------------------------------------- | ------ |
| 2025-10-29 | 0.1     | Initial draft for rerank telemetry story                                  | Bob    |
| 2025-10-29 | 0.2     | Story marked ready for handoff (Signoff)                                  | Bob    |
| 2025-10-29 | 0.3     | Implemented rerank fallback telemetry, docs, and tests                    | James  |
| 2025-10-30 | 1.0     | Status updated to Done after QA gate PASS (Ready for Done) recommendation | James  |

## Dev Agent Record

### Agent Model Used

GitHub Copilot (GPT-5-Codex)

### Debug Log References

- 2025-10-29: Telemetry regression sweep
  - `pytest tests/test_processing_summary.py` â†’ 17 passed
  - `pytest tests/test_telemetry_smoke.py` â†’ 23 passed
  - `pytest tests/test_embed_collections_cli.py` â†’ 7 passed

### Completion Notes List

- **Telemetry instrumentation**
  - Propagated rerank fallback counters through `processor/ultimate_embedder/core.py` spans, metrics emission, and CLI rendering paths.
  - Updated `tests/test_processing_summary.py` to assert fallback count, reason, and source across executed, disabled, and failure scenarios.
- **Runbook and observability updates**
  - Expanded `docs/telemetry/rerank_sparse_signals.md` with fallback metric definitions, alerting thresholds, response steps, and dashboard guidance.
  - Refreshed `docs/architecture/observability.md` to cover rerank fallback counter panels, alert routing, and CLI summary expectations.
- **Validation and evidence**
  - Enhanced telemetry smoke and CLI tests to exercise fallback attributes and Prometheus counter emission.
  - Captured CLI evidence (`docs/qa/assessments/1.4-telemetry-smoke-evidence/cli-output-fallback-20251029.txt`) showing fallback counters rendered for operators.

### File List

- processor/ultimate_embedder/core.py (ensure fallback counters flow into telemetry spans, metrics, and summaries)
- tests/test_processing_summary.py (add fallback assertions for rerank error/disabled cases)
- tests/test_telemetry_smoke.py (cover rerank fallback span attributes and metrics details)
- tests/test_embed_collections_cli.py (verify CLI logs include fallback counter metadata)
- docs/telemetry/rerank_sparse_signals.md (document fallback metrics, alerts, dashboards, and runbook steps)
- docs/architecture/observability.md (update observability overview with fallback counters and alert routing)
- docs/qa/assessments/1.4-telemetry-smoke-evidence/cli-output-fallback-20251029.txt (new CLI output evidence with fallback counter)
- docs/stories/3.3.story.md (status, tasks, and record updates)

## QA Results

### Review Date: 2025-10-30

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

- Rerank telemetry now surfaces fallback counters across spans, Prometheus payloads, processing summaries, and CLI output without breaking existing data contracts.
- Regression suites exercise toggle-disabled, error, and success paths, keeping fallback provenance consistent for operators.
- Documentation updates align runbook, alerting thresholds, and dashboard guidance with the new signals.

### Refactoring Performed

- None.

### Compliance Check

- Coding Standards: âœ“ Code follows repository guidance for telemetry helpers and CLI logging.
- Project Structure: âœ“ Updates stay within documented processor and docs modules.
- Testing Strategy: âœ“ Targeted pytest suites cover new instrumentation paths.
- All ACs Met: âœ“ AC1-AC3 validated via automated tests and documentation review.

### Improvements Checklist

- [x] Verified fallback counters recorded in `processing_summary.json` and CLI summary logs.
- [x] Confirmed telemetry spans and metrics include fallback_reason/source attributes across skip/error states.
- [ ] Schedule staging run with Prometheus enabled to observe `rerank_fallback_total` alerts in practice.
- [ ] Extend monitoring to capture long-term fallback rate trends once stage lands in production.

### Security Review

- No new data exposure; fallback metadata uses existing redacted telemetry channels.

### Performance Considerations

- Counter emission only triggers on fallback events and showed no measurable overhead in regression runs.

### Files Modified During Review

- None (QA assessment only).

### Gate Status

Gate: PASS â†’ docs/qa/gates/3.3-telemetry-and-observability-enhancements.yml
Risk profile: not generated for this review cycle.
NFR assessment: covered inline within gate decision.

### Recommended Status

[âœ“ Ready for Done]
(Story owner decides final status)
