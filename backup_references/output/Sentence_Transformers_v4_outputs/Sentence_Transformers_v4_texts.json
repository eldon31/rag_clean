[
  "cosine_similarity = util.cos_sim(query_embedding, passage_embedding) dot_product_similarity = util.dot_score(query_embedding, passage_embedding) print(\"Cosine similarity:\", cosine_similarity) ``` ### Cross-Encoder Model Usage ```python from sentence_transformers import CrossEncoder import torch # Load cross-encoder with sigmoid activation for 0-1 scores model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\", activation_fn=torch.nn.Sigmoid()) # Score query-passage pairs scores = model.predict([ (\"How big is London\", \"London has 9,787,426 inhabitants at the 2011 census\"), (\"How big is London\", \"London is well known for its museums\") ]) # Returns array([0.9998173, 0.01312432], dtype=float32) ``` ### Batch Processing for Production ```python",
  "This document covers the comprehensive training system for `SparseEncoder` models in the sentence-transformers library. It explains the specialized components, architectures, and workflows required to train sparse representation models such as SPLADE and CSR (Contrastive Sparse Representation) models. For information about general training concepts that apply across all model types, see [Training](#3). For specific loss function details, see [Loss Functions for SparseEncoder](#3.5). For evaluation during training, see [SparseEncoder Evaluators](#4.2). ## Training System Architecture The `SparseEncoder` training system consists of specialized components that handle the unique requirements of sparse representation learning, including sparsity regularization and architecture-specific optimizations. ```mermaid graph TB subgraph \"Training Components\" SparseEncoderTrainer[\"SparseEncoderTrainer\"] SparseEncoderTrainingArguments[\"SparseEncoderTrainingArguments\"] DataCollator[\"DataCollator\"] end subgraph \"Model Architectures\" MLMTransformer[\"MLMTransformer\"] SpladePooling[\"SpladePooling\"] SparseAutoEncoder[\"SparseAutoEncoder\"] SparseStaticEmbedding[\"SparseStaticEmbedding\"] Router[\"Router\"] end subgraph \"Loss Functions\" SpladeLoss[\"SpladeLoss\"] CSRLoss[\"CSRLoss\"] FlopsLoss[\"FlopsLoss\"] SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"] end subgraph \"Evaluators\" SparseNanoBEIREvaluator[\"SparseNanoBEIREvaluator\"] SparseInformationRetrievalEvaluator[\"SparseInformationRetrievalEvaluator\"] SparseEmbeddingSimilarityEvaluator[\"SparseEmbeddingSimilarityEvaluator\"] end SparseEncoderTrainer --> MLMTransformer SparseEncoderTrainer --> SpladePooling SparseEncoderTrainer --> SparseAutoEncoder SparseEncoderTrainer --> Router SparseEncoderTrainingArguments --> SparseEncoderTrainer DataCollator --> SparseEncoderTrainer SpladeLoss --> SparseEncoderTrainer CSRLoss --> SparseEncoderTrainer FlopsLoss --> SpladeLoss SparseMultipleNegativesRankingLoss --> SpladeLoss SparseMultipleNegativesRankingLoss --> CSRLoss SparseNanoBEIREvaluator --> SparseEncoderTrainer SparseInformationRetrievalEvaluator --> SparseEncoderTrainer SparseEmbeddingSimilarityEvaluator --> SparseEncoderTrainer ``` **Sources:** [docs/sparse_encoder/training_overview.md:17-46](), [sentence_transformers/sparse_encoder/__init__.py:1-14](), [sentence_transformers/sparse_encoder/losses/__init__.py:1-29]() ## Sparse Encoder Architectures The training system supports three primary sparse encoder architectures, each requiring different components and training strategies. ### SPLADE Architecture SPLADE models use `MLMTransformer` followed by `SpladePooling` to create sparse lexical representations from masked language model logits. ```mermaid graph LR Input[\"Text Input\"] --> MLMTransformer[\"MLMTransformer<br/>(BERT/RoBERTa/DistilBERT)\"] MLMTransformer --> Logits[\"MLM Head Logits<br/>(vocab_size)\"] Logits --> SpladePooling[\"SpladePooling<br/>(max/sum pooling)\"] SpladePooling --> Activation[\"ReLU + log1p\"] Activation --> SparseEmbedding[\"Sparse Embedding<br/>(vocab_size dimensions)\"] ``` **Sources:** [docs/sparse_encoder/training_overview.md:59-98](), [sentence_transformers/sparse_encoder/models/MLMTransformer.py:26-55](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:13-40]() ### Inference-Free SPLADE Architecture This architecture uses `Router` to process queries and documents differently, with lightweight `SparseStaticEmbedding` for queries and full SPLADE processing for documents. ```mermaid graph TB subgraph \"Router Module\" QueryRoute[\"Query Route\"] DocumentRoute[\"Document Route\"] end QueryInput[\"Query Text\"] --> QueryRoute DocumentInput[\"Document Text\"] --> DocumentRoute QueryRoute --> SparseStaticEmbedding[\"SparseStaticEmbedding<br/>(Static Weights)\"] DocumentRoute --> MLMTransformer[\"MLMTransformer\"] MLMTransformer --> SpladePooling[\"SpladePooling\"] SparseStaticEmbedding --> QueryEmbedding[\"Query Sparse Embedding\"] SpladePooling --> DocumentEmbedding[\"Document Sparse Embedding\"] ``` **Sources:** [docs/sparse_encoder/training_overview.md:99-168](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py:24-62]() ### CSR (Contrastive Sparse Representation) Architecture CSR models apply `SparseAutoEncoder` on top of dense sentence transformer embeddings to create sparse representations.",
  "```mermaid graph LR Input[\"Text Input\"] --> Transformer[\"Transformer<br/>(BERT/etc)\"] Transformer --> Pooling[\"Pooling<br/>(mean/cls)\"] Pooling --> DenseEmbedding[\"Dense Embedding\"] DenseEmbedding --> SparseAutoEncoder[\"SparseAutoEncoder<br/>(k=256, k_aux=512)\"] SparseAutoEncoder --> SparseEmbedding[\"Sparse Embedding<br/>(k dimensions)\"] ``` **Sources:** [docs/sparse_encoder/training_overview.md:169-228](), [sentence_transformers/sparse_encoder/models/__init__.py:1-9]() ## Training Components ### Model Initialization Models are initialized differently based on the target architecture: | Architecture | Initialization Method | Key Components | |-------------|----------------------|----------------| | SPLADE | `SparseEncoder(\"bert-base-uncased\")` | `MLMTransformer` + `SpladePooling` | | Inference-Free SPLADE | `Router.for_query_document()` | `SparseStaticEmbedding` + `MLMTransformer` + `SpladePooling` | | CSR | `SparseEncoder(\"sentence-transformer-model\")` | `Transformer` + `Pooling` + `SparseAutoEncoder` | **Sources:** [docs/sparse_encoder/training_overview.md:48-229]() ### Loss Function Requirements Sparse encoder training requires specialized loss functions that incorporate sparsity regularization: ```mermaid graph TB subgraph \"Wrapper Losses\" SpladeLoss[\"SpladeLoss<br/>(for SPLADE models)\"] CSRLoss[\"CSRLoss<br/>(for CSR models)\"] end subgraph \"Main Loss Functions\" SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"] SparseMarginMSELoss[\"SparseMarginMSELoss\"] SparseDistillKLDivLoss[\"SparseDistillKLDivLoss\"] end subgraph \"Regularization\" FlopsLoss[\"FlopsLoss<br/>(default regularizer)\"] CustomRegularizer[\"Custom Regularizer\"] end subgraph \"Specialized Losses\" CSRReconstructionLoss[\"CSRReconstructionLoss\"] SparseMSELoss[\"SparseMSELoss<br/>(standalone distillation)\"] end SpladeLoss --> SparseMultipleNegativesRankingLoss SpladeLoss --> SparseMarginMSELoss SpladeLoss --> SparseDistillKLDivLoss SpladeLoss --> FlopsLoss SpladeLoss --> CustomRegularizer CSRLoss --> SparseMultipleNegativesRankingLoss CSRLoss --> CSRReconstructionLoss ``` **Sources:** [docs/sparse_encoder/training_overview.md:346-393](), [docs/sparse_encoder/loss_overview.md:4-28](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-187]() ### Training Arguments `SparseEncoderTrainingArguments` extends standard training arguments with sparse-specific parameters: | Parameter | Purpose | Example | |-----------|---------|---------| | `router_mapping` | Maps dataset columns to Router tasks | `{\"question\": \"query\", \"answer\": \"document\"}` | | `learning_rate_mapping` | Sets different learning rates per component | `{\"SparseStaticEmbedding.*\": 1e-3}` | | `batch_sampler` | Controls batch composition | `BatchSamplers.NO_DUPLICATES` | | `prompts` | Task-specific prompts | `{\"query\": \"Represent this query:\"}` | **Sources:** [docs/sparse_encoder/training_overview.md:394-473](), [docs/sparse_encoder/training_overview.md:149-168]() ## Training Workflow The complete training workflow integrates all components through the `SparseEncoderTrainer`: ```mermaid sequenceDiagram participant User participant SparseEncoderTrainer participant Model as \"SparseEncoder\" participant Loss as \"SpladeLoss/CSRLoss\" participant Evaluator participant TrainingArgs as \"SparseEncoderTrainingArguments\" User->>Model: Initialize architecture User->>Loss: Configure loss function User->>TrainingArgs: Set training parameters User->>Evaluator: Setup evaluation User->>SparseEncoderTrainer: Create trainer SparseEncoderTrainer->>Model: Forward pass Model->>Loss: Compute loss + regularization Loss->>SparseEncoderTrainer: Return loss dict SparseEncoderTrainer->>Model: Backward pass alt Evaluation Step SparseEncoderTrainer->>Evaluator: Run evaluation Evaluator->>Model: Generate embeddings Evaluator->>SparseEncoderTrainer: Return metrics end SparseEncoderTrainer->>User: Training complete ```",
  "**Sources:** [docs/sparse_encoder/training_overview.md:475-552]() ### Dataset Format Requirements Training datasets must match the loss function requirements: | Loss Function | Input Columns | Label Column | Example | |---------------|---------------|--------------|---------| | `SparseMultipleNegativesRankingLoss` | `(anchor, positive)` or `(anchor, positive, negative)` | None | `[\"query\", \"answer\"]` | | `SparseCoSENTLoss` | `(sentence_A, sentence_B)` | `score` (0-1) | `[\"text1\", \"text2\", \"score\"]` | | `SparseMarginMSELoss` | `(query, positive, negative)` | `margin_scores` | `[\"query\", \"pos\", \"neg\", \"margins\"]` | **Sources:** [docs/sparse_encoder/training_overview.md:328-344](), [docs/sparse_encoder/loss_overview.md:29-62]() ## Advanced Training Features ### Router-Based Training When using `Router` modules, special configuration is required to map dataset columns to routing paths: ```python",
  "ranked_results = cross_encoder.rank(query, candidates, return_documents=True) ``` **Sources:** [sentence_transformers/cross_encoder/CrossEncoder.py:48-226](), [sentence_transformers/cross_encoder/CrossEncoder.py:394-486](), [sentence_transformers/cross_encoder/CrossEncoder.py:488-586](), [tests/cross_encoder/test_cross_encoder.py:71-96]() ## Module Architecture All model types in sentence-transformers use a modular architecture where functionality is composed of discrete, reusable modules. ### Core Module Hierarchy ```mermaid graph TB subgraph \"Base Module System\" Module[\"Module<br/>Base class\"] InputModule[\"InputModule<br/>Input processing\"] Transformer[\"Transformer<br/>Token embeddings\"] Pooling[\"Pooling<br/>Sentence embeddings\"] end subgraph \"Processing Modules\" Dense[\"Dense<br/>Linear transformation\"] Normalize[\"Normalize<br/>L2 normalization\"] LayerNorm[\"LayerNorm<br/>Layer normalization\"] Router[\"Router<br/>Task routing\"] end subgraph \"Specialized Modules\" MLMTransformer[\"MLMTransformer<br/>Masked LM head\"] SpladePooling[\"SpladePooling<br/>SPLADE activation\"] CLIPModel[\"CLIPModel<br/>Multimodal encoding\"] end Module --> InputModule Module --> Dense Module --> Normalize InputModule --> Transformer InputModule --> Pooling Module --> Router Module --> MLMTransformer Module --> SpladePooling Module --> CLIPModel ``` ### Router Module for Asymmetric Architectures The `Router` module enables asymmetric architectures where queries and documents follow different processing paths: ```mermaid graph TB subgraph \"Router Architecture\" Input[\"Input + Task\"] Router[\"Router Module\"] QueryPath[\"Query Submodules\"] DocumentPath[\"Document Submodules\"] Output[\"Task-specific Output\"] end Input --> Router Router -->|task=\"query\"| QueryPath Router -->|task=\"document\"| DocumentPath QueryPath --> Output DocumentPath --> Output ``` This enables models like the inference-free SPLADE variants where queries use only necessary tokens while documents use expanded representations. **Sources:** [sentence_transformers/models/Transformer.py:36-58](), [sentence_transformers/models/Pooling.py:9-41](), [sentence_transformers/models/Router.py](), [tests/sparse_encoder/test_sparse_encoder.py:171-196]() ## Model Comparison and Selection | Model Type | Input Format | Output Format | Use Case | Computational Cost | |------------|--------------|---------------|----------|-------------------| | `SentenceTransformer` | Single texts | Dense vectors | Semantic similarity, clustering | Moderate | | `SparseEncoder` | Single texts | Sparse vectors | Lexical search, hybrid retrieval | Moderate | | `CrossEncoder` | Text pairs | Similarity scores | Reranking, pairwise classification | High | ### When to Use Each Type - **SentenceTransformer**: General-purpose semantic tasks, when you need individual text embeddings - **SparseEncoder**: When interpretability matters, hybrid search scenarios, or working with search engines - **CrossEncoder**: When highest accuracy is needed for pairwise decisions, reranking scenarios ### Backend Support All three model types support multiple inference backends: - **PyTorch**: Full functionality, training support - **ONNX**: Optimized inference, 2-3x speedup - **OpenVINO**: Intel hardware optimization **Sources:** [sentence_transformers/SentenceTransformer.py:408-414](), [sentence_transformers/cross_encoder/CrossEncoder.py:259-265](), [README.md:89-167](), [index.rst:12-13]() # Module Architecture This page details the modular building blocks that compose sentence transformer models. The sentence-transformers library uses a sequential pipeline architecture where modules are chained together to transform input text into final embeddings or representations. ## Overview of Modular Design The sentence-transformers library implements a modular architecture where models are composed of sequential modules. Each module performs a specific transformation on input features, passing the result to the next module in the pipeline. #### Core Model Architecture ```mermaid graph LR Input[\"Input Text\"] --> M1[\"Module 1<br/>(InputModule)\"] M1 --> M2[\"Module 2<br/>(Module)\"] M2 --> M3[\"Module 3<br/>(Module)\"] M3 --> Output[\"Final Output\"] subgraph \"Sequential Pipeline\" M1 M2 M3 end ``` The core models inherit from `nn.Sequential` and store modules in an ordered collection:",
  "- **`SentenceTransformer`**: Inherits from `nn.Sequential`, produces dense embeddings - **`SparseEncoder`**: Inherits from `SentenceTransformer`, produces sparse embeddings - **`CrossEncoder`**: Separate architecture (`nn.Module`), scores text pairs directly Sources: [sentence_transformers/SentenceTransformer.py:61](), [sentence_transformers/sparse_encoder/SparseEncoder.py:27](), [sentence_transformers/cross_encoder/CrossEncoder.py:48]() ## Module Type Hierarchy The library defines two main types of modules that form the building blocks of all models: #### Module Class Hierarchy ```mermaid classDiagram class Module { <<abstract>> +forward(features) +save(output_path) +load(model_name_or_path) +get_sentence_embedding_dimension() #config_keys: list[str] #config_file_name: str } class InputModule { <<abstract>> +tokenize(texts) +save_tokenizer(output_path) +tokenizer +save_in_root: bool } Module <|-- InputModule class Transformer { +auto_model: PreTrainedModel +tokenizer: PreTrainedTokenizer +max_seq_length: int +backend: str +do_lower_case: bool } class Pooling { +pooling_mode_cls_token: bool +pooling_mode_mean_tokens: bool +pooling_mode_max_tokens: bool +word_embedding_dimension: int +include_prompt: bool } class Router { +sub_modules: ModuleDict +default_route: str +allow_empty_key: bool +forward_kwargs: list[str] } InputModule <|-- Transformer InputModule <|-- Router Module <|-- Pooling ``` #### Module Types - **`Module`**: Base class defining `forward()`, `save()`, `load()` interface - **`InputModule`**: Subclass adding `tokenize()` capability (must be first in pipeline) - **Dense modules**: `Transformer`, `Pooling`, `Normalize`, `Dense` - **Sparse modules**: `MLMTransformer`, `SpladePooling`, `SparseStaticEmbedding` - **Routing modules**: `Router` for asymmetric architectures Sources: [sentence_transformers/models/Module.py:33-89](), [sentence_transformers/models/InputModule.py:13-93](), [sentence_transformers/models/Transformer.py:36](), [sentence_transformers/models/Pooling.py:9](), [sentence_transformers/models/Router.py:22]() ## Core Module Types ### Transformer Module The `Transformer` module is the most common input module, wrapping Hugging Face transformer models: ```mermaid graph TD subgraph \"Transformer Module\" Input[\"Input Texts\"] --> Tokenizer[\"AutoTokenizer\"] Tokenizer --> Features[\"TokenizedFeatures<br/>{input_ids, attention_mask}\"] Features --> AutoModel[\"AutoModel<br/>(BERT, RoBERTa, etc.)\"] AutoModel --> TokenEmb[\"token_embeddings\"] TokenEmb --> Output[\"{token_embeddings,<br/>attention_mask,<br/>all_layer_embeddings?}\"] end ``` Key attributes: - `auto_model`: The wrapped Hugging Face model - `tokenizer`: The associated tokenizer - `max_seq_length`: Maximum sequence length - `backend`: Inference backend (`torch`, `onnx`, `openvino`) Sources: [sentence_transformers/models/Transformer.py:37-646]() ### Pooling Module The `Pooling` module aggregates token embeddings into sentence embeddings: ```mermaid graph TD Input[\"{token_embeddings,<br/>attention_mask}\"] --> Strategy[\"Pooling Strategy\"] Strategy --> CLS[\"CLS Token<br/>pooling_mode_cls_token\"] Strategy --> Mean[\"Mean Pooling<br/>pooling_mode_mean_tokens\"] Strategy --> Max[\"Max Pooling<br/>pooling_mode_max_tokens\"] Strategy --> Last[\"Last Token<br/>pooling_mode_lasttoken\"] Strategy --> Weighted[\"Weighted Mean<br/>pooling_mode_weightedmean_tokens\"] CLS --> Concat[\"Concatenate Outputs\"] Mean --> Concat Max --> Concat Last --> Concat Weighted --> Concat Concat --> SentEmb[\"{sentence_embedding}\"] ``` Pooling strategies can be combined by setting multiple `pooling_mode_*` flags to `True`. Sources: [sentence_transformers/models/Pooling.py:9-248]() ### Router Module The `Router` module enables asymmetric architectures with different processing paths based on task type: #### Router Architecture",
  "```mermaid graph TD Input[\"{features, task='query'}\"] --> RouterForward[\"Router.forward()\"] RouterForward --> TaskCheck{task in sub_modules?} TaskCheck -->|\"query\"| QuerySeq[\"self.sub_modules['query']<br/>nn.Sequential\"] TaskCheck -->|\"document\"| DocSeq[\"self.sub_modules['document']<br/>nn.Sequential\"] TaskCheck -->|None| DefaultRoute[\"self.default_route<br/>or raise ValueError\"] QuerySeq --> QueryMods[\"Query-specific modules<br/>(e.g. SparseStaticEmbedding)\"] DocSeq --> DocMods[\"Document-specific modules<br/>(e.g. MLMTransformer + SpladePooling)\"] QueryMods --> QueryEmb[\"{sentence_embedding}\"] DocMods --> DocEmb[\"{sentence_embedding}\"] ``` #### Router Configuration ```python",
  "trainer = CrossEncoderTrainer( model=model, train_dataset=train_dataset, loss=loss, ) trainer.train() ``` ### Configuration Parameters Common parameters across learning-to-rank losses: | Parameter | Type | Purpose | |-----------|------|---------| | `model` | `CrossEncoder` | Model to train | | `activation_fn` | `nn.Module` | Applied to logits before loss computation | | `mini_batch_size` | `int` | Controls memory usage and processing speed | Sources: [examples/cross_encoder/training/ms_marco/training_ms_marco_listmle.py:93-94](), [examples/cross_encoder/training/ms_marco/training_ms_marco_plistmle.py:96]() ## Performance Recommendations Based on the documentation and implementation comments: 1. **LambdaLoss with NDCGLoss2PPScheme**: Generally performs best for ranking tasks 2. **PListMLELoss**: Outperforms standard ListMLELoss due to position weighting 3. **Mini-batch size**: Critical for memory management when processing many documents per query 4. **Hard negative mining**: Use `mine_hard_negatives` with `output_format=\"labeled-list\"` for better training data The learning-to-rank losses are optimized for handling variable numbers of documents per query and support both binary and continuous relevance labels. Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:175-176](), [sentence_transformers/cross_encoder/losses/PListMLELoss.py:105-111](), [examples/cross_encoder/training/ms_marco/training_ms_marco_cmnrl.py:62-71]() # Memory-Efficient Training This document covers memory-efficient training techniques in sentence-transformers that allow training with large batch sizes and complex loss functions while maintaining reasonable memory usage. These techniques are essential for achieving optimal performance on modern embedding models without requiring excessive GPU memory. For general training information, see [SentenceTransformer Training](#3.1). For specific loss functions, see [Loss Functions for SentenceTransformer](#3.4). ## Cached Loss Functions The primary memory-efficient training technique uses **GradCache**, which enables training with much larger effective batch sizes while maintaining constant memory usage. This is implemented through cached versions of standard loss functions. ### GradCache Architecture ```mermaid flowchart TD Input[\"Input Batch (e.g., 1024 samples)\"] --> CMNRL[\"CachedMultipleNegativesRankingLoss.forward()\"] CMNRL --> Split[\"Split into mini_batch_size chunks\"] Split --> Step1[\"Step 1: Forward Pass (torch.no_grad)\"] Step1 --> EmbedIter[\"embed_minibatch_iter()\"] EmbedIter --> EmbedMB[\"embed_minibatch()\"] EmbedMB --> RandCtx[\"RandContext.copy_random_state\"] RandCtx --> CacheEmb[\"reps.detach().requires_grad_()\"] CacheEmb --> Step2[\"Step 2: Loss Calculation\"] Step2 --> CalcLoss[\"calculate_loss_and_cache_gradients()\"] CalcLoss --> GradCache[\"self.cache = [[r.grad for r in rs]]\"] GradCache --> Step3[\"Step 3: Second Forward Pass (torch.enable_grad)\"] Step3 --> Hook[\"loss.register_hook(_backward_hook)\"] Hook --> Surrogate[\"torch.dot(reps_mb.flatten(), grad_mb.flatten())\"] Surrogate --> Backprop[\"surrogate.backward()\"] ``` **Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:278-305](), [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:42-62](), [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:18-39]() ### Available Cached Loss Functions | Standard Loss | Cached Version | Memory Benefit | |---------------|----------------|----------------| | `MultipleNegativesRankingLoss` | `CachedMultipleNegativesRankingLoss` | Constant memory for any batch size | | `GISTEmbedLoss` | `CachedGISTEmbedLoss` | Large batch sizes with guide model | | `MultipleNegativesSymmetricRankingLoss` | `CachedMultipleNegativesSymmetricRankingLoss` | Symmetric loss with caching | **Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:64](), [sentence_transformers/losses/CachedGISTEmbedLoss.py:65](), [sentence_transformers/losses/CachedMultipleNegativesSymmetricRankingLoss.py:41]() ### Mini-batch Processing Implementation",
  "```mermaid graph LR Batch[\"Full Batch\"] --> Iterator[\"embed_minibatch_iter()\"] Iterator --> MB1[\"Mini-batch 1<br/>embed_minibatch()\"] Iterator --> MB2[\"Mini-batch 2<br/>embed_minibatch()\"] Iterator --> MB3[\"Mini-batch N<br/>embed_minibatch()\"] MB1 --> RS1[\"RandContext<br/>(Random State)\"] MB2 --> RS2[\"RandContext<br/>(Random State)\"] MB3 --> RS3[\"RandContext<br/>(Random State)\"] RS1 --> Cache1[\"Cached Embeddings\"] RS2 --> Cache2[\"Cached Embeddings\"] RS3 --> Cache3[\"Cached Embeddings\"] ``` The `mini_batch_size` parameter controls memory usage during training. Each mini-batch is processed through `embed_minibatch()` with `RandContext` ensuring reproducible embeddings across forward passes. **Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:175-223](), [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:18-39]() ## Matryoshka Training Matryoshka training allows models to work efficiently at multiple embedding dimensions, reducing storage and computation costs for downstream applications. ### MatryoshkaLoss Architecture ```mermaid flowchart TD Model[\"SentenceTransformer\"] --> Forward[\"ForwardDecorator\"] Loss[\"Base Loss Function\"] --> Matryoshka[\"MatryoshkaLoss\"] Dims[\"matryoshka_dims<br/>[768, 512, 256, 128, 64]\"] --> Matryoshka Weights[\"matryoshka_weights<br/>[1, 1, 1, 1, 1]\"] --> Matryoshka Forward --> Cache[\"Cache Full Embeddings\"] Cache --> Shrink1[\"shrink(embeddings, 768)\"] Cache --> Shrink2[\"shrink(embeddings, 512)\"] Cache --> Shrink3[\"shrink(embeddings, 256)\"] Cache --> Shrink4[\"shrink(embeddings, 128)\"] Cache --> Shrink5[\"shrink(embeddings, 64)\"] Shrink1 --> Loss1[\"Loss at 768d\"] Shrink2 --> Loss2[\"Loss at 512d\"] Shrink3 --> Loss3[\"Loss at 256d\"] Shrink4 --> Loss4[\"Loss at 128d\"] Shrink5 --> Loss5[\"Loss at 64d\"] Loss1 --> Combine[\"Weighted Sum\"] Loss2 --> Combine Loss3 --> Combine Loss4 --> Combine Loss5 --> Combine ``` **Sources:** [sentence_transformers/losses/MatryoshkaLoss.py:113-253](), [sentence_transformers/losses/MatryoshkaLoss.py:30-111]() ### Cached Matryoshka Integration For cached losses, Matryoshka uses `CachedLossDecorator` instead of `ForwardDecorator`: ```mermaid graph TD CachedLoss[\"CachedMultipleNegativesRankingLoss\"] --> Decorator[\"CachedLossDecorator\"] Embeddings[\"Pre-computed Embeddings\"] --> Decorator Decorator --> Shrink[\"shrink() for each dimension\"] Shrink --> Calculate[\"calculate_loss()\"] Calculate --> Weighted[\"Apply matryoshka_weights\"] ``` **Sources:** [sentence_transformers/losses/MatryoshkaLoss.py:67-111](), [sentence_transformers/losses/MatryoshkaLoss.py:195-204]() ## Router-based Asymmetric Models The `Router` module enables memory-efficient asymmetric architectures where different encoders are used for queries and documents. ### Router Architecture",
  "```mermaid graph TD Input[\"features: dict[str, Tensor]\"] --> RouterFwd[\"Router.forward()\"] RouterFwd --> TaskCheck{\"task = features.get('task', self.default_route)\"} TaskCheck -->|\"task='query'\"| QuerySeq[\"self.sub_modules['query']\"] TaskCheck -->|\"task='document'\"| DocSeq[\"self.sub_modules['document']\"] TaskCheck -->|\"None\"| DefaultRoute[\"self.default_route\"] QuerySeq --> QMod1[\"SparseStaticEmbedding\"] QuerySeq --> QMod2[\"Additional Query Modules\"] DocSeq --> DMod1[\"MLMTransformer\"] DocSeq --> DMod2[\"SpladePooling\"] Training[\"Training Phase\"] --> RouterMap[\"router_mapping in TrainingArguments\"] RouterMap --> DataCollator[\"SentenceTransformerDataCollator\"] DataCollator --> TaskAssign[\"task = router_mapping.get(column_name)\"] TaskAssign --> TokenizeFn[\"self.tokenize_fn(inputs, task=task)\"] ``` This enables memory-efficient asymmetric training where lightweight query encoders (e.g., `SparseStaticEmbedding`) can be combined with powerful document encoders, reducing both training and inference costs. **Sources:** [sentence_transformers/models/Router.py:217-245](), [sentence_transformers/models/Router.py:287-324](), [sentence_transformers/data_collator.py:90-118]() ## Batch Sampling and Multi-Dataset Training The `SentenceTransformerTrainer` provides memory-efficient batch sampling strategies for large-scale training: ### Batch Sampler Architecture ```mermaid graph TD TrainingArgs[\"SentenceTransformerTrainingArguments\"] --> BatchSampler[\"args.batch_sampler\"] BatchSampler --> DefaultBS[\"DefaultBatchSampler\"] BatchSampler --> NoDupBS[\"NoDuplicatesBatchSampler\"] BatchSampler --> GroupBS[\"GroupByLabelBatchSampler\"] MultiDataset[\"Multi-Dataset Training\"] --> MultiBS[\"args.multi_dataset_batch_sampler\"] MultiBS --> PropBS[\"ProportionalBatchSampler\"] MultiBS --> RoundRobinBS[\"RoundRobinBatchSampler\"] Trainer[\"SentenceTransformerTrainer\"] --> GetBatchSampler[\"get_batch_sampler()\"] GetBatchSampler --> ConcatDS[\"ConcatDataset\"] ConcatDS --> GetMultiBS[\"get_multi_dataset_batch_sampler()\"] ``` **Sources:** [sentence_transformers/trainer.py:623-684](), [sentence_transformers/trainer.py:685-737](), [sentence_transformers/sampler.py:28-35]() ### Memory Usage Patterns ```mermaid graph LR subgraph Traditional[\"Traditional Training\"] TB[\"Batch Size: 32\"] --> TM[\"Memory: Base\"] TB2[\"Batch Size: 1024\"] --> TM2[\"Memory: 32x Base<br/>(OOM)\"] end subgraph Cached[\"Cached Training\"] CB[\"mini_batch_size: 32\"] --> CM[\"Memory: Base + Cache\"] CB2[\"per_device_train_batch_size: 1024\"] --> CM2[\"Memory: Base + Cache<br/>(Constant!)\"] end subgraph MultiDataset[\"Multi-Dataset Training\"] MDS[\"DatasetDict\"] --> CDS[\"ConcatDataset\"] CDS --> TrackDS[\"track dataset_name\"] TrackDS --> LossSelect[\"loss[dataset_name]\"] end ``` **Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:100-107](), [sentence_transformers/trainer.py:416-422]() ### Data Collator Memory Optimizations The `SentenceTransformerDataCollator` includes several memory-efficient features: ```mermaid graph TD DataCollator[\"SentenceTransformerDataCollator.__call__()\"] --> RouterMap[\"self.router_mapping\"] DataCollator --> Prompts[\"self.prompts\"] DataCollator --> PromptCache[\"self._prompt_length_mapping\"] RouterMap --> TaskAssign[\"task = router_mapping.get(column_name)\"] Prompts --> PromptCheck[\"if isinstance(prompts, str)\"] PromptCheck --> PromptPrefix[\"prompt + row[column_name]\"] PromptCache --> GetPromptLen[\"_get_prompt_length()\"] GetPromptLen --> TokenizeOnce[\"tokenize_fn([prompt], task=task)\"] TokenizeOnce --> CacheLen[\"_prompt_length_mapping[(prompt, task)]\"] TaskAssign --> TokenizeFn[\"tokenize_fn(inputs, task=task)\"] TokenizeFn --> BatchKeys[\"batch[f'{column_name}_{key}'] = value\"] ``` The prompt length caching in `_get_prompt_length()` prevents repeated tokenization of the same prompts, significantly reducing memory overhead during data loading. **Sources:** [sentence_transformers/data_collator.py:35-119](), [sentence_transformers/data_collator.py:121-138]() ## Implementation Examples",
  "### Using Cached Losses ```python # Standard approach - memory scales with batch size loss = MultipleNegativesRankingLoss(model) # Cached approach - constant memory usage loss = CachedMultipleNegativesRankingLoss( model, mini_batch_size=32, # Controls actual memory usage show_progress_bar=True ) ``` ## Trainer Memory Optimizations The `SentenceTransformerTrainer` includes several memory-efficient features beyond cached losses: ### Loss Component Tracking ```mermaid graph TD ComputeLoss[\"SentenceTransformerTrainer.compute_loss()\"] --> TrackLoss[\"track_loss_components()\"] TrackLoss --> AccumLoss[\"self.accum_loss_components[training_type]\"] AccumLoss --> LogLoss[\"self.log()\"] LogLoss --> NestedGather[\"self._nested_gather()\"] NestedGather --> AvgLoss[\"value.sum() / steps\"] LossDict[\"loss: dict[str, torch.Tensor]\"] --> Stack[\"torch.stack(list(loss.values())).sum()\"] Stack --> SingleLoss[\"Final Loss Tensor\"] ``` This prevents memory spikes when losses return dictionaries with multiple components by accumulating and averaging them efficiently. **Sources:** [sentence_transformers/trainer.py:443-462](), [sentence_transformers/trainer.py:464-494](), [sentence_transformers/trainer.py:431-441]() ### Training Arguments for Memory Efficiency ```python args = SentenceTransformerTrainingArguments( per_device_train_batch_size=1024, # Large effective batch size gradient_accumulation_steps=1, # No additional accumulation needed dataloader_drop_last=True, # Avoid uneven batches batch_sampler=BatchSamplers.NO_DUPLICATES, # Memory-efficient sampling multi_dataset_batch_sampler=MultiDatasetBatchSamplers.PROPORTIONAL, ) ``` **Sources:** [sentence_transformers/trainer.py:623-684](), [sentence_transformers/training_args.py:37-39]() ### Router Training Configuration ```python",
  "query_embeddings = model.encode_query(\"machine learning algorithms\") document_embeddings = model.encode_document([ \"Machine learning uses statistical techniques...\", \"Deep learning is a subset of machine learning...\" ]) ``` **Sources:** [sentence_transformers/sparse_encoder/SparseEncoder.py:181-293](), [sentence_transformers/sparse_encoder/SparseEncoder.py:295-410](), [README.md:134-167]() ## CrossEncoder: Reranking and Classification `CrossEncoder` models take pairs of texts as input and output similarity scores or classification labels, providing high-precision reranking capabilities. ### Basic Usage ```python from sentence_transformers import CrossEncoder # Load a cross-encoder model model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\") # Define query and candidate passages query = \"How many people live in Berlin?\" passages = [ \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\", \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\", \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\", ] # Predict similarity scores for query-passage pairs scores = model.predict([(query, passage) for passage in passages]) print(scores) # [8.607139 5.506266 6.352977] ``` ### Ranking Documents ```python",
  "if Router in [module.__class__ for module in model.children()] and not args.router_mapping: raise ValueError( \"You are using a Router module in your model, but you did not provide a \" \"`router_mapping` in the training arguments.\" ) args = SentenceTransformerTrainingArguments( router_mapping={ \"question\": \"query\", \"answer\": \"document\" }, learning_rate_mapping={ r\"SparseStaticEmbedding\\.*\": 1e-3, # Higher LR for static embeddings r\".*\": 2e-5 # Lower LR for transformer layers } ) ``` **Sources:** [sentence_transformers/trainer.py:206-212](), [sentence_transformers/models/Router.py:47-68](), [tests/models/test_router.py:414-454]() ## Performance Considerations ### Memory vs Speed Trade-offs | Technique | Memory Reduction | Speed Impact | Use Case | |-----------|------------------|--------------|----------| | Cached Losses | ~75% | -20% | Large batch training | | Mini-batching | Configurable | Variable | Memory-constrained environments | | Matryoshka | None | +10% | Multi-resolution embeddings | | Router | ~50% for queries | +5% | Asymmetric retrieval | ### Optimal Configuration Guidelines 1. **Cached losses**: Use `mini_batch_size` as large as your GPU memory allows 2. **Matryoshka**: Set `n_dims_per_step=1` for memory efficiency, `-1` for speed 3. **Router**: Use lightweight query encoders with powerful document encoders 4. **Gradient accumulation**: Generally unnecessary with cached losses **Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:88-92](), [sentence_transformers/losses/MatryoshkaLoss.py:140-144]() # Evaluation The evaluation system in sentence-transformers provides comprehensive metrics and benchmarking capabilities for assessing model performance across diverse tasks. This system supports evaluation during training for model selection, standalone evaluation for benchmarking, and integration with various downstream applications. For training-specific guidance including how evaluators integrate with the training loop, see [Training](#3). For task-specific evaluator details, see [SentenceTransformer Evaluators](#4.1), [SparseEncoder Evaluators](#4.2), and [CrossEncoder Evaluators](#4.3). ## Evaluation Architecture The evaluation system is built around a modular architecture where all evaluators inherit from the `SentenceEvaluator` base class. This design provides consistent interfaces while allowing specialized implementations for different task types. ### Core Evaluation Flow ```mermaid graph TD Model[\"SentenceTransformer\"] --> Evaluator[\"SentenceEvaluator\"] Evaluator --> EmbedInputs[\"embed_inputs()\"] Evaluator --> ComputeMetrics[\"compute_metrics()\"] ComputeMetrics --> Metrics[\"Primary & Secondary Metrics\"] Metrics --> CSV[\"CSV Results\"] Metrics --> ModelCard[\"Model Card Data\"] Metrics --> Selection[\"Model Selection\"] EmbedInputs --> Encode[\"model.encode()\"] Encode --> Embeddings[\"Text Embeddings\"] Embeddings --> ComputeMetrics ``` **Evaluation Flow in sentence-transformers** The base `SentenceEvaluator` class defines the evaluation interface and common functionality shared across all evaluators. Every evaluator implements the `__call__` method to perform evaluation and return metrics. Sources: [sentence_transformers/evaluation/SentenceEvaluator.py:13-121]() ### Key Evaluation Concepts | Concept | Description | Implementation | |---------|-------------|----------------| | **Primary Metric** | Main metric used for model selection | `evaluator.primary_metric` attribute | | **Greater is Better** | Whether higher scores indicate better performance | `evaluator.greater_is_better` boolean | | **CSV Logging** | Track metrics over training epochs/steps | `write_csv` parameter across evaluators | | **Model Card Integration** | Store evaluation results in model metadata | `store_metrics_in_model_card_data()` method | ```mermaid graph LR subgraph \"Base Class\" SentenceEvaluator[\"SentenceEvaluator\"] end subgraph \"Task-Specific Evaluators\" IRE[\"InformationRetrievalEvaluator\"] BCE[\"BinaryClassificationEvaluator\"] ESE[\"EmbeddingSimilarityEvaluator\"] RE[\"RerankingEvaluator\"] TE[\"TripletEvaluator\"] PME[\"ParaphraseMiningEvaluator\"] TRE[\"TranslationEvaluator\"] MSE[\"MSEEvaluator\"] LAE[\"LabelAccuracyEvaluator\"] end SentenceEvaluator --> IRE SentenceEvaluator --> BCE SentenceEvaluator --> ESE SentenceEvaluator --> RE SentenceEvaluator --> TE SentenceEvaluator --> PME SentenceEvaluator --> TRE SentenceEvaluator --> MSE SentenceEvaluator --> LAE ``` **Evaluator Class Hierarchy**",
  "Sources: [sentence_transformers/evaluation/SentenceEvaluator.py:13-121](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-568](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379]() ## Evaluation Categories ### Information Retrieval Evaluation The `InformationRetrievalEvaluator` is the most comprehensive evaluator, designed for search and retrieval tasks. It computes standard IR metrics including Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), precision, recall, and accuracy at various k values. ```mermaid graph TD Queries[\"queries: Dict[str, str]\"] --> IRE[\"InformationRetrievalEvaluator\"] Corpus[\"corpus: Dict[str, str]\"] --> IRE RelevantDocs[\"relevant_docs: Dict[str, Set[str]]\"] --> IRE IRE --> QueryEmb[\"Query Embeddings\"] IRE --> CorpusEmb[\"Corpus Embeddings\"] QueryEmb --> SimilarityScores[\"Similarity Computation\"] CorpusEmb --> SimilarityScores SimilarityScores --> TopK[\"Top-k Retrieval\"] TopK --> IRMetrics[\"IR Metrics Computation\"] IRMetrics --> MRR[\"MRR@k\"] IRMetrics --> NDCG[\"NDCG@k\"] IRMetrics --> MAP[\"MAP@k\"] IRMetrics --> Accuracy[\"Accuracy@k\"] IRMetrics --> PrecisionRecall[\"Precision/Recall@k\"] ``` **Information Retrieval Evaluation Pipeline** Key parameters for IR evaluation: - `corpus_chunk_size`: Controls memory usage during large corpus evaluation - `score_functions`: Multiple similarity functions can be evaluated simultaneously - `write_predictions`: Enable writing retrieval results for downstream analysis Sources: [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-568]() ### Similarity and Classification Evaluation For tasks involving sentence pair similarity or binary classification: | Evaluator | Primary Use Case | Key Metrics | |-----------|------------------|-------------| | `EmbeddingSimilarityEvaluator` | Semantic textual similarity | Pearson, Spearman correlation | | `BinaryClassificationEvaluator` | Binary similarity classification | Accuracy, F1, Precision, Recall, AP | | `TripletEvaluator` | Triplet-based ranking | Accuracy with configurable margins | ```mermaid graph LR subgraph \"Similarity Tasks\" STS[\"Semantic Textual Similarity\"] BinClass[\"Binary Classification\"] Triplets[\"Triplet Ranking\"] end subgraph \"Evaluators\" ESE[\"EmbeddingSimilarityEvaluator\"] BCE[\"BinaryClassificationEvaluator\"] TE[\"TripletEvaluator\"] end subgraph \"Similarity Functions\" Cosine[\"cosine\"] Dot[\"dot\"] Euclidean[\"euclidean\"] Manhattan[\"manhattan\"] end STS --> ESE BinClass --> BCE Triplets --> TE ESE --> Cosine ESE --> Dot ESE --> Euclidean ESE --> Manhattan BCE --> Cosine BCE --> Dot BCE --> Euclidean BCE --> Manhattan TE --> Cosine TE --> Dot TE --> Euclidean TE --> Manhattan ``` **Similarity and Classification Evaluation Framework** Sources: [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:27-272](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379](), [sentence_transformers/evaluation/TripletEvaluator.py:26-271]() ### Specialized Task Evaluation Several evaluators target specific applications: **Reranking Evaluation**: The `RerankingEvaluator` assesses models on reranking tasks where queries are paired with positive and negative documents. It computes MAP, MRR@k, and NDCG@k metrics. **Paraphrase Mining**: The `ParaphraseMiningEvaluator` evaluates paraphrase detection by mining similar sentence pairs from a corpus and comparing against gold standard duplicates. **Translation/Multilingual**: The `TranslationEvaluator` tests cross-lingual sentence alignment by checking if translated sentence pairs have the highest mutual similarity. **Knowledge Distillation**: The `MSEEvaluator` measures mean squared error between teacher and student model embeddings for distillation tasks. Sources: [sentence_transformers/evaluation/RerankingEvaluator.py:25-372](), [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:18-279](), [sentence_transformers/evaluation/TranslationEvaluator.py:22-192](), [sentence_transformers/evaluation/MSEEvaluator.py:18-158]() ## Integration with Training Evaluators integrate seamlessly with the training pipeline for model selection and progress tracking. During training, evaluators are called at specified intervals to assess model performance.",
  "```mermaid graph TD TrainingLoop[\"Training Loop\"] --> EvalInterval[\"Evaluation Interval\"] EvalInterval --> CallEvaluator[\"evaluator(model, output_path, epoch, steps)\"] CallEvaluator --> ComputeMetrics[\"Compute Evaluation Metrics\"] ComputeMetrics --> LogResults[\"Log Results\"] ComputeMetrics --> SaveCSV[\"Save CSV Results\"] ComputeMetrics --> UpdateModelCard[\"Update Model Card\"] LogResults --> BestModelSelection[\"Best Model Selection\"] BestModelSelection --> LoadBestModel[\"load_best_model_at_end\"] SaveCSV --> TrackingFile[\"evaluation_results.csv\"] UpdateModelCard --> ModelCardData[\"model.model_card_data\"] ``` **Evaluation Integration in Training Pipeline** ### Model Selection Mechanism The training system uses the `primary_metric` and `greater_is_better` attributes to determine the best performing checkpoint: - Each evaluator defines a `primary_metric` (e.g., `\"ndcg@10\"`, `\"spearman_cosine\"`) - The `greater_is_better` boolean indicates optimization direction - Best model loading occurs when `load_best_model_at_end=True` in training arguments ### Configuration and Extensibility Common configuration options across evaluators: | Parameter | Purpose | Available In | |-----------|---------|--------------| | `truncate_dim` | Embedding dimension truncation | Most evaluators | | `batch_size` | Encoding batch size | All evaluators | | `show_progress_bar` | Progress display | Most evaluators | | `write_csv` | CSV result logging | All evaluators | | `name` | Evaluator identifier | All evaluators | Sources: [sentence_transformers/evaluation/SentenceEvaluator.py:71-75](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:272-290]() ## Advanced Features ### Multi-Function Evaluation Many evaluators support multiple similarity functions simultaneously, enabling comprehensive comparison: ```python",
  "document_embeddings = model.encode_document([ \"Climate change affects marine ecosystems...\", \"AI development began in the 1950s...\" ]) ``` **Sources:** [sentence_transformers/SentenceTransformer.py:416-543](), [sentence_transformers/SentenceTransformer.py:545-675](), [README.md:60-87]() ## SparseEncoder: Sparse Embeddings `SparseEncoder` models create sparse vector representations where most values are zero, enabling efficient neural lexical search and hybrid retrieval systems. ### Basic Usage ```python from sentence_transformers import SparseEncoder # Load a sparse encoder model model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") sentences = [ \"The weather is lovely today.\", \"It's so sunny outside!\", \"He drove to the stadium.\", ] # Generate sparse embeddings embeddings = model.encode(sentences) print(embeddings.shape) # (3, 30522) - vocabulary size dimensions # Calculate similarities (using dot product for sparse vectors) similarities = model.similarity(embeddings, embeddings) print(similarities) # tensor([[35.629, 9.154, 0.098], # [9.154, 27.478, 0.019], # [0.098, 0.019, 29.553]]) # Check sparsity statistics stats = SparseEncoder.sparsity(embeddings) print(f\"Sparsity: {stats['sparsity_ratio']:.2%}\") # Sparsity: 99.84% ``` ### Query and Document Encoding ```python",
  "Multimodal applications in sentence-transformers enable processing and encoding of multiple input modalities (text, images) within unified embedding spaces. The primary implementation uses CLIP (Contrastive Language-Image Pre-training) models through the `CLIPModel` class, which supports image-text similarity, cross-modal retrieval, and multimodal semantic search. The multimodal functionality integrates with the broader sentence-transformers ecosystem including multi-processing, evaluation frameworks, and deployment optimizations. For text-only dense embeddings, see [SentenceTransformer Models](#5.1). For sparse embeddings, see [SparseEncoder Models](#5.2). For retrieval applications, see [Semantic Search](#6.1). ## CLIPModel Architecture The `CLIPModel` class provides the core functionality for multimodal applications by implementing the CLIP architecture within the sentence-transformers framework. **CLIPModel Component Architecture** ```mermaid graph TB subgraph Input[\"Input Processing\"] IMG[\"PIL.Image objects\"] TXT[\"Text strings\"] end subgraph CLIPModel[\"CLIPModel Class\"] PROC[\"CLIPProcessor\"] VMODEL[\"model.vision_model\"] TMODEL[\"model.text_model\"] VPROJ[\"model.visual_projection\"] TPROJ[\"model.text_projection\"] end subgraph Processing[\"tokenize() Method\"] IMGPROC[\"image_processor\"] TXTPROC[\"tokenizer\"] INFO[\"image_text_info tracking\"] end subgraph Output[\"forward() Output\"] VEMB[\"Image embeddings\"] TEMB[\"Text embeddings\"] UNIFIED[\"sentence_embedding tensor\"] end IMG --> Processing TXT --> Processing Processing --> CLIPModel CLIPModel --> Output ``` The `CLIPModel` class inherits from `InputModule` and wraps `transformers.CLIPModel` and `transformers.CLIPProcessor` components. It implements the `tokenize()` and `forward()` methods required by the sentence-transformers module system. Sources: [sentence_transformers/models/CLIPModel.py:15-26](), [sentence_transformers/models/CLIPModel.py:70-92]() ## Input Processing and Tokenization The CLIP model handles mixed input types through its `tokenize` method, which can process both PIL Images and text strings in the same batch. ```mermaid graph LR subgraph InputBatch[\"Mixed Input Batch\"] IMG1[\"PIL.Image\"] TXT1[\"'A dog in snow'\"] IMG2[\"PIL.Image\"] TXT2[\"'A cat on table'\"] end subgraph Tokenization[\"tokenize() Method\"] CLASSIFY[\"Classify Input Types\"] IMGPROC[\"Image Processing\"] TXTPROC[\"Text Tokenization\"] MERGE[\"Merge Features\"] end subgraph Features[\"Feature Tensors\"] PIXELS[\"pixel_values\"] TOKENS[\"input_ids\"] MASK[\"attention_mask\"] INFO[\"image_text_info\"] end InputBatch --> CLASSIFY CLASSIFY --> IMGPROC CLASSIFY --> TXTPROC IMGPROC --> PIXELS TXTPROC --> TOKENS TXTPROC --> MASK CLASSIFY --> INFO MERGE --> Features ``` The `image_text_info` list tracks which inputs are images (0) versus text (1), enabling proper routing during the forward pass. Sources: [sentence_transformers/models/CLIPModel.py:70-92]() ## Forward Pass and Embedding Generation The forward method processes mixed inputs and generates embeddings in a unified vector space: | Processing Step | Image Inputs | Text Inputs | |----------------|--------------|-------------| | Feature Extraction | `vision_model(pixel_values)` | `text_model(input_ids, attention_mask)` | | Projection | `visual_projection()` | `text_projection()` | | Output Format | `image_embeds` | `text_embeds` | ```mermaid graph TB subgraph Forward[\"forward() Method\"] CHECK[\"Check Input Types\"] VISION[\"Process Images\"] TEXT[\"Process Text\"] REORDER[\"Reorder by image_text_info\"] end subgraph VisionPath[\"Vision Processing\"] VFORWARD[\"vision_model()\"] VPROJECTION[\"visual_projection()\"] end subgraph TextPath[\"Text Processing\"] TFORWARD[\"text_model()\"] TPROJECTION[\"text_projection()\"] end CHECK --> VISION CHECK --> TEXT VISION --> VisionPath TEXT --> TextPath VisionPath --> REORDER TextPath --> REORDER REORDER --> UNIFIED[\"sentence_embedding tensor\"] ``` The method maintains input order by using the `image_text_info` list to correctly sequence embeddings in the output tensor.",
  "Sources: [sentence_transformers/models/CLIPModel.py:38-68]() ## Usage Patterns ### Basic Image-Text Similarity ```python from sentence_transformers import SentenceTransformer from PIL import Image model = SentenceTransformer('clip-ViT-B-32') # Encode image image = Image.open('path/to/image.jpg') img_embedding = model.encode(image) # Encode text descriptions texts = [\"A dog in the snow\", \"A cat on a table\"] text_embeddings = model.encode(texts) # Compute similarities similarities = model.similarity(img_embedding, text_embeddings) ``` ### Mixed Batch Processing The CLIP model can process images and text in the same batch call: ```python",
  "- [Introduction](sentence-transformers/Introduction.md) - [Overview](sentence-transformers/Overview.md) - [Core Model Types](sentence-transformers/Core_Model_Types.md) - [Basic encoding](sentence-transformers/Basic_encoding.md) - [Task-specific encoding with prompts](sentence-transformers/Task-specific_encoding_with_prompts.md) - [Returns machine 0.85 learning 0.72 algorithm 0.45 ...](sentence-transformers/Returns_machine_0.85_learning_0.72_algorithm_0.45_....md) - [Reranking pipeline](sentence-transformers/Reranking_pipeline.md) - [Score all pairs](sentence-transformers/Score_all_pairs.md) - [Or rank documents directly](sentence-transformers/Or_rank_documents_directly.md) - [Created via Router.for query document](sentence-transformers/Created_via_Router.for_query_document.md) - [Or manual configuration](sentence-transformers/Or_manual_configuration.md) - [Training arguments must specify router mapping](sentence-transformers/Training_arguments_must_specify_router_mapping.md) - [Default installation](sentence-transformers/Default_installation.md) - [Training setup](sentence-transformers/Training_setup.md) - [Development setup](sentence-transformers/Development_setup.md) - [Test basic model loading](sentence-transformers/Test_basic_model_loading.md) - [Test encoding](sentence-transformers/Test_encoding.md) - [Quickstart Guide](sentence-transformers/Quickstart_Guide.md) - [Query-optimized encoding](sentence-transformers/Query-optimized_encoding.md) - [Document-optimized encoding](sentence-transformers/Document-optimized_encoding.md) - [Specialized encoding for retrieval](sentence-transformers/Specialized_encoding_for_retrieval.md) - [Automatically rank documents by relevance](sentence-transformers/Automatically_rank_documents_by_relevance.md) - [Query How many people live in Berlin](sentence-transformers/Query_How_many_people_live_in_Berlin.md) - [- 0 8.61 Berlin had a population of 3520031 registered inhabitants...](sentence-transformers/-_0_8.61_Berlin_had_a_population_of_3520031_registered_inhabitants....md) - [- 2 6.35 In 2013 around 600000 Berliners were registered...](sentence-transformers/-_2_6.35_In_2013_around_600000_Berliners_were_registered....md) - [- 1 5.51 Berlin has a yearly total of about 135 million day visitors...](sentence-transformers/-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md) - [Single loss](sentence-transformers/Single_loss.md) - [Multi-dataset losses](sentence-transformers/Multi-dataset_losses.md) - [Training arguments with router mapping](sentence-transformers/Training_arguments_with_router_mapping.md) - [SparseEncoder Training](sentence-transformers/SparseEncoder_Training.md) - [Training arguments for Router models](sentence-transformers/Training_arguments_for_Router_models.md) - [CrossEncoder Training](sentence-transformers/CrossEncoder_Training.md) - [Model with existing classification head](sentence-transformers/Model_with_existing_classification_head.md) - [Model without classification head will be added](sentence-transformers/Model_without_classification_head_will_be_added.md) - [CSR model training setup](sentence-transformers/CSR_model_training_setup.md) - [Loss Functions for CrossEncoder](sentence-transformers/Loss_Functions_for_CrossEncoder.md) - [Key computation from ListNetLoss.forward](sentence-transformers/Key_computation_from_ListNetLoss.forward.md) - [... populate matrices ...](sentence-transformers/..._populate_matrices_....md) - [Choose appropriate loss function](sentence-transformers/Choose_appropriate_loss_function.md) - [loss losses.ListNetLossmodel Alternative listwise approach](sentence-transformers/loss__losses.ListNetLossmodel___Alternative_listwise_approach.md) - [loss losses.PListMLELossmodel Position-aware MLE](sentence-transformers/loss__losses.PListMLELossmodel___Position-aware_MLE.md) - [Router module validation in trainer](sentence-transformers/Router_module_validation_in_trainer.md) - [Example Multiple similarity functions in one evaluation](sentence-transformers/Example_Multiple_similarity_functions_in_one_evaluation.md) - [SparseEncoder Evaluators](sentence-transformers/SparseEncoder_Evaluators.md) - [Example from SparseInformationRetrievalEvaluator](sentence-transformers/Example_from_SparseInformationRetrievalEvaluator.md) - [CrossEncoder Evaluators](sentence-transformers/CrossEncoder_Evaluators.md) - [Prepare samples with queries positives and negatives](sentence-transformers/Prepare_samples_with_queries_positives_and_negatives.md) - [NanoBEIR Evaluation](sentence-transformers/NanoBEIR_Evaluation.md) - [Basic usage with all datasets](sentence-transformers/Basic_usage_with_all_datasets.md) - [Subset of datasets with prompts](sentence-transformers/Subset_of_datasets_with_prompts.md) - [SparseEncoder - Sparse vector embeddings](sentence-transformers/SparseEncoder_-_Sparse_vector_embeddings.md) - [Check sparsity statistics](sentence-transformers/Check_sparsity_statistics.md) - [Load general purpose model](sentence-transformers/Load_general_purpose_model.md) - [Load with specific configuration](sentence-transformers/Load_with_specific_configuration.md) - [Model with prompt support](sentence-transformers/Model_with_prompt_support.md) - [Automatic prompt selection](sentence-transformers/Automatic_prompt_selection.md) - [Manual prompt specification](sentence-transformers/Manual_prompt_specification.md) - [SparseEncoder Models](sentence-transformers/SparseEncoder_Models.md) - [MSMARCO Models](sentence-transformers/MSMARCO_Models.md) - [Load cosine similarity optimized model](sentence-transformers/Load_cosine_similarity_optimized_model.md) - [Encode query and passage](sentence-transformers/Encode_query_and_passage.md) - [Calculate similarity choose based on model optimization](sentence-transformers/Calculate_similarity_choose_based_on_model_optimization.md) - [For large-scale retrieval](sentence-transformers/For_large-scale_retrieval.md) - [Efficient batch encoding](sentence-transformers/Efficient_batch_encoding.md) - [Compute similarity matrix](sentence-transformers/Compute_similarity_matrix.md) - [Semantic Search](sentence-transformers/Semantic_Search.md) - [Sparse Search Integration](sentence-transformers/Sparse_Search_Integration.md) - [Retrieve Rerank Architecture](sentence-transformers/Retrieve__Rerank_Architecture.md) - [Semantic Textual Similarity](sentence-transformers/Semantic_Textual_Similarity.md) - [Multiple similarity functions evaluated together](sentence-transformers/Multiple_similarity_functions_evaluated_together.md) - [Multimodal Applications](sentence-transformers/Multimodal_Applications.md) - [Mixed input batch](sentence-transformers/Mixed_input_batch.md) - [Through SentenceTransformer](sentence-transformers/Through_SentenceTransformer.md) - [Direct CLIPModel construction](sentence-transformers/Direct_CLIPModel_construction.md) - [Advanced Topics](sentence-transformers/Advanced_Topics.md) - [Example of automatic statistics generation](sentence-transformers/Example_of_automatic_statistics_generation.md) - [From SparseEncoderModelCardData.register model](sentence-transformers/From_SparseEncoderModelCardData.register_model.md) - [Multi-Processing and Optimization](sentence-transformers/Multi-Processing_and_Optimization.md) - [Testing and Development](sentence-transformers/Testing_and_Development.md) - [Example test structure from test pretrained stsb.py](sentence-transformers/Example_test_structure_from_test_pretrained_stsb.py.md) - [Run all tests excluding slow tests](sentence-transformers/Run_all_tests_excluding_slow_tests.md) - [Run specific test categories](sentence-transformers/Run_specific_test_categories.md) - [Run tests for specific model type](sentence-transformers/Run_tests_for_specific_model_type.md) - [Run with coverage](sentence-transformers/Run_with_coverage.md)",
  "similarity_fn_names = [\"cosine\", \"dot\", \"euclidean\", \"manhattan\"] ``` This generates metrics for each function, with optional `max_*` aggregated metrics for overall performance assessment. ### Prediction Output The `InformationRetrievalEvaluator` supports `write_predictions=True` to output retrieval results in JSONL format, enabling downstream analysis and fusion with other retrieval systems. ### Embedding Optimization Several evaluators support advanced embedding configurations: - **Precision Control**: `precision` parameter for quantized embeddings (`\"int8\"`, `\"binary\"`, etc.) - **Dimension Truncation**: `truncate_dim` for reduced-dimension evaluation - **Normalization**: Automatic normalization for certain precision modes Sources: [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:171-176](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:374-396]() # SentenceTransformer Evaluators This document covers the evaluation system for `SentenceTransformer` models, including the base evaluator architecture and all available evaluator implementations. These evaluators are used during training to assess model performance on various downstream tasks, enabling automatic model selection and performance monitoring. For evaluation of sparse encoder models, see [SparseEncoder Evaluators](#4.2). For cross-encoder evaluation, see [CrossEncoder Evaluators](#4.3). For comprehensive benchmark evaluation, see [NanoBEIR Evaluation](#4.4). ## Evaluator Architecture All SentenceTransformer evaluators inherit from the `SentenceEvaluator` base class, which provides a standardized interface for evaluation during training and inference. ### Base Evaluator Structure ```mermaid classDiagram class SentenceEvaluator { +bool greater_is_better +str primary_metric +__call__(model, output_path, epoch, steps) +prefix_name_to_metrics(metrics, name) +store_metrics_in_model_card_data(model, metrics, epoch, steps) +embed_inputs(model, sentences) +get_config_dict() +description : str } class InformationRetrievalEvaluator { +dict queries +dict corpus +dict relevant_docs +compute_metrices(model) +compute_metrics(queries_result_list) } class EmbeddingSimilarityEvaluator { +list sentences1 +list sentences2 +list scores +compute_metrices(model) } class BinaryClassificationEvaluator { +list sentences1 +list sentences2 +list labels +find_best_acc_and_threshold() +find_best_f1_and_threshold() } class RerankingEvaluator { +list samples +int at_k +compute_metrices_batched(model) +compute_metrices_individual(model) } class TripletEvaluator { +list anchors +list positives +list negatives +dict margin } class ParaphraseMiningEvaluator { +dict sentences_map +dict duplicates +add_transitive_closure() } SentenceEvaluator <|-- InformationRetrievalEvaluator SentenceEvaluator <|-- EmbeddingSimilarityEvaluator SentenceEvaluator <|-- BinaryClassificationEvaluator SentenceEvaluator <|-- RerankingEvaluator SentenceEvaluator <|-- TripletEvaluator SentenceEvaluator <|-- ParaphraseMiningEvaluator ``` **Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:13-121]() ### Key Base Class Features The `SentenceEvaluator` base class provides several critical features: | Feature | Purpose | Key Methods | |---------|---------|-------------| | **Primary Metric** | Identifies the main metric for model selection | `primary_metric` attribute | | **Metric Direction** | Indicates if higher scores are better | `greater_is_better` attribute | | **Metric Prefixing** | Adds evaluator names to metric keys | `prefix_name_to_metrics()` | | **Model Card Integration** | Stores evaluation results in model metadata | `store_metrics_in_model_card_data()` | | **Embedding Interface** | Standardized text encoding | `embed_inputs()` | **Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:26-121]() ## Core Evaluator Types ### Information Retrieval Evaluator The `InformationRetrievalEvaluator` is designed for search and retrieval tasks, computing standard IR metrics across large corpora.",
  "```mermaid graph TB subgraph \"InformationRetrievalEvaluator\" Queries[\"queries: Dict[str, str]\"] Corpus[\"corpus: Dict[str, str]\"] RelevantDocs[\"relevant_docs: Dict[str, Set[str]]\"] subgraph \"Metrics Computed\" MRR[\"MRR@k (Mean Reciprocal Rank)\"] NDCG[\"NDCG@k (Normalized DCG)\"] MAP[\"MAP@k (Mean Average Precision)\"] Accuracy[\"Accuracy@k\"] PrecisionRecall[\"Precision@k / Recall@k\"] end subgraph \"Configuration\" ChunkSize[\"corpus_chunk_size: 50000\"] BatchSize[\"batch_size: 32\"] ScoreFunctions[\"score_functions: Dict\"] Prompts[\"query_prompt / corpus_prompt\"] end end Queries --> MRR Corpus --> MRR RelevantDocs --> MRR ChunkSize --> MRR ``` Key features: - **Chunked Processing**: Handles large corpora via `corpus_chunk_size` parameter [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:130]() - **Multiple Score Functions**: Supports different similarity functions via `score_functions` [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:141]() - **Asymmetric Encoding**: Different prompts for queries vs documents [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:143-146]() - **Prediction Export**: Optional JSONL output for downstream analysis [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:147]() **Sources:** [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-568]() ### Embedding Similarity Evaluator The `EmbeddingSimilarityEvaluator` measures correlation between predicted and ground-truth similarity scores. ```mermaid graph LR subgraph \"Input Data\" S1[\"sentences1: List[str]\"] S2[\"sentences2: List[str]\"] GoldScores[\"scores: List[float]\"] end subgraph \"Similarity Functions\" Cosine[\"cosine: pairwise_cos_sim\"] Dot[\"dot: pairwise_dot_score\"] Euclidean[\"euclidean: pairwise_euclidean_sim\"] Manhattan[\"manhattan: pairwise_manhattan_sim\"] end subgraph \"Correlation Metrics\" Pearson[\"Pearson Correlation\"] Spearman[\"Spearman Correlation\"] end S1 --> Cosine S2 --> Cosine Cosine --> Pearson Cosine --> Spearman GoldScores --> Pearson GoldScores --> Spearman ``` Key features: - **Multiple Similarity Functions**: Supports cosine, dot product, Euclidean, and Manhattan distance [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:184-189]() - **Precision Support**: Handles quantized embeddings (int8, uint8, binary, ubinary) [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:93]() - **Automatic Deduplication**: Avoids re-encoding identical sentences [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:225-237]() **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:27-272]() ### Binary Classification Evaluator The `BinaryClassificationEvaluator` treats similarity as a binary classification problem. ```mermaid graph TD subgraph \"Input Processing\" Pairs[\"sentence pairs + binary labels\"] Embeddings[\"encode sentence pairs\"] Similarities[\"compute pairwise similarities\"] end subgraph \"Threshold Optimization\" AccThreshold[\"find_best_acc_and_threshold()\"] F1Threshold[\"find_best_f1_and_threshold()\"] end subgraph \"Metrics Output\" Accuracy[\"Accuracy + optimal threshold\"] F1Score[\"F1, Precision, Recall + threshold\"] AP[\"Average Precision\"] MCC[\"Matthews Correlation Coefficient\"] end Pairs --> Embeddings Embeddings --> Similarities Similarities --> AccThreshold Similarities --> F1Threshold AccThreshold --> Accuracy F1Threshold --> F1Score Similarities --> AP F1Threshold --> MCC ``` **Sources:** [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379]() ### Reranking Evaluator The `RerankingEvaluator` evaluates models on reranking tasks with query-document relevance.",
  "```mermaid graph TB subgraph \"Sample Structure\" Query[\"query: str\"] Positive[\"positive: List[str]\"] Negative[\"negative: List[str]\"] end subgraph \"Processing Modes\" Batched[\"compute_metrices_batched()\"] Individual[\"compute_metrices_individual()\"] end subgraph \"Ranking Metrics\" MAP[\"Mean Average Precision\"] MRR[\"MRR@k\"] NDCG[\"NDCG@k\"] end Query --> Batched Positive --> Batched Negative --> Batched Batched --> MAP Batched --> MRR Batched --> NDCG ``` Key features: - **Flexible Processing**: Choice between batched and individual encoding [sentence_transformers/evaluation/RerankingEvaluator.py:98]() - **Memory Optimization**: Batched mode for speed, individual mode for memory efficiency [sentence_transformers/evaluation/RerankingEvaluator.py:210-214]() **Sources:** [sentence_transformers/evaluation/RerankingEvaluator.py:25-372]() ## Specialized Evaluators ### Triplet Evaluator Evaluates triplet ranking performance where positive examples should be closer than negative examples. ```mermaid graph LR subgraph \"Triplet Components\" Anchors[\"anchors: List[str]\"] Positives[\"positives: List[str]\"] Negatives[\"negatives: List[str]\"] end subgraph \"Margin Configuration\" MarginDict[\"margin: Dict[str, float]\"] DefaultMargin[\"default: 0.0 for all metrics\"] end subgraph \"Evaluation Logic\" Condition[\"similarity(anchor, positive) > similarity(anchor, negative) + margin\"] Accuracy[\"accuracy per similarity function\"] end Anchors --> Condition Positives --> Condition Negatives --> Condition MarginDict --> Condition Condition --> Accuracy ``` **Sources:** [sentence_transformers/evaluation/TripletEvaluator.py:26-271]() ### Paraphrase Mining Evaluator Evaluates paraphrase detection performance by mining similar sentences from a corpus. Key features: - **Transitive Closure**: Optional transitive relationship enforcement [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:97]() - **Scalable Mining**: Uses `paraphrase_mining` utility for efficient processing [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:172-182]() - **F1 Optimization**: Finds optimal similarity threshold for paraphrase detection [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:187-212]() **Sources:** [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:18-279]() ### Translation Evaluator Measures cross-lingual alignment by checking if translations have highest mutual similarity. **Sources:** [sentence_transformers/evaluation/TranslationEvaluator.py:22-192]() ### Knowledge Distillation Evaluators Two evaluators support knowledge distillation scenarios: - **MSEEvaluator**: Computes MSE between teacher and student embeddings [sentence_transformers/evaluation/MSEEvaluator.py:18-158]() - **MSEEvaluatorFromDataFrame**: Structured multilingual distillation evaluation [sentence_transformers/evaluation/MSEEvaluatorFromDataFrame.py:20-139]() ## Common Usage Patterns ### Evaluation During Training All evaluators follow the same calling convention for integration with training loops: ```python # Standard evaluator call signature results = evaluator( model=sentence_transformer_model, output_path=\"./evaluation_results\", epoch=current_epoch, steps=current_step ) ``` ### Metric Organization ```mermaid graph TD subgraph \"Metric Processing Pipeline\" RawMetrics[\"evaluator.compute_metrices()\"] PrefixedMetrics[\"evaluator.prefix_name_to_metrics()\"] ModelCard[\"evaluator.store_metrics_in_model_card_data()\"] CSVOutput[\"CSV file output (optional)\"] end subgraph \"Metric Structure\" PrimaryMetric[\"evaluator.primary_metric\"] AllMetrics[\"Dict[str, float] return value\"] GreaterIsBetter[\"evaluator.greater_is_better\"] end RawMetrics --> PrefixedMetrics PrefixedMetrics --> ModelCard PrefixedMetrics --> CSVOutput PrefixedMetrics --> AllMetrics PrimaryMetric --> AllMetrics ``` **Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:57-75]() ### Configuration Management Each evaluator provides configuration introspection via `get_config_dict()`:",
  "| Evaluator | Key Configuration Parameters | |-----------|------------------------------| | `InformationRetrievalEvaluator` | `truncate_dim`, `query_prompt`, `corpus_prompt` | | `EmbeddingSimilarityEvaluator` | `truncate_dim`, `precision` | | `BinaryClassificationEvaluator` | `truncate_dim` | | `RerankingEvaluator` | `at_k`, `truncate_dim` | | `TripletEvaluator` | `margin`, `truncate_dim` | **Sources:** Multiple evaluator `get_config_dict()` methods across evaluation files ## Integration with Training Evaluators integrate seamlessly with the SentenceTransformer training system: ```mermaid sequenceDiagram participant Trainer as \"SentenceTransformerTrainer\" participant Evaluator as \"SentenceEvaluator\" participant Model as \"SentenceTransformer\" participant ModelCard as \"ModelCardData\" Trainer->>Evaluator: __call__(model, output_path, epoch, steps) Evaluator->>Model: encode(sentences, **kwargs) Model-->>Evaluator: embeddings Evaluator->>Evaluator: compute_metrices() Evaluator->>Evaluator: prefix_name_to_metrics() Evaluator->>ModelCard: store_metrics_in_model_card_data() Evaluator-->>Trainer: metrics dict with primary_metric ``` The training system uses `evaluator.primary_metric` and `evaluator.greater_is_better` for: - **Model Selection**: Choosing best checkpoint when `load_best_model_at_end=True` - **Early Stopping**: Monitoring convergence based on evaluation metrics - **Logging**: Tracking primary metrics across training runs **Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:26-75]()",
  "all_modules = [module.__class__ for module in model.modules()] if Router in all_modules or Asym in all_modules: model_type += [\"Asymmetric\"] if SparseStaticEmbedding in all_modules: model_type += [\"Inference-free\"] ``` Sources: [sentence_transformers/sparse_encoder/model_card.py:87-111]() ## Integration with Training ### Trainer Integration Model card callbacks are automatically added during trainer initialization: **Callback Registration**: The `SentenceTransformerModelCardCallback` is automatically integrated during trainer initialization and responds to training lifecycle events. **Hyperparameter Filtering** ([sentence_transformers/model_card.py:97-129]()): The system tracks only meaningful hyperparameters, filtering out logging and infrastructure settings like `output_dir`, `logging_dir`, `eval_steps`, etc. ### Carbon Emissions Tracking Integration with CodeCarbon for environmental impact measurement: **Automatic Detection** ([sentence_transformers/model_card.py:63-68]()): ```python callbacks = [callback for callback in trainer.callback_handler.callbacks if isinstance(callback, CodeCarbonCallback)] if callbacks: model.model_card_data.code_carbon_callback = callbacks[0] ``` Sources: [sentence_transformers/trainer.py:315-333](), [sentence_transformers/model_card.py:47-192]() ## Model Card Generation Workflow ### Version and Citation Management The system automatically manages framework versions and academic citations: **Version Tracking** ([sentence_transformers/model_card.py:217-236]()): - Python, sentence-transformers, transformers, PyTorch versions - Optional accelerate, datasets, tokenizers versions **Citation Generation** ([sentence_transformers/model_card.py:411-440]()): - Loss function citations from `loss.citation` attributes - Automatic deduplication of identical citations - BibTeX formatting for academic references ### Final Model Card Assembly Model cards are generated through the `generate_model_card()` function which: 1. Compiles all tracked metadata into template variables 2. Renders the Jinja2 template with collected data 3. Generates usage code snippets based on model type 4. Creates evaluation tables from stored metrics 5. Produces the final README.md file Sources: [sentence_transformers/model_card.py:217-263](), [sentence_transformers/model_card.py:411-441]()",
  "emb = model.encode(\"Text to encode\", prompt=\"Represent this text: \") ``` **Performance Optimization**: - `normalize_embeddings=True`: Enable dot-product similarity - `convert_to_tensor=False`: Return numpy arrays for storage - `precision=\"int8\"`: Quantized embeddings for memory efficiency - `batch_size=64`: Adjust for your hardware Sources: [sentence_transformers/SentenceTransformer.py:309-386](), [sentence_transformers/SentenceTransformer.py:424-432]() ## Model Discovery and Metadata The interactive model browser provides comprehensive model information: ```mermaid graph TB subgraph \"Model Browser Interface\" HTML[\"models_en_sentence_embeddings.html\"] --> Filters[\"Performance Filters\"] HTML --> Sort[\"Sortable Columns\"] HTML --> Details[\"Expandable Details\"] end subgraph \"Model Metadata\" Name[\"Model Name\"] --> HFHub[\"Hugging Face Hub\"] Perf[\"Performance Metrics\"] --> Benchmarks[\"14 Sentence Tasks<br/>6 Search Tasks\"] Speed[\"Encoding Speed\"] --> Hardware[\"V100 GPU Benchmarks\"] Size[\"Model Size\"] --> Storage[\"MB Requirements\"] end subgraph \"Selection Criteria\" Task[\"Task Requirements\"] --> Filter1[\"Performance Threshold\"] Hardware[\"Hardware Constraints\"] --> Filter2[\"Speed/Size Limits\"] Quality[\"Quality Needs\"] --> Filter3[\"Metric Requirements\"] end ``` The browser enables filtering by performance, speed, and size to find optimal models for specific requirements. Sources: [docs/_static/html/models_en_sentence_embeddings.html:106-228](), [docs/sentence_transformer/pretrained_models.md:41-49]()",
  "This document covers the evaluation framework specifically designed for sparse encoder models in the sentence-transformers library. These evaluators extend the standard dense evaluators to handle sparse embeddings and provide additional sparsity-related metrics. For general evaluation concepts and dense model evaluators, see [SentenceTransformer Evaluators](#4.1). For information about sparse encoder models themselves, see [SparseEncoder Training](#3.2). ## Architecture Overview SparseEncoder evaluators follow an inheritance-based architecture where each sparse evaluator extends its corresponding dense evaluator while adding sparse-specific functionality. ### Class Hierarchy ```mermaid graph TD subgraph \"Dense Evaluators\" IRE[\"InformationRetrievalEvaluator\"] NBE[\"NanoBEIREvaluator\"] ESE[\"EmbeddingSimilarityEvaluator\"] RE[\"RerankingEvaluator\"] BCE[\"BinaryClassificationEvaluator\"] MSE[\"MSEEvaluator\"] TE[\"TripletEvaluator\"] TransE[\"TranslationEvaluator\"] end subgraph \"Sparse Evaluators\" SIRE[\"SparseInformationRetrievalEvaluator\"] SNBE[\"SparseNanoBEIREvaluator\"] SESE[\"SparseEmbeddingSimilarityEvaluator\"] SRE[\"SparseRerankingEvaluator\"] SBCE[\"SparseBinaryClassificationEvaluator\"] SMSE[\"SparseMSEEvaluator\"] STE[\"SparseTripletEvaluator\"] STrE[\"SparseTranslationEvaluator\"] end IRE --> SIRE NBE --> SNBE ESE --> SESE RE --> SRE BCE --> SBCE MSE --> SMSE TE --> STE TransE --> STrE subgraph \"Sparse Model\" SE[\"SparseEncoder\"] end SIRE --> SE SNBE --> SE SESE --> SE SRE --> SE SBCE --> SE SMSE --> SE STE --> SE STrE --> SE ``` Sources: [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:23-281](), [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:26-263](), [sentence_transformers/sparse_encoder/evaluation/SparseEmbeddingSimilarityEvaluator.py:22-169]() ## Core Functionality Differences ### Sparse Embedding Generation All sparse evaluators override the `embed_inputs` method to generate sparse embeddings instead of dense ones: | Component | Dense Evaluators | Sparse Evaluators | |-----------|------------------|-------------------| | Embedding Format | Dense tensors | Sparse tensors via `convert_to_sparse_tensor=True` | | Memory Management | Standard tensor operations | `save_to_cpu=True` for memory efficiency | | Dimension Control | `truncate_dim` for reducing dimensions | `max_active_dims` for sparsity control | | Similarity Functions | Cosine, dot product, euclidean, manhattan | Same functions but optimized for sparse tensors | ### Sparsity Statistics Tracking Each sparse evaluator tracks and reports sparsity statistics: ```mermaid graph LR subgraph \"Embedding Process\" Input[\"Text Input\"] SE[\"SparseEncoder.encode()\"] SparseTensor[\"Sparse Tensor\"] Stats[\"model.sparsity()\"] end subgraph \"Tracked Metrics\" ActiveDims[\"active_dims\"] SparsityRatio[\"sparsity_ratio\"] end Input --> SE SE --> SparseTensor SparseTensor --> Stats Stats --> ActiveDims Stats --> SparsityRatio subgraph \"Output\" CSVHeaders[\"CSV Headers\"] LogOutput[\"Logger Output\"] Metrics[\"Return Metrics\"] end ActiveDims --> CSVHeaders SparsityRatio --> CSVHeaders ActiveDims --> LogOutput SparsityRatio --> LogOutput ActiveDims --> Metrics SparsityRatio --> Metrics ``` Sources: [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:263-269](), [sentence_transformers/sparse_encoder/evaluation/SparseEmbeddingSimilarityEvaluator.py:154-157]() ## Sparsity Statistics ### Active Dimensions and Sparsity Ratio All sparse evaluators compute two key sparsity metrics: - **Active Dimensions**: Average number of non-zero dimensions per embedding - **Sparsity Ratio**: Proportion of zero values in the embedding (closer to 1.0 means more sparse) These statistics are computed using [sentence_transformers/sparse_encoder/SparseEncoder.py]() model's `sparsity()` method and are: - Added to CSV output headers via `_append_csv_headers()` - Logged to console during evaluation - Included in returned metrics dictionary - Stored in model card data ### Context-Specific Statistics Some evaluators track sparsity statistics for different contexts:",
  "| Evaluator | Sparsity Context | Statistics Tracked | |-----------|------------------|-------------------| | `SparseInformationRetrievalEvaluator` | Query vs Corpus embeddings | `query_active_dims`, `query_sparsity_ratio`, `corpus_active_dims`, `corpus_sparsity_ratio` | | `SparseRerankingEvaluator` | Query vs Document embeddings | Same as above | | `SparseTripletEvaluator` | Anchor, Positive, Negative | `anchor_*`, `positive_*`, `negative_*` for each statistic | | Other evaluators | Single context | `active_dims`, `sparsity_ratio` | Sources: [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:190-194](), [sentence_transformers/sparse_encoder/evaluation/SparseTripletEvaluator.py:129-133]() ## Individual Evaluators ### SparseInformationRetrievalEvaluator Extends `InformationRetrievalEvaluator` for information retrieval tasks with sparse embeddings. **Key Features:** - Computes standard IR metrics (MRR, NDCG, MAP, Precision, Recall) - Tracks separate sparsity statistics for queries and corpus - Supports `max_active_dims` parameter for controlling sparsity during evaluation - Handles corpus chunking for memory efficiency **Usage Pattern:** ```python from sentence_transformers.sparse_encoder.evaluation import SparseInformationRetrievalEvaluator evaluator = SparseInformationRetrievalEvaluator( queries=query_dict, corpus=corpus_dict, relevant_docs=qrels_dict, max_active_dims=1000, batch_size=16 ) ``` Sources: [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:23-281]() ### SparseNanoBEIREvaluator Extends `NanoBEIREvaluator` to evaluate sparse models on multiple NanoBEIR datasets simultaneously. **Key Features:** - Evaluates across 13 NanoBEIR datasets - Aggregates sparsity statistics across all datasets using weighted averages - Supports dataset-specific prompts via `query_prompts` and `corpus_prompts` - Uses `SparseInformationRetrievalEvaluator` as the underlying evaluator class **Aggregation Logic:** - Query statistics: Simple average across datasets - Corpus statistics: Weighted average by corpus size - Final metrics: Aggregated using `aggregate_fn` (default: `np.mean`) Sources: [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:26-263]() ### SparseEmbeddingSimilarityEvaluator Extends `EmbeddingSimilarityEvaluator` for semantic similarity tasks using sparse embeddings. **Key Features:** - Computes Pearson and Spearman correlations - Single sparsity context (combined statistics for both sentence sets) - Supports multiple similarity functions (cosine, dot, euclidean, manhattan) - Memory-efficient sparse tensor operations Sources: [sentence_transformers/sparse_encoder/evaluation/SparseEmbeddingSimilarityEvaluator.py:22-169]() ### Other Sparse Evaluators | Evaluator | Purpose | Key Difference | |-----------|---------|----------------| | `SparseRerankingEvaluator` | Re-ranking evaluation | Tracks query/corpus sparsity separately | | `SparseBinaryClassificationEvaluator` | Binary classification | Single sparsity context | | `SparseMSEEvaluator` | Knowledge distillation | Handles sparse-to-sparse MSE computation | | `SparseTripletEvaluator` | Triplet accuracy | Tracks sparsity for anchor/positive/negative separately | | `SparseTranslationEvaluator` | Translation matching | Single sparsity context for both languages | ## Common Usage Patterns ### Memory Management Sparse evaluators include several memory optimization strategies: ```python # Common parameters across sparse evaluators embeddings = model.encode( sentences, convert_to_sparse_tensor=True, # Generate sparse tensors save_to_cpu=True, # Move to CPU to free GPU memory max_active_dims=max_active_dims # Control sparsity level ) ``` ### CSV Output Enhancement All sparse evaluators extend CSV headers to include sparsity statistics: ```python",
  "def pretrained_model_score(model_name, expected_score: float, max_test_samples: int = 100): model = SentenceTransformer(model_name, cache_folder=cache_dir) # Download and load STS benchmark data # Evaluate model performance # Assert performance meets threshold ``` **Key Test Parameters:** - `max_test_samples`: Limits test data size (100 for fast tests, -1 for complete evaluation) - `expected_score`: Minimum performance threshold for each model - Similarity functions tested: cosine, euclidean, manhattan, dot product Sources: [tests/test_pretrained_stsb.py:18-49]() ### Training Validation Tests The `test_train_stsb.py` module validates training functionality across different scenarios: ```mermaid graph TD subgraph \"Training Test Types\" STSBTrain[\"STSB Training Tests\"] NLITrain[\"NLI Training Tests\"] SlowTrain[\"Slow Training Tests\"] FastTrain[\"Fast Training Tests\"] end subgraph \"Test Conditions\" FullDataset[\"Full Dataset (slow)\"] SubsetDataset[\"Subset Dataset (fast)\"] CI_Skip[\"CI Environment Skip\"] end STSBTrain --> FullDataset STSBTrain --> SubsetDataset NLITrain --> FullDataset NLITrain --> SubsetDataset SlowTrain --> CI_Skip FastTrain --> SubsetDataset ``` Sources: [tests/test_train_stsb.py:82-187]() ### Test Execution Control Tests use pytest markers to control execution: - `@pytest.mark.slow`: Comprehensive tests with full datasets - `@pytest.mark.skipif(\"CI\" in os.environ)`: Skip resource-intensive tests in CI - `@pytest.mark.parametrize`: Run same test with multiple parameter sets ## Running Tests ### Local Development ```bash",
  "from sentence_transformers.models import CLIPModel clip_model = CLIPModel( model_name='openai/clip-vit-base-patch32', processor_name='openai/clip-vit-base-patch32' ) ``` The `CLIPModel.load()` class method handles model restoration from disk, ensuring both the `transformers.CLIPModel` and `transformers.CLIPProcessor` components are properly loaded. Sources: [sentence_transformers/models/CLIPModel.py:18-25](), [sentence_transformers/models/CLIPModel.py:102-121]() ## Serialization and Persistence The `CLIPModel` implements custom save/load methods to properly handle both the model and processor components: ```mermaid graph LR subgraph Save[\"save() Method\"] SAVEMODEL[\"model.save_pretrained()\"] SAVEPROC[\"processor.save_pretrained()\"] end subgraph Load[\"load() Method\"] LOADPATH[\"load_dir_path()\"] CONSTRUCT[\"CLIPModel(local_path)\"] end Save --> DISK[\"Disk Storage\"] DISK --> Load ``` This ensures that both the vision and text processing components are preserved during model serialization. Sources: [sentence_transformers/models/CLIPModel.py:98-121]()",
  "args = SentenceTransformerTrainingArguments( router_mapping={ \"question\": \"query\", \"positive\": \"document\", \"negative\": \"document\" } ) ``` ### Data Collator Integration The data collator uses `router_mapping` to pass task information to the `Router.tokenize()` method: Sources: [sentence_transformers/data_collator.py:55-68](), [sentence_transformers/data_collator.py:92-94](), [sentence_transformers/models/Router.py:287-324]() ### Validation The trainer validates that models using `Router` modules have proper `router_mapping` configured: Sources: [sentence_transformers/trainer.py:206-212]() ## Memory-Efficient Training Features The training system includes several memory optimization features: | Feature | Purpose | Implementation | |---------|---------|----------------| | Gradient Caching | Enables larger effective batch sizes | `CachedMultipleNegativesRankingLoss` | | Multi-Dataset Batching | Efficient sampling across datasets | `MultiDatasetBatchSampler` classes | | Loss Component Tracking | Monitors complex loss breakdowns | `track_loss_components()` | | Model Card Generation | Automatic documentation | `SentenceTransformerModelCardCallback` | Sources: [sentence_transformers/trainer.py:443-462](), [sentence_transformers/trainer.py:327-345]() ## Next Steps For detailed training guides specific to each model type: - [SentenceTransformer Training](#3.1) - Dense embedding model training - [SparseEncoder Training](#3.2) - Sparse embedding model training - [CrossEncoder Training](#3.3) - Cross-encoder reranking model training - [Memory-Efficient Training](#3.7) - Advanced memory optimization techniques # SentenceTransformer Training This document covers the comprehensive training system for SentenceTransformer models, including the trainer architecture, data processing pipeline, loss functions, and evaluation mechanisms. It focuses on the `SentenceTransformerTrainer` class and its supporting infrastructure for training dense embedding models. For information about training sparse encoder models, see [SparseEncoder Training](#3.2). For training cross-encoder models, see [CrossEncoder Training](#3.3). For detailed information about available loss functions, see [Loss Functions for SentenceTransformer](#3.4). ## Training System Architecture The SentenceTransformer training system is built around the `SentenceTransformerTrainer` class, which extends the Hugging Face Transformers `Trainer` with specialized functionality for embedding model training. ```mermaid graph TB subgraph \"Training Infrastructure\" ST[\"SentenceTransformerTrainer\"] STA[\"SentenceTransformerTrainingArguments\"] DC[\"SentenceTransformerDataCollator\"] MC[\"SentenceTransformerModelCardCallback\"] end subgraph \"Model Components\" Model[\"SentenceTransformer\"] Router[\"Router Module\"] Transformer[\"Transformer\"] Pooling[\"Pooling\"] end subgraph \"Data Processing\" Dataset[\"Dataset/DatasetDict\"] Prompts[\"Prompts System\"] RouterMapping[\"Router Mapping\"] BatchSampler[\"Batch Samplers\"] end subgraph \"Loss & Evaluation\" Loss[\"Loss Functions\"] Evaluator[\"SentenceEvaluator\"] SequentialEvaluator[\"SequentialEvaluator\"] end ST --> Model ST --> STA ST --> DC ST --> MC ST --> Dataset ST --> Loss ST --> Evaluator STA --> Prompts STA --> RouterMapping STA --> BatchSampler DC --> Prompts DC --> RouterMapping Model --> Router Model --> Transformer Model --> Pooling Evaluator --> SequentialEvaluator ``` **Sources:** [sentence_transformers/trainer.py:59-127](), [sentence_transformers/training_args.py](), [sentence_transformers/data_collator.py:13-23]() ## Core Training Flow The training process follows a structured pipeline from data input to model optimization:",
  "```mermaid graph TD Input[\"Input Dataset\"] --> Validate[\"validate_column_names()\"] Validate --> Preprocess[\"preprocess_dataset()\"] Preprocess --> DC[\"SentenceTransformerDataCollator\"] subgraph \"Data Collation\" DC --> ExtractLabels[\"Extract Labels\"] DC --> ApplyPrompts[\"Apply Prompts\"] DC --> Tokenize[\"tokenize_fn()\"] DC --> RouterMap[\"Apply Router Mapping\"] end Tokenize --> Features[\"Tokenized Features\"] Features --> ComputeLoss[\"compute_loss()\"] subgraph \"Loss Computation\" ComputeLoss --> CollectFeatures[\"collect_features()\"] CollectFeatures --> LossForward[\"loss.forward()\"] LossForward --> TrackComponents[\"track_loss_components()\"] end TrackComponents --> Optimizer[\"Optimizer Step\"] Optimizer --> Evaluate[\"evaluation_loop()\"] subgraph \"Evaluation\" Evaluate --> EvalDataset[\"Eval Dataset Loss\"] Evaluate --> Evaluator[\"evaluator()\"] Evaluator --> Metrics[\"Evaluation Metrics\"] end Metrics --> Log[\"log()\"] Log --> SaveModel[\"Save Model/Checkpoint\"] ``` **Sources:** [sentence_transformers/trainer.py:391-441](), [sentence_transformers/trainer.py:531-592](), [sentence_transformers/data_collator.py:35-119]() ## SentenceTransformerTrainer The `SentenceTransformerTrainer` class is the central component that orchestrates the entire training process. It extends the Hugging Face `Trainer` with specialized functionality for embedding models. ### Key Features - **Multi-dataset training support** through `DatasetDict` - **Loss component tracking** for complex loss functions that return dictionaries - **Router module integration** for asymmetric training architectures - **Prompt system support** for instruction-based training - **Automatic model card generation** during training ### Initialization and Configuration The trainer accepts several key parameters: ```python # Key trainer initialization parameters from trainer.py:129-148 model: SentenceTransformer | None = None args: SentenceTransformerTrainingArguments | None = None train_dataset: Dataset | DatasetDict | IterableDataset | dict[str, Dataset] | None = None eval_dataset: Dataset | DatasetDict | IterableDataset | dict[str, Dataset] | None = None loss: nn.Module | dict[str, nn.Module] | Callable | dict[str, Callable] | None = None evaluator: SentenceEvaluator | list[SentenceEvaluator] | None = None ``` **Sources:** [sentence_transformers/trainer.py:129-148](), [sentence_transformers/trainer.py:291-310]() ### Loss Computation Pipeline The trainer implements a sophisticated loss computation system that supports both single and multi-dataset training: ```mermaid graph TD ComputeLoss[\"compute_loss()\"] --> ExtractDataset[\"Extract dataset_name\"] ExtractDataset --> CollectFeatures[\"collect_features()\"] CollectFeatures --> SelectLoss[\"Select Loss Function\"] subgraph \"Feature Collection\" CollectFeatures --> ParseColumns[\"Parse Input Columns\"] ParseColumns --> ExtractLabels[\"Extract Labels\"] ParseColumns --> GroupFeatures[\"Group by Prefix\"] end subgraph \"Loss Selection\" SelectLoss --> SingleLoss[\"Single Loss\"] SelectLoss --> DictLoss[\"Dictionary Loss\"] DictLoss --> DatasetMapping[\"Map dataset_name to loss\"] end GroupFeatures --> LossForward[\"loss.forward(features, labels)\"] DatasetMapping --> LossForward SingleLoss --> LossForward LossForward --> CheckDict[\"Loss is dict?\"] CheckDict -->|Yes| TrackComponents[\"track_loss_components()\"] CheckDict -->|No| ReturnLoss[\"Return Loss\"] TrackComponents --> SumComponents[\"Sum Loss Components\"] SumComponents --> ReturnLoss ``` **Sources:** [sentence_transformers/trainer.py:391-441](), [sentence_transformers/trainer.py:496-529](), [sentence_transformers/trainer.py:443-462]() ## Training Arguments The `SentenceTransformerTrainingArguments` class extends Hugging Face's `TrainingArguments` with additional parameters specific to embedding model training. ### Key SentenceTransformer-Specific Arguments - **`batch_sampler`**: Controls how batches are constructed (e.g., `NO_DUPLICATES`, `GROUP_BY_LABEL`) - **`multi_dataset_batch_sampler`**: Strategy for sampling from multiple datasets - **`prompts`**: System for adding prompts to input text - **`router_mapping`**: Maps dataset columns to Router module routes - **`learning_rate_mapping`**: Allows different learning rates for different model components **Sources:** [sentence_transformers/training_args.py](), [sentence_transformers/trainer.py:156-163]() ## Data Processing System ### SentenceTransformerDataCollator The data collator handles the conversion from raw dataset samples to tokenized model inputs:",
  "```mermaid graph TD DataCollator[\"SentenceTransformerDataCollator\"] --> ProcessFeatures[\"Process Features\"] subgraph \"Feature Processing\" ProcessFeatures --> ExtractDatasetName[\"Extract dataset_name\"] ProcessFeatures --> ExtractLabels[\"Extract Labels\"] ProcessFeatures --> ProcessColumns[\"Process Text Columns\"] end subgraph \"Column Processing\" ProcessColumns --> GetTask[\"Get Router Task\"] ProcessColumns --> GetPrompt[\"Get Column Prompt\"] ProcessColumns --> ApplyPrompt[\"Apply Prompt Prefix\"] ProcessColumns --> TokenizeColumn[\"tokenize_fn(texts, task)\"] end subgraph \"Prompt System\" GetPrompt --> SinglePrompt[\"Single String Prompt\"] GetPrompt --> DatasetPrompts[\"Dataset-specific Prompts\"] GetPrompt --> ColumnPrompts[\"Column-specific Prompts\"] end TokenizeColumn --> PrefixKeys[\"Prefix with column_name\"] PrefixKeys --> BatchOutput[\"Final Batch Dict\"] ``` **Sources:** [sentence_transformers/data_collator.py:35-119](), [sentence_transformers/data_collator.py:90-118]() ### Prompt System The training system supports a flexible prompting mechanism: - **Single prompt**: Applied to all columns and datasets - **Column-specific prompts**: Different prompts for different input columns - **Dataset-specific prompts**: Different prompts for different datasets in multi-dataset training - **Combined prompts**: Dataset and column-specific combinations **Sources:** [sentence_transformers/data_collator.py:69-89](), [sentence_transformers/data_collator.py:96-101]() ## Router Module Integration The Router module enables asymmetric training architectures where different input types (e.g., queries vs documents) are processed through different model paths. ### Router Training Requirements When using a Router module, specific training arguments are required: ```mermaid graph TD RouterModel[\"Model with Router\"] --> CheckMapping[\"Check router_mapping\"] CheckMapping -->|Missing| Error[\"ValueError: router_mapping required\"] CheckMapping -->|Present| ValidateMapping[\"Validate Mapping\"] subgraph \"Router Mapping\" ValidateMapping --> ColumnMapping[\"Column → Route Mapping\"] ColumnMapping --> QueryRoute[\"'query' route\"] ColumnMapping --> DocumentRoute[\"'document' route\"] end subgraph \"Data Collation\" ColumnMapping --> DataCollator[\"SentenceTransformerDataCollator\"] DataCollator --> ApplyRouting[\"Apply router_mapping\"] ApplyRouting --> TokenizeWithTask[\"tokenize_fn(texts, task)\"] end TokenizeWithTask --> RouterForward[\"router.forward(features, task)\"] ``` **Sources:** [sentence_transformers/trainer.py:206-212](), [sentence_transformers/models/Router.py:217-245](), [sentence_transformers/data_collator.py:92-94]() ### Router Configuration Example ```python # Router mapping example from training arguments router_mapping = { \"question\": \"query\", \"positive\": \"document\", \"negative\": \"document\" } ``` **Sources:** [sentence_transformers/models/Router.py:45-54](), [tests/models/test_router.py:432-433]() ## Evaluation System The training system supports evaluation through both dataset-based metrics and custom evaluators. ### Evaluation Pipeline ```mermaid graph TD EvaluationLoop[\"evaluation_loop()\"] --> EvalDataset[\"Eval Dataset Loss\"] EvaluationLoop --> CheckEvaluator[\"evaluator exists?\"] CheckEvaluator -->|No| ReturnMetrics[\"Return Dataset Metrics\"] CheckEvaluator -->|Yes| RunEvaluator[\"evaluator(model, output_path, epoch, steps)\"] subgraph \"Evaluator Execution\" RunEvaluator --> SingleEvaluator[\"Single Evaluator\"] RunEvaluator --> SequentialEvaluator[\"SequentialEvaluator\"] SequentialEvaluator --> MultipleEvaluators[\"Multiple Evaluators\"] end SingleEvaluator --> EvaluatorMetrics[\"Evaluator Metrics\"] MultipleEvaluators --> EvaluatorMetrics EvaluatorMetrics --> PrefixMetrics[\"Prefix with eval_\"] PrefixMetrics --> MergeMetrics[\"Merge with Dataset Metrics\"] MergeMetrics --> FinalMetrics[\"Final Evaluation Output\"] ``` **Sources:** [sentence_transformers/trainer.py:545-592](), [sentence_transformers/trainer.py:312-315]() ## Batch Sampling Strategies The training system provides several batch sampling strategies to optimize training performance: ### Available Batch Samplers - **`DefaultBatchSampler`**: Standard random sampling - **`NoDuplicatesBatchSampler`**: Ensures no duplicate samples in batch (useful for in-batch negatives) - **`GroupByLabelBatchSampler`**: Groups samples by label - **`ProportionalBatchSampler`**: Maintains dataset proportions in multi-dataset training - **`RoundRobinBatchSampler`**: Alternates between datasets",
  "**Sources:** [sentence_transformers/trainer.py:623-684](), [sentence_transformers/sampler.py]() ## Model Card Generation The training system automatically generates model cards that document the training process: ```mermaid graph TD TrainingStart[\"Training Initialization\"] --> AddCallback[\"add_model_card_callback()\"] AddCallback --> ModelCardCallback[\"SentenceTransformerModelCardCallback\"] subgraph \"Callback Lifecycle\" ModelCardCallback --> OnInitEnd[\"on_init_end()\"] ModelCardCallback --> OnTrainEnd[\"on_train_end()\"] ModelCardCallback --> OnEvaluate[\"on_evaluate()\"] end OnInitEnd --> TrackArgs[\"Track Training Arguments\"] OnTrainEnd --> TrackMetrics[\"Track Training Metrics\"] OnEvaluate --> TrackEvalMetrics[\"Track Evaluation Metrics\"] TrackArgs --> GenerateCard[\"Generate Model Card\"] TrackMetrics --> GenerateCard TrackEvalMetrics --> GenerateCard GenerateCard --> SaveCard[\"Save README.md\"] ``` **Sources:** [sentence_transformers/trainer.py:327-345](), [sentence_transformers/model_card.py]()",
  "## Purpose and Scope The sentence-transformers library is a comprehensive Python framework for accessing, using, and training state-of-the-art embedding and reranker models. It provides three core model types that serve different purposes in natural language processing pipelines: `SentenceTransformer` for dense embeddings, `SparseEncoder` for sparse embeddings, and `CrossEncoder` for pairwise scoring and reranking. This document covers the high-level architecture and core concepts of the sentence-transformers library. For specific usage instructions, see [Quickstart Guide](#2.1). For detailed training procedures, see [Training](#3). For performance optimization, see [Advanced Topics](#7). Sources: [README.md:15-21](), [sentence_transformers/__init__.py:27-34]() ## Core Architecture The sentence-transformers library is built around three fundamental model architectures that can be used independently or in combination for various NLP tasks: ```mermaid graph TB subgraph \"sentence_transformers Library\" ST[\"SentenceTransformer<br/>Dense Embeddings\"] SE[\"SparseEncoder<br/>Sparse Embeddings\"] CE[\"CrossEncoder<br/>Pairwise Scoring\"] end subgraph \"Core Functionality\" ST --> |\"encode()\"| DenseEmb[\"Dense Vector<br/>Embeddings\"] SE --> |\"encode_query()<br/>encode_document()\"| SparseEmb[\"Sparse Vector<br/>Embeddings\"] CE --> |\"predict()<br/>rank()\"| Scores[\"Relevance<br/>Scores\"] end subgraph \"Primary Applications\" DenseEmb --> SemanticSearch[\"Semantic Search\"] DenseEmb --> Clustering[\"Clustering\"] SparseEmb --> NeuralLexical[\"Neural Lexical<br/>Search\"] SparseEmb --> HybridRetrieval[\"Hybrid Retrieval\"] Scores --> Reranking[\"Reranking\"] Scores --> Classification[\"Text Classification\"] end subgraph \"Training Infrastructure\" STTrainer[\"SentenceTransformerTrainer\"] SETrainer[\"SparseEncoderTrainer\"] CETrainer[\"CrossEncoderTrainer\"] STTrainer --> ST SETrainer --> SE CETrainer --> CE end ``` Each model type has corresponding trainer classes and specialized loss functions optimized for their specific use cases. The library provides over 15,000 pretrained models available through Hugging Face Hub. Sources: [sentence_transformers/__init__.py:15-36](), [README.md:19](), [index.rst:12-15]() ## Model Types ### SentenceTransformer The `SentenceTransformer` class produces dense vector embeddings where semantically similar texts have similar vector representations. These models use bi-encoder architectures that independently encode each input text. **Key characteristics:** - Output: Dense vectors (typically 384-1024 dimensions) - Use case: Semantic similarity, clustering, dense retrieval - Similarity functions: Cosine similarity, dot product, Euclidean distance - Example models: `all-MiniLM-L6-v2`, `all-mpnet-base-v2` **Basic usage pattern:** ```python from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = model.encode(sentences) similarities = model.similarity(embeddings, embeddings) ``` Sources: [README.md:56-87](), [sentence_transformers/__init__.py:27]() ### SparseEncoder The `SparseEncoder` class generates sparse vector embeddings where most dimensions are zero, creating interpretable representations that combine neural and lexical matching signals. **Key characteristics:** - Output: Sparse vectors (vocabulary-size dimensions, ~99% zeros) - Use case: Neural lexical search, hybrid retrieval, interpretability - Similarity functions: Dot product on sparse representations - Example models: `naver/splade-cocondenser-ensembledistil` **Basic usage pattern:** ```python from sentence_transformers import SparseEncoder model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") embeddings = model.encode(sentences) stats = SparseEncoder.sparsity(embeddings) ``` Sources: [README.md:133-167](), [sentence_transformers/__init__.py:29-34]() ### CrossEncoder The `CrossEncoder` class performs joint encoding of text pairs to produce similarity scores, making it ideal for reranking and classification tasks where high precision is required. **Key characteristics:** - Output: Scalar similarity scores - Use case: Reranking, text pair classification, high-precision ranking - Architecture: Joint encoding (both texts processed together) - Example models: `cross-encoder/ms-marco-MiniLM-L6-v2`",
  "**Basic usage pattern:** ```python from sentence_transformers import CrossEncoder model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\") scores = model.predict([(query, passage) for passage in passages]) ranks = model.rank(query, passages, return_documents=True) ``` Sources: [README.md:89-132](), [sentence_transformers/__init__.py:15-20]() ## Training Infrastructure Each model type has specialized training infrastructure with corresponding trainer classes, loss functions, and evaluation metrics: ```mermaid graph LR subgraph \"Model Classes\" ST_Model[\"SentenceTransformer\"] SE_Model[\"SparseEncoder\"] CE_Model[\"CrossEncoder\"] end subgraph \"Trainer Classes\" ST_Trainer[\"SentenceTransformerTrainer\"] SE_Trainer[\"SparseEncoderTrainer\"] CE_Trainer[\"CrossEncoderTrainer\"] end subgraph \"Training Arguments\" ST_Args[\"SentenceTransformerTrainingArguments\"] SE_Args[\"SparseEncoderTrainingArguments\"] CE_Args[\"CrossEncoderTrainingArguments\"] end subgraph \"Model Cards\" ST_Card[\"SentenceTransformerModelCardData\"] SE_Card[\"SparseEncoderModelCardData\"] CE_Card[\"CrossEncoderModelCardData\"] end ST_Model --> ST_Trainer SE_Model --> SE_Trainer CE_Model --> CE_Trainer ST_Args --> ST_Trainer SE_Args --> SE_Trainer CE_Args --> CE_Trainer ST_Trainer --> ST_Card SE_Trainer --> SE_Card CE_Trainer --> CE_Card ``` The library provides 20+ loss functions for SentenceTransformer training, 10+ for SparseEncoder training, and 10+ for CrossEncoder training, enabling fine-tuning for specific tasks and domains. Sources: [sentence_transformers/__init__.py:35-64](), [README.md:195]() ## Integration Ecosystem The sentence-transformers library integrates with a wide ecosystem of tools and platforms: **Backend Support:** - PyTorch (default) - ONNX Runtime for optimized inference - Intel OpenVINO for CPU optimization **Vector Databases:** - Pinecone, Weaviate, Qdrant, ChromaDB **Search Engines:** - Elasticsearch, OpenSearch, Apache Solr **ML Frameworks:** - Hugging Face ecosystem (transformers, datasets, hub) - LangChain, Haystack, LlamaIndex **Specialized Libraries:** - BERTopic, KeyBERT, SetFit for domain-specific applications Sources: [sentence_transformers/__init__.py:10-14](), [README.md:172-189]() ## Module Architecture The library follows a modular design where models are composed of reusable components: ```mermaid graph TB subgraph \"Core Modules\" Transformer[\"Transformer<br/>Base encoding\"] Pooling[\"Pooling<br/>Sequence aggregation\"] Router[\"Router<br/>Asymmetric routing\"] end subgraph \"Specialized Modules\" MLMTransformer[\"MLMTransformer<br/>Masked language modeling\"] SpladePooling[\"SpladePooling<br/>Sparse activation\"] CLIPModel[\"CLIPModel<br/>Vision-text joint encoding\"] end subgraph \"Backend Options\" PyTorchBackend[\"PyTorch Backend\"] ONNXBackend[\"ONNX Backend\"] OpenVINOBackend[\"OpenVINO Backend\"] end Transformer --> Pooling MLMTransformer --> SpladePooling Router --> Transformer Router --> MLMTransformer Router --> CLIPModel PyTorchBackend --> Router ONNXBackend --> Router OpenVINOBackend --> Router ``` This modular architecture enables flexible model composition and optimization for different use cases. For detailed information about the module system, see [Module Architecture](#1.2). Sources: [sentence_transformers/__init__.py:10-14](), [pyproject.toml:52-54]()",
  "This guide provides essential examples for getting started with the three core model types in sentence-transformers: **SentenceTransformer** for dense embeddings, **SparseEncoder** for sparse embeddings, and **CrossEncoder** for reranking. Each model type serves different use cases in natural language processing and information retrieval systems. For installation instructions, see [Installation & Setup](#2). For detailed training information, see [Training](#3). For comprehensive usage documentation, see the specific model sections: [SentenceTransformer Models](#5.1), [SparseEncoder Models](#5.2), and [CrossEncoder Models](#5.3). ## Core Model Types Overview The sentence-transformers library provides three main model architectures that complement each other in modern NLP workflows: ```mermaid graph TB subgraph \"Core Classes\" ST[\"SentenceTransformer<br/>sentence_transformers/SentenceTransformer.py\"] SE[\"SparseEncoder<br/>sentence_transformers/sparse_encoder/SparseEncoder.py\"] CE[\"CrossEncoder<br/>sentence_transformers/cross_encoder/CrossEncoder.py\"] end subgraph \"Output Types\" ST --> Dense[\"Dense Vectors<br/>[batch_size, embedding_dim]\"] SE --> Sparse[\"Sparse Vectors<br/>[batch_size, vocab_size]\"] CE --> Scores[\"Similarity Scores<br/>[batch_size] or [batch_size, num_labels]\"] end subgraph \"Primary Use Cases\" Dense --> SemanticSearch[\"Semantic Search<br/>Clustering<br/>Similarity\"] Sparse --> LexicalSearch[\"Neural Lexical Search<br/>Hybrid Retrieval\"] Scores --> Reranking[\"Reranking<br/>Classification\"] end ``` **Sources:** [sentence_transformers/SentenceTransformer.py:61](), [sentence_transformers/sparse_encoder/SparseEncoder.py:27](), [sentence_transformers/cross_encoder/CrossEncoder.py:48](), [README.md:15-17]() ## SentenceTransformer: Dense Embeddings `SentenceTransformer` models encode text into fixed-size dense vector representations, ideal for semantic similarity tasks and vector databases. ### Basic Usage ```python from sentence_transformers import SentenceTransformer # Load a pre-trained model model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Encode sentences into dense vectors sentences = [ \"The weather is lovely today.\", \"It's so sunny outside!\", \"He drove to the stadium.\", ] embeddings = model.encode(sentences) print(embeddings.shape) # (3, 384) # Calculate similarity scores similarities = model.similarity(embeddings, embeddings) print(similarities) # tensor([[1.0000, 0.6660, 0.1046], # [0.6660, 1.0000, 0.1411], # [0.1046, 0.1411, 1.0000]]) ``` ### Specialized Encoding Methods For information retrieval tasks, use task-specific encoding methods: ```python",
  "ranks = model.rank(query, passages, return_documents=True) print(\"Query:\", query) for rank in ranks: print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")",
  "similarity_matrix = util.cos_sim(query_embeddings, passage_embeddings) ``` **Sources:** [docs/pretrained-models/msmarco-v3.md:7-16](), [docs/cross_encoder/pretrained_models.md:16-23]() ## Model Training Evolution The MS MARCO models have evolved through multiple versions with different training methodologies: ### Training Code Flow ```mermaid graph TB subgraph \"Training Components\" TRAINER[\"SentenceTransformerTrainer\"] LOSS_FUNC[\"MultipleNegativesRankingLoss\"] EVALUATOR[\"InformationRetrievalEvaluator\"] MINING[\"util.mine_hard_negatives()\"] end subgraph \"Version Evolution\" V1_DATA[\"Basic Pairs<br/>msmarco-v1 datasets\"] V2_DATA[\"Improved Negatives<br/>msmarco-v2 datasets\"] V3_PROCESS[\"Hard Negative Mining<br/>cross-encoder/ms-marco-electra-base\"] V4_DATA[\"Refined Training<br/>msmarco-v4 models\"] end subgraph \"Training Pipeline\" LOAD_MODEL[\"SentenceTransformer.load()\"] TRAIN[\"trainer.train()\"] EVAL[\"evaluator.evaluate()\"] SAVE[\"model.save()\"] end V1_DATA --> V2_DATA V2_DATA --> V3_PROCESS V3_PROCESS --> V4_DATA TRAINER --> TRAIN LOSS_FUNC --> TRAINER EVALUATOR --> EVAL MINING --> V3_PROCESS LOAD_MODEL --> TRAIN TRAIN --> EVAL EVAL --> SAVE ``` ### Version 3 Hard Negative Mining Process The v3 models used an automated hard negative mining pipeline implemented with sentence-transformers utilities: 1. **Initial Retrieval**: v2 `SentenceTransformer` models encoded queries and retrieved similar passages 2. **Cross-Encoder Scoring**: `CrossEncoder(\"cross-encoder/ms-marco-electra-base\")` scored query-passage pairs 3. **Hard Negative Mining**: `util.mine_hard_negatives()` identified passages with high bi-encoder similarity but low cross-encoder relevance scores 4. **Retraining**: Models trained with `MultipleNegativesRankingLoss` using the mined hard negatives ### Training Loss Functions Used | Model Version | Primary Loss | Secondary Loss | Evaluation | |---------------|-------------|----------------|------------| | v1-v2 | `MultipleNegativesRankingLoss` | - | `InformationRetrievalEvaluator` | | v3 | `MultipleNegativesRankingLoss` | Hard negative augmentation | `InformationRetrievalEvaluator` | | v4 | `MultipleNegativesRankingLoss` | Advanced hard negatives | `InformationRetrievalEvaluator` | **Sources:** [docs/pretrained-models/msmarco-v3.md:53-58](), [docs/sentence_transformer/dataset_overview.md:78-89]() ## Model Selection Guidelines ### Choose Based on Similarity Method - **Cosine Similarity Models**: Use when you need normalized similarity scores and prefer shorter, focused passages - **Dot Product Models**: Use when longer, comprehensive passages are preferred and unnormalized scores are acceptable ### Choose Based on Architecture ```mermaid graph TD RETRIEVAL_TASK[\"Retrieval Task\"] RETRIEVAL_TASK --> FIRST_STAGE[\"First Stage Retrieval<br/>encode() + similarity search\"] RETRIEVAL_TASK --> SECOND_STAGE[\"Second Stage Reranking<br/>predict() on pairs\"] FIRST_STAGE --> ST_CLASS[\"SentenceTransformer class\"] SECOND_STAGE --> CE_CLASS[\"CrossEncoder class\"] ST_CLASS --> ST_FAST[\"Fast: msmarco-MiniLM-L6-v3<br/>18k queries/sec GPU\"] ST_CLASS --> ST_BALANCED[\"Balanced: msmarco-distilbert-base-v4<br/>7k queries/sec, 70.24 NDCG@10\"] ST_CLASS --> ST_ACCURATE[\"Accurate: msmarco-distilbert-base-tas-b<br/>71.04 NDCG@10, 34.43 MRR@10\"] CE_CLASS --> CE_FAST[\"Fast: cross-encoder/ms-marco-TinyBERT-L2-v2<br/>9k docs/sec\"] CE_CLASS --> CE_ACCURATE[\"Accurate: cross-encoder/ms-marco-MiniLM-L6-v2<br/>74.30 NDCG@10, 39.01 MRR@10\"] subgraph \"Integration Methods\" UTIL_COS[\"util.cos_sim()\"] UTIL_DOT[\"util.dot_score()\"] PREDICT_METHOD[\"predict() method\"] end ST_FAST --> UTIL_COS ST_BALANCED --> UTIL_COS ST_ACCURATE --> UTIL_DOT CE_FAST --> PREDICT_METHOD CE_ACCURATE --> PREDICT_METHOD ```",
  "### Performance vs Speed Trade-offs - **Fastest**: `msmarco-MiniLM-L6-v3` (18,000 queries/sec GPU) - **Best Balance**: `msmarco-distilbert-base-v4` (7,000 queries/sec GPU, highest accuracy) - **Highest Quality**: `msmarco-distilbert-base-tas-b` (34.43 MRR@10) **Sources:** [docs/pretrained-models/msmarco-v3.md:45-50]() ## Integration with Search Systems MS MARCO models integrate with various search architectures for production deployment. For detailed integration patterns, see [Retrieve & Rerank Architecture](#6.3) and [Semantic Search](#6.1). **Sources:** [docs/pretrained-models/msmarco-v3.md:19](), [docs/cross_encoder/pretrained_models.md:44]() # Applications This page provides an overview of real-world applications and integration patterns using sentence-transformers models. It covers how the three core model types (`SentenceTransformer`, `SparseEncoder`, and `CrossEncoder`) are deployed in production systems for semantic search, retrieval, reranking, and other natural language processing tasks. For specific implementation details of individual applications, see [Semantic Search](#6.1), [Sparse Search Integration](#6.2), [Retrieve & Rerank Architecture](#6.3), [Semantic Textual Similarity](#6.4), and [Multimodal Applications](#6.5). For information about available pretrained models optimized for specific applications, see [Pretrained Models](#5). ## Core Application Categories The sentence-transformers library enables three primary categories of applications, each leveraging different model architectures optimized for specific use cases: ### Application Architecture Overview ```mermaid graph TB subgraph \"Dense Embedding Applications\" ST[\"SentenceTransformer\"] ST --> SemanticSearch[\"Semantic Search\"] ST --> Clustering[\"Document Clustering\"] ST --> STS[\"Semantic Textual Similarity\"] ST --> Recommendation[\"Content Recommendation\"] end subgraph \"Sparse Embedding Applications\" SE[\"SparseEncoder\"] SE --> NeuralLexical[\"Neural Lexical Search\"] SE --> HybridRetrieval[\"Hybrid Dense-Sparse Retrieval\"] SE --> KeywordSearch[\"Enhanced Keyword Search\"] end subgraph \"Cross-Attention Applications\" CE[\"CrossEncoder\"] CE --> Reranking[\"Search Result Reranking\"] CE --> Classification[\"Text Pair Classification\"] CE --> ScoreRegression[\"Similarity Score Regression\"] end subgraph \"Output Formats\" SemanticSearch --> DenseVectors[\"Dense Vectors (384-1024 dim)\"] NeuralLexical --> SparseVectors[\"Sparse Vectors (30k+ dim)\"] Reranking --> SimilarityScores[\"Similarity Scores (0-1)\"] end ``` **Dense embedding applications** use `SentenceTransformer` models to convert text into fixed-size dense vectors that capture semantic meaning. These applications excel at finding semantically similar content even when lexical overlap is minimal. **Sparse embedding applications** use `SparseEncoder` models to generate high-dimensional sparse vectors that preserve lexical information while adding semantic understanding. These applications bridge the gap between traditional keyword search and semantic search. **Cross-attention applications** use `CrossEncoder` models that jointly process text pairs to produce precise similarity scores. These applications provide the highest accuracy for ranking and classification tasks but with higher computational cost. Sources: [docs/pretrained-models/msmarco-v2.md:1-39]() ## Integration Patterns Production systems typically integrate sentence-transformers models through several common patterns, each optimized for different scalability and accuracy requirements: ### System Integration Architecture",
  "```mermaid graph LR subgraph \"Data Sources\" Documents[\"Document Corpus\"] Queries[\"User Queries\"] TextPairs[\"Text Pairs\"] end subgraph \"sentence_transformers\" STModel[\"SentenceTransformer.encode()\"] SEModel[\"SparseEncoder.encode_query()\"] CEModel[\"CrossEncoder.predict()\"] end subgraph \"Storage Systems\" VectorDB[\"Vector Databases<br/>Pinecone, Weaviate, Qdrant\"] SearchEngines[\"Search Engines<br/>Elasticsearch, OpenSearch\"] Cache[\"Embedding Cache<br/>Redis, Memcached\"] end subgraph \"Application Layer\" SearchAPI[\"Search API\"] RerankAPI[\"Reranking API\"] SimilarityAPI[\"Similarity API\"] end Documents --> STModel Documents --> SEModel STModel --> VectorDB SEModel --> SearchEngines Queries --> STModel Queries --> SEModel Queries --> CEModel VectorDB --> SearchAPI SearchEngines --> SearchAPI Cache --> SearchAPI TextPairs --> CEModel CEModel --> RerankAPI CEModel --> SimilarityAPI SearchAPI --> RerankAPI ``` **Vector database integration** stores dense embeddings from `SentenceTransformer.encode()` in specialized vector databases optimized for similarity search. Common databases include Pinecone, Weaviate, and Qdrant, which provide approximate nearest neighbor search capabilities. **Search engine integration** indexes sparse embeddings from `SparseEncoder.encode_query()` and `SparseEncoder.encode_document()` in traditional search engines like Elasticsearch or OpenSearch, enabling hybrid lexical-semantic search. **API-based reranking** uses `CrossEncoder.predict()` to refine initial retrieval results, typically processing the top-k candidates from a faster first-stage retrieval system. Sources: [docs/pretrained-models/msmarco-v2.md:7-16]() ## Production Deployment Patterns ### Two-Stage Retrieval Architecture The most common production pattern combines fast retrieval with precise reranking: ```mermaid graph TD UserQuery[\"User Query\"] subgraph \"Stage 1: Fast Retrieval\" BiEncoder[\"SentenceTransformer<br/>or SparseEncoder\"] CandidateRetrieval[\"Retrieve Top-100<br/>Candidates\"] end subgraph \"Stage 2: Precise Reranking\" CrossEncoder[\"CrossEncoder.predict()\"] FinalRanking[\"Return Top-10<br/>Results\"] end UserQuery --> BiEncoder BiEncoder --> CandidateRetrieval CandidateRetrieval --> CrossEncoder CrossEncoder --> FinalRanking ``` This architecture balances computational efficiency with accuracy by using fast bi-encoder models for initial retrieval and slower but more accurate cross-encoder models for final ranking. ### Model Selection by Application Requirements | Application Type | Model Architecture | Latency | Accuracy | Storage Requirements | |------------------|-------------------|---------|----------|---------------------| | Real-time search | `SentenceTransformer` | ~1ms | Good | Dense vectors (384-1024 dim) | | Hybrid search | `SparseEncoder` | ~2ms | Better | Sparse vectors (30k+ dim) | | Reranking | `CrossEncoder` | ~10ms | Best | No storage (computed on-demand) | | Batch processing | Any | Variable | Highest | Depends on architecture | ### Memory and Scaling Considerations Production deployments must consider memory requirements and scaling patterns: - **Dense embeddings**: Require 4 bytes per dimension per document (e.g., 1.5KB for 384-dim embeddings) - **Sparse embeddings**: Store only non-zero values, typically 50-200 active dimensions per document - **Cross-encoders**: No storage overhead but higher compute cost per query ## Common Integration Libraries The sentence-transformers library integrates with numerous downstream frameworks and applications: | Integration Type | Libraries | Use Case | |------------------|-----------|----------| | RAG Frameworks | LangChain, LlamaIndex, Haystack | Document retrieval for LLMs | | Topic Modeling | BERTopic, Top2Vec | Document clustering and topic discovery | | Few-shot Learning | SetFit | Classification with minimal training data | | Keyword Extraction | KeyBERT | Semantic keyword extraction | | Search Applications | txtai | End-to-end search applications | These integrations typically use the standard `encode()` and `predict()` methods provided by the core model classes, enabling drop-in replacement of embedding models without changing application code. Sources: [docs/pretrained-models/msmarco-v2.md:19-38]()",
  "This document covers the three core model architectures in the sentence-transformers library: `SentenceTransformer`, `SparseEncoder`, and `CrossEncoder`. Each serves distinct use cases in text encoding and similarity tasks. For information about training these model types, see pages [3.1](#3.1), [3.2](#3.2), and [3.3](#3.3). For details on available pretrained models, see pages [5.1](#5.1), [5.2](#5.2), and [5.3](#5.3). ## Architecture Overview The sentence-transformers library provides three main model architectures that differ in their encoding approach and use cases: ```mermaid graph TB subgraph \"Input Processing\" Text[\"Text Input(s)\"] end subgraph \"Core Model Types\" ST[\"SentenceTransformer<br/>Dense Embeddings\"] SE[\"SparseEncoder<br/>Sparse Embeddings\"] CE[\"CrossEncoder<br/>Pairwise Scoring\"] end subgraph \"Output Types\" Dense[\"Dense Vectors<br/>[batch_size, embedding_dim]\"] Sparse[\"Sparse Vectors<br/>[batch_size, vocab_size]\"] Scores[\"Similarity Scores<br/>[batch_size] or [batch_size, num_labels]\"] end subgraph \"Use Cases\" SemanticSearch[\"Semantic Search\"] Clustering[\"Clustering\"] LexicalSearch[\"Neural Lexical Search\"] HybridRetrieval[\"Hybrid Retrieval\"] Reranking[\"Reranking\"] Classification[\"Text Classification\"] end Text --> ST Text --> SE Text --> CE ST --> Dense SE --> Sparse CE --> Scores Dense --> SemanticSearch Dense --> Clustering Sparse --> LexicalSearch Sparse --> HybridRetrieval Scores --> Reranking Scores --> Classification ``` **Sources:** [sentence_transformers/SentenceTransformer.py:61-163](), [sentence_transformers/sparse_encoder/SparseEncoder.py:27-129](), [sentence_transformers/cross_encoder/CrossEncoder.py:48-116](), [README.md:15-17]() ## SentenceTransformer The `SentenceTransformer` class is the primary model for generating dense vector embeddings from text. It encodes individual sentences or documents into fixed-size dense vectors suitable for semantic similarity tasks. ### Core Architecture ```mermaid graph LR subgraph \"SentenceTransformer Pipeline\" Input[\"Text Input\"] Tokenizer[\"tokenize()\"] Transformer[\"Transformer Module\"] Pooling[\"Pooling Module\"] Optional[\"Optional Modules<br/>(Normalize, Dense, etc.)\"] Output[\"Dense Embedding\"] end Input --> Tokenizer Tokenizer --> Transformer Transformer --> Pooling Pooling --> Optional Optional --> Output subgraph \"Key Methods\" Encode[\"encode()\"] EncodeQuery[\"encode_query()\"] EncodeDoc[\"encode_document()\"] Similarity[\"similarity()\"] end ``` The `SentenceTransformer` class inherits from `nn.Sequential`, `FitMixin`, and `PeftAdapterMixin`, allowing it to function as a sequential pipeline of modules while supporting training and PEFT adapters. ### Key Features - **Modular Design**: Composed of sequential modules like `Transformer`, `Pooling`, `Normalize` - **Prompt Support**: Configurable prompts for different tasks via `prompts` dictionary - **Task-Specific Encoding**: `encode_query()` and `encode_document()` methods for asymmetric retrieval - **Multiple Backends**: Supports PyTorch, ONNX, and OpenVINO backends - **Similarity Functions**: Built-in similarity computation with configurable functions ### Usage Patterns ```python",
  "def _append_csv_headers(self, similarity_fn_names): super()._append_csv_headers(similarity_fn_names) self.csv_headers.extend([ \"query_active_dims\", \"query_sparsity_ratio\", \"corpus_active_dims\", \"corpus_sparsity_ratio\" ]) ``` ### Model Card Integration Sparse evaluators store evaluation metrics in the model's card data for documentation: ```python def store_metrics_in_model_card_data(self, model, metrics, epoch=0, step=0): model.model_card_data.set_evaluation_metrics(self, metrics, epoch=epoch, step=step) ``` Sources: [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:271-274](), [sentence_transformers/sparse_encoder/evaluation/SparseEmbeddingSimilarityEvaluator.py:159-162]()",
  "args = SentenceTransformerTrainingArguments( router_mapping={ \"question\": \"query\", \"positive\": \"document\", \"negative\": \"document\" } ) ``` The `router_mapping` tells the data collator which dataset columns correspond to which router tasks. Sources: [sentence_transformers/trainer.py:198-204](), [sentence_transformers/sparse_encoder/trainer.py:180-186]() ## Advanced Module Types ### Sparse Encoder Modules For sparse embeddings, specialized modules are used that produce high-dimensional sparse vectors: #### Sparse Module Pipeline ```mermaid graph LR Text[\"Input Text\"] --> MLMTrans[\"MLMTransformer<br/>auto_model + MLM head\"] MLMTrans --> TokenLogits[\"Token Logits<br/>[batch, seq_len, vocab_size]\"] TokenLogits --> SpladePool[\"SpladePooling<br/>pooling_strategy + activation\"] SpladePool --> SparseEmb[\"Sparse Embedding<br/>[batch, vocab_size]\"] subgraph SparseModules[\"Sparse Module Types\"] MLMTrans SpladePool end ``` #### Sparse Module Types | Module | Purpose | Output Shape | |--------|---------|--------------| | `MLMTransformer` | Contextual token predictions | `[batch, seq_len, vocab_size]` | | `SpladePooling` | Aggregate tokens to sparse vector | `[batch, vocab_size]` | | `SparseStaticEmbedding` | Pre-computed static weights | `[batch, vocab_size]` | | `SparseAutoEncoder` | Learned sparse representations | `[batch, latent_dim]` | Key configuration: - `pooling_strategy`: `\"max\"`, `\"mean\"`, `\"sum\"` - `activation_function`: `\"relu\"`, `\"log1p_relu\"`, `\"identity\"` - `k`: Number of top-k active dimensions (for `SparseAutoEncoder`) Sources: [sentence_transformers/sparse_encoder/models/MLMTransformer.py](), [sentence_transformers/sparse_encoder/models/SpladePooling.py](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py](), [sentence_transformers/sparse_encoder/models/SparseAutoEncoder.py]() ### Backend Support The `Transformer` module supports multiple inference backends: | Backend | Description | Requirements | |---------|-------------|--------------| | `torch` | Standard PyTorch | Default | | `onnx` | ONNX Runtime optimization | `optimum[onnxruntime]` | | `openvino` | Intel hardware acceleration | `optimum[openvino]` | Backend selection affects model loading and inference performance but not the module interface. Sources: [sentence_transformers/models/Transformer.py:174-195](), [sentence_transformers/models/Transformer.py:196-248]() ## Module Forward Pass Interface ### Feature Dictionary Flow All modules operate on a shared features dictionary that flows through the pipeline: ```mermaid graph TD Input[\"features = {}\"] --> InputMod[\"InputModule.tokenize()\"] InputMod --> TokenFeats[\"{input_ids, attention_mask, ...}\"] TokenFeats --> Mod1[\"Module 1.forward(features)\"] Mod1 --> F1[\"{input_ids, attention_mask,<br/>token_embeddings, ...}\"] F1 --> Mod2[\"Module 2.forward(features)\"] Mod2 --> F2[\"{..., sentence_embedding}\"] F2 --> ModN[\"Module N.forward(features)\"] ModN --> Final[\"{final features}\"] ``` Common feature keys: - `input_ids`, `attention_mask`: From tokenization - `token_embeddings`: Per-token representations - `sentence_embedding`: Final sentence representation - `task`: Task type for Router modules ### Module Interface Requirements Each module must implement: ```python def forward(self, features: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]: # Transform features and return updated dictionary pass def save(self, output_path: str, **kwargs) -> None: # Save module configuration and weights pass ``` InputModules additionally require: ```python def tokenize(self, texts: list[str], **kwargs) -> dict[str, torch.Tensor]: # Convert text to model features pass ``` Sources: [sentence_transformers/models/Module.py:33-89](), [sentence_transformers/models/InputModule.py:60-73]() ## Use Cases and Applications The CrossEncoder is commonly used in several scenarios: 1. **Reranking**: As the second stage in a retrieve-and-rerank pipeline ``` Query → SentenceTransformer (retrieval) → Top-k documents → CrossEncoder (reranking) → Reranked results ```",
  "2. **Text Pair Classification**: Directly scoring or classifying pairs of texts - Natural language inference (entailment/contradiction) - Semantic textual similarity - Question-answer relevance 3. **High-Precision Scoring**: When maximum accuracy is needed for a limited number of text pairs CrossEncoder models typically achieve higher accuracy than SentenceTransformer models for direct text pair comparison tasks, but they are computationally more expensive for large-scale comparisons. Sources: [README.md:93-131]() ## Selection Guide: CrossEncoder vs SentenceTransformer | Aspect | CrossEncoder | SentenceTransformer | |--------|-------------|---------------------| | **Output** | Scores/labels for text pairs | Embeddings for individual texts | | **Comparison** | Direct scoring of pairs | Similarity between embeddings | | **Scaling** | O(n) comparisons need O(n) inference calls | O(n) comparisons need O(1) inference calls | | **Accuracy** | Generally higher for direct comparisons | Good for retrieval, less precise for direct comparison | | **Use case** | Reranking, high-precision scoring of few pairs | Retrieval, semantic search, clustering | Typically, the optimal approach is to combine both: use SentenceTransformer for efficient retrieval of candidate documents, then use CrossEncoder for precise reranking of the top results. Sources: [README.md:93-131](), [index.rst:65-98]() # Installation & Setup This document covers installation procedures and dependency requirements for the sentence-transformers library. It explains the different installation options available for various use cases including basic inference, training, backend optimization, and development. For basic usage examples after installation, see [Quickstart Guide](#2.1). For training-specific setup details, see [Training](#3). ## System Requirements The sentence-transformers library requires **Python 3.9+**, **PyTorch 1.11.0+**, and **transformers v4.41.0+**. The library supports multiple backends and deployment scenarios through optional dependencies. ### Installation Options Overview The library provides five main installation configurations that correspond to different usage patterns: ```mermaid graph TB subgraph \"Installation Options\" Default[\"Default<br/>Basic inference\"] ONNX[\"ONNX<br/>Optimized inference\"] OpenVINO[\"OpenVINO<br/>Intel optimization\"] Training[\"Default + Training<br/>Model training\"] Development[\"Development<br/>Contributing\"] end subgraph \"Core Capabilities\" Default --> LoadSave[\"Model loading/saving\"] Default --> Inference[\"Embedding generation\"] ONNX --> ONNXOpt[\"ONNX optimization\"] ONNX --> Quantization[\"Model quantization\"] OpenVINO --> IntelOpt[\"Intel hardware optimization\"] Training --> TrainLoop[\"Training loops\"] Training --> Evaluation[\"Model evaluation\"] Development --> Testing[\"Unit testing\"] Development --> Linting[\"Code formatting\"] end subgraph \"Backend Support\" LoadSave --> PyTorchBackend[\"PyTorch backend\"] ONNXOpt --> ONNXBackend[\"ONNX Runtime backend\"] IntelOpt --> OpenVINOBackend[\"OpenVINO backend\"] end subgraph \"Model Types\" PyTorchBackend --> SentenceTransformer[\"SentenceTransformer\"] PyTorchBackend --> SparseEncoder[\"SparseEncoder\"] PyTorchBackend --> CrossEncoder[\"CrossEncoder\"] ONNXBackend --> OptimizedModels[\"Optimized models\"] OpenVINOBackend --> IntelModels[\"Intel-optimized models\"] end ``` **Sources:** [docs/installation.md:3-8]() ## Package Manager Installation ### pip Installation The recommended installation method uses pip with specific extras for different use cases: | Installation Type | Command | Use Case | |------------------|---------|----------| | Default | `pip install -U sentence-transformers` | Basic inference only | | ONNX (GPU+CPU) | `pip install -U \"sentence-transformers[onnx-gpu]\"` | Optimized inference with GPU support | | ONNX (CPU only) | `pip install -U \"sentence-transformers[onnx]\"` | CPU-only optimized inference | | OpenVINO | `pip install -U \"sentence-transformers[openvino]\"` | Intel hardware optimization | | Training | `pip install -U \"sentence-transformers[train]\"` | Model training capabilities | | Development | `pip install -U \"sentence-transformers[dev]\"` | Full development environment | ### conda Installation For conda users, the base package is available through conda-forge: ```bash",
  "sentences = [\"This is a test sentence\", \"This is another test\"] embeddings = model.encode(sentences) print(f\"Generated embeddings shape: {embeddings.shape}\") ``` ### Backend-Specific Verification Test different backends if installed: ```python # Test ONNX backend (if installed) from sentence_transformers import SentenceTransformer model = SentenceTransformer('all-MiniLM-L6-v2', backend='onnx') # Test OpenVINO backend (if installed) model = SentenceTransformer('all-MiniLM-L6-v2', backend='openvino') # Check GPU availability import torch print(f\"CUDA available: {torch.cuda.is_available()}\") print(f\"GPU count: {torch.cuda.device_count()}\") ``` **Sources:** [docs/installation.md:1-177]()",
  "model = CrossEncoder(\"google-bert/bert-base-uncased\") ``` Sources: [docs/cross_encoder/training_overview.md:42-72]() ### Dataset Format Requirements CrossEncoder training datasets must match the chosen loss function requirements. The validation involves three steps: 1. **Input Columns**: All columns except \"label\", \"labels\", \"score\", or \"scores\" are treated as inputs 2. **Label Columns**: If the loss function requires labels, the dataset must have a column named \"label\", \"labels\", \"score\", or \"scores\" 3. **Model Output Compatibility**: The number of model output labels must match the loss function requirements **Dataset Format Validation Flow** ```mermaid graph TD InputDataset[Input Dataset] InputDataset --> CheckColumns{Check Column Names} CheckColumns --> LabelCols[Extract Label Columns] CheckColumns --> InputCols[Extract Input Columns] LabelCols --> ValidateLabels{Validate Labels} InputCols --> ValidateInputs{Validate Input Count} ValidateLabels --> CheckModelOutput{Check Model Output} ValidateInputs --> CheckModelOutput CheckModelOutput --> Compatible[Compatible Format] CheckModelOutput --> Error[Format Error] Compatible --> Training[Begin Training] Error --> FixDataset[Fix Dataset Format] FixDataset --> CheckColumns ``` Sources: [docs/cross_encoder/training_overview.md:171-189]() ### Loss Functions CrossEncoder loss functions are designed for ranking and classification tasks. The choice depends on your data format and task type: | Input Format | Labels | Model Outputs | Common Loss Functions | |--------------|--------|---------------|----------------------| | `(sentence_A, sentence_B)` pairs | `class` | `num_classes` | `CrossEntropyLoss` | | `(anchor, positive)` pairs | `none` | `1` | `MultipleNegativesRankingLoss` | | `(anchor, positive/negative)` pairs | `1/0` | `1` | `BinaryCrossEntropyLoss` | | `(query, [doc1, ..., docN])` | `[score1, ..., scoreN]` | `1` | `LambdaLoss`, `ListNetLoss` | Sources: [docs/cross_encoder/loss_overview.md:20-28]() ### Hard Negatives Mining CrossEncoder performance often depends on the quality of negative examples. The `mine_hard_negatives` function helps generate challenging negatives: ```python from sentence_transformers.util import mine_hard_negatives hard_train_dataset = mine_hard_negatives( train_dataset, embedding_model, num_negatives=5, range_min=10, range_max=100, max_score=0.8, output_format=\"labeled-pair\" ) ``` Sources: [docs/cross_encoder/training_overview.md:204-242]() ## Training Process Integration CrossEncoder training integrates with the broader sentence-transformers training infrastructure while maintaining its specialized functionality. **CrossEncoder Training Infrastructure** ```mermaid graph TB subgraph \"CrossEncoder Specific\" CEModel[CrossEncoder] CETrainer[CrossEncoderTrainer] CEArgs[CrossEncoderTrainingArguments] CELoss[CrossEncoder Losses] CEEval[CrossEncoder Evaluators] end subgraph \"Shared Infrastructure\" HFTrainer[transformers.Trainer] DataCollator[SentenceTransformerDataCollator] ModelCard[Model Card Callbacks] Optimizers[Optimizers & Schedulers] end subgraph \"External Integration\" HFHub[Hugging Face Hub] WandB[Weights & Biases] TensorBoard[TensorBoard] end CETrainer --> HFTrainer CEArgs --> HFTrainer DataCollator --> CETrainer ModelCard --> CETrainer Optimizers --> CETrainer CEModel --> CELoss CELoss --> CETrainer CEEval --> CETrainer CETrainer --> HFHub CETrainer --> WandB CETrainer --> TensorBoard ``` Sources: [sentence_transformers/trainer.py:59-128](), [docs/cross_encoder/training_overview.md:314-400]() ### Training Arguments `CrossEncoderTrainingArguments` extends the standard transformers training arguments with CrossEncoder-specific parameters:",
  "**Key Training Arguments:** - **Performance**: `learning_rate`, `per_device_train_batch_size`, `num_train_epochs`, `gradient_accumulation_steps` - **Optimization**: `fp16`, `bf16`, `optim`, `lr_scheduler_type`, `warmup_ratio` - **Evaluation**: `eval_strategy`, `eval_steps`, `load_best_model_at_end`, `metric_for_best_model` - **Tracking**: `report_to`, `run_name`, `logging_steps`, `push_to_hub` Sources: [docs/cross_encoder/training_overview.md:320-344]() ### Evaluation System CrossEncoder evaluators assess model performance during training with task-specific metrics: - `BinaryClassificationEvaluator`: For binary classification tasks - `CrossEncoderReranking`: For ranking performance evaluation - `EmbeddingSimilarityEvaluator`: For similarity scoring tasks - `InformationRetrievalEvaluator`: For retrieval performance Sources: [docs/cross_encoder/training_overview.md:365-400]() ## Relationship to Other Training Systems CrossEncoder training shares infrastructure with other sentence-transformers training systems while maintaining its distinct characteristics. **Training System Relationships** ```mermaid graph TB subgraph \"Base Training Infrastructure\" BaseTrainer[SentenceTransformerTrainer] BaseArgs[SentenceTransformerTrainingArguments] BaseCollator[SentenceTransformerDataCollator] BaseCard[SentenceTransformerModelCardCallback] end subgraph \"CrossEncoder Training\" CETrainer[CrossEncoderTrainer] CEArgs[CrossEncoderTrainingArguments] CEModel[CrossEncoder] CELosses[CrossEncoder Losses] CEEvals[CrossEncoder Evaluators] end subgraph \"SparseEncoder Training\" SETrainer[SparseEncoderTrainer] SEArgs[SparseEncoderTrainingArguments] SEModel[SparseEncoder] SELosses[SparseEncoder Losses] end subgraph \"SentenceTransformer Training\" STModel[SentenceTransformer] STLosses[SentenceTransformer Losses] STEvals[SentenceTransformer Evaluators] end BaseTrainer --> CETrainer BaseTrainer --> SETrainer BaseArgs --> CEArgs BaseArgs --> SEArgs BaseCollator --> CETrainer BaseCollator --> SETrainer BaseCard --> CETrainer STModel --> BaseTrainer CEModel --> CETrainer SEModel --> SETrainer STLosses --> BaseTrainer CELosses --> CETrainer SELosses --> SETrainer STEvals --> BaseTrainer CEEvals --> CETrainer ``` Sources: [sentence_transformers/trainer.py:59-128](), [sentence_transformers/sparse_encoder/trainer.py:31-98]() The CrossEncoder training system leverages the shared infrastructure while providing specialized components for joint text encoding and ranking tasks, making it suitable for reranking applications and text pair classification scenarios. # Loss Functions for SentenceTransformer ## Introduction This page documents the loss functions available for training SentenceTransformer models. Loss functions are a critical component that defines the training objective and directly influences the quality and properties of the resulting sentence embeddings. For information about how to use these loss functions in training, see [SentenceTransformer Training](#3.1) and [CrossEncoder Training](#3.2). The SentenceTransformer library offers a diverse set of loss functions, each designed for specific use cases and data formats: ```mermaid graph TD subgraph \"Overview of Loss Functions\" ST[SentenceTransformer] Loss[Loss Functions] Training[Training Process] ST --> Loss Loss --> Training Training --> ST end ``` Sources: - [sentence_transformers/losses/__init__.py:1-67]() ## Loss Function Taxonomy SentenceTransformer provides several categories of loss functions, each with different approaches to learning sentence embeddings: Loss Function Taxonomy",
  "```mermaid graph TD subgraph \"Complete Loss Function Taxonomy\" LF[\"Loss Functions\"] Ranking[\"Ranking-based Losses\"] Contrastive[\"Contrastive Losses\"] Triplet[\"Triplet Losses\"] MSE[\"MSE-based Losses\"] Special[\"Specialized Losses\"] LF --> Ranking LF --> Contrastive LF --> Triplet LF --> MSE LF --> Special Ranking --> MNRL[\"MultipleNegativesRankingLoss\"] Ranking --> CMNRL[\"CachedMultipleNegativesRankingLoss\"] Ranking --> MNRSL[\"MultipleNegativesSymmetricRankingLoss\"] Ranking --> CMNRSL[\"CachedMultipleNegativesSymmetricRankingLoss\"] Ranking --> GISTL[\"GISTEmbedLoss\"] Ranking --> CGISTL[\"CachedGISTEmbedLoss\"] Ranking --> MBML[\"MegaBatchMarginLoss\"] Contrastive --> CL[\"ContrastiveLoss\"] Contrastive --> OCL[\"OnlineContrastiveLoss\"] Contrastive --> CTL[\"ContrastiveTensionLoss\"] Contrastive --> CTLBN[\"ContrastiveTensionLossInBatchNegatives\"] Contrastive --> COSENT[\"CoSENTLoss\"] Contrastive --> AnglE[\"AnglELoss\"] Triplet --> TL[\"TripletLoss\"] Triplet --> BHTL[\"BatchHardTripletLoss\"] Triplet --> BHSM[\"BatchHardSoftMarginTripletLoss\"] Triplet --> BSHT[\"BatchSemiHardTripletLoss\"] Triplet --> BATL[\"BatchAllTripletLoss\"] MSE --> MSEL[\"MSELoss\"] MSE --> MMSE[\"MarginMSELoss\"] MSE --> CSL[\"CosineSimilarityLoss\"] MSE --> DKL[\"DistillKLDivLoss\"] Special --> ML[\"MatryoshkaLoss\"] Special --> M2D[\"Matryoshka2dLoss\"] Special --> ALL[\"AdaptiveLayerLoss\"] Special --> DAEL[\"DenoisingAutoEncoderLoss\"] Special --> SL[\"SoftmaxLoss\"] end ``` Sources: - [sentence_transformers/losses/__init__.py:1-67]() ## Ranking-Based Loss Functions Ranking-based loss functions are commonly used for training retrieval models. They focus on learning representations where relevant pairs have higher similarity than irrelevant pairs. ### MultipleNegativesRankingLoss This is one of the most widely used loss functions for training sentence embeddings. It treats other samples in the batch as negatives, creating an effective training signal that improves with larger batch sizes. ```mermaid flowchart TD subgraph \"MultipleNegativesRankingLoss Flow\" A[\"Anchor Embeddings\"] P[\"Positive Embeddings\"] S[\"Similarity Matrix\"] L[\"Loss Computation\"] A --> S P --> S S --> L L -->|\"Cross Entropy\"| RL[\"Ranking Loss\"] end ``` **Key Properties**: - Also known as InfoNCE loss, SimCSE loss, or in-batch negatives loss - Performance generally improves with increasing batch size - Requires (anchor, positive) pairs or (anchor, positive, negative) triplets - Each anchor should be most similar to its corresponding positive from all candidates in the batch Sources: - [sentence_transformers/losses/MultipleNegativesRankingLoss.py:13-132]() ### CachedMultipleNegativesRankingLoss A memory-efficient version of MultipleNegativesRankingLoss based on the GradCache technique, allowing for extremely large batch sizes with constant memory usage. **Key Properties**: - Divides computation into embedding and loss calculation stages - Allows training with much larger batch sizes on limited hardware - Approximately 20-30% slower than non-cached version - Technically superior for large batch sizes ```mermaid flowchart TD subgraph \"CachedMultipleNegativesRankingLoss Stages\" Stage1[\"Stage 1: Embed without gradients\"] Stage2[\"Stage 2: Calculate loss and cache gradients\"] Stage3[\"Stage 3: Embed with gradients and apply cached gradients\"] Stage1 --> Stage2 Stage2 --> Stage3 end ``` Sources: - [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:63-300]() ### GISTEmbedLoss An improved ranking loss that uses a guide model to guide the in-batch negative sample selection, providing a stronger training signal. **Key Properties**: - Uses a teacher/guide model to identify and suppress false negatives - Supports different margin strategies for negative filtering - Better training signal than MultipleNegativesRankingLoss",
  "Sources: - [sentence_transformers/losses/GISTEmbedLoss.py:13-221]() - [sentence_transformers/losses/CachedGISTEmbedLoss.py:63-382]() ## Contrastive Loss Functions Contrastive losses optimize embeddings so that similar items are closer together and dissimilar items are farther apart in the embedding space. ### ContrastiveLoss The standard contrastive loss function that minimizes distance between positive pairs and maximizes distance between negative pairs. **Key Properties**: - Expects pairs of texts with binary labels (1 for similar, 0 for dissimilar) - Uses a specified distance metric (cosine, euclidean, manhattan) - Includes a margin hyperparameter ```mermaid flowchart TD subgraph \"ContrastiveLoss Flow\" Input1[\"Text A\"] --> Embedding1[\"Embedding A\"] Input2[\"Text B\"] --> Embedding2[\"Embedding B\"] Embedding1 --> Distance[\"Distance Calculation\"] Embedding2 --> Distance Distance --> Contrastive[\"Contrastive Loss\"] Label[\"Label (0 or 1)\"] --> Contrastive end ``` Sources: - [sentence_transformers/losses/ContrastiveLoss.py:13-120]() ### CoSENTLoss An improved contrastive loss that provides a stronger training signal than standard CosineSimilarityLoss. **Key Properties**: - Uses a logsum formulation comparing multiple pairs in the batch - Faster convergence and better performance than CosineSimilarityLoss - Requires sentence pairs with similarity scores Sources: - [sentence_transformers/losses/CoSENTLoss.py:13-115]() ### ContrastiveTensionLoss Designed for unsupervised learning, this loss creates positive and negative pairs automatically. **Key Properties**: - Works without explicit labels - Creates a copy of the encoder model to produce embeddings for the first sentence in each pair - Requires using `ContrastiveTensionDataLoader` for proper pair generation Sources: - [sentence_transformers/losses/ContrastiveTensionLoss.py:17-204]() ## Triplet Loss Functions Triplet losses use triplets of (anchor, positive, negative) to learn embeddings where the anchor is closer to the positive than to the negative by a certain margin. ### TripletLoss The basic triplet loss function minimizes the distance between anchor and positive while maximizing the distance between anchor and negative. **Key Properties**: - Requires (anchor, positive, negative) triplets - Uses a specified distance metric and margin - Optimizes: max(||anchor - positive|| - ||anchor - negative|| + margin, 0) ```mermaid flowchart TD subgraph \"TripletLoss Structure\" A[\"Anchor\"] --> E_A[\"Anchor Embedding\"] P[\"Positive\"] --> E_P[\"Positive Embedding\"] N[\"Negative\"] --> E_N[\"Negative Embedding\"] E_A --> D_AP[\"Distance(Anchor, Positive)\"] E_P --> D_AP E_A --> D_AN[\"Distance(Anchor, Negative)\"] E_N --> D_AN D_AP --> TL[\"Triplet Loss\"] D_AN --> TL M[\"Margin\"] --> TL end ``` Sources: - [sentence_transformers/losses/TripletLoss.py:13-112]() ### Batch Triplet Losses These are more advanced variants of triplet loss that use different strategies for mining triplets within a batch: 1. **BatchHardTripletLoss**: Selects hardest positive and negative samples for each anchor. 2. **BatchSemiHardTripletLoss**: Focuses on semi-hard triplets (not too easy, not too hard). 3. **BatchAllTripletLoss**: Uses all valid triplets in the batch. 4. **BatchHardSoftMarginTripletLoss**: Similar to BatchHardTripletLoss but with a soft margin. **Key Properties**: - Require single sentences with class labels - Create triplets on-the-fly from the batch - Recommend using batches with multiple examples per class - Different mining strategies for different training dynamics Sources: - [sentence_transformers/losses/BatchHardTripletLoss.py:12-267]() - [sentence_transformers/losses/BatchSemiHardTripletLoss.py:13-188]() - [sentence_transformers/losses/BatchAllTripletLoss.py:13-151]() - [sentence_transformers/losses/BatchHardSoftMarginTripletLoss.py:13-153]() ## MSE-Based Loss Functions These loss functions use Mean Squared Error (MSE) to optimize embeddings against a target. ### MSELoss MSE Loss computes the squared error between computed sentence embeddings and target embeddings.",
  "**Key Properties**: - Often used for knowledge distillation and multilingual model extension - Requires sentences with corresponding target embeddings - Simple and effective for teacher-student learning ```mermaid flowchart TD subgraph \"MSELoss Flow\" Input[\"Input Text\"] --> StudentModel[\"Student Model\"] StudentModel --> Embedding[\"Student Embedding\"] TargetEmb[\"Target Embedding\"] --> MSE[\"MSE Loss\"] Embedding --> MSE end ``` Sources: - [sentence_transformers/losses/MSELoss.py:11-98]() ### MarginMSELoss An extension of MSE loss that focuses on the margin between pairs of passages for a query. **Key Properties**: - Computes MSE between predicted margins and gold margins - More suitable for ranking tasks - Does not require strict positive/negative distinction - Often used with a teacher model in knowledge distillation Sources: - [sentence_transformers/losses/MarginMSELoss.py:10-143]() ### CosineSimilarityLoss Computes cosine similarity between sentence pairs and optimizes against a similarity score. **Key Properties**: - Expects text pairs with a similarity score - Minimizes the difference between predicted and target similarity - Used for Semantic Textual Similarity (STS) tasks Sources: - [sentence_transformers/losses/CosineSimilarityLoss.py:13-85]() ## Specialized Loss Functions ### MatryoshkaLoss A loss function modifier that enables training models to produce effective embeddings at multiple dimensions. This allows users to reduce the embedding dimension at inference time without retraining. **Key Properties**: - Trains on multiple embedding dimensions simultaneously - Allows flexible trade-off between quality and dimensionality at inference time - Compatible with other base losses (wraps another loss function) ```mermaid flowchart TD subgraph \"MatryoshkaLoss Architecture\" BaseL[\"Base Loss Function\"] ML[\"MatryoshkaLoss\"] Dims[\"Multiple Dimensions\"] --> ML Weights[\"Dimension Weights\"] --> ML BaseL --> ML ML --> MDim1[\"Train at Dim 768\"] ML --> MDim2[\"Train at Dim 384\"] ML --> MDim3[\"Train at Dim 128\"] MDim1 --> CombLoss[\"Combined Loss\"] MDim2 --> CombLoss MDim3 --> CombLoss end ``` Sources: - [sentence_transformers/losses/MatryoshkaLoss.py:113-253]() ### AdaptiveLayerLoss Trains model to produce good embeddings with fewer transformer layers, enabling faster inference. **Key Properties**: - Applies loss to intermediate transformer layers - Allows layer reduction at inference time - KL divergence regularization between layer outputs Sources: - [sentence_transformers/losses/AdaptiveLayerLoss.py:106-274]() ### Matryoshka2dLoss Combines MatryoshkaLoss and AdaptiveLayerLoss to enable both dimension and layer reduction. **Key Properties**: - 2D flexibility in both dimensions and layers - Allows for different performance vs. efficiency trade-offs Sources: - [sentence_transformers/losses/Matryoshka2dLoss.py:13-152]() ### DenoisingAutoEncoderLoss Trains a model to reconstruct original sentences from damaged versions, useful for unsupervised learning. **Key Properties**: - Requires pairs of damaged and original sentences - Creates a decoder component to reconstruct from embeddings - Used in TSDAE (Transformer-based Sequential Denoising Auto-Encoder) Sources: - [sentence_transformers/losses/DenoisingAutoEncoderLoss.py:15-203]() ### SoftmaxLoss Used for classification with text pairs, adds a softmax classifier on top of the embedding outputs. **Key Properties**: - Original loss from the SBERT paper - Uses sentence pairs with class labels - Multiple ways to combine embeddings (concatenation, difference, multiplication) Sources: - [sentence_transformers/losses/SoftmaxLoss.py:17-156]() ## Loss Function Selection Guide The table below provides guidance on which loss function to use based on your data and task:",
  "| Loss Function | Best For | Input Format | Special Requirements | |---------------|----------|--------------|---------------------| | MultipleNegativesRankingLoss | Retrieval, general purpose | (anchor, positive) pairs | Large batch size beneficial | | CoSENTLoss | Semantic similarity | Text pairs with scores | - | | TripletLoss | Clustering, similarity | (anchor, positive, negative) triplets | - | | BatchHardTripletLoss | Classification | Single texts with labels | Multiple examples per class | | MSELoss | Distillation, transfer | Texts with target embeddings | Teacher model | | MatryoshkaLoss | Size-efficient models | Depends on base loss | - | | AdaptiveLayerLoss | Speed-efficient models | Depends on base loss | - | | ContrastiveTensionLoss | Unsupervised learning | Single sentences | Special dataloader | | GISTEmbedLoss | Better negative sampling | Same as MNRL | Guide model | ## Memory-Efficient Variants For training with large batch sizes on limited hardware, consider the cached variants: ```mermaid graph TD subgraph \"Memory-Efficient Variants\" MNRL[\"MultipleNegativesRankingLoss\"] --> CMNRL[\"CachedMultipleNegativesRankingLoss\"] MNRSL[\"MultipleNegativesSymmetricRankingLoss\"] --> CMNRSL[\"CachedMultipleNegativesSymmetricRankingLoss\"] GISTL[\"GISTEmbedLoss\"] --> CGISTL[\"CachedGISTEmbedLoss\"] CMNRL -->|\"Uses\"| GC[\"GradCache Technique\"] CMNRSL -->|\"Uses\"| GC CGISTL -->|\"Uses\"| GC GC --> S1[\"1. Embed without gradients\"] GC --> S2[\"2. Calculate loss & cache gradients\"] GC --> S3[\"3. Embed with gradients & apply cached gradients\"] end ``` **Key Properties**: - Allow for much larger batch sizes with constant memory usage - Approximately 20-30% slower than non-cached versions - Recommended when batch size is a limiting factor Sources: - [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:63-300]() - [sentence_transformers/losses/CachedGISTEmbedLoss.py:63-382]() - [sentence_transformers/losses/CachedMultipleNegativesSymmetricRankingLoss.py:40-256]() ## Implementation Details All loss functions in SentenceTransformer follow a common pattern: 1. They are subclasses of `torch.nn.Module` 2. They implement a `forward` method that: - Takes sentence features and optional labels as input - Computes sentence embeddings using the model - Calculates and returns the loss value Many loss functions also provide: - `get_config_dict` method for configuration serialization - `citation` property for academic references - Documentation about input requirements and recommendations The loss function is typically passed to a `SentenceTransformerTrainer` along with the model and dataset, as shown in this example pattern: ```python from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses from datasets import Dataset model = SentenceTransformer(\"model_name\") train_dataset = Dataset.from_dict({ \"anchor\": [\"Text A\", \"Text B\"], \"positive\": [\"Similar to A\", \"Similar to B\"], }) loss = losses.MultipleNegativesRankingLoss(model) trainer = SentenceTransformerTrainer( model=model, train_dataset=train_dataset, loss=loss, ) trainer.train() ``` Sources: - [sentence_transformers/losses/MultipleNegativesRankingLoss.py:13-132]() - [sentence_transformers/losses/__init__.py:1-67]() # Loss Functions for SparseEncoder This document covers the specialized loss functions designed for training `SparseEncoder` models. These losses are specifically tailored for sparse neural information retrieval models like SPLADE and CSR architectures that require both effectiveness and efficiency through sparsity regularization. For dense embedding loss functions, see [Loss Functions for SentenceTransformer](#3.4). For reranking model loss functions, see [Loss Functions for CrossEncoder](#3.6). ## Overview SparseEncoder loss functions follow a hierarchical architecture where wrapper losses combine base contrastive/similarity losses with regularization terms to control sparsity. The two main wrapper losses are: - **`SpladeLoss`**: For SPLADE-style models that use MLM heads with pooling - **`CSRLoss`**: For CSR (Contrastive Sparse Representation) models with autoencoder components These wrapper losses are required for training `SparseEncoder` models, as they provide the necessary sparsity regularization on top of standard contrastive learning objectives. ## Loss Function Architecture",
  "```mermaid graph TD subgraph \"Wrapper Losses\" SpladeLoss[\"SpladeLoss<br/>SPLADE Models\"] CSRLoss[\"CSRLoss<br/>CSR Models\"] end subgraph \"Base Losses\" SMNRL[\"SparseMultipleNegativesRankingLoss\"] SMarginMSE[\"SparseMarginMSELoss\"] SDistillKL[\"SparseDistillKLDivLoss\"] STriplet[\"SparseTripletLoss\"] SCosine[\"SparseCosineSimilarityLoss\"] SCoSENT[\"SparseCoSENTLoss\"] SAnglE[\"SparseAnglELoss\"] SparseMSE[\"SparseMSELoss\"] end subgraph \"Regularization\" FlopsLoss[\"FlopsLoss<br/>Sparsity Regularization\"] CSRReconstruction[\"CSRReconstructionLoss<br/>Autoencoder Reconstruction\"] end SpladeLoss --> SMNRL SpladeLoss --> SMarginMSE SpladeLoss --> SDistillKL SpladeLoss --> STriplet SpladeLoss --> SCosine SpladeLoss --> SCoSENT SpladeLoss --> SAnglE CSRLoss --> SMNRL CSRLoss --> SMarginMSE CSRLoss --> SDistillKL CSRLoss --> STriplet CSRLoss --> SCosine CSRLoss --> SCoSENT CSRLoss --> SAnglE SpladeLoss --> FlopsLoss CSRLoss --> CSRReconstruction SparseMSE -.-> |\"Standalone Only\"| SparseMSE ``` Sources: [sentence_transformers/sparse_encoder/losses/__init__.py:1-29](), [sentence_transformers/sparse_encoder/losses/SpladeLoss.py:15-136](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-188]() ## Wrapper Losses ### SpladeLoss `SpladeLoss` is the primary wrapper loss for SPLADE-style models. It combines a base contrastive loss with regularization terms to encourage sparsity in both query and document representations. **Key Components:** - **Base Loss**: Any SparseEncoder loss except CSR-related and FLOPS losses - **Document Regularizer**: Typically `FlopsLoss` applied to positive documents and negatives - **Query Regularizer**: Typically `FlopsLoss` applied to query embeddings **Configuration Parameters:** - `document_regularizer_weight`: Weight for document sparsity regularization (λ_d) - `query_regularizer_weight`: Weight for query sparsity regularization (λ_q) - `document_regularizer_threshold`: Optional threshold for document regularization - `query_regularizer_threshold`: Optional threshold for query regularization - `use_document_regularizer_only`: Treat all inputs as documents (for symmetric training) ```mermaid graph LR subgraph \"SpladeLoss Forward Pass\" Input[\"sentence_features<br/>List[Dict[str, Tensor]]\"] Model[\"SparseEncoder.forward()\"] Embeddings[\"embeddings<br/>List[Tensor]\"] BaseLoss[\"base_loss.compute_loss_from_embeddings()\"] DocReg[\"document_regularizer.compute_loss_from_embeddings()\"] QueryReg[\"query_regularizer.compute_loss_from_embeddings()\"] Input --> Model Model --> Embeddings Embeddings --> BaseLoss Embeddings --> DocReg Embeddings --> QueryReg BaseLoss --> TotalLoss[\"Combined Loss Dict\"] DocReg --> TotalLoss QueryReg --> TotalLoss end ``` Sources: [sentence_transformers/sparse_encoder/losses/SpladeLoss.py:15-163]() ### CSRLoss `CSRLoss` is designed for CSR (Contrastive Sparse Representation) models that use autoencoder components for reconstruction-based training. **Key Components:** - **Base Loss**: Typically `SparseMultipleNegativesRankingLoss` for contrastive learning - **Reconstruction Loss**: `CSRReconstructionLoss` with three components: - L_k: Reconstruction loss using top-k sparse components - L_4k: Reconstruction loss using top-4k sparse components - L_aux: Auxiliary loss for residual information **Configuration Parameters:** - `beta`: Weight for L_aux component in reconstruction loss - `gamma`: Weight for the main contrastive loss component",
  "```mermaid graph TD subgraph \"CSRLoss Components\" CSRMain[\"CSRLoss\"] MainLoss[\"base_loss<br/>(e.g., SparseMultipleNegativesRankingLoss)\"] ReconLoss[\"CSRReconstructionLoss\"] subgraph \"Reconstruction Components\" LossK[\"L_k: MSE(x, recons_k)\"] Loss4K[\"L_4k: MSE(x, recons_4k)\"] LossAux[\"L_aux: NMSE(recons_aux, residual)\"] end CSRMain --> MainLoss CSRMain --> ReconLoss ReconLoss --> LossK ReconLoss --> Loss4K ReconLoss --> LossAux end ``` Sources: [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-215](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:28-127]() ## Regularization Losses ### FlopsLoss `FlopsLoss` implements sparsity regularization by calculating the squared L2 norm of the mean embedding vector. This encourages more zero values in embeddings, reducing floating-point operations during inference. **Formula**: `torch.sum(torch.mean(embeddings, dim=0) ** 2)` **Key Features:** - Optional threshold parameter to ignore overly sparse embeddings - Used as a component within `SpladeLoss` rather than standalone - Supports L0 masking when threshold is specified ```mermaid graph LR subgraph \"FlopsLoss Computation\" Embeddings[\"embeddings<br/>Tensor[batch, vocab_size]\"] Threshold{\"threshold<br/>!= None?\"} L0Mask[\"L0 Masking<br/>(embeddings != 0).sum()\"] MeanEmb[\"torch.mean(embeddings, dim=0)\"] SquaredNorm[\"torch.sum(mean_emb ** 2)\"] Embeddings --> Threshold Threshold -->|Yes| L0Mask Threshold -->|No| MeanEmb L0Mask --> MeanEmb MeanEmb --> SquaredNorm end ``` Sources: [sentence_transformers/sparse_encoder/losses/FlopsLoss.py:11-54]() ## Base Losses The following base losses can be used within the wrapper losses to provide the main learning signal: | Loss Function | Purpose | Key Features | |---------------|---------|--------------| | `SparseMultipleNegativesRankingLoss` | Contrastive learning with in-batch negatives | Most common base loss, InfoNCE-style | | `SparseMarginMSELoss` | Knowledge distillation with margin | Used with teacher models | | `SparseDistillKLDivLoss` | KL divergence distillation | Probability distribution matching | | `SparseTripletLoss` | Triplet-based contrastive learning | Anchor-positive-negative relationships | | `SparseCosineSimilarityLoss` | Cosine similarity regression | Direct similarity score prediction | | `SparseCoSENTLoss` | Cosine sentence embeddings | Variant of cosine similarity | | `SparseAnglELoss` | Angle-based similarity | Complex number representation | | `SparseMSELoss` | Direct embedding distillation | **Standalone only** - no wrapper needed | **Important Note**: `SparseMSELoss` is the only loss that can be used independently without a wrapper, as it performs direct embedding-level distillation from a teacher model. Sources: [sentence_transformers/sparse_encoder/losses/__init__.py:15-28](), [docs/package_reference/sparse_encoder/losses.md:8-10]() ## Usage Patterns ### Training with SpladeLoss ```python # Typical SPLADE training setup student_model = SparseEncoder(\"distilbert/distilbert-base-uncased\") teacher_model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") loss = SpladeLoss( model=student_model, loss=SparseMarginMSELoss(student_model), document_regularizer_weight=3e-5, query_regularizer_weight=5e-5, ) ``` ### Training with CSRLoss ```python",
  "model = SentenceTransformer( \"all-mpnet-base-v2\", device=\"cuda\", truncate_dim=512, # Reduce dimensions model_kwargs={\"torch_dtype\": \"float16\"} # Memory optimization ) ``` The `SentenceTransformer.__init__()` method in [sentence_transformers/SentenceTransformer.py:167-187]() handles model loading with various configuration options. ### Encoding Methods ```mermaid graph LR subgraph \"Encoding Methods\" GenEnc[\"encode()\"] --> Text1[\"General text encoding\"] QueryEnc[\"encode_query()\"] --> Text2[\"Query-optimized encoding\"] DocEnc[\"encode_document()\"] --> Text3[\"Document-optimized encoding\"] end subgraph \"Internal Flow\" Input[\"Input Text\"] --> Prompt[\"Apply Prompts\"] Prompt --> Task[\"Task Routing\"] Task --> Modules[\"Sequential Modules\"] Modules --> Embeddings[\"Dense Embeddings\"] end subgraph \"Configuration\" Prompts[\"model.prompts\"] --> QueryEnc DefaultPrompt[\"model.default_prompt_name\"] --> GenEnc TaskType[\"task parameter\"] --> Router[\"Router Module\"] end ``` **Method Selection**: - `encode()`: General-purpose encoding for similarity tasks - `encode_query()`: Optimized for search queries, applies \"query\" prompt if available - `encode_document()`: Optimized for documents, applies \"document\"/\"passage\"/\"corpus\" prompts Sources: [sentence_transformers/SentenceTransformer.py:416-543](), [sentence_transformers/SentenceTransformer.py:545-675]() ### Advanced Configuration **Prompts and Task Types**: Many models support prompts for different use cases: ```python",
  "model.prompts = {\"query\": \"query: \", \"document\": \"passage: \"} query_emb = model.encode_query(\"What is AI?\") doc_emb = model.encode_document(\"AI is artificial intelligence\") ``` **Sources:** [sentence_transformers/SentenceTransformer.py:61-407](), [sentence_transformers/SentenceTransformer.py:416-675](), [tests/test_sentence_transformer.py:309-346]() ## SparseEncoder The `SparseEncoder` class extends `SentenceTransformer` to produce sparse vector representations where most dimensions are zero. This architecture is particularly effective for lexical matching and hybrid retrieval scenarios. ### Sparse Architecture Components ```mermaid graph TB subgraph \"SparseEncoder Components\" MLMTransformer[\"MLMTransformer<br/>Token-level predictions\"] SpladePooling[\"SpladePooling<br/>Sparsification\"] SparseAutoEncoder[\"SparseAutoEncoder<br/>k-sparse activation\"] Router[\"Router<br/>Query/Document paths\"] end subgraph \"Output Processing\" ActiveDims[\"max_active_dims<br/>Sparsity control\"] SparseOutput[\"Sparse COO Tensor<br/>[batch_size, vocab_size]\"] end Input[\"Text\"] --> Router Router --> MLMTransformer MLMTransformer --> SpladePooling SpladePooling --> ActiveDims ActiveDims --> SparseOutput ``` ### Key Differences from SentenceTransformer - **Vocabulary-Sized Output**: Embeddings have dimensions equal to tokenizer vocabulary size - **Sparsity Control**: `max_active_dims` parameter limits non-zero dimensions - **Sparse Tensor Format**: Outputs can be sparse COO tensors for memory efficiency - **Term Importance**: Non-zero values represent importance of vocabulary terms ### Decoding and Interpretation The `SparseEncoder` provides a `decode()` method to interpret sparse embeddings as weighted vocabulary terms: ```python model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") embeddings = model.encode(\"machine learning\") tokens_weights = model.decode(embeddings, top_k=10)",
  "router = Router( sub_modules={ \"query\": [efficient_module], \"document\": [contextual_module, pooling_module] }, default_route=\"document\" ) ``` Key attributes: - `sub_modules`: `nn.ModuleDict` mapping task types to `nn.Sequential` module chains - `default_route`: Default task when `task` not specified in features - `allow_empty_key`: Whether to allow no default route - `forward_kwargs`: List of kwargs forwarded to modules (includes `\"task\"`) Sources: [sentence_transformers/models/Router.py:22-418](), [sentence_transformers/models/Router.py:187-215](), [sentence_transformers/models/Router.py:217-245]() ## Module Composition Examples ### Dense Embedding Model (SentenceTransformer) ```mermaid graph LR Text[\"Input Text\"] --> Transformer[\"Transformer<br/>(bert-base-uncased)\"] Transformer --> Pooling[\"Pooling<br/>(mean pooling)\"] Pooling --> Normalize[\"Normalize<br/>(L2 normalization)\"] Normalize --> Embedding[\"Dense Embedding<br/>(768-dim vector)\"] subgraph \"modules[0]\" Transformer end subgraph \"modules[1]\" Pooling end subgraph \"modules[2]\" Normalize end ``` ### Asymmetric Model with Router #### Asymmetric SparseEncoder Architecture ```mermaid graph TD EncodeQuery[\"model.encode_query()\"] --> RouterQuery[\"Router(task='query')\"] EncodeDoc[\"model.encode_document()\"] --> RouterDoc[\"Router(task='document')\"] RouterQuery --> QueryPath[\"sub_modules['query']\"] RouterDoc --> DocPath[\"sub_modules['document']\"] QueryPath --> StaticEmb[\"SparseStaticEmbedding<br/>Pre-computed static weights\"] DocPath --> MLMTrans[\"MLMTransformer<br/>Contextual MLM head\"] StaticEmb --> QueryEmb[\"Sparse Query Embedding<br/>(efficient)\"] MLMTrans --> SpladePool[\"SpladePooling<br/>max + log1p_relu activation\"] SpladePool --> DocEmb[\"Sparse Document Embedding<br/>(contextual)\"] subgraph RouterModule[\"Router Module\"] QueryPath DocPath StaticEmb MLMTrans SpladePool end ``` #### Router Training Requirements ```python # Training args must specify router_mapping args = SparseEncoderTrainingArguments( router_mapping={ \"question\": \"query\", # Dataset column -> router task \"positive\": \"document\", \"negative\": \"document\" } ) # Data collator uses mapping to set task in features collator = SparseEncoderDataCollator( tokenize_fn=model.tokenize, router_mapping=args.router_mapping ) ``` Sources: [sentence_transformers/models/Router.py:104-156](), [sentence_transformers/sparse_encoder/trainer.py:180-186](), [sentence_transformers/sparse_encoder/data_collator.py:55-68]() ### Training Pipeline Integration ```mermaid flowchart TD Dataset[\"Training Dataset\"] --> Collator[\"SentenceTransformerDataCollator\"] Collator --> RouterMap{router_mapping?} RouterMap -->|Yes| TaskRoute[\"Add task to features<br/>based on column mapping\"] RouterMap -->|No| DirectProcess[\"Process normally\"] TaskRoute --> Model[\"Model.forward()\"] DirectProcess --> Model Model --> Features[\"Processed Features\"] Features --> Loss[\"Loss Function<br/>(MultipleNegativesRankingLoss, etc.)\"] ``` Sources: [sentence_transformers/trainer.py:198-204](), [sentence_transformers/data_collator.py:55-68]() ## Module Loading and Saving ### Module Configuration System Each module uses a configuration system for persistence: ```mermaid graph TD Module[\"Module Instance\"] --> Config[\"get_config_dict()\"] Config --> Keys[\"config_keys<br/>['param1', 'param2']\"] Keys --> JSON[\"config.json<br/>{param1: value1, param2: value2}\"] subgraph \"Save Process\" Config Keys JSON end LoadJSON[\"config.json\"] --> LoadConfig[\"load_config()\"] LoadConfig --> LoadModule[\"Module.load()\"] LoadModule --> NewInstance[\"Module Instance\"] subgraph \"Load Process\" LoadJSON LoadConfig LoadModule NewInstance end ``` Key configuration attributes: - `config_keys`: List of attributes to save/load - `config_file_name`: Name of config file (usually `\"config.json\"`) - `save_in_root`: Whether to save in model root or subfolder ### Model Directory Structure",
  "```mermaid graph TD ModelDir[\"model_directory/\"] --> ModulesJSON[\"modules.json<br/>(module metadata)\"] ModelDir --> ConfigST[\"config_sentence_transformers.json<br/>(model-level config)\"] ModelDir --> Module0[\"0_Transformer/<br/>(or root if save_in_root=True)\"] ModelDir --> Module1[\"1_Pooling/\"] ModelDir --> Module2[\"2_Normalize/\"] Module0 --> TransConfig[\"sentence_bert_config.json\"] Module0 --> ModelFiles[\"model.safetensors<br/>tokenizer files\"] Module1 --> PoolConfig[\"config.json\"] Module2 --> NormConfig[\"(empty - no config needed)\"] ``` The `modules.json` file contains metadata about each module: | Field | Description | |-------|-------------| | `idx` | Module index in pipeline | | `name` | Module identifier | | `path` | Directory path relative to model root | | `type` | Full Python class path | Sources: [docs/sentence_transformer/usage/custom_models.rst:43-101]() ## Training Integration ### Data Flow in Training ```mermaid sequenceDiagram participant Dataset participant Collator as SentenceTransformerDataCollator participant Model as Model Pipeline participant Loss as Loss Function Dataset->>Collator: Raw text columns Note over Collator: Apply prompts, router_mapping Collator->>Model: Tokenized features + task info loop For each module Model->>Model: module.forward(features) Note over Model: Update features dict end Model->>Loss: Final features Loss->>Loss: Compute loss value ``` ### Router Training Requirements When using `Router` modules, additional training configuration is required: ```python",
  "pytest tests/ --cov=sentence_transformers ``` ### Test Environment Variables | Variable | Purpose | Default | |----------|---------|---------| | `CI` | Indicates CI environment, skips slow tests | `None` | | Cache directories | Managed by `cache_dir` fixture | Temporary in CI | Sources: [tests/conftest.py:108-114](), [tests/test_train_stsb.py:106-110]() ## Documentation Development The documentation system uses Sphinx with custom extensions and styling. ### Documentation Build Configuration ```mermaid graph TB subgraph \"Sphinx Configuration\" ConfPy[\"docs/conf.py\"] Extensions[\"Extensions:<br/>napoleon, autodoc,<br/>myst_parser, mermaid\"] Theme[\"sphinx_rtd_theme\"] end subgraph \"Custom Assets\" CSS[\"docs/_static/css/custom.css\"] JS[\"docs/_static/js/custom.js\"] Images[\"docs/_static/img/\"] end subgraph \"Content Sources\" Markdown[\"*.md files\"] Docstrings[\"Python docstrings\"] Examples[\"examples/ directory\"] end ConfPy --> Extensions ConfPy --> Theme Extensions --> CSS Extensions --> JS Markdown --> Content[\"Generated Documentation\"] Docstrings --> Content Examples --> Content ``` ### Key Documentation Extensions | Extension | Purpose | |-----------|---------| | `sphinx.ext.napoleon` | Google/NumPy docstring parsing | | `sphinx.ext.autodoc` | Automatic API documentation | | `myst_parser` | Markdown support | | `sphinxcontrib.mermaid` | Diagram rendering | | `sphinx.ext.linkcode` | Source code linking | Sources: [docs/conf.py:38-49]() ### Custom Documentation Features The documentation includes custom JavaScript for GitHub integration and enhanced styling: **GitHub Integration:** ```javascript // From custom.js - adds GitHub star button function addGithubButton() { // Inserts GitHub star button and Hugging Face link } ``` **Custom Styling:** - Responsive layout with max-width 1280px - Component boxes with gradient headers - Training arguments grid layout - Enhanced navigation styling Sources: [docs/_static/js/custom.js:1-39](), [docs/_static/css/custom.css:1-124]() ## Development Environment Setup ### Prerequisites ```bash # Install development dependencies pip install -e \".[dev,train]\" # For documentation building pip install sphinx sphinx-rtd-theme myst-parser sphinxcontrib-mermaid ``` ### Code Quality Tools The project maintains code quality through: - **Type Hints**: Extensive use of type annotations with `from __future__ import annotations` - **Import Organization**: Consistent import ordering and structure - **Documentation Standards**: Comprehensive docstrings and examples ### Testing Best Practices 1. **Fixture Usage**: Use appropriate fixture scopes to minimize resource usage 2. **Test Isolation**: Each test should be independent and reproducible 3. **Performance Thresholds**: Set realistic but strict performance expectations 4. **Resource Management**: Use temporary directories and cleanup in CI environments Sources: [tests/conftest.py:1-115](), [tests/test_pretrained_stsb.py:1-145]() ## Continuous Integration Considerations The testing framework is designed to work efficiently in CI environments: ### CI Optimizations ```mermaid graph LR subgraph \"CI Environment Detection\" EnvCheck[\"os.environ.get('CI')\"] TempDir[\"SafeTemporaryDirectory\"] CacheDir[\"Temporary cache_dir\"] end subgraph \"Test Adaptations\" SkipSlow[\"Skip slow tests\"] LimitSamples[\"Limit test samples\"] TempStorage[\"Use temporary storage\"] end EnvCheck --> SkipSlow TempDir --> TempStorage CacheDir --> TempStorage SkipSlow --> FastExecution[\"Fast CI execution\"] TempStorage --> FastExecution ``` ### Memory and Storage Management - **Temporary Directories**: CI tests use temporary directories that are automatically cleaned up - **Model Caching**: Session-scoped fixtures prevent redundant model loading - **Sample Limiting**: Tests can run with reduced datasets in CI environments Sources: [tests/conftest.py:108-114](), [tests/test_train_stsb.py:106-163]()",
  "``` **Sources:** [sentence_transformers/sparse_encoder/SparseEncoder.py:27-180](), [sentence_transformers/sparse_encoder/models/MLMTransformer.py](), [sentence_transformers/sparse_encoder/models/SpladePooling.py](), [tests/sparse_encoder/test_sparse_encoder.py:15-169]() ## CrossEncoder The `CrossEncoder` architecture differs fundamentally from the other two by taking pairs of texts as input and producing similarity scores rather than individual embeddings. ### CrossEncoder Pipeline ```mermaid graph TB subgraph \"CrossEncoder Architecture\" Pairs[\"Text Pairs<br/>[(query, document), ...]\"] TokenizerCE[\"AutoTokenizer<br/>Joint encoding\"] ModelCE[\"AutoModelForSequenceClassification\"] Activation[\"Activation Function<br/>(Sigmoid/Identity)\"] ScoresOut[\"Similarity Scores\"] end subgraph \"Key Methods\" Predict[\"predict()<br/>Score pairs\"] Rank[\"rank()<br/>Rank documents\"] end Pairs --> TokenizerCE TokenizerCE --> ModelCE ModelCE --> Activation Activation --> ScoresOut ScoresOut --> Predict ScoresOut --> Rank ``` ### Architecture Characteristics - **No Individual Embeddings**: Cannot encode single texts independently - **Joint Processing**: Both texts processed together through transformer layers - **Classification Head**: Uses sequence classification architecture - **Configurable Labels**: Supports regression (`num_labels=1`) or multi-class classification - **Higher Accuracy**: Generally more accurate than bi-encoder approaches for pairwise tasks ### Usage in Retrieve-Rerank Pipeline ```python",
  "This section covers advanced features and development topics for power users and contributors working with the sentence-transformers library. It focuses on sophisticated functionality that extends beyond basic model usage, including automatic model documentation generation, performance optimization techniques, and development infrastructure. For basic model training concepts, see [Training](#3). For standard evaluation procedures, see [Evaluation](#4). For application integration patterns, see [Applications](#6). ## Model Card Generation System The sentence-transformers library includes a comprehensive automatic model card generation system that creates detailed documentation during training. This system captures training metadata, dataset information, evaluation metrics, and generates standardized model cards for sharing on the Hugging Face Hub. ### Model Card Data Architecture The model card system is built around specialized data classes that automatically collect and organize model information: ```mermaid graph TB subgraph \"Base Classes\" STMCD[\"SentenceTransformerModelCardData\"] STMCC[\"SentenceTransformerModelCardCallback\"] end subgraph \"Model-Specific Implementations\" SEMCD[\"SparseEncoderModelCardData\"] CEMCD[\"CrossEncoderModelCardData\"] SEMCC[\"SparseEncoderModelCardCallback\"] CEMCC[\"CrossEncoderModelCardCallback\"] end subgraph \"Template System\" STTemplate[\"model_card_template.md\"] SETemplate[\"sparse_encoder/model_card_template.md\"] CETemplate[\"cross_encoder/model_card_template.md\"] end subgraph \"Integration Points\" Trainer[\"SentenceTransformerTrainer\"] Model[\"SentenceTransformer/SparseEncoder/CrossEncoder\"] HFHub[\"Hugging Face Hub\"] end STMCD --> SEMCD STMCD --> CEMCD STMCC --> SEMCC STMCC --> CEMCC STMCD --> STTemplate SEMCD --> SETemplate CEMCD --> CETemplate STMCC --> Trainer Model --> STMCD STTemplate --> HFHub ``` Sources: [sentence_transformers/model_card.py:265-359](), [sentence_transformers/sparse_encoder/model_card.py:22-86](), [sentence_transformers/cross_encoder/model_card.py:27-89]() ### Automatic Data Collection During Training The model card callback system integrates with the training process to automatically capture relevant information: ```mermaid graph LR subgraph \"Training Lifecycle\" Init[\"on_init_end\"] TrainBegin[\"on_train_begin\"] Evaluate[\"on_evaluate\"] Log[\"on_log\"] end subgraph \"Data Collection\" DatasetMeta[\"extract_dataset_metadata\"] LossInfo[\"set_losses\"] Hyperparams[\"hyperparameters\"] Metrics[\"training_logs\"] Examples[\"set_widget_examples\"] end subgraph \"Model Card Data\" TrainDatasets[\"train_datasets\"] EvalDatasets[\"eval_datasets\"] Citations[\"citations\"] TrainingLogs[\"training_logs\"] Widget[\"widget\"] end Init --> DatasetMeta Init --> LossInfo Init --> Examples TrainBegin --> Hyperparams Evaluate --> Metrics Log --> Metrics DatasetMeta --> TrainDatasets DatasetMeta --> EvalDatasets LossInfo --> Citations Hyperparams --> TrainingLogs Metrics --> TrainingLogs Examples --> Widget ``` Sources: [sentence_transformers/model_card.py:47-199](), [sentence_transformers/model_card.py:445-570]() ### Dataset Metadata Extraction The system automatically analyzes training and evaluation datasets to generate comprehensive statistics and examples: | Metadata Type | Information Collected | Implementation | |---------------|----------------------|----------------| | **Size & Structure** | Dataset size, column names, data types | `compute_dataset_metrics` | | **Content Statistics** | Token/character counts, value distributions | Statistical analysis per column | | **Hub Integration** | Dataset ID, revision, download checksums | `extract_dataset_metadata` | | **Sample Examples** | Representative examples for documentation | First 3 samples with formatting | | **Loss Configuration** | Loss function details and parameters | `get_config_dict` introspection | The dataset analysis includes automatic tokenization to provide meaningful statistics: ```python",
  "This document covers specialized pretrained models trained on the MS MARCO dataset for passage retrieval tasks. These models are optimized for semantic search scenarios where queries need to be matched against relevant passages. For general dense embedding models, see [SentenceTransformer Models](#5.1). For cross-encoder models trained on other datasets, see [CrossEncoder Models](#5.3). ## Dataset Overview MS MARCO (Microsoft Machine Reading Comprehension) is a large-scale information retrieval corpus created from real user search queries using the Bing search engine. The dataset consists of: - **Training data**: Over 500,000 query-passage examples - **Complete corpus**: Over 8.8 million passages - **Evaluation**: TREC Deep Learning 2019 and MS MARCO Passage Retrieval datasets - **Task type**: Asymmetric semantic search (short queries → longer passages) The dataset enables training models that can find semantically relevant passages given natural language queries, making it ideal for search and question-answering applications. **Sources:** [docs/pretrained-models/msmarco-v3.md:1-5](), [docs/pretrained-models/ce-msmarco.md:1-6]() ## Model Architecture Types MS MARCO models are available across multiple architectures, each optimized for different use cases in the retrieval pipeline: ### Code Entity Mapping ```mermaid graph TB subgraph \"sentence_transformers Classes\" ST[SentenceTransformer] CE[CrossEncoder] ST_INIT[\"SentenceTransformer(__init__)\"] ST_ENCODE[\"encode()\"] CE_INIT[\"CrossEncoder(__init__)\"] CE_PREDICT[\"predict()\"] ST --> ST_INIT ST --> ST_ENCODE CE --> CE_INIT CE --> CE_PREDICT end subgraph \"Utility Functions\" UTIL_COS[\"util.cos_sim()\"] UTIL_DOT[\"util.dot_score()\"] TORCH_SIG[\"torch.nn.Sigmoid()\"] end subgraph \"Model Identifiers\" DENSE_MODELS[\"msmarco-distilbert-base-v4<br/>msmarco-MiniLM-L6-v3<br/>msmarco-roberta-base-v3<br/>msmarco-distilbert-base-tas-b\"] CE_MODELS[\"cross-encoder/ms-marco-MiniLM-L6-v2<br/>cross-encoder/ms-marco-electra-base<br/>cross-encoder/ms-marco-TinyBERT-L2-v2\"] end ST_INIT --> DENSE_MODELS CE_INIT --> CE_MODELS CE_INIT --> TORCH_SIG ST_ENCODE --> UTIL_COS ST_ENCODE --> UTIL_DOT CE_PREDICT --> SCORES[\"Relevance Scores Array\"] ``` ### Training Components ```mermaid graph LR subgraph \"Loss Functions\" MSE_LOSS[\"MultipleNegativesRankingLoss\"] COSENT_LOSS[\"CoSENTLoss\"] MARGIN_MSE[\"MarginMSELoss\"] end subgraph \"Evaluators\" IR_EVAL[\"InformationRetrievalEvaluator\"] EMB_EVAL[\"EmbeddingSimilarityEvaluator\"] CE_EVAL[\"CrossEncoderReranking\"] end subgraph \"Datasets\" MSMARCO_TRAIN[\"sentence-transformers/msmarco-*\"] TREC_DL[\"TREC Deep Learning 2019\"] MSMARCO_DEV[\"MS MARCO Dev Set\"] end MSE_LOSS --> MSMARCO_TRAIN MARGIN_MSE --> MSMARCO_TRAIN IR_EVAL --> TREC_DL IR_EVAL --> MSMARCO_DEV CE_EVAL --> TREC_DL ``` **Sources:** [docs/pretrained-models/msmarco-v3.md:6-16](), [docs/cross_encoder/pretrained_models.md:27-44]() ## Dense Embedding Models Dense embedding models use the `SentenceTransformer` class and encode queries and passages into dense vector representations for efficient similarity search: ### Cosine Similarity Models These models are optimized for cosine similarity computation and tend to prefer shorter, more focused passages:",
  "| Model Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Queries/sec (GPU/CPU) | Docs/sec (GPU/CPU) | |------------|---------------------|----------------------|---------------------|-------------------| | `msmarco-MiniLM-L6-v3` | 67.46 | 32.27 | 18,000 / 750 | 2,800 / 180 | | `msmarco-MiniLM-L12-v3` | 65.14 | 32.75 | 11,000 / 400 | 1,500 / 90 | | `msmarco-distilbert-base-v3` | 69.02 | 33.13 | 7,000 / 350 | 1,100 / 70 | | `msmarco-distilbert-base-v4` | **70.24** | **33.79** | 7,000 / 350 | 1,100 / 70 | | `msmarco-roberta-base-v3` | 69.08 | 33.01 | 4,000 / 170 | 540 / 30 | ### Dot Product Models These models are optimized for dot product similarity and tend to prefer longer, more comprehensive passages: | Model Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Queries/sec (GPU/CPU) | Docs/sec (GPU/CPU) | |------------|---------------------|----------------------|---------------------|-------------------| | `msmarco-distilbert-base-dot-prod-v3` | 68.42 | 33.04 | 7,000 / 350 | 1,100 / 70 | | `msmarco-roberta-base-ance-firstp` | 67.84 | 33.01 | 4,000 / 170 | 540 / 30 | | `msmarco-distilbert-base-tas-b` | **71.04** | **34.43** | 7,000 / 350 | 1,100 / 70 | **Sources:** [docs/pretrained-models/msmarco-v3.md:27-44]() ## Cross-Encoder Models Cross-encoder models use the `CrossEncoder` class for reranking retrieved passages. They process query-passage pairs jointly and output relevance scores: | Model Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Docs/sec | |------------|---------------------|----------------------|----------| | `cross-encoder/ms-marco-TinyBERT-L2-v2` | 69.84 | 32.56 | 9,000 | | `cross-encoder/ms-marco-MiniLM-L2-v2` | 71.01 | 34.85 | 4,100 | | `cross-encoder/ms-marco-MiniLM-L4-v2` | 73.04 | 37.70 | 2,500 | | `cross-encoder/ms-marco-MiniLM-L6-v2` | **74.30** | **39.01** | 1,800 | | `cross-encoder/ms-marco-MiniLM-L12-v2` | 74.31 | 39.02 | 960 | | `cross-encoder/ms-marco-electra-base` | 71.99 | 36.41 | 340 | **Sources:** [docs/cross_encoder/pretrained_models.md:35-43](), [docs/pretrained-models/ce-msmarco.md:41-48]() ## Usage Examples ### Dense Embedding Model Usage ```python from sentence_transformers import SentenceTransformer, util",
  "This page covers performance optimization techniques available in sentence-transformers, including multi-processing for distributed encoding, backend optimization with ONNX and OpenVINO, quantization strategies, and hardware acceleration. These optimization methods can significantly improve inference speed and reduce memory usage when working with large datasets or resource-constrained environments. For general model usage and basic inference, see the quickstart guide in [2.1](#2.1). For training optimization techniques, see [3.7](#3.7). ## Multi-Processing Architecture The sentence-transformers library provides built-in multi-processing capabilities that allow distributing encoding tasks across multiple devices or CPU processes. This is particularly useful when processing large datasets or when multiple GPUs are available. ```mermaid graph TD MainProcess[\"Main Process<br/>SentenceTransformer\"] --> ProcessPool[\"Multi-Process Pool<br/>start_multi_process_pool()\"] ProcessPool --> Worker1[\"Worker Process 1<br/>Device: cuda:0\"] ProcessPool --> Worker2[\"Worker Process 2<br/>Device: cuda:1\"] ProcessPool --> Worker3[\"Worker Process 3<br/>Device: cpu\"] MainProcess --> ChunkDistribution[\"Data Chunking<br/>chunk_size parameter\"] ChunkDistribution --> Worker1 ChunkDistribution --> Worker2 ChunkDistribution --> Worker3 Worker1 --> Results1[\"Partial Results 1\"] Worker2 --> Results2[\"Partial Results 2\"] Worker3 --> Results3[\"Partial Results 3\"] Results1 --> Aggregation[\"Result Aggregation<br/>_encode_multi_process()\"] Results2 --> Aggregation Results3 --> Aggregation Aggregation --> FinalOutput[\"Final Embeddings<br/>numpy.ndarray or torch.Tensor\"] ``` **Sources:** [sentence_transformers/SentenceTransformer.py:1046-1158](), [tests/test_multi_process.py:14-42]() ### Pool Management Multi-processing pools are managed through dedicated methods that handle worker process lifecycle: | Method | Purpose | Returns | |--------|---------|---------| | `start_multi_process_pool(target_devices)` | Creates worker processes for specified devices | Pool dictionary | | `stop_multi_process_pool(pool)` | Terminates worker processes and cleanup | None | | `encode(..., pool=pool)` | Uses existing pool for encoding | Embeddings | | `encode(..., device=[\"cuda:0\", \"cuda:1\"])` | Auto-creates temporary pool | Embeddings | **Sources:** [sentence_transformers/SentenceTransformer.py:1915-1970](), [tests/test_multi_process.py:61-81]() ### Distributed Encoding Process ```mermaid sequenceDiagram participant Client as \"Client Code\" participant ST as \"SentenceTransformer\" participant Pool as \"Process Pool\" participant W1 as \"Worker 1\" participant W2 as \"Worker 2\" Client->>ST: encode(sentences, device=[\"cuda:0\", \"cuda:1\"]) ST->>ST: _encode_multi_process() ST->>Pool: Create temporary pool Pool->>W1: Initialize on cuda:0 Pool->>W2: Initialize on cuda:1 ST->>ST: Split sentences into chunks ST->>W1: Send chunk 1 ST->>W2: Send chunk 2 W1->>W1: Process batch W2->>W2: Process batch W1->>ST: Return embeddings 1 W2->>ST: Return embeddings 2 ST->>ST: Concatenate results ST->>Pool: Cleanup processes ST->>Client: Return final embeddings ``` **Sources:** [sentence_transformers/SentenceTransformer.py:1077-1158](), [sentence_transformers/sparse_encoder/SparseEncoder.py:514-532]() ## Backend Optimization Sentence-transformers supports multiple inference backends beyond PyTorch, enabling significant performance improvements for production deployments. ### Backend Architecture",
  "```mermaid graph LR Model[\"Model Loading\"] --> BackendChoice{\"Backend Selection\"} BackendChoice -->|\"backend='torch'\"| PyTorchBackend[\"PyTorch Backend<br/>AutoModel.from_pretrained()\"] BackendChoice -->|\"backend='onnx'\"| ONNXBackend[\"ONNX Backend<br/>load_onnx_model()\"] BackendChoice -->|\"backend='openvino'\"| OpenVINOBackend[\"OpenVINO Backend<br/>load_openvino_model()\"] PyTorchBackend --> PyTorchInference[\"PyTorch Inference<br/>model(**features)\"] ONNXBackend --> ONNXInference[\"ONNX Runtime<br/>session.run()\"] OpenVINOBackend --> OpenVINOInference[\"OpenVINO Runtime<br/>compiled_model()\"] PyTorchInference --> Output[\"Embeddings Output\"] ONNXInference --> Output OpenVINOInference --> Output ``` **Sources:** [sentence_transformers/models/Transformer.py:173-203](), [sentence_transformers/cross_encoder/CrossEncoder.py:236-257]() ### Backend Configuration Each model type supports backend selection through the `backend` parameter: | Model Type | Backend Support | Configuration | |------------|----------------|---------------| | `SentenceTransformer` | torch, onnx, openvino | `SentenceTransformer(model_name, backend=\"onnx\")` | | `SparseEncoder` | torch, onnx, openvino | `SparseEncoder(model_name, backend=\"openvino\")` | | `CrossEncoder` | torch, onnx, openvino | `CrossEncoder(model_name, backend=\"torch\")` | **Sources:** [sentence_transformers/SentenceTransformer.py:186](), [sentence_transformers/sparse_encoder/SparseEncoder.py:151](), [sentence_transformers/cross_encoder/CrossEncoder.py:135]() ### Backend-Specific Parameters Additional configuration options are available through `model_kwargs`: ```python # ONNX provider selection model = SentenceTransformer( \"model-name\", backend=\"onnx\", model_kwargs={\"provider\": \"CUDAExecutionProvider\"} ) # Optimized model file selection model = SentenceTransformer( \"model-name\", backend=\"openvino\", model_kwargs={\"file_name\": \"model_optimized.xml\"} ) # Auto-export control model = SentenceTransformer( \"model-name\", backend=\"onnx\", model_kwargs={\"export\": True} ) ``` **Sources:** [sentence_transformers/SentenceTransformer.py:113-119](), [sentence_transformers/backend.py]() ## Quantization and Precision The library provides multiple quantization strategies to reduce memory usage and improve inference speed with minimal accuracy loss. ### Quantization Pipeline ```mermaid graph TD FloatEmbeddings[\"Float32 Embeddings<br/>model.encode()\"] --> QuantizationChoice{\"Precision Parameter\"} QuantizationChoice -->|\"precision='float32'\"| Float32[\"Float32 Output<br/>No quantization\"] QuantizationChoice -->|\"precision='int8'\"| Int8Quant[\"INT8 Quantization<br/>quantize_embeddings()\"] QuantizationChoice -->|\"precision='uint8'\"| UInt8Quant[\"UINT8 Quantization<br/>quantize_embeddings()\"] QuantizationChoice -->|\"precision='binary'\"| BinaryQuant[\"Binary Quantization<br/>quantize_embeddings()\"] QuantizationChoice -->|\"precision='ubinary'\"| UBinaryQuant[\"Unsigned Binary<br/>quantize_embeddings()\"] Int8Quant --> MemoryReduction[\"Memory Usage<br/>4x reduction\"] UInt8Quant --> MemoryReduction BinaryQuant --> MemoryReduction2[\"Memory Usage<br/>32x reduction\"] UBinaryQuant --> MemoryReduction2 ``` **Sources:** [sentence_transformers/SentenceTransformer.py:424](), [sentence_transformers/quantization.py]() ### Precision Performance Characteristics | Precision | Memory Factor | Speed Factor | Use Case | |-----------|---------------|--------------|----------| | `float32` | 1x | 1x | Highest accuracy | | `int8` | 4x smaller | ~2x faster | Balanced accuracy/speed | | `uint8` | 4x smaller | ~2x faster | Positive-only embeddings | | `binary` | 32x smaller | ~10x faster | Similarity search | | `ubinary` | 32x smaller | ~10x faster | Unsigned binary encoding | **Sources:** [sentence_transformers/quantization.py:15-142](), [sentence_transformers/SentenceTransformer.py:469-473]() ## Hardware Acceleration ### Device Management The library automatically detects and utilizes available hardware acceleration:",
  "```mermaid graph TD DeviceDetection[\"Device Detection<br/>get_device_name()\"] --> AvailableCheck{\"Hardware Available?\"} AvailableCheck -->|\"torch.cuda.is_available()\"| CUDA[\"CUDA Devices<br/>cuda:0, cuda:1, ...\"] AvailableCheck -->|\"torch.backends.mps.is_available()\"| MPS[\"Apple MPS<br/>mps device\"] AvailableCheck -->|\"is_torch_npu_available()\"| NPU[\"Neural Processing Unit<br/>npu device\"] AvailableCheck -->|\"HPU available\"| HPU[\"Habana HPU<br/>hpu device\"] AvailableCheck -->|\"Default\"| CPU[\"CPU Fallback<br/>cpu device\"] CUDA --> OptimumHabana[\"Optimum Habana<br/>adapt_transformers_to_gaudi()\"] NPU --> OptimumHabana HPU --> OptimumHabana ``` **Sources:** [sentence_transformers/SentenceTransformer.py:217-224](), [sentence_transformers/util.py:47-77]() ### Mixed Precision Support Hardware acceleration includes mixed precision training and inference: | `torch_dtype` | Description | Memory Savings | |---------------|-------------|----------------| | `\"auto\"` | Use model's default dtype | Varies | | `torch.float16` | Half precision | 50% reduction | | `torch.bfloat16` | Brain floating point | 50% reduction | | `torch.float32` | Full precision | Baseline | **Sources:** [sentence_transformers/SentenceTransformer.py:95-106](), [tests/test_sentence_transformer.py:96-106]() ## Memory Optimization Strategies ### Efficient Encoding Parameters Several parameters help optimize memory usage during encoding: ```mermaid graph LR Input[\"Large Dataset\"] --> BatchSize[\"batch_size<br/>Control memory per batch\"] BatchSize --> ChunkSize[\"chunk_size<br/>Multi-process distribution\"] ChunkSize --> TruncateDim[\"truncate_dim<br/>Matryoshka model truncation\"] TruncateDim --> Precision[\"precision<br/>Quantization strategy\"] Precision --> Output[\"Optimized Embeddings\"] TruncateDim --> MatryoshkaNote[\"Matryoshka Models<br/>Maintain quality with<br/>reduced dimensions\"] ``` **Sources:** [sentence_transformers/SentenceTransformer.py:485-491](), [sentence_transformers/util.py:436-455]() ### Sparse Encoding Optimization For `SparseEncoder` models, additional memory optimizations are available: | Parameter | Effect | Usage | |-----------|--------|-------| | `max_active_dims` | Limits non-zero dimensions | Reduce memory and computation | | `convert_to_sparse_tensor` | Use sparse tensor format | Memory efficient storage | | `save_to_cpu` | Move results to CPU | Free GPU memory | **Sources:** [sentence_transformers/sparse_encoder/SparseEncoder.py:192](), [sentence_transformers/sparse_encoder/SparseEncoder.py:467-469]() ### Pooling Configuration for Memory The `Pooling` module provides memory-efficient pooling strategies: ```mermaid graph TD TokenEmbeddings[\"Token Embeddings<br/>[batch_size, seq_len, hidden_dim]\"] --> PoolingChoice{\"Pooling Strategy\"} PoolingChoice -->|\"pooling_mode='mean'\"| MeanPool[\"Mean Pooling<br/>Average across sequence\"] PoolingChoice -->|\"pooling_mode='max'\"| MaxPool[\"Max Pooling<br/>Maximum values\"] PoolingChoice -->|\"pooling_mode='cls'\"| CLSPool[\"CLS Token<br/>First token only\"] PoolingChoice -->|\"pooling_mode='lasttoken'\"| LastPool[\"Last Token<br/>Final valid token\"] MeanPool --> SentenceEmbedding[\"Sentence Embedding<br/>[batch_size, hidden_dim]\"] MaxPool --> SentenceEmbedding CLSPool --> SentenceEmbedding LastPool --> SentenceEmbedding SentenceEmbedding --> IncludePrompt{\"include_prompt=False\"} IncludePrompt --> PromptExclusion[\"Exclude prompt tokens<br/>from pooling calculation\"] ``` **Sources:** [sentence_transformers/models/Pooling.py:135-241](), [sentence_transformers/models/Pooling.py:142-152]()",
  "def compute_dataset_metrics(self, dataset, dataset_info, loss): # String columns: token/character length analysis if isinstance(first, str): tokenized = self.tokenize(subsection, task=\"document\") if isinstance(tokenized, dict) and \"attention_mask\" in tokenized: lengths = tokenized[\"attention_mask\"].sum(dim=1).tolist() suffix = \"tokens\" else: lengths = [len(sentence) for sentence in subsection] suffix = \"characters\" ``` Sources: [sentence_transformers/model_card.py:609-756]() ### Template-Based Generation Model cards are generated using Jinja2 templates that create standardized documentation: | Template Section | Content Generated | Data Source | |------------------|-------------------|-------------| | **Model Description** | Base model, architecture, dimensions | `base_model`, `output_dimensionality` | | **Usage Examples** | Code snippets with actual model ID | `predict_example`, `model_id` | | **Training Details** | Dataset information, hyperparameters | `train_datasets`, `all_hyperparameters` | | **Evaluation Metrics** | Performance tables and charts | `eval_results_dict`, `training_logs` | | **Citations** | Automatic BibTeX generation | `citations` from loss functions | Templates automatically adapt based on model type: - Information Retrieval models get separate `encode_query`/`encode_document` examples - Sparse encoders include sparsity and dimensionality information - Cross encoders focus on reranking and classification use cases Sources: [sentence_transformers/model_card_template.md:76-126](), [sentence_transformers/sparse_encoder/model_card_template.md:99-126]() ## Backend Architecture and Optimization The library supports multiple backend implementations for optimized inference across different deployment scenarios. ### Multi-Backend Support ```mermaid graph TB subgraph \"Input Processing\" TextInput[\"Text Input\"] ImageInput[\"Image Input\"] TokenProcessor[\"Tokenization\"] end subgraph \"Backend Selection\" PyTorchBackend[\"PyTorch Backend\"] ONNXBackend[\"ONNX Runtime\"] OpenVINOBackend[\"OpenVINO Backend\"] end subgraph \"Model Components\" TransformerModule[\"Transformer Module\"] PoolingModule[\"Pooling Module\"] RouterModule[\"Router Module\"] SpladePooling[\"SpladePooling\"] end subgraph \"Output Processing\" Quantization[\"Quantization\"] Normalization[\"Normalization\"] SparsityControl[\"Sparsity Control\"] end TextInput --> TokenProcessor ImageInput --> TokenProcessor TokenProcessor --> PyTorchBackend TokenProcessor --> ONNXBackend TokenProcessor --> OpenVINOBackend PyTorchBackend --> TransformerModule PyTorchBackend --> RouterModule ONNXBackend --> TransformerModule OpenVINOBackend --> TransformerModule TransformerModule --> PoolingModule RouterModule --> SpladePooling PoolingModule --> Quantization SpladePooling --> SparsityControl Quantization --> Normalization ``` Sources: Based on architecture patterns seen in [sentence_transformers/model_card.py:602-607]() and backend references ### Performance Optimization Techniques The library implements several optimization strategies for production deployment: | Optimization | Implementation | Use Case | |-------------|----------------|----------| | **Multi-Processing** | Process pools for batch encoding | Large-scale text processing | | **ONNX Conversion** | Model quantization and optimization | CPU inference optimization | | **Backend Selection** | Runtime backend switching | Hardware-specific optimization | | **Memory Management** | Gradient caching, efficient batching | Memory-constrained training | | **Sparse Operations** | Optimized sparse tensor operations | Sparse encoder efficiency | ## Development and Extension Points ### Custom Model Card Integration Developers can extend the model card system for custom model types: ```python @dataclass class CustomModelCardData(SentenceTransformerModelCardData): custom_field: str = \"default_value\" custom_metrics: dict = field(default_factory=dict) def get_model_specific_metadata(self) -> dict[str, Any]: return { \"custom_dimension\": self.model.get_custom_dimension(), \"special_config\": self.model.get_special_config(), } ``` ### Callback Extension The callback system can be extended to capture custom training information: ```python class CustomModelCardCallback(SentenceTransformerModelCardCallback): def on_custom_event(self, args, state, control, model, **kwargs): # Custom data collection logic model.model_card_data.custom_metrics.update(kwargs.get('metrics', {})) ``` Sources: [sentence_transformers/model_card.py:47-199](), [sentence_transformers/sparse_encoder/model_card.py:18-20]() ### Version and Dependency Management The system automatically tracks framework versions and dependencies for reproducibility:",
  "```python def get_versions() -> dict[str, Any]: versions = { \"python\": python_version(), \"sentence_transformers\": sentence_transformers_version, \"transformers\": transformers.__version__, \"torch\": torch.__version__, } # Conditional imports for optional dependencies if is_accelerate_available(): versions[\"accelerate\"] = accelerate_version ``` This ensures model cards include complete environment information for reproducible results. Sources: [sentence_transformers/model_card.py:217-236]() # Model Card Generation This document covers the automatic model card generation system in sentence-transformers, which creates comprehensive documentation and metadata for trained models. The system automatically tracks training data, hyperparameters, evaluation metrics, and generates standardized model cards during the training process. For information about manual model configuration, see other training documentation pages. For details about evaluation metrics collection, see [4](#4). ## Model Card Architecture Overview The model card generation system consists of three main components working together to automatically document models during training: ```mermaid graph TB subgraph \"Model Card Data Classes\" STMCD[\"SentenceTransformerModelCardData\"] SEMCD[\"SparseEncoderModelCardData\"] CEMCD[\"CrossEncoderModelCardData\"] end subgraph \"Model Card Callbacks\" STMCC[\"SentenceTransformerModelCardCallback\"] SEMCC[\"SparseEncoderModelCardCallback\"] CEMCC[\"CrossEncoderModelCardCallback\"] end subgraph \"Trainers\" STT[\"SentenceTransformerTrainer\"] SET[\"SparseEncoderTrainer\"] CET[\"CrossEncoderTrainer\"] end subgraph \"Templates\" STTemplate[\"model_card_template.md\"] SETemplate[\"sparse_encoder/model_card_template.md\"] CETemplate[\"cross_encoder/model_card_template.md\"] end subgraph \"Generated Output\" README[\"README.md\"] end STT --> STMCC SET --> SEMCC CET --> CEMCC STMCC --> STMCD SEMCC --> SEMCD CEMCC --> CEMCD STMCD --> STTemplate SEMCD --> SETemplate CEMCD --> CETemplate STTemplate --> README SETemplate --> README CETemplate --> README STMCD -.-> SEMCD STMCD -.-> CEMCD ``` **Model Card Data Collection Flow** Sources: [sentence_transformers/model_card.py:265-355](), [sentence_transformers/sparse_encoder/model_card.py:22-132](), [sentence_transformers/cross_encoder/model_card.py:27-161]() ## Data Collection Process The model card system automatically collects training metadata through callback integration with the trainer lifecycle: ```mermaid sequenceDiagram participant Trainer as \"SentenceTransformerTrainer\" participant Callback as \"SentenceTransformerModelCardCallback\" participant ModelCard as \"SentenceTransformerModelCardData\" participant Model as \"SentenceTransformer\" Trainer->>Callback: on_init_end() Callback->>ModelCard: extract_dataset_metadata() Callback->>ModelCard: set_losses() Callback->>ModelCard: set_widget_examples() Trainer->>Callback: on_train_begin() Callback->>ModelCard: store hyperparameters loop During Training Trainer->>Callback: on_log() Callback->>ModelCard: track training_logs Trainer->>Callback: on_evaluate() Callback->>ModelCard: track evaluation metrics end Model->>ModelCard: save() ModelCard->>ModelCard: generate_model_card() ``` **Automatic Data Collection Timeline** Note: The legacy `ModelCardCallback` class has been deprecated in favor of `SentenceTransformerModelCardCallback` ([sentence_transformers/model_card.py:193-199]()). Sources: [sentence_transformers/model_card.py:47-192](), [sentence_transformers/trainer.py:315-333]() ## Model Card Types and Features The system supports three model card types with specialized features for each model architecture: | Model Type | Data Class | Callback Class | Key Features | |------------|------------|----------------|--------------| | `SentenceTransformer` | `SentenceTransformerModelCardData` | `SentenceTransformerModelCardCallback` | Dense embeddings, similarity functions, widget examples | | `SparseEncoder` | `SparseEncoderModelCardData` | `SparseEncoderModelCardCallback` | Sparse embeddings, sparsity metrics, active dimensions | | `CrossEncoder` | `CrossEncoderModelCardData` | `CrossEncoderModelCardCallback` | Pairwise scoring, ranking metrics, text classification | Sources: [sentence_transformers/model_card.py:266-355](), [sentence_transformers/sparse_encoder/model_card.py:23-132](), [sentence_transformers/cross_encoder/model_card.py:28-161]() ## Automatic Data Tracking",
  "### Training Metadata Collection The `SentenceTransformerModelCardCallback` automatically captures training information through trainer hooks: **Initialization Phase** ([sentence_transformers/model_card.py:52-88]()): - Dataset metadata extraction via `extract_dataset_metadata()` - Loss function registration via `set_losses()` - Widget example generation via `set_widget_examples()` - CodeCarbon integration for emissions tracking **Training Phase** ([sentence_transformers/model_card.py:89-130]()): - Hyperparameter tracking (default vs. non-default values) - Training and validation loss logging - Information retrieval model detection **Evaluation Phase** ([sentence_transformers/model_card.py:131-191]()): - Evaluation metrics collection - Primary metric identification for model selection - Training log consolidation ### Dataset Information Extraction The system automatically infers dataset metadata from Hugging Face Hub information: ```python # From extract_dataset_metadata method def extract_dataset_metadata(self, dataset, existing_datasets, loss, dataset_type): # Automatically detects: # - Dataset ID and revision from download checksums # - Dataset size and column statistics # - Language information from dataset cards # - Loss function configuration ``` Sources: [sentence_transformers/model_card.py:758-809]() ### Widget Example Generation The `set_widget_examples()` method automatically creates interactive examples from training or evaluation datasets: **Example Selection Process** ([sentence_transformers/model_card.py:445-522]()): 1. Sample 1000 examples from random datasets 2. Sort by text length to find representative examples 3. Generate 4-text combinations for similarity demonstrations 4. Handle Router module compatibility for asymmetric models **CrossEncoder Widget Handling** ([sentence_transformers/cross_encoder/model_card.py:90-136]()): CrossEncoder models have specialized widget handling that only generates prediction examples rather than interactive widgets, since HuggingFace Hub doesn't support pairwise text ranking widgets. ## Template System ### Template Structure Model cards are generated using Jinja2 templates with conditional sections: **Base Template Features** ([sentence_transformers/model_card_template.md:1-277]()): - YAML metadata header with HuggingFace Hub integration - Model description with training dataset links - Usage examples with code snippets - Architecture documentation - Evaluation metrics tables - Training details and hyperparameters - Framework version tracking - Citation generation **Sparse Encoder Specializations** ([sentence_transformers/sparse_encoder/model_card_template.md:1-276]()): - Sparsity statistics and active dimension reporting - Asymmetric model detection (Router/Asym modules) - SPLADE and CSR model type identification - Sparse retrieval usage examples ### Model Type Detection The system automatically detects model characteristics for specialized documentation: ```python",
  "``` **Sources:** [sentence_transformers/cross_encoder/CrossEncoder.py:392-486](), [sentence_transformers/cross_encoder/CrossEncoder.py:488-590](), [README.md:93-132]() ## Data Flow and Processing Pipeline The following diagram shows how text data flows through the different model architectures and their underlying components: ```mermaid graph LR subgraph \"Input Processing\" Text[\"Text Input\"] Text --> Tokenizer[\"Tokenizer<br/>transformers.AutoTokenizer\"] Tokenizer --> Features[\"Token Features<br/>{input_ids, attention_mask}\"] end subgraph \"Model Components\" Features --> Transformer[\"Transformer<br/>sentence_transformers/models/Transformer.py\"] Transformer --> TokenEmb[\"Token Embeddings<br/>[batch, seq_len, hidden_dim]\"] TokenEmb --> Pooling[\"Pooling<br/>sentence_transformers/models/Pooling.py\"] TokenEmb --> SpladePooling[\"SpladePooling<br/>sparse_encoder/models/SpladePooling.py\"] TokenEmb --> CrossScore[\"Classification Head<br/>transformers Model\"] end subgraph \"Output Processing\" Pooling --> DenseOut[\"Dense Embeddings<br/>model.encode()\"] SpladePooling --> SparseOut[\"Sparse Embeddings<br/>sparse_model.encode()\"] CrossScore --> ScoreOut[\"Similarity Scores<br/>cross_model.predict()\"] end ``` **Sources:** [sentence_transformers/models/Transformer.py:226-257](), [sentence_transformers/models/Pooling.py:135-241](), [sentence_transformers/sparse_encoder/models/SpladePooling.py](), [sentence_transformers/cross_encoder/CrossEncoder.py:341-342]() ## Common Usage Patterns ### Typical Model Selection | Task | Model Type | Example Model | Key Method | |------|------------|---------------|------------| | Semantic similarity | `SentenceTransformer` | `all-mpnet-base-v2` | `encode()` | | Vector database search | `SentenceTransformer` | `all-MiniLM-L6-v2` | `encode()` | | Lexical + semantic search | `SparseEncoder` | `naver/splade-cocondenser-ensembledistil` | `encode()` | | Reranking search results | `CrossEncoder` | `cross-encoder/ms-marco-MiniLM-L6-v2` | `rank()` | | Text pair classification | `CrossEncoder` | `cross-encoder/nli-MiniLM2-L6-H768` | `predict()` | ### Hybrid Retrieval Pipeline ```mermaid graph TB Query[\"User Query\"] subgraph \"Stage 1: Initial Retrieval\" Query --> DenseRetrieval[\"Dense Retrieval<br/>SentenceTransformer.encode_query()\"] Query --> SparseRetrieval[\"Sparse Retrieval<br/>SparseEncoder.encode_query()\"] DenseRetrieval --> DenseCandidates[\"Dense Candidates<br/>(semantic similarity)\"] SparseRetrieval --> SparseCandidates[\"Sparse Candidates<br/>(lexical matching)\"] end subgraph \"Stage 2: Fusion & Reranking\" DenseCandidates --> Fusion[\"Candidate Fusion\"] SparseCandidates --> Fusion Fusion --> TopK[\"Top-K Candidates\"] TopK --> Reranker[\"CrossEncoder.rank()\"] end Reranker --> FinalResults[\"Final Ranked Results\"] ``` **Sources:** [README.md:213-216](), [sentence_transformers/SentenceTransformer.py:416-675](), [sentence_transformers/sparse_encoder/SparseEncoder.py:181-410](), [sentence_transformers/cross_encoder/CrossEncoder.py:488-590]() ## Next Steps - **Model Selection**: Browse available models in [Pretrained Models](#5) or on the [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers) - **Training**: Learn to fine-tune models for your domain in [Training](#3) - **Applications**: Explore real-world use cases in [Applications](#6) - **Performance**: Optimize inference speed using techniques in [Advanced Topics](#7) **Sources:** [index.rst:133-154](), [docs/sentence_transformer/pretrained_models.md:1-49]() # Training This page covers the training system architecture and common concepts shared across all model types in sentence-transformers. The training system provides a unified interface for training `SentenceTransformer`, `SparseEncoder`, and `CrossEncoder` models using PyTorch and 🤗 Transformers.",
  "For specific training guides, see [SentenceTransformer Training](#3.1), [SparseEncoder Training](#3.2), and [CrossEncoder Training](#3.3). For loss function details, see [Loss Functions for SentenceTransformer](#3.4), [Loss Functions for SparseEncoder](#3.5), and [Loss Functions for CrossEncoder](#3.6). ## Training Architecture Overview The sentence-transformers training system is built on top of 🤗 Transformers `Trainer` with specialized components for each model type: ```mermaid graph TB subgraph \"Model Types\" ST[\"SentenceTransformer\"] SE[\"SparseEncoder\"] CE[\"CrossEncoder\"] end subgraph \"Trainer Classes\" STTrainer[\"SentenceTransformerTrainer\"] SETrainer[\"SparseEncoderTrainer\"] CETrainer[\"CrossEncoderTrainer\"] end subgraph \"🤗 Transformers Base\" BaseTrainer[\"transformers.Trainer\"] end subgraph \"Common Components\" Args[\"TrainingArguments\"] DataCollator[\"DataCollator\"] Loss[\"Loss Functions\"] Evaluator[\"Evaluators\"] Router[\"Router Module\"] end ST --> STTrainer SE --> SETrainer CE --> CETrainer STTrainer --> BaseTrainer SETrainer --> STTrainer CETrainer --> BaseTrainer STTrainer --> Args SETrainer --> Args CETrainer --> Args STTrainer --> DataCollator SETrainer --> DataCollator CETrainer --> DataCollator STTrainer --> Loss SETrainer --> Loss CETrainer --> Loss STTrainer --> Evaluator SETrainer --> Evaluator CETrainer --> Evaluator STTrainer --> Router SETrainer --> Router ``` **Training Architecture Overview** Sources: [sentence_transformers/trainer.py:59-127](), [sentence_transformers/sparse_encoder/trainer.py:31-98](), [docs/sentence_transformer/training_overview.md:17-46]() ## Trainer Class Hierarchy The training system uses specialized trainer classes that inherit from the 🤗 Transformers `Trainer`: ```mermaid graph TB BaseTrainer[\"transformers.Trainer\"] STTrainer[\"SentenceTransformerTrainer<br/>sentence_transformers/trainer.py\"] SETrainer[\"SparseEncoderTrainer<br/>sparse_encoder/trainer.py\"] CETrainer[\"CrossEncoderTrainer<br/>cross_encoder/trainer.py\"] STArgs[\"SentenceTransformerTrainingArguments\"] SEArgs[\"SparseEncoderTrainingArguments\"] CEArgs[\"CrossEncoderTrainingArguments\"] STDataCollator[\"SentenceTransformerDataCollator\"] SEDataCollator[\"SparseEncoderDataCollator\"] CEDataCollator[\"CrossEncoderDataCollator\"] BaseTrainer --> STTrainer BaseTrainer --> CETrainer STTrainer --> SETrainer STTrainer --> STArgs SETrainer --> SEArgs CETrainer --> CEArgs STTrainer --> STDataCollator SETrainer --> SEDataCollator CETrainer --> CEDataCollator ``` **Trainer Class Hierarchy and Components** Sources: [sentence_transformers/trainer.py:59](), [sentence_transformers/sparse_encoder/trainer.py:31](), [sentence_transformers/data_collator.py:13]() ## Data Flow During Training The training process follows a consistent data flow across all model types:",
  "```mermaid graph LR subgraph \"Input Data\" Dataset[\"datasets.Dataset<br/>or DatasetDict\"] Features[\"Raw Text Features\"] end subgraph \"Data Processing\" DataCollator[\"DataCollator.call()\"] Tokenize[\"tokenize_fn()\"] Prompts[\"Prompt Addition\"] RouterMapping[\"router_mapping\"] end subgraph \"Model Forward\" ModelForward[\"model.forward()\"] LossCompute[\"compute_loss()\"] Features_[\"Tokenized Features\"] Labels[\"Labels\"] end subgraph \"Training Step\" Optimizer[\"Optimizer.step()\"] Scheduler[\"LR Scheduler\"] Backward[\"loss.backward()\"] end subgraph \"Evaluation\" EvalDataset[\"eval_dataset\"] Evaluator[\"evaluator()\"] Metrics[\"Evaluation Metrics\"] end Dataset --> DataCollator Features --> DataCollator DataCollator --> Tokenize DataCollator --> Prompts DataCollator --> RouterMapping Tokenize --> Features_ Prompts --> Features_ RouterMapping --> Features_ DataCollator --> Labels Features_ --> ModelForward Labels --> LossCompute ModelForward --> LossCompute LossCompute --> Backward Backward --> Optimizer Optimizer --> Scheduler EvalDataset --> Evaluator Evaluator --> Metrics ``` **Training Data Flow** Sources: [sentence_transformers/trainer.py:391-441](), [sentence_transformers/data_collator.py:35-119](), [sentence_transformers/trainer.py:545-592]() ## Common Training Components ### Data Collator The `SentenceTransformerDataCollator` handles tokenization and batch preparation: | Component | Purpose | |-----------|---------| | `tokenize_fn` | Tokenizes text inputs using the model's tokenizer | | `router_mapping` | Maps dataset columns to router tasks (e.g., \"query\", \"document\") | | `prompts` | Adds prompts to input texts before tokenization | | `valid_label_columns` | Identifies label columns (\"label\", \"score\") | Sources: [sentence_transformers/data_collator.py:13-31]() ### Training Arguments Each model type has specialized training arguments: - `SentenceTransformerTrainingArguments` - Basic sentence transformer training - `SparseEncoderTrainingArguments` - Adds sparse-specific parameters - `CrossEncoderTrainingArguments` - Cross encoder specific settings Key parameters include `batch_sampler`, `multi_dataset_batch_sampler`, `router_mapping`, and `learning_rate_mapping`. Sources: [sentence_transformers/trainer.py:36-40]() ### Loss Functions Training supports both single loss functions and multi-dataset loss dictionaries: ```python",
  "model = SparseEncoder(\"sentence-transformers/all-MiniLM-L6-v2\") loss = CSRLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model), beta=0.1, # L_aux weight gamma=1.0 # Main loss weight ) ``` ### Architecture Requirements ```mermaid graph TD subgraph \"Model Architecture Requirements\" SPLADE[\"SPLADE Models\"] CSR[\"CSR Models\"] MLMTransformer[\"MLMTransformer<br/>MLM head access\"] SpladePooling[\"SpladePooling<br/>Sparse pooling\"] Autoencoder[\"Autoencoder Components<br/>encode/decode methods\"] BackboneEmb[\"sentence_embedding_backbone\"] DecodedEmb[\"decoded_embedding_k/4k/aux\"] SPLADE --> MLMTransformer SPLADE --> SpladePooling CSR --> Autoencoder CSR --> BackboneEmb CSR --> DecodedEmb end ``` Sources: [sentence_transformers/sparse_encoder/models/MLMTransformer.py:26-54](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:13-39](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:68-98]()",
  "This document explains the two-stage retrieval architecture that combines bi-encoders (`SentenceTransformer`) for efficient retrieval with cross-encoders (`CrossEncoder`) for high-precision reranking. This approach balances computational efficiency with ranking quality by leveraging the strengths of both model types. For information about individual model types, see [SentenceTransformer Models](#5.1) and [CrossEncoder Models](#5.3). For basic semantic search implementations, see [Semantic Search](#6.1). ## Architecture Overview The retrieve & rerank architecture consists of two sequential stages that process user queries against a document corpus: ```mermaid graph TB subgraph \"Input\" Query[\"User Query\"] Corpus[\"Document Corpus\"] end subgraph \"Stage 1: Retrieval\" ST[\"SentenceTransformer<br/>(Bi-Encoder)\"] QueryEmb[\"Query Embedding\"] DocEmbs[\"Document Embeddings<br/>(Pre-computed)\"] VectorSearch[\"Vector Similarity Search\"] TopK[\"Top-K Candidates<br/>(e.g., k=100)\"] end subgraph \"Stage 2: Reranking\" CE[\"CrossEncoder\"] Pairs[\"Query-Document Pairs\"] Scores[\"Relevance Scores\"] FinalRank[\"Final Ranking<br/>(e.g., top-10)\"] end Query --> ST ST --> QueryEmb Corpus --> DocEmbs QueryEmb --> VectorSearch DocEmbs --> VectorSearch VectorSearch --> TopK Query --> Pairs TopK --> Pairs Pairs --> CE CE --> Scores Scores --> FinalRank ``` **Two-Stage Retrieve & Rerank Pipeline** This architecture optimizes the trade-off between efficiency and quality. The `SentenceTransformer` handles the computationally expensive task of searching through millions of documents efficiently, while the `CrossEncoder` provides more accurate scoring for a smaller set of candidates. Sources: [docs/pretrained-models/msmarco-v3.md:54-58](), [docs/cross_encoder/pretrained_models.md:44](), [docs/pretrained-models/ce-msmarco.md:36-48]() ## Retrieval Stage: SentenceTransformer The first stage uses a `SentenceTransformer` model to encode queries and documents into dense vector embeddings. This bi-encoder approach processes queries and documents independently, enabling efficient pre-computation and fast similarity search. ```mermaid graph LR subgraph \"SentenceTransformer Models\" MiniLM[\"msmarco-MiniLM-L6-v3\"] DistilBERT[\"msmarco-distilbert-base-v3\"] RoBERTa[\"msmarco-roberta-base-v3\"] end subgraph \"Encoding Process\" QueryEncode[\"model.encode(query)\"] DocEncode[\"model.encode(documents)\"] CosineSim[\"util.cos_sim()\"] DotProduct[\"util.dot_score()\"] end subgraph \"Similarity Functions\" CosineModels[\"Cosine Similarity Models\"] DotProdModels[\"Dot Product Models\"] end MiniLM --> CosineModels DistilBERT --> CosineModels RoBERTa --> CosineModels CosineModels --> CosineSim DotProdModels --> DotProduct QueryEncode --> CosineSim DocEncode --> CosineSim ``` **SentenceTransformer Retrieval Components** ### Model Selection for Retrieval The choice of retrieval model depends on similarity function and performance requirements: | Model | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Similarity Function | |-------|---------------------|----------------------|-------------------| | `msmarco-MiniLM-L6-v3` | 67.46 | 32.27 | Cosine | | `msmarco-distilbert-base-v3` | 69.02 | 33.13 | Cosine | | `msmarco-distilbert-base-v4` | 70.24 | 33.79 | Cosine | | `msmarco-distilbert-base-dot-prod-v3` | 68.42 | 33.04 | Dot Product | Models tuned for cosine similarity prefer shorter passages, while dot-product models prefer longer passages. Sources: [docs/pretrained-models/msmarco-v3.md:27-48](), [docs/pretrained-models/msmarco-v3.md:6-16]() ## Reranking Stage: CrossEncoder The second stage uses a `CrossEncoder` model to perform joint encoding of query-document pairs, producing more accurate relevance scores for the top-k candidates from the retrieval stage.",
  "```mermaid graph TB subgraph \"CrossEncoder Models\" TinyBERT[\"cross-encoder/ms-marco-TinyBERT-L2-v2\"] MiniLMSmall[\"cross-encoder/ms-marco-MiniLM-L2-v2\"] MiniLMBase[\"cross-encoder/ms-marco-MiniLM-L6-v2\"] Electra[\"cross-encoder/ms-marco-electra-base\"] end subgraph \"Reranking Process\" Pairs[\"(query, document) pairs\"] CrossEncode[\"model.predict(pairs)\"] Sigmoid[\"torch.nn.Sigmoid()\"] RelevanceScores[\"Relevance Scores 0-1\"] Ranking[\"Final Ranking\"] end subgraph \"Performance Metrics\" Speed[\"Processing Speed<br/>(docs/sec)\"] Quality[\"Ranking Quality<br/>(NDCG@10)\"] end TinyBERT --> Speed MiniLMBase --> Quality Pairs --> CrossEncode CrossEncode --> Sigmoid Sigmoid --> RelevanceScores RelevanceScores --> Ranking ``` **CrossEncoder Reranking Pipeline** ### Model Selection for Reranking CrossEncoder models provide different speed-quality trade-offs: | Model | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Docs/Sec | |-------|---------------------|----------------------|----------| | `cross-encoder/ms-marco-TinyBERT-L2-v2` | 69.84 | 32.56 | 9000 | | `cross-encoder/ms-marco-MiniLM-L6-v2` | 74.30 | 39.01 | 1800 | | `cross-encoder/ms-marco-MiniLM-L12-v2` | 74.31 | 39.02 | 960 | | `cross-encoder/ms-marco-electra-base` | 71.99 | 36.41 | 340 | The `cross-encoder/ms-marco-MiniLM-L6-v2` model provides the best balance of quality and speed for most applications. Sources: [docs/cross_encoder/pretrained_models.md:35-43](), [docs/pretrained-models/ce-msmarco.md:41-53]() ## Training Data Integration The MSMARCO v3 models demonstrate how retrieve & rerank architectures are used during training to improve model quality: ```mermaid graph TD subgraph \"Training Pipeline\" V2Models[\"MSMARCO v2 Models\"] BiEncoder[\"Bi-Encoder Retrieval\"] CrossEncoderScore[\"Cross-Encoder Scoring\"] HardNegatives[\"Hard Negatives\"] V3Training[\"V3 Model Training\"] end subgraph \"Hard Negative Mining\" HighBiScore[\"High Bi-Encoder Score\"] LowCrossScore[\"Low Cross-Encoder Score\"] SaveNegative[\"Save as Hard Negative\"] end V2Models --> BiEncoder BiEncoder --> CrossEncoderScore CrossEncoderScore --> HighBiScore CrossEncoderScore --> LowCrossScore HighBiScore --> SaveNegative LowCrossScore --> SaveNegative SaveNegative --> HardNegatives HardNegatives --> V3Training ``` **Hard Negative Mining in MSMARCO v3 Training** This process identifies passages that receive high scores from the bi-encoder but low scores from the (more accurate) cross-encoder, creating challenging training examples that improve model quality. Sources: [docs/pretrained-models/msmarco-v3.md:53-58]() ## Performance Characteristics The retrieve & rerank architecture provides several performance advantages: ### Computational Efficiency - **Pre-computation**: Document embeddings are computed once and stored - **Fast Retrieval**: Vector similarity search scales to millions of documents - **Selective Reranking**: Only top-k candidates require expensive cross-encoder processing ### Quality Improvements - **Broader Recall**: Bi-encoders capture semantic similarity across large corpora - **Precise Ranking**: Cross-encoders provide accurate relevance scoring for final results - **Hard Negative Training**: Improved training data quality through cross-encoder feedback ### Typical Configuration - **Retrieval**: Return top-100 to top-1000 candidates - **Reranking**: Score and return top-10 to top-20 final results - **Latency**: ~10-50ms for retrieval + ~50-200ms for reranking Sources: [docs/pretrained-models/msmarco-v3.md:27-50](), [docs/cross_encoder/pretrained_models.md:35-43]() ## Implementation Considerations ### Model Compatibility Both retrieval and reranking models should be trained on similar datasets (e.g., MSMARCO) to ensure consistent relevance understanding.",
  "### Activation Functions CrossEncoder models may require specific activation functions: `activation_fn=torch.nn.Sigmoid()` ensures scores are normalized between 0 and 1. ### Similarity Functions Ensure consistency between training and inference similarity functions (cosine vs. dot product) for optimal performance. Sources: [docs/cross_encoder/pretrained_models.md:17-33](), [docs/pretrained-models/msmarco-v3.md:46-47]()",
  "similarity_fn_names = [\"cosine\", \"euclidean\", \"manhattan\", \"dot\"] evaluator = EmbeddingSimilarityEvaluator( sentences1=sentences1, sentences2=sentences2, scores=scores, similarity_fn_names=similarity_fn_names ) ``` **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:223-238]() ### Configuration and Extensibility Each evaluator implements a `get_config_dict()` method for serializing evaluation configuration: ```mermaid graph TD subgraph \"Configuration Parameters\" TRUNC[\"truncate_dim\"] PREC[\"precision\"] MARGIN[\"margin (TripletEvaluator)\"] SIMFNS[\"similarity_fn_names\"] end subgraph \"Config Dict Generation\" GETCONFIG[\"get_config_dict()\"] CONFIGOUT[\"serializable configuration\"] end subgraph \"Model Card Integration\" MODELCARD[\"store_metrics_in_model_card_data()\"] METADATA[\"evaluation metadata\"] end TRUNC --> GETCONFIG PREC --> GETCONFIG MARGIN --> GETCONFIG SIMFNS --> GETCONFIG GETCONFIG --> CONFIGOUT CONFIGOUT --> MODELCARD MODELCARD --> METADATA ``` **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:265-271](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:374-378](), [sentence_transformers/evaluation/TripletEvaluator.py:264-270]()",
  "This page covers the evaluation and measurement of semantic textual similarity using sentence-transformers. It focuses on the various evaluators, similarity functions, and metrics available for assessing how semantically similar two pieces of text are to each other. For information about using similarity for information retrieval tasks, see [Information Retrieval](#6.1). For details on training models with similarity-based objectives, see [Loss Functions for SentenceTransformer](#3.4). ## Core Similarity Evaluators The sentence-transformers library provides several specialized evaluators for measuring semantic textual similarity, each designed for different evaluation scenarios and data formats. ### EmbeddingSimilarityEvaluator The `EmbeddingSimilarityEvaluator` is the primary evaluator for semantic similarity tasks. It computes Spearman and Pearson rank correlations between predicted similarities and gold standard similarity scores. ```mermaid graph TD ESE[\"EmbeddingSimilarityEvaluator\"] subgraph \"Input Data\" S1[\"sentences1: List[str]\"] S2[\"sentences2: List[str]\"] SC[\"scores: List[float]\"] end subgraph \"Similarity Functions\" COS[\"cosine\"] DOT[\"dot\"] EUC[\"euclidean\"] MAN[\"manhattan\"] end subgraph \"Metrics\" PEAR[\"pearson\"] SPEAR[\"spearman\"] end subgraph \"Embedding Process\" EMB1[\"model.encode(sentences1)\"] EMB2[\"model.encode(sentences2)\"] SIMCALC[\"pairwise_similarity_functions\"] end S1 --> EMB1 S2 --> EMB2 EMB1 --> SIMCALC EMB2 --> SIMCALC SIMCALC --> COS SIMCALC --> DOT SIMCALC --> EUC SIMCALC --> MAN COS --> PEAR COS --> SPEAR DOT --> PEAR DOT --> SPEAR EUC --> PEAR EUC --> SPEAR MAN --> PEAR MAN --> SPEAR SC --> PEAR SC --> SPEAR ESE --> S1 ESE --> S2 ESE --> SC ``` **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:27-272]() ### BinaryClassificationEvaluator The `BinaryClassificationEvaluator` treats semantic similarity as a binary classification problem, determining whether pairs of sentences are similar (1) or dissimilar (0). ```mermaid graph TD BCE[\"BinaryClassificationEvaluator\"] subgraph \"Input Processing\" PAIRS[\"sentence pairs + binary labels\"] EMBED[\"model.encode()\"] HASHOPT[\"hashable optimization\"] end subgraph \"Similarity Computation\" COSF[\"pairwise_cos_sim\"] DOTF[\"pairwise_dot_score\"] EUCF[\"pairwise_euclidean_sim\"] MANF[\"pairwise_manhattan_sim\"] end subgraph \"Threshold Optimization\" ACCTHRESH[\"find_best_acc_and_threshold\"] F1THRESH[\"find_best_f1_and_threshold\"] end subgraph \"Metrics\" ACC[\"accuracy\"] F1[\"f1\"] PREC[\"precision\"] REC[\"recall\"] AP[\"average_precision\"] MCC[\"matthews_corrcoef\"] end BCE --> PAIRS PAIRS --> EMBED EMBED --> HASHOPT HASHOPT --> COSF HASHOPT --> DOTF HASHOPT --> EUCF HASHOPT --> MANF COSF --> ACCTHRESH COSF --> F1THRESH DOTF --> ACCTHRESH DOTF --> F1THRESH EUCF --> ACCTHRESH EUCF --> F1THRESH MANF --> ACCTHRESH MANF --> F1THRESH ACCTHRESH --> ACC F1THRESH --> F1 F1THRESH --> PREC F1THRESH --> REC COSF --> AP F1THRESH --> MCC ``` **Sources:** [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379]() ### TripletEvaluator The `TripletEvaluator` evaluates models using triplets of (anchor, positive, negative) sentences, ensuring that the anchor is more similar to the positive than to the negative example.",
  "```mermaid graph TD TE[\"TripletEvaluator\"] subgraph \"Triplet Input\" ANC[\"anchors: List[str]\"] POS[\"positives: List[str]\"] NEG[\"negatives: List[str]\"] MAR[\"margin: float | Dict[str, float]\"] end subgraph \"Embedding Generation\" EAANC[\"embed_inputs(model, anchors)\"] EPOS[\"embed_inputs(model, positives)\"] ENEG[\"embed_inputs(model, negatives)\"] end subgraph \"Similarity Calculation\" SIMPOS[\"similarity(anchor, positive)\"] SIMNEG[\"similarity(anchor, negative)\"] end subgraph \"Evaluation Logic\" COMP[\"positive_scores > negative_scores + margin\"] ACCUR[\"accuracy = mean(comparisons)\"] end TE --> ANC TE --> POS TE --> NEG TE --> MAR ANC --> EAANC POS --> EPOS NEG --> ENEG EAANC --> SIMPOS EPOS --> SIMPOS EAANC --> SIMNEG ENEG --> SIMNEG SIMPOS --> COMP SIMNEG --> COMP MAR --> COMP COMP --> ACCUR ``` **Sources:** [sentence_transformers/evaluation/TripletEvaluator.py:26-271]() ## Similarity Functions and Metrics The evaluators support multiple similarity functions, each with different mathematical properties and use cases. | Similarity Function | Implementation | Use Case | Greater is Better | |-------------------|----------------|----------|------------------| | `cosine` | `pairwise_cos_sim` | General semantic similarity | ✓ | | `dot` | `pairwise_dot_score` | When magnitude matters | ✓ | | `euclidean` | `pairwise_euclidean_sim` | Distance-based similarity | ✗ | | `manhattan` | `pairwise_manhattan_sim` | L1 distance similarity | ✗ | ```mermaid graph LR subgraph \"SimilarityFunction Enum\" COSINE_ENUM[\"SimilarityFunction.COSINE\"] DOT_ENUM[\"SimilarityFunction.DOT_PRODUCT\"] EUC_ENUM[\"SimilarityFunction.EUCLIDEAN\"] MAN_ENUM[\"SimilarityFunction.MANHATTAN\"] end subgraph \"Implementation Functions\" COS_FUNC[\"pairwise_cos_sim\"] DOT_FUNC[\"pairwise_dot_score\"] EUC_FUNC[\"pairwise_euclidean_sim\"] MAN_FUNC[\"pairwise_manhattan_sim\"] end subgraph \"Evaluator Integration\" SIM_DICT[\"similarity_functions dict\"] SCORE_COMP[\"score computation\"] METRIC_CALC[\"metric calculation\"] end COSINE_ENUM --> COS_FUNC DOT_ENUM --> DOT_FUNC EUC_ENUM --> EUC_FUNC MAN_ENUM --> MAN_FUNC COS_FUNC --> SIM_DICT DOT_FUNC --> SIM_DICT EUC_FUNC --> SIM_DICT MAN_FUNC --> SIM_DICT SIM_DICT --> SCORE_COMP SCORE_COMP --> METRIC_CALC ``` **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:184-189](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:238-259](), [sentence_transformers/evaluation/TripletEvaluator.py:187-204]() ## STS Benchmark Integration The library includes extensive testing and evaluation capabilities for the STS (Semantic Textual Similarity) benchmark, a standard dataset for evaluating semantic similarity models. ### STS Benchmark Testing Framework ```mermaid graph TD subgraph \"STS Dataset Loading\" STSDATA[\"stsbenchmark.tsv.gz\"] DOWNLOAD[\"util.http_get()\"] PARSE[\"csv.DictReader parsing\"] end subgraph \"Data Processing\" NORM[\"score normalization (0-5 → 0-1)\"] SPLIT[\"train/test split\"] INPUTEX[\"InputExample creation\"] end subgraph \"Evaluation Pipeline\" EVALCREATE[\"EmbeddingSimilarityEvaluator.from_input_examples()\"] MODELEVAL[\"model.evaluate()\"] SCORERET[\"primary_metric score\"] end subgraph \"Pretrained Model Testing\" MODLIST[\"pretrained model list\"] PERFTEST[\"pretrained_model_score()\"] ASSERT[\"performance assertions\"] end STSDATA --> DOWNLOAD DOWNLOAD --> PARSE PARSE --> NORM NORM --> SPLIT SPLIT --> INPUTEX INPUTEX --> EVALCREATE EVALCREATE --> MODELEVAL MODELEVAL --> SCORERET MODLIST --> PERFTEST PERFTEST --> EVALCREATE SCORERET --> ASSERT ```",
  "**Sources:** [tests/test_pretrained_stsb.py:18-49](), [tests/test_train_stsb.py:33-51]() ### Training with STS Data The library supports training models specifically for semantic similarity using the STS benchmark: ```mermaid graph TD subgraph \"Training Setup\" STSLOAD[\"STS data loading\"] DATASET[\"SentencesDataset creation\"] DATALOADER[\"DataLoader setup\"] end subgraph \"Loss Function\" COSLOSS[\"CosineSimilarityLoss\"] MODELREF[\"model reference\"] end subgraph \"Training Process\" FIT[\"model.fit()\"] TRAINOBJ[\"train_objectives\"] EPOCHS[\"epoch configuration\"] end subgraph \"Evaluation\" EVALFUNC[\"evaluate_stsb_test()\"] THRESHOLD[\"expected score threshold\"] ASSERTION[\"performance assertion\"] end STSLOAD --> DATASET DATASET --> DATALOADER DATALOADER --> TRAINOBJ COSLOSS --> TRAINOBJ MODELREF --> COSLOSS TRAINOBJ --> FIT EPOCHS --> FIT FIT --> EVALFUNC EVALFUNC --> THRESHOLD THRESHOLD --> ASSERTION ``` **Sources:** [tests/test_train_stsb.py:74-103](), [tests/test_train_stsb.py:111-127]() ## Advanced Evaluation Features ### Precision and Quantization Support The `EmbeddingSimilarityEvaluator` supports various embedding precisions for memory-efficient evaluation: | Precision | Description | Binary Unpacking | |-----------|-------------|------------------| | `float32` | Standard floating point | ✗ | | `int8` | 8-bit signed integer | ✗ | | `uint8` | 8-bit unsigned integer | ✗ | | `binary` | Binary with signed conversion | ✓ | | `ubinary` | Unsigned binary | ✓ | **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:171-176]() ### Multi-Metric Evaluation All similarity evaluators support evaluation with multiple similarity functions simultaneously, computing max metrics across functions: ```python",
  "args = SparseEncoderTrainingArguments( router_mapping={ \"question\": \"query\", \"answer\": \"document\", }, learning_rate_mapping={ r\"SparseStaticEmbedding\\.*\": 1e-3, # Higher LR for static embeddings } ) ``` **Sources:** [docs/sparse_encoder/training_overview.md:149-168]() ### Multi-Dataset Training The system supports training on multiple datasets simultaneously with different batch sampling strategies: ```python args = SparseEncoderTrainingArguments( multi_dataset_batch_sampler=BatchSamplers.PROPORTIONAL, batch_sampler=BatchSamplers.NO_DUPLICATES, ) ``` **Sources:** [docs/sparse_encoder/training_overview.md:419-425]() ### Memory Optimization For large models, several memory optimization techniques are available: - **Gradient Checkpointing**: `gradient_checkpointing=True` - **Mixed Precision**: `fp16=True` or `bf16=True` - **Chunked Processing**: Configure `chunk_size` in `SpladePooling` - **Gradient Accumulation**: `gradient_accumulation_steps=N` **Sources:** [docs/sparse_encoder/training_overview.md:400-425](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:92-128]()",
  "loss = { \"dataset1\": CoSENTLoss(model), \"dataset2\": MultipleNegativesRankingLoss(model) } ``` Sources: [sentence_transformers/trainer.py:291-310]() ## Multi-Dataset Training The training system supports training on multiple datasets simultaneously using `DatasetDict`: ```mermaid graph TB subgraph \"Multi-Dataset Input\" DD[\"DatasetDict\"] DS1[\"Dataset 'nli'\"] DS2[\"Dataset 'sts'\"] DS3[\"Dataset 'quora'\"] end subgraph \"Loss Mapping\" LossDict[\"Loss Dictionary\"] L1[\"nli: CoSENTLoss\"] L2[\"sts: CosineSimilarityLoss\"] L3[\"quora: MNRL\"] end subgraph \"Batch Sampling\" BatchSampler[\"MultiDatasetBatchSampler\"] RoundRobin[\"RoundRobinBatchSampler\"] Proportional[\"ProportionalBatchSampler\"] end subgraph \"Training Process\" DataCollator[\"add_dataset_name_column()\"] ComputeLoss[\"compute_loss()\"] LossSelect[\"Select loss by dataset_name\"] end DD --> DS1 DD --> DS2 DD --> DS3 LossDict --> L1 LossDict --> L2 LossDict --> L3 DS1 --> BatchSampler DS2 --> BatchSampler DS3 --> BatchSampler BatchSampler --> RoundRobin BatchSampler --> Proportional BatchSampler --> DataCollator DataCollator --> ComputeLoss LossDict --> LossSelect ComputeLoss --> LossSelect ``` **Multi-Dataset Training Architecture** Sources: [sentence_transformers/trainer.py:295-310](), [sentence_transformers/trainer.py:416-422](), [sentence_transformers/trainer.py:785-800]() ## Router Support for Asymmetric Training The training system integrates with the `Router` module to enable asymmetric architectures where different paths are used for queries vs documents: ### Router Configuration ```python",
  "loss = self.cross_entropy_loss(logits_matrix, labels_matrix.softmax(dim=1)) ``` Sources: [sentence_transformers/cross_encoder/losses/ListNetLoss.py:10-198]() ### Position-Aware ListMLE The `PListMLELoss` and `ListMLELoss` implement maximum likelihood estimation for permutations with optional position-aware weighting: ```python # Core PListMLE computation from PListMLELoss.forward() scores = sorted_logits.exp() cumsum_scores = torch.flip(torch.cumsum(torch.flip(scores, [1]), 1), [1]) log_probs = sorted_logits - torch.log(cumsum_scores + self.eps) if self.lambda_weight is not None: lambda_weight = self.lambda_weight(mask) log_probs = log_probs * lambda_weight ``` Sources: [sentence_transformers/cross_encoder/losses/PListMLELoss.py:45-295](), [sentence_transformers/cross_encoder/losses/ListMLELoss.py:9-127]() ## Common Implementation Patterns All learning-to-rank losses share several implementation patterns: ### Mini-Batch Processing Large document lists are processed in mini-batches to manage memory usage: ```python mini_batch_size = self.mini_batch_size or batch_size if mini_batch_size <= 0: mini_batch_size = len(pairs) for i in range(0, len(pairs), mini_batch_size): mini_batch_pairs = pairs[i : i + mini_batch_size] # Process mini-batch... ``` ### Padding Handling Variable document counts per query are handled using padding and masking: ```python # Create padded matrices logits_matrix = torch.full((batch_size, max_docs), -1e16, device=self.model.device) labels_matrix = torch.full_like(logits_matrix, float(\"-inf\")) # Place valid logits and labels doc_indices = torch.cat([torch.arange(len(docs)) for docs in docs_list], dim=0) batch_indices = torch.repeat_interleave(torch.arange(batch_size), torch.tensor(docs_per_query)) logits_matrix[batch_indices, doc_indices] = logits ``` Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:247-287](), [sentence_transformers/cross_encoder/losses/ListNetLoss.py:132-176]() ## Configuration and Usage ### Basic Usage Pattern ```python from sentence_transformers.cross_encoder import CrossEncoder, CrossEncoderTrainer, losses from datasets import Dataset model = CrossEncoder(\"microsoft/mpnet-base\") train_dataset = Dataset.from_dict({ \"query\": [\"What are pandas?\", \"What is the capital of France?\"], \"docs\": [ [\"Pandas are a kind of bear.\", \"Pandas are kind of like fish.\"], [\"The capital of France is Paris.\", \"Paris is quite large.\"], ], \"labels\": [[1, 0], [1, 0]], })",
  "stats = SparseEncoder.sparsity(sparse_embeddings) print(f\"Sparsity: {stats['sparsity_ratio']:.2%}\") ``` ```python # CrossEncoder - Pairwise scoring and ranking from sentence_transformers import CrossEncoder model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\") # Predict similarity scores query = \"What is machine learning?\" passages = [\"ML is a subset of AI\", \"Weather prediction models\"] scores = model.predict([(query, passage) for passage in passages]) # Rank passages by relevance ranked_results = model.rank(query, passages, return_documents=True) ``` ### Model Organization and Naming | Model Source | Loading Pattern | Example | |---|---|---| | Official sentence-transformers | Direct name | `SentenceTransformer(\"all-mpnet-base-v2\")` | | Community models | Full path | `SentenceTransformer(\"BAAI/bge-large-en\")` | | Organization-specific | Org/model format | `SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")` | **Sources:** [README.md:58-167](), [index.rst:37-132](), [docs/sentence_transformer/pretrained_models.md:16-27]() ## Model Versioning and Evolution The library maintains version histories for major model series to track improvements over time: ```mermaid graph TD subgraph \"MSMARCO Evolution\" V1[\"v1: Initial Models<br/>MultipleNegativesRankingLoss<br/>In-batch negatives\"] V2[\"v2: Improved Training<br/>Better hard negatives<br/>Performance gains\"] V3[\"v3: Cross-Encoder Mining<br/>electra-base cross-encoder<br/>Hard negative mining\"] V5[\"v5: Normalized + MarginMSE<br/>normalized_embeddings<br/>MarginMSE loss\"] end V1 --> V2 V2 --> V3 V3 --> V5 subgraph \"Performance Trends\" P1[\"MRR@10: ~23\"] P2[\"MRR@10: ~29\"] P3[\"MRR@10: ~33\"] P5[\"MRR@10: ~37\"] end V1 --> P1 V2 --> P2 V3 --> P3 V5 --> P5 ``` **Sources:** [docs/pretrained-models/msmarco-v1.md:10-11](), [docs/pretrained-models/msmarco-v3.md:53-58](), [docs/pretrained-models/msmarco-v5.md:53-65]() Each version incorporates training improvements, better negative sampling strategies, and architectural refinements that progressively enhance model performance on downstream tasks. # SentenceTransformer Models This page provides a comprehensive guide to pretrained SentenceTransformer models for dense text embeddings. SentenceTransformer models encode text into fixed-size vector representations that capture semantic meaning, enabling applications like semantic search, clustering, and similarity comparison. For sparse embedding models, see [SparseEncoder Models](#5.2). For pairwise scoring models, see [CrossEncoder Models](#5.3). For MSMARCO-specific models, see [MSMARCO Models](#5.4). ## Model Architecture Overview SentenceTransformer models generate dense embeddings by combining transformer layers with pooling mechanisms: ```mermaid graph LR subgraph \"SentenceTransformer Architecture\" Input[\"Input Text\"] --> Tokenizer[\"AutoTokenizer\"] Tokenizer --> Transformer[\"Transformer Module\"] Transformer --> Pooling[\"Pooling Module\"] Pooling --> Normalize[\"Normalize (Optional)\"] Normalize --> Output[\"Dense Embeddings\"] end subgraph \"Core Components\" ST[\"SentenceTransformer\"] --> TF[\"sentence_transformers.models.Transformer\"] ST --> PL[\"sentence_transformers.models.Pooling\"] ST --> NM[\"sentence_transformers.models.Normalize\"] end ``` The `SentenceTransformer` class in [sentence_transformers/SentenceTransformer.py:61-163]() serves as the main interface, orchestrating sequential modules to transform text into embeddings. Sources: [sentence_transformers/SentenceTransformer.py:61-163](), [sentence_transformers/models/Transformer.py](), [sentence_transformers/models/Pooling.py]() ## Model Categories ### General Purpose Models **All-series Models**: Trained on diverse datasets (1B+ training pairs) for broad applicability.",
  "| Model | Dimensions | Speed (GPU/CPU) | Performance | Use Case | |-------|------------|-----------------|-------------|----------| | `all-mpnet-base-v2` | 768 | 2,800 / 170 | 67.97 | Best quality | | `all-MiniLM-L6-v2` | 384 | 14,200 / 750 | 64.82 | Balanced speed/quality | | `all-MiniLM-L12-v2` | 384 | 7,500 / 400 | 66.01 | Higher quality MiniLM | **Paraphrase Models**: Optimized for sentence similarity and paraphrase detection. | Model | Base Architecture | Training Data | Performance | |-------|------------------|---------------|-------------| | `paraphrase-mpnet-base-v2` | MPNet | Multi-domain paraphrases | 67.97 | | `paraphrase-MiniLM-L6-v2` | MiniLM | Efficient paraphrase model | 64.82 | | `paraphrase-distilroberta-base-v2` | DistilRoBERTa | RoBERTa-based paraphrase | 66.27 | Sources: [docs/_static/html/models_en_sentence_embeddings.html:342-355](), [docs/sentence_transformer/pretrained_models.md:41-49]() ### Semantic Search Models Specialized for query-document retrieval tasks: ```mermaid graph TB subgraph \"Multi-QA Models\" MQ1[\"multi-qa-mpnet-base-cos-v1<br/>57.46 performance<br/>4,000 qps\"] MQ2[\"multi-qa-distilbert-cos-v1<br/>52.83 performance<br/>7,000 qps\"] MQ3[\"multi-qa-MiniLM-L6-cos-v1<br/>51.83 performance<br/>18,000 qps\"] end subgraph \"MSMARCO Models\" MS1[\"msmarco-bert-base-dot-v5<br/>38.08 MRR@10<br/>4,000 qps\"] MS2[\"msmarco-distilbert-dot-v5<br/>37.25 MRR@10<br/>7,000 qps\"] MS3[\"msmarco-distilbert-cos-v5<br/>33.79 MRR@10<br/>7,000 qps\"] end subgraph \"Usage Patterns\" Query[\"encode_query()\"] --> MQ1 Documents[\"encode_document()\"] --> MQ1 MQ1 --> Similarity[\"similarity()\"] end ``` **Multi-QA Models** are trained on 215M+ question-answer pairs from diverse sources. **MSMARCO Models** are trained on Bing search queries with web passages. Sources: [docs/sentence_transformer/pretrained_models.md:84-124](), [docs/pretrained-models/msmarco-v5.md:29-44]() ### Multilingual Models Support 50+ languages with aligned vector spaces: | Model | Languages | Architecture | Use Case | |-------|-----------|--------------|----------| | `distiluse-base-multilingual-cased-v2` | 50+ | DistilUSE | General multilingual | | `paraphrase-multilingual-mpnet-base-v2` | 50+ | MPNet | High-quality multilingual | | `LaBSE` | 109 | BERT | Bitext mining/translation | Sources: [docs/sentence_transformer/pretrained_models.md:128-144]() ## Model Selection Guide ```mermaid flowchart TD Start[\"Model Selection\"] --> Task{\"Task Type?\"} Task -->|\"General Purpose\"| Speed{\"Speed vs Quality?\"} Task -->|\"Semantic Search\"| Domain{\"Domain?\"} Task -->|\"Multilingual\"| Languages{\"Language Count?\"} Speed -->|\"Best Quality\"| MPNet[\"all-mpnet-base-v2\"] Speed -->|\"Balanced\"| MiniLM6[\"all-MiniLM-L6-v2\"] Speed -->|\"Fastest\"| MiniLM3[\"paraphrase-MiniLM-L3-v2\"] Domain -->|\"Web Search\"| MSMARCO[\"msmarco-distilbert-dot-v5\"] Domain -->|\"QA Diverse\"| MultiQA[\"multi-qa-mpnet-base-cos-v1\"] Domain -->|\"Scientific\"| Specter[\"allenai-specter\"] Languages -->|\"15 Major\"| DistilUSE1[\"distiluse-base-multilingual-cased-v1\"] Languages -->|\"50+\"| DistilUSE2[\"distiluse-base-multilingual-cased-v2\"] Languages -->|\"Translation\"| LaBSE[\"LaBSE\"] ``` ### Performance Considerations",
  "**Embedding Dimensions**: Higher dimensions generally provide better quality but require more storage and compute: - 384-dim: Efficient for most applications - 768-dim: Better quality for complex tasks - 1024-dim: Highest quality for specialized domains **Normalization**: Models with normalized embeddings enable efficient dot-product similarity: - Normalized: Use `util.dot_score()` for fastest similarity - Non-normalized: Use `util.cos_sim()` for cosine similarity Sources: [docs/_static/html/models_en_sentence_embeddings.html:184-202](), [sentence_transformers/SentenceTransformer.py:139-163]() ## Loading and Usage Patterns ### Basic Model Loading ```python from sentence_transformers import SentenceTransformer",
  "This document covers the training system for CrossEncoder models in sentence-transformers. CrossEncoders are designed for reranking and classification tasks where two texts are jointly encoded to produce similarity scores or class predictions. For information about training SentenceTransformer models (bi-encoders), see [3.1](#3.1). For SparseEncoder training, see [3.2](#3.2). For loss function details specific to CrossEncoders, see [3.6](#3.6). ## CrossEncoder Training Architecture CrossEncoder training follows a similar pattern to other model types in sentence-transformers but with specific adaptations for joint text encoding and ranking/classification tasks. **CrossEncoder Training System Overview** ```mermaid graph TB subgraph \"Core Components\" CE[CrossEncoder] CETrainer[CrossEncoderTrainer] CEArgs[CrossEncoderTrainingArguments] CELosses[CrossEncoder Losses] CEEvals[CrossEncoder Evaluators] end subgraph \"Data Processing\" Dataset[datasets.Dataset] DataCollator[Data Collator] HardNegMining[Hard Negatives Mining] end subgraph \"Loss Functions\" BCE[BinaryCrossEntropyLoss] MNR[MultipleNegativesRankingLoss] Lambda[LambdaLoss] ListNet[ListNetLoss] CrossEntropy[CrossEntropyLoss] end subgraph \"Training Infrastructure\" HFTrainer[Transformers Trainer] ModelCard[Model Card Generation] HFHub[Hugging Face Hub] end CE --> CETrainer Dataset --> DataCollator CEArgs --> CETrainer CELosses --> CETrainer CEEvals --> CETrainer BCE --> CELosses MNR --> CELosses Lambda --> CELosses ListNet --> CELosses CrossEntropy --> CELosses CETrainer --> HFTrainer CETrainer --> ModelCard ModelCard --> HFHub HardNegMining --> Dataset ``` Sources: [docs/cross_encoder/training_overview.md:1-500](), [docs/cross_encoder/loss_overview.md:1-100]() ## Training Components CrossEncoder training involves six main components that work together to fine-tune models for ranking and classification tasks. **CrossEncoder Training Data Flow** ```mermaid graph LR subgraph \"Input Data\" TextPairs[\"(text_A, text_B) pairs\"] Triplets[\"(query, positive, negative)\"] Rankings[\"(query, [doc1, doc2, ...])\"] Labels[Class Labels / Scores] end subgraph \"Data Processing\" DataCollator[\"Data Collator\"] Tokenization[Tokenization] BatchFormat[Batch Formatting] end subgraph \"Model & Loss\" CrossEncoder[CrossEncoder Model] LossFunction[Loss Function] ForwardPass[Forward Pass] end subgraph \"Training Loop\" Optimizer[Optimizer] BackwardPass[Backward Pass] WeightUpdate[Weight Update] end subgraph \"Evaluation\" Evaluator[CrossEncoder Evaluator] Metrics[Metrics Calculation] end TextPairs --> DataCollator Triplets --> DataCollator Rankings --> DataCollator Labels --> DataCollator DataCollator --> Tokenization Tokenization --> BatchFormat BatchFormat --> CrossEncoder CrossEncoder --> ForwardPass ForwardPass --> LossFunction LossFunction --> BackwardPass BackwardPass --> Optimizer Optimizer --> WeightUpdate CrossEncoder --> Evaluator Evaluator --> Metrics ``` Sources: [docs/cross_encoder/training_overview.md:170-190](), [sentence_transformers/data_collator.py:35-120]() ### Model Initialization CrossEncoder models are initialized by loading a pretrained transformers model with a sequence classification head. If the model doesn't have such a head, it's added automatically. ```python from sentence_transformers import CrossEncoder",
  "evaluator = NanoBEIREvaluator( dataset_names=[\"msmarco\", \"nfcorpus\"], query_prompts={ \"msmarco\": \"Retrieve relevant passages: \", \"nfcorpus\": \"Find related documents: \" } ) ``` ### Sparse Model Evaluation ```python from sentence_transformers import SparseEncoder from sentence_transformers.sparse_encoder.evaluation import SparseNanoBEIREvaluator # Evaluation with sparsity constraints evaluator = SparseNanoBEIREvaluator( dataset_names=[\"msmarco\", \"scifact\"], max_active_dims=100, show_progress_bar=True ) results = evaluator(sparse_model) ``` Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:101-120](), [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:58-77]() ## Configuration Options ### Core Parameters | Parameter | Type | Description | Default | |-----------|------|-------------|---------| | `dataset_names` | `List[DatasetNameType]` | Datasets to evaluate on | All 13 datasets | | `aggregate_fn` | `Callable` | Function to aggregate scores | `np.mean` | | `aggregate_key` | `str` | Key for aggregated results | `\"mean\"` | | `batch_size` | `int` | Batch size for encoding | `32` | | `show_progress_bar` | `bool` | Show evaluation progress | `False` | ### Metric Configuration | Parameter | Type | Description | Default | |-----------|------|-------------|---------| | `mrr_at_k` | `List[int]` | MRR calculation values | `[10]` | | `ndcg_at_k` | `List[int]` | NDCG calculation values | `[10]` | | `accuracy_at_k` | `List[int]` | Accuracy calculation values | `[1, 3, 5, 10]` | | `precision_recall_at_k` | `List[int]` | P/R calculation values | `[1, 3, 5, 10]` | | `map_at_k` | `List[int]` | MAP calculation values | `[100]` | ### Prompt Configuration Both `query_prompts` and `corpus_prompts` can be: - `str`: Same prompt for all datasets - `Dict[str, str]`: Dataset-specific prompts - `None`: No prompts used Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:193-211](), [sentence_transformers/evaluation/NanoBEIREvaluator.py:447-466]() ## Key Methods and Data Flow ### Evaluation Process ```mermaid graph TD __call__[\"__call__(model, output_path, epoch, steps)<br/>Main evaluation entry\"] _validate_dataset_names[\"_validate_dataset_names()<br/>Check dataset validity against<br/>dataset_name_to_id.keys()\"] _validate_prompts[\"_validate_prompts()<br/>Validate prompt configuration<br/>for each dataset_name\"] create_evaluators[\"Create self.evaluators list<br/>via _load_dataset() in __init__\"] per_dataset_loop[\"for evaluator in tqdm(self.evaluators)<br/>desc='Evaluating datasets'\"] evaluator_call[\"evaluation = evaluator(model)<br/>Returns dict[metric_name, score]\"] split_metrics[\"splits = full_key.split('_', maxsplit=<br/>num_underscores_in_name)\"] collect_per_metric[\"per_metric_results[metric].append(<br/>metric_value)\"] aggregate_metrics[\"agg_results[metric] = <br/>self.aggregate_fn(per_metric_results[metric])\"] determine_primary[\"if not self.primary_metric:<br/>score_function with max ndcg@k\"] store_metrics[\"store_metrics_in_model_card_data()<br/>Save to model.model_card_data\"] write_csv[\"Write to self.csv_file<br/>(if self.write_csv)\"] __call__ --> _validate_dataset_names __call__ --> _validate_prompts __call__ --> create_evaluators create_evaluators --> per_dataset_loop per_dataset_loop --> evaluator_call evaluator_call --> split_metrics split_metrics --> collect_per_metric collect_per_metric --> aggregate_metrics aggregate_metrics --> determine_primary determine_primary --> store_metrics determine_primary --> write_csv ``` Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:283-396](), [sentence_transformers/evaluation/NanoBEIREvaluator.py:314-325](), [sentence_transformers/evaluation/NanoBEIREvaluator.py:358-366]() ### Dataset Loading Process The `_load_dataset` method handles the conversion from Hugging Face dataset format to the format expected by `InformationRetrievalEvaluator`:",
  "```mermaid graph LR load_dataset[\"load_dataset()<br/>corpus, queries, qrels\"] corpus_dict[\"corpus_dict<br/>{sample['_id']: sample['text']}\"] queries_dict[\"queries_dict<br/>{sample['_id']: sample['text']}\"] qrels_dict[\"qrels_dict<br/>{query-id: set(corpus-ids)}\"] apply_prompts[\"Apply query_prompts/<br/>corpus_prompts\"] create_evaluator[\"self.information_retrieval_class()<br/>InformationRetrievalEvaluator or<br/>SparseInformationRetrievalEvaluator\"] load_dataset --> corpus_dict load_dataset --> queries_dict load_dataset --> qrels_dict corpus_dict --> apply_prompts queries_dict --> apply_prompts qrels_dict --> apply_prompts apply_prompts --> create_evaluator ``` 1. **Load dataset splits**: `corpus`, `queries`, `qrels` from Hub using `datasets.load_dataset` 2. **Convert to dictionaries**: Transform to `{sample[\"_id\"]: sample[\"text\"]}` format 3. **Build relevance mapping**: Create `{sample[\"query-id\"]: set(sample[\"corpus-id\"])}` from qrels 4. **Apply prompts**: Add dataset-specific `query_prompt`/`corpus_prompt` if configured 5. **Create evaluator**: Instantiate via `self.information_retrieval_class` attribute Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:404-434](), [sentence_transformers/evaluation/NanoBEIREvaluator.py:415-421]() ## Output Format and Metrics ### Individual Dataset Results Each dataset evaluation produces metrics with the pattern: `{dataset_name}_{score_function}_{metric}@{k}`: - `NanoMSMARCO_cosine_ndcg@10` - `NanoSciFact_dot_mrr@10` - `NanoQuoraRetrieval_cosine_map@100` ### Aggregated Results Aggregated metrics follow the pattern: `NanoBEIR_{aggregate_key}_{score_function}_{metric}@{k}`: - `NanoBEIR_mean_cosine_ndcg@10` - `NanoBEIR_mean_dot_mrr@10` ### Sparse Model Additional Metrics `SparseNanoBEIREvaluator` extends the base functionality with sparsity tracking via `defaultdict(list)` collections: | Metric | Calculation | Description | |--------|-------------|-------------| | `{name}_query_active_dims` | Weighted average by query count | Average active dimensions across all queries | | `{name}_query_sparsity_ratio` | Weighted average by query count | Sparsity ratio (1 - active/total) for queries | | `{name}_corpus_active_dims` | Weighted average by corpus size | Average active dimensions across all documents | | `{name}_corpus_sparsity_ratio` | Weighted average by corpus size | Sparsity ratio for corpus documents | The sparsity calculation process: ```mermaid graph LR per_evaluator[\"Each evaluator.sparsity_stats<br/>{key: value}\"] collect[\"self.sparsity_stats[key]<br/>.append(value)\"] weight_calc[\"sum(val * length for val, length in <br/>zip(value, self.lengths[key.split('_')[0]])\"] normalize[\"/ sum(self.lengths[key.split('_')[0]])\"] prefix_metrics[\"self.prefix_name_to_metrics(<br/>self.sparsity_stats, self.name)\"] per_evaluator --> collect collect --> weight_calc weight_calc --> normalize normalize --> prefix_metrics ``` Sources: [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:222-231](), [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:202-212]() ## Primary Metric Selection The primary metric is determined by: 1. **Explicit main_score_function**: Use `{main_score_function}_ndcg@{max(ndcg_at_k)}` 2. **Automatic selection**: Choose score function with highest NDCG@k score 3. **Format**: `NanoBEIR_{aggregate_key}_{score_function}_ndcg@{k}` The primary metric is used for model selection and optimization during training. Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:358-366]() # Pretrained Models This page provides an overview of the extensive collection of pretrained models available in the sentence-transformers library and guidance on selecting the right model for your task. With over 15,000 models available on the Hugging Face Hub, this overview helps you navigate the three main model architectures and understand when to use each type.",
  "The library offers three distinct model architectures, each optimized for different use cases: - **SentenceTransformer**: Dense vector embeddings for semantic similarity and clustering - **SparseEncoder**: Sparse vector embeddings for efficient retrieval and search engine integration - **CrossEncoder**: Pairwise scoring models for reranking and classification For detailed model catalogs and specific recommendations, see [SentenceTransformer Models](#5.1), [SparseEncoder Models](#5.2), [CrossEncoder Models](#5.3), and [MSMARCO Models](#5.4). ## Model Architecture Overview Model Architecture Comparison ```mermaid graph TB subgraph \"sentence_transformers.SentenceTransformer\" ST_CLASS[\"SentenceTransformer\"] ST_ENCODE[\"model.encode()\"] ST_SIM[\"model.similarity()\"] ST_MODELS[\"all-mpnet-base-v2<br/>all-MiniLM-L6-v2<br/>paraphrase-mpnet-base-v2\"] end subgraph \"sentence_transformers.SparseEncoder\" SE_CLASS[\"SparseEncoder\"] SE_ENCODE[\"model.encode()\"] SE_SPARSE[\"model.sparsity()\"] SE_MODELS[\"naver/splade-cocondenser-ensembledistil<br/>prithivida/Splade_PP_en_v1\"] end subgraph \"sentence_transformers.CrossEncoder\" CE_CLASS[\"CrossEncoder\"] CE_PREDICT[\"model.predict()\"] CE_RANK[\"model.rank()\"] CE_MODELS[\"cross-encoder/ms-marco-MiniLM-L6-v2<br/>cross-encoder/ms-marco-TinyBERT-L2-v2\"] end subgraph \"Output_Types\" DENSE[\"Dense Vectors<br/>torch.Tensor[batch, 384-1024]<br/>util.cos_sim()\"] SPARSE[\"Sparse Vectors<br/>torch.Tensor[batch, vocab_size]<br/>util.dot_score()\"] SCORES[\"Scalar Scores<br/>torch.Tensor[batch]<br/>torch.sigmoid()\"] end ST_CLASS --> DENSE SE_CLASS --> SPARSE CE_CLASS --> SCORES ``` Model Selection Decision Tree ```mermaid graph TD START[\"What is your use case?\"] START --> SEMANTIC[\"Semantic Search<br/>Similarity Comparison<br/>Clustering\"] START --> RETRIEVAL[\"Document Retrieval<br/>Search Engine Integration<br/>Keyword + Semantic\"] START --> RANKING[\"Reranking<br/>Pairwise Classification<br/>Relevance Scoring\"] SEMANTIC --> ST_CHOICE[\"Use SentenceTransformer<br/>See page 5.1\"] RETRIEVAL --> SE_CHOICE[\"Use SparseEncoder<br/>See page 5.2\"] RANKING --> CE_CHOICE[\"Use CrossEncoder<br/>See page 5.3\"] ST_CHOICE --> ST_SPEED[\"Speed Priority?\"] ST_SPEED --> ST_FAST[\"all-MiniLM-L6-v2<br/>14,200 sent/sec\"] ST_SPEED --> ST_QUALITY[\"all-mpnet-base-v2<br/>Best quality\"] SE_CHOICE --> SE_LANG[\"Language?\"] SE_LANG --> SE_EN[\"English:<br/>splade-cocondenser-ensembledistil\"] SE_LANG --> SE_MULTI[\"Multilingual:<br/>Available models limited\"] CE_CHOICE --> CE_DOMAIN[\"Domain?\"] CE_DOMAIN --> CE_GENERAL[\"General:<br/>cross-encoder/ms-marco-MiniLM-L6-v2\"] CE_DOMAIN --> CE_SPECIFIC[\"Domain-specific:<br/>See MSMARCO page 5.4\"] ``` **Sources:** [README.md:19](), [docs/sentence_transformer/pretrained_models.md:16-27](), [index.rst:37-132]() ## Model Discovery and Selection",
  "Model Discovery Pathways ```mermaid graph TB subgraph \"Official_Sources\" HF_ORG[\"huggingface.co/sentence-transformers<br/>Official model organization<br/>~100 curated models\"] DOCS_HTML[\"docs/_static/html/models_en_sentence_embeddings.html<br/>Interactive Vue.js browser<br/>Sortable performance tables\"] end subgraph \"Community_Sources\" HF_COMMUNITY[\"huggingface.co/models?library=sentence-transformers<br/>15,000+ community models<br/>Diverse domains and languages\"] MTEB_BOARD[\"huggingface.co/spaces/mteb/leaderboard<br/>MTEB benchmark rankings<br/>State-of-the-art performance\"] end subgraph \"Selection_Criteria\" TASK_FIT[\"Task Compatibility<br/>sentence_performance: 49-70<br/>semantic_search: 22-57\"] SPEED_REQ[\"Speed Requirements<br/>GPU: 800-34000 sent/sec<br/>CPU: 30-750 sent/sec\"] RESOURCE_LIMIT[\"Resource Constraints<br/>Model size: 43-1360 MB<br/>Memory requirements\"] end HF_ORG --> TASK_FIT DOCS_HTML --> SPEED_REQ MTEB_BOARD --> RESOURCE_LIMIT ``` Selection Criteria and Properties | Property | Code Reference | Typical Values | Impact | |---|---|---|---| | Dimensions | `model.get_sentence_embedding_dimension()` | 384, 768, 1024 | Memory usage, similarity computation speed | | Normalized Embeddings | `normalized_embeddings: true/false` | Boolean | Score function compatibility | | Score Functions | `score_functions: [\"cos\", \"dot\", \"eucl\"]` | Array of strings | Similarity computation method | | Max Sequence Length | `max_seq_length` | 128, 256, 512 | Input text limitations | | Model Size | File size in MB | 43-1360 MB | Storage and loading time | **Sources:** [README.md:2](), [docs/_static/html/models_en_sentence_embeddings.html:236-550](), [docs/sentence_transformer/pretrained_models.md:4-7]() ## Benchmark Evaluation Framework Evaluation Metrics and Datasets ```mermaid graph TB subgraph \"SentenceTransformer_Evaluation\" ST_SENTENCE[\"Sentence Performance<br/>14 diverse tasks<br/>STS, classification, clustering\"] ST_SEARCH[\"Semantic Search<br/>6 retrieval datasets<br/>Query-passage matching\"] ST_SPEED[\"Inference Speed<br/>V100 GPU / 8-core CPU<br/>sentences per second\"] end subgraph \"SparseEncoder_Evaluation\" SE_SPARSITY[\"Sparsity Metrics<br/>SparseEncoder.sparsity()<br/>sparsity_ratio calculation\"] SE_RETRIEVAL[\"Sparse Retrieval<br/>BEIR benchmark<br/>Neural-lexical search\"] SE_EFFICIENCY[\"Memory Efficiency<br/>Storage compression<br/>Index size optimization\"] end subgraph \"CrossEncoder_Evaluation\" CE_RERANK[\"Reranking Performance<br/>TREC-DL datasets<br/>NDCG@10 scores\"] CE_CLASSIFICATION[\"Classification Tasks<br/>Binary/multi-class<br/>Accuracy and F1\"] CE_SPEED[\"Prediction Speed<br/>Pairs per second<br/>GPU/CPU throughput\"] end ``` Performance Ranges by Model Type | Model Type | Performance Metric | Range | Best Models | |---|---|---|---| | SentenceTransformer | Sentence Performance | 49-70 | `all-mpnet-base-v2` (69.57) | | SentenceTransformer | Semantic Search | 22-57 | `all-mpnet-base-v2` (57.02) | | SparseEncoder | Sparsity Ratio | 99.5-99.9% | `splade-cocondenser-ensembledistil` | | CrossEncoder | NDCG@10 | 60-75 | `ms-marco-MiniLM-L6-v2` (74.30) | | All Types | Inference Speed | 800-34000 sent/sec | GPU performance varies by size | **Sources:** [docs/_static/html/models_en_sentence_embeddings.html:113-151](), [README.md:164-166](), [docs/pretrained-models/msmarco-v5.md:29-44]() ## Quick Start Recommendations The following table provides starting points for common use cases, with links to detailed model catalogs:",
  "| Use Case | Recommended Model | Performance | Speed | Documentation | |---|---|---|---|---| | **General Embeddings** | `all-mpnet-base-v2` | Best quality (69.57) | 2800 sent/sec | [Page 5.1](#5.1) | | **Fast Embeddings** | `all-MiniLM-L6-v2` | Good quality (68.06) | 14200 sent/sec | [Page 5.1](#5.1) | | **Semantic Search** | `multi-qa-mpnet-base-cos-v1` | High search (57.46) | 4000 sent/sec | [Page 5.1](#5.1) | | **Sparse Retrieval** | `naver/splade-cocondenser-ensembledistil` | SPLADE architecture | Memory efficient | [Page 5.2](#5.2) | | **Reranking** | `cross-encoder/ms-marco-MiniLM-L6-v2` | NDCG@10: 74.30 | 39.01 MRR@10 | [Page 5.3](#5.3) | | **MSMARCO Tasks** | `msmarco-distilbert-dot-v5` | MRR@10: 37.25 | 7000 sent/sec | [Page 5.4](#5.4) | ### Model Series Overview **General Purpose (`all-*` series)** - Trained on 1B+ training pairs from diverse sources - Best for general semantic understanding tasks - Available in multiple sizes: MiniLM (fast), DistilRoBERTa (balanced), MPNet (quality) **Search-Optimized (`multi-qa-*` and `msmarco-*` series)** - Fine-tuned for question-answering and information retrieval - Optimized for query-passage similarity measurement - Available in dot-product and cosine similarity variants **Sparse Models (SPLADE variants)** - Neural sparse representations for efficient retrieval - Compatible with inverted index search engines - High sparsity (99%+) while maintaining semantic understanding **Cross-Encoder Rerankers** - Highest accuracy for pairwise relevance scoring - Computationally intensive but precise - Ideal for reranking small candidate sets ### Navigation to Detailed Catalogs - **[SentenceTransformer Models](#5.1)**: Complete catalog of dense embedding models with performance comparisons and specialized variants - **[SparseEncoder Models](#5.2)**: Sparse model architectures, SPLADE variants, and search engine integration guides - **[CrossEncoder Models](#5.3)**: Reranking and classification models across different domains and tasks - **[MSMARCO Models](#5.4)**: Specialized documentation for MSMARCO-trained models with version histories and performance evolution **Sources:** [docs/sentence_transformer/pretrained_models.md:45-124](), [README.md:169-176](), [docs/_static/html/models_en_sentence_embeddings.html:470-550]() ## Loading and Usage Patterns ### Basic Loading Patterns All pretrained models follow consistent loading and usage patterns through their respective classes from the `sentence_transformers` package: ```python # SentenceTransformer - Dense vector embeddings from sentence_transformers import SentenceTransformer # Official models (no prefix needed) model = SentenceTransformer(\"all-mpnet-base-v2\") # Community models (full path required) model = SentenceTransformer(\"BAAI/bge-large-en\") sentences = [\"The weather is lovely today.\", \"It's sunny outside!\"] embeddings = model.encode(sentences) similarities = model.similarity(embeddings, embeddings) ``` ```python",
  "conda install -c conda-forge sentence-transformers accelerate datasets pre-commit pytest ruff ``` Note that ONNX and OpenVINO extras still require pip installation even when using conda for the base package. **Sources:** [docs/installation.md:12-110]() ## Backend Dependencies ### Training Dependencies For training workflows, additional dependencies provide enhanced functionality: ```mermaid graph LR subgraph \"Core Training\" SentenceTransformers[\"sentence-transformers[train]\"] Accelerate[\"accelerate\"] Datasets[\"datasets\"] end subgraph \"Optional Training Tools\" WandB[\"wandb<br/>(tracking)\"] CodeCarbon[\"codecarbon<br/>(emissions)\"] end subgraph \"Training Components\" SentenceTransformers --> Trainers[\"SentenceTransformerTrainer<br/>SparseEncoderTrainer<br/>CrossEncoderTrainer\"] Accelerate --> DistributedTraining[\"Distributed training\"] Datasets --> DataLoading[\"Dataset loading\"] WandB --> LogTracking[\"Training log tracking\"] CodeCarbon --> ModelCards[\"Automatic model card generation\"] end ``` Install recommended training tools: ```bash pip install wandb # For experiment tracking pip install codecarbon # For carbon emissions tracking ``` ### Optimization Backends Different backends provide specific optimization capabilities: | Backend | Installation | Optimization Focus | |---------|-------------|-------------------| | PyTorch | Default | Standard deep learning operations | | ONNX Runtime | `[onnx]` or `[onnx-gpu]` | Cross-platform inference optimization | | OpenVINO | `[openvino]` | Intel CPU/GPU/VPU optimization | **Sources:** [docs/installation.md:46-52](), [docs/installation.md:96-102]() ## Source Installation ### Latest Development Version Install directly from the GitHub repository to access the latest features: ```bash # Default from source pip install git+https://github.com/UKPLab/sentence-transformers.git # Training from source pip install -U \"sentence-transformers[train] @ git+https://github.com/UKPLab/sentence-transformers.git\" # ONNX from source pip install -U \"sentence-transformers[onnx-gpu] @ git+https://github.com/UKPLab/sentence-transformers.git\" ``` ### Editable Development Install For contributors and developers making changes to the library: ```bash git clone https://github.com/UKPLab/sentence-transformers cd sentence-transformers pip install -e \".[train,dev]\" ``` This creates a link between the cloned repository and your Python environment, enabling immediate testing of code changes. **Sources:** [docs/installation.md:112-174]() ## GPU and CUDA Setup ### PyTorch CUDA Installation For GPU acceleration, install PyTorch with CUDA support before installing sentence-transformers: ```mermaid graph TD subgraph \"CUDA Setup Process\" CheckCUDA[\"Check CUDA availability\"] InstallPyTorch[\"Install PyTorch with CUDA\"] InstallST[\"Install sentence-transformers\"] VerifyGPU[\"Verify GPU detection\"] end subgraph \"Verification Commands\" CheckCUDA --> CUDACheck[\"torch.cuda.is_available()\"] InstallPyTorch --> PyTorchInstall[\"pip install torch torchvision<br/>--index-url https://download.pytorch.org/whl/cu121\"] VerifyGPU --> GPUCheck[\"model.device<br/>torch.cuda.device_count()\"] end ``` Follow the [PyTorch installation guide](https://pytorch.org/get-started/locally/) for your specific CUDA version and system configuration. **Sources:** [docs/installation.md:175-177]() ## Installation Verification ### Basic Functionality Test After installation, verify the setup works correctly: ```python",
  "This page documents the pretrained SparseEncoder models available in the sentence-transformers library, including their characteristics, performance metrics, and usage patterns. SparseEncoder models generate sparse vector representations that enable efficient neural search while maintaining interpretability through token-level activation patterns. For information about training SparseEncoder models, see [SparseEncoder Training](#3.2). For evaluation strategies, see [SparseEncoder Evaluators](#4.2). ## Model Architecture Types SparseEncoder models in the sentence-transformers ecosystem fall into two primary architectural categories, each optimized for different use cases and performance requirements. ```mermaid graph TB subgraph \"SparseEncoder Model Types\" CoreSPLADE[\"Core SPLADE Models<br/>Full Neural Inference\"] InferenceFree[\"Inference-Free SPLADE Models<br/>Hybrid Architecture\"] end subgraph \"Core SPLADE Architecture\" CoreQuery[\"Query: MLMTransformer + SpladePooling\"] CoreDoc[\"Document: MLMTransformer + SpladePooling\"] CoreSPLADE --> CoreQuery CoreSPLADE --> CoreDoc end subgraph \"Inference-Free Architecture\" IFQuery[\"Query: SparseStaticEmbedding<br/>(Pre-computed scores)\"] IFDoc[\"Document: MLMTransformer + SpladePooling\"] InferenceFree --> IFQuery InferenceFree --> IFDoc end subgraph \"Common Interface\" EncodeQuery[\"encode_query()\"] EncodeDoc[\"encode_document()\"] Similarity[\"similarity()\"] end CoreQuery --> EncodeQuery CoreDoc --> EncodeDoc IFQuery --> EncodeQuery IFDoc --> EncodeDoc EncodeQuery --> Similarity EncodeDoc --> Similarity subgraph \"Output Characteristics\" CoreOutput[\"Sparse Vectors<br/>Query Expansion: Yes<br/>Latency: Higher\"] IFOutput[\"Sparse Vectors<br/>Query Expansion: No<br/>Latency: Near-instant\"] end CoreSPLADE --> CoreOutput InferenceFree --> IFOutput ``` **SparseEncoder Model Architecture Types** Sources: [docs/sparse_encoder/pretrained_models.md:62-76]() ## Core SPLADE Models Core SPLADE models use neural inference for both queries and documents, providing query expansion capabilities and optimal retrieval performance. These models are trained on datasets like MS MARCO Passage Retrieval and evaluated on BEIR benchmarks.",
  "| Model Name | MS MARCO MRR@10 | BEIR-13 avg nDCG@10 | Parameters | Architecture | |------------|:---------------:|:-------------------:|-----------:|-------------| | [opensearch-project/opensearch-neural-sparse-encoding-v2-distill](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v2-distill) | NA | **52.8** | 67M | DistilBERT | | [opensearch-project/opensearch-neural-sparse-encoding-v1](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v1) | NA | 52.4 | 133M | BERT | | [naver/splade-v3](https://huggingface.co/naver/splade-v3) | **40.2** | 51.7 | 109M | BERT | | [ibm-granite/granite-embedding-30m-sparse](https://huggingface.co/ibm-granite/granite-embedding-30m-sparse) | NA | 50.8 | 30M | Custom | | [naver/splade-cocondenser-selfdistil](https://huggingface.co/naver/splade-cocondenser-selfdistil) | 37.6 | 50.7 | 109M | BERT | | [naver/splade_v2_distil](https://huggingface.co/naver/splade_v2_distil) | 36.8 | 50.6 | 67M | DistilBERT | | [naver/splade-v3-distilbert](https://huggingface.co/naver/splade-v3-distilbert) | 38.7 | 50.0 | 67M | DistilBERT | | [naver/splade-v3-lexical](https://huggingface.co/naver/splade-v3-lexical) | 40.0 | 49.1 | 109M | BERT | | [rasyosef/splade-mini](https://huggingface.co/rasyosef/splade-mini) | 33.2 | 42.5 | 11M | Mini | | [rasyosef/splade-tiny](https://huggingface.co/rasyosef/splade-tiny) | 30.9 | 40.6 | 4M | Tiny | **Note:** BM25 baseline achieves 18.4 MS MARCO MRR@10 and 45.6 BEIR-13 avg nDCG@10. Sources: [docs/sparse_encoder/pretrained_models.md:36-60]() ## Inference-Free SPLADE Models Inference-free SPLADE models use `SparseStaticEmbedding` for queries (pre-computed token scores) and traditional SPLADE architecture for documents. This design sacrifices query expansion for near-instant query processing speed. | Model Name | BEIR-13 avg nDCG@10 | Parameters | Document Architecture | |------------|:-------------------:|-----------:|---------------------| | [opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte) | **54.6** | 137M | BERT-Large | | [opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill) | 51.7 | 67M | DistilBERT | | [opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill) | 50.4 | 67M | DistilBERT | | [opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini) | 49.7 | 23M | Mini | | [opensearch-project/opensearch-neural-sparse-encoding-doc-v1](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v1) | 49.0 | 133M | BERT | | [naver/splade-v3-doc](https://huggingface.co/naver/splade-v3-doc) | 47.0 | 109M | BERT | Sources: [docs/sparse_encoder/pretrained_models.md:62-76]()",
  "## Basic Usage Pattern All SparseEncoder models follow a consistent interface for encoding queries and documents: ```python from sentence_transformers import SparseEncoder # Load any SparseEncoder model model = SparseEncoder(\"naver/splade-v3\") # Encode queries and documents queries = [\"what causes aging fast\"] documents = [\"UV-A light causes skin aging...\", \"Alzheimer's disease...\"] query_embeddings = model.encode_query(queries) document_embeddings = model.encode_document(documents) # Compute similarities similarities = model.similarity(query_embeddings, document_embeddings) ``` The `encode_query()` and `encode_document()` methods return sparse tensors with shape `[batch_size, vocab_size]`, where non-zero values indicate token activations. Sources: [docs/sparse_encoder/pretrained_models.md:12-33]() ## Integration with Search Systems SparseEncoder models integrate seamlessly with various search infrastructures, leveraging their sparse vector representations for efficient retrieval. ```mermaid graph TD subgraph \"SparseEncoder Integration Architecture\" QueryInput[\"Query Input<br/>encode_query()\"] DocInput[\"Document Corpus<br/>encode_document()\"] SparseModel[\"SparseEncoder<br/>(naver/splade-v3)\"] QueryInput --> SparseModel DocInput --> SparseModel end subgraph \"Sparse Vector Processing\" QuerySparse[\"Query Sparse Vector<br/>[1, 30522]\"] DocSparse[\"Document Sparse Vectors<br/>[N, 30522]\"] SparseModel --> QuerySparse SparseModel --> DocSparse end subgraph \"Search Engine Integration\" Elasticsearch[\"Elasticsearch<br/>sparse_vector field\"] OpenSearch[\"OpenSearch<br/>neural-sparse plugin\"] Qdrant[\"Qdrant<br/>sparse vectors\"] SpladeIndex[\"splade-index<br/>specialized library\"] end subgraph \"Application Layer\" SemanticSearch[\"Semantic Search<br/>semantic_search_splade_index.py\"] HybridRetrieval[\"Hybrid Retrieval<br/>Dense + Sparse\"] NeuralLexical[\"Neural Lexical Search<br/>Token-level matching\"] end QuerySparse --> Elasticsearch DocSparse --> Elasticsearch QuerySparse --> OpenSearch DocSparse --> OpenSearch QuerySparse --> Qdrant DocSparse --> Qdrant QuerySparse --> SpladeIndex DocSparse --> SpladeIndex Elasticsearch --> SemanticSearch OpenSearch --> SemanticSearch SpladeIndex --> SemanticSearch SemanticSearch --> HybridRetrieval SemanticSearch --> NeuralLexical ``` **SparseEncoder Integration with Search Systems** Sources: [examples/sparse_encoder/applications/semantic_search/semantic_search_splade_index.py:1-52](), [docs/sparse_encoder/pretrained_models.md:1-83]() ## Model Selection Guidelines ### Performance Considerations - **Highest BEIR Performance**: `opensearch-project/opensearch-neural-sparse-encoding-v2-distill` (52.8 nDCG@10) - **Highest MS MARCO Performance**: `naver/splade-v3` (40.2 MRR@10) - **Best Efficiency Trade-off**: `rasyosef/splade-tiny` (4M parameters, 40.6 BEIR nDCG@10) - **Fastest Query Processing**: Inference-free models with `SparseStaticEmbedding` ### Use Case Recommendations | Use Case | Recommended Model | Rationale | |----------|-------------------|-----------| | Production Search | `naver/splade-v3` | Best MS MARCO performance, proven reliability | | Resource-Constrained | `rasyosef/splade-tiny` | Minimal parameters, decent performance | | High-Throughput Queries | `opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte` | Inference-free query processing | | Research/Experimentation | `opensearch-project/opensearch-neural-sparse-encoding-v2-distill` | Highest BEIR performance | Sources: [docs/sparse_encoder/pretrained_models.md:36-76]() ## Model Collections Pre-organized collections of SparseEncoder models are available on the Hugging Face Hub: - **[SPLADE Models](https://huggingface.co/collections/sparse-encoder/splade-models-6862be100374b320d826eeaa)**: Complete collection of core SPLADE models - **[Inference-Free SPLADE Models](https://huggingface.co/collections/sparse-encoder/inference-free-splade-models-6862be3a1d72eab38920bc6a)**: Models optimized for query speed These collections provide curated access to models with consistent naming conventions and documented performance characteristics.",
  "Sources: [docs/sparse_encoder/pretrained_models.md:77-83]() # CrossEncoder Models This document covers the pretrained CrossEncoder models available in the sentence-transformers library, their characteristics, performance metrics, and usage patterns. CrossEncoder models are designed for pairwise text scoring and classification tasks, making them particularly effective for reranking, semantic similarity measurement, and natural language inference. For information about training CrossEncoder models, see [CrossEncoder Training](#3.3). For details about CrossEncoder evaluation methods, see [CrossEncoder Evaluators](#4.3). ## Model Architecture and Purpose CrossEncoder models process pairs of texts jointly through a single transformer model, producing similarity scores or classification outputs. Unlike bi-encoder architectures that encode texts independently, CrossEncoders perform cross-attention between the input texts, enabling more precise but computationally expensive comparisons. ```mermaid graph TD subgraph \"CrossEncoder Architecture\" TextPair[\"Text Pair Input<br/>(query, passage)\"] --> Tokenizer[\"Tokenizer\"] Tokenizer --> CrossAttention[\"Cross-Attention<br/>Transformer\"] CrossAttention --> Classifier[\"Classification Head\"] Classifier --> ActivationFn[\"Activation Function<br/>(Sigmoid/Identity)\"] ActivationFn --> Score[\"Relevance Score<br/>(0-1 or logit)\"] end subgraph \"Use Cases\" Score --> Reranking[\"Reranking Pipeline\"] Score --> STS[\"Semantic Similarity\"] Score --> NLI[\"Natural Language Inference\"] Score --> QA[\"Question Answering\"] end ``` **Sources:** [docs/cross_encoder/pretrained_models.md:1-33](), [docs/pretrained-models/ce-msmarco.md:1-63]() ## Available Model Categories The sentence-transformers library provides several categories of pretrained CrossEncoder models, each optimized for specific tasks and domains. | Category | Purpose | Key Models | Performance Metric | |----------|---------|------------|-------------------| | MS MARCO | Information Retrieval | `cross-encoder/ms-marco-MiniLM-L6-v2` | NDCG@10, MRR@10 | | STSbenchmark | Semantic Similarity | `cross-encoder/stsb-roberta-base` | Pearson Correlation | | NLI | Natural Language Inference | `cross-encoder/nli-deberta-v3-base` | Accuracy | | QNLI | Question-Passage Matching | `cross-encoder/qnli-electra-base` | Accuracy | | Quora | Duplicate Detection | `cross-encoder/quora-roberta-base` | Average Precision | **Sources:** [docs/cross_encoder/pretrained_models.md:27-112]() ### MS MARCO Models MS MARCO CrossEncoder models are specifically trained for information retrieval and reranking tasks using real user queries from Bing search engine. ```mermaid graph LR subgraph \"MS MARCO Model Hierarchy\" TinyBERT[\"cross-encoder/ms-marco-TinyBERT-L2-v2<br/>NDCG@10: 69.84<br/>9000 docs/sec\"] MiniLM2[\"cross-encoder/ms-marco-MiniLM-L2-v2<br/>NDCG@10: 71.01<br/>4100 docs/sec\"] MiniLM4[\"cross-encoder/ms-marco-MiniLM-L4-v2<br/>NDCG@10: 73.04<br/>2500 docs/sec\"] MiniLM6[\"cross-encoder/ms-marco-MiniLM-L6-v2<br/>NDCG@10: 74.30<br/>1800 docs/sec\"] MiniLM12[\"cross-encoder/ms-marco-MiniLM-L12-v2<br/>NDCG@10: 74.31<br/>960 docs/sec\"] end TinyBERT --> |\"Higher Performance\"| MiniLM2 MiniLM2 --> MiniLM4 MiniLM4 --> MiniLM6 MiniLM6 --> MiniLM12 TinyBERT --> |\"Higher Speed\"| MiniLM2 ``` **Sources:** [docs/cross_encoder/pretrained_models.md:35-42](), [docs/pretrained-models/ce-msmarco.md:41-48]() ### Community Models The ecosystem includes high-quality community-contributed models for specialized domains:",
  "- **BAAI BGE Rerankers**: `BAAI/bge-reranker-base`, `BAAI/bge-reranker-large`, `BAAI/bge-reranker-v2-m3` - **Jina AI Models**: `jinaai/jina-reranker-v1-tiny-en`, `jinaai/jina-reranker-v1-turbo-en` - **Mixedbread AI**: `mixedbread-ai/mxbai-rerank-base-v1`, `mixedbread-ai/mxbai-rerank-large-v1` - **Alibaba GTE**: `Alibaba-NLP/gte-reranker-modernbert-base`, `Alibaba-NLP/gte-multilingual-reranker-base` **Sources:** [docs/cross_encoder/pretrained_models.md:114-130]() ## Usage Patterns ### Basic Usage with SentenceTransformers ```python from sentence_transformers import CrossEncoder import torch # Load with sigmoid activation for 0-1 scores model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\", activation_fn=torch.nn.Sigmoid()) scores = model.predict([ (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants.\"), (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"), ]) # => array([0.9998173 , 0.01312432], dtype=float32) ``` ### Integration with Transformers Library ```python from transformers import AutoTokenizer, AutoModelForSequenceClassification import torch model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L6-v2\") tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L6-v2\") features = tokenizer([\"Query\", \"Query\"], [\"Paragraph1\", \"Paragraph2\"], padding=True, truncation=True, return_tensors=\"pt\") with torch.no_grad(): scores = model(**features).logits ``` **Sources:** [docs/cross_encoder/pretrained_models.md:12-33](), [docs/pretrained-models/ce-msmarco.md:19-33]() ## Performance Characteristics ### Speed vs Accuracy Trade-offs ```mermaid graph TD subgraph \"Performance Matrix\" FastLow[\"Fast & Lower Accuracy<br/>TinyBERT-L2: 9000 docs/sec<br/>NDCG@10: 69.84\"] MediumMed[\"Medium Speed & Accuracy<br/>MiniLM-L6: 1800 docs/sec<br/>NDCG@10: 74.30\"] SlowHigh[\"Slower & Higher Accuracy<br/>MiniLM-L12: 960 docs/sec<br/>NDCG@10: 74.31\"] end FastLow --> |\"Diminishing Returns\"| MediumMed MediumMed --> SlowHigh subgraph \"Use Case Mapping\" RealTime[\"Real-time Applications\"] --> FastLow BatchProcessing[\"Batch Processing\"] --> MediumMed HighPrecision[\"High Precision Tasks\"] --> SlowHigh end ``` ### Activation Function Impact | Activation Function | Output Range | Use Case | |-------------------|--------------|----------| | `torch.nn.Sigmoid()` | 0.0 - 1.0 | Probability-like scores | | `None` (Identity) | -∞ to +∞ | Raw logits for ranking | **Sources:** [docs/cross_encoder/pretrained_models.md:31-42](), [docs/pretrained-models/ce-msmarco.md:41-62]() ## Integration with Retrieval Systems CrossEncoder models are typically used in the reranking stage of two-stage retrieval systems, where they refine results from faster bi-encoder models. ```mermaid graph LR subgraph \"Retrieve & Rerank Pipeline\" Query[\"User Query\"] --> BiEncoder[\"Bi-Encoder<br/>SentenceTransformer\"] Corpus[\"Document Corpus<br/>8.8M passages\"] --> BiEncoder BiEncoder --> CandidateSet[\"Top-k Candidates<br/>(e.g., 100-1000)\"] CandidateSet --> CrossEncoder[\"CrossEncoder<br/>cross-encoder/ms-marco-MiniLM-L6-v2\"] CrossEncoder --> RankedResults[\"Final Ranked Results<br/>(e.g., top 10)\"] end subgraph \"Performance Benefits\" BiEncoder --> |\"Fast Retrieval<br/>Dense Search\"| Speed[\"Speed: ~10k docs/sec\"] CrossEncoder --> |\"Precise Reranking<br/>Cross-Attention\"| Accuracy[\"Accuracy: NDCG@10 74.30\"] end ``` ### Hard Negatives Mining CrossEncoder models are used to generate hard negatives for training bi-encoder models, creating a feedback loop for model improvement.",
  "```mermaid graph TD subgraph \"Hard Negatives Pipeline\" BiEncoderV2[\"Bi-Encoder v2<br/>msmarco-distilbert-base-v2\"] --> SimilarPassages[\"Retrieved Similar Passages\"] SimilarPassages --> CrossEncoderElectra[\"CrossEncoder<br/>electra-base-msmarco\"] CrossEncoderElectra --> LowScores[\"Low Cross-Encoder Scores<br/>(Hard Negatives)\"] LowScores --> TrainingData[\"Enhanced Training Data\"] TrainingData --> BiEncoderV3[\"Bi-Encoder v3<br/>msmarco-distilbert-base-v3\"] end ``` **Sources:** [docs/pretrained-models/msmarco-v3.md:53-58](), [docs/cross_encoder/pretrained_models.md:44]() ## Model Selection Guidelines | Task Type | Recommended Model | Key Considerations | |-----------|------------------|-------------------| | Information Retrieval | `cross-encoder/ms-marco-MiniLM-L6-v2` | Best balance of speed/accuracy | | Semantic Similarity | `cross-encoder/stsb-roberta-large` | Optimized for similarity scoring | | Question Answering | `cross-encoder/qnli-electra-base` | Passage-question relevance | | Duplicate Detection | `cross-encoder/quora-roberta-base` | Text pair classification | | Multilingual Tasks | `Alibaba-NLP/gte-multilingual-reranker-base` | Cross-lingual support | **Sources:** [docs/cross_encoder/pretrained_models.md:1-130](), [docs/pretrained-models/ce-msmarco.md:1-63]()",
  "This document covers the NanoBEIR evaluation system in sentence-transformers, which provides rapid multi-dataset information retrieval evaluation using a collection of smaller BEIR-based datasets. For comprehensive single-dataset IR evaluation, see [SentenceTransformer Evaluators](#4.1). For sparse encoder specific evaluations, see [SparseEncoder Evaluators](#4.2). ## Overview The NanoBEIR evaluation system enables quick assessment of model performance across multiple information retrieval tasks using significantly smaller datasets compared to the full BEIR benchmark. The system supports both dense embedding models (`SentenceTransformer`) and sparse embedding models (`SparseEncoder`), providing the same metrics as standard IR evaluation but aggregated across multiple datasets. The core evaluators are `NanoBEIREvaluator` for dense models and `SparseNanoBEIREvaluator` for sparse models, both extending the functionality of `InformationRetrievalEvaluator` to handle multiple datasets efficiently. Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:72-79](), [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:26-35]() ## Architecture ### Evaluator Class Hierarchy ```mermaid graph TD SentenceEvaluator[\"SentenceEvaluator<br/>__call__(), primary_metric\"] InformationRetrievalEvaluator[\"InformationRetrievalEvaluator<br/>compute_metrices(), embed_inputs()\"] SparseInformationRetrievalEvaluator[\"SparseInformationRetrievalEvaluator<br/>+ sparsity_stats, max_active_dims\"] NanoBEIREvaluator[\"NanoBEIREvaluator<br/>_load_dataset(), aggregate_fn\"] SparseNanoBEIREvaluator[\"SparseNanoBEIREvaluator<br/>information_retrieval_class\"] SentenceEvaluator --> InformationRetrievalEvaluator SentenceEvaluator --> NanoBEIREvaluator InformationRetrievalEvaluator --> SparseInformationRetrievalEvaluator NanoBEIREvaluator --> SparseNanoBEIREvaluator SparseNanoBEIREvaluator -.->|\"information_retrieval_class = <br/>SparseInformationRetrievalEvaluator\"| SparseInformationRetrievalEvaluator NanoBEIREvaluator -.->|\"information_retrieval_class = <br/>InformationRetrievalEvaluator\"| InformationRetrievalEvaluator ``` Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:191](), [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:157](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23](), [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:23]() ### Dataset Collection and Evaluation Flow ```mermaid graph LR subgraph \"Dataset Loading\" DatasetNameType[\"DatasetNameType<br/>(13 supported datasets)\"] dataset_name_to_id[\"dataset_name_to_id<br/>mapping dictionary\"] HuggingFaceHub[\"🤗 Hub<br/>zeta-alpha-ai/Nano*\"] end subgraph \"Per-Dataset Evaluation\" _load_dataset[\"_load_dataset()<br/>creates IR evaluator\"] InformationRetrievalEvaluator[\"InformationRetrievalEvaluator<br/>or SparseInformationRetrievalEvaluator\"] compute_metrics[\"compute_metrics()<br/>MRR, NDCG, MAP, etc.\"] end subgraph \"Aggregation\" aggregate_fn[\"aggregate_fn<br/>(default: np.mean)\"] per_metric_results[\"per_metric_results<br/>dict[metric, list[values]]\"] agg_results[\"aggregated results<br/>dict[metric, float]\"] end DatasetNameType --> dataset_name_to_id dataset_name_to_id --> HuggingFaceHub HuggingFaceHub --> _load_dataset _load_dataset --> InformationRetrievalEvaluator InformationRetrievalEvaluator --> compute_metrics compute_metrics --> per_metric_results per_metric_results --> aggregate_fn aggregate_fn --> agg_results ``` Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:404-434](), [sentence_transformers/evaluation/NanoBEIREvaluator.py:310-325]() ## Dataset Collection The NanoBEIR collection consists of 13 datasets, each significantly smaller than their full BEIR counterparts:",
  "| Dataset | Full Name | Hub Path | |---------|-----------|----------| | `climatefever` | ClimateFEVER | `zeta-alpha-ai/NanoClimateFEVER` | | `dbpedia` | DBPedia | `zeta-alpha-ai/NanoDBPedia` | | `fever` | FEVER | `zeta-alpha-ai/NanoFEVER` | | `fiqa2018` | FiQA2018 | `zeta-alpha-ai/NanoFiQA2018` | | `hotpotqa` | HotpotQA | `zeta-alpha-ai/NanoHotpotQA` | | `msmarco` | MSMARCO | `zeta-alpha-ai/NanoMSMARCO` | | `nfcorpus` | NFCorpus | `zeta-alpha-ai/NanoNFCorpus` | | `nq` | NQ | `zeta-alpha-ai/NanoNQ` | | `quoraretrieval` | QuoraRetrieval | `zeta-alpha-ai/NanoQuoraRetrieval` | | `scidocs` | SCIDOCS | `zeta-alpha-ai/NanoSCIDOCS` | | `arguana` | ArguAna | `zeta-alpha-ai/NanoArguAna` | | `scifact` | SciFact | `zeta-alpha-ai/NanoSciFact` | | `touche2020` | Touche2020 | `zeta-alpha-ai/NanoTouche2020` | Each dataset contains three splits: `corpus`, `queries`, and `qrels` (query relevance judgments). Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:39-69]() ## Usage Patterns ### Dense Model Evaluation ```python from sentence_transformers import SentenceTransformer from sentence_transformers.evaluation import NanoBEIREvaluator",
  "CrossEncoder evaluators assess CrossEncoder models that take pairs of texts as input and output classification or ranking scores. While SentenceTransformer evaluators focus on embedding quality, CrossEncoder evaluators measure pairwise scoring accuracy for classification, ranking, and retrieval tasks. ## Overview CrossEncoder models are evaluated using specialized evaluator classes that measure their ability to correctly score text pairs. The main evaluators applicable to CrossEncoder models include `BinaryClassificationEvaluator` for classification tasks, `RerankingEvaluator` for ranking tasks, and `InformationRetrievalEvaluator` for retrieval tasks. **CrossEncoder Evaluation Architecture** ```mermaid flowchart TD subgraph models[\"Model Types\"] CE[\"CrossEncoder\"] end subgraph evaluators[\"Evaluator Classes\"] BCE[\"BinaryClassificationEvaluator\"] RE[\"RerankingEvaluator\"] IRE[\"InformationRetrievalEvaluator\"] end subgraph metrics[\"Evaluation Metrics\"] ACC[\"Accuracy\"] F1[\"F1 Score\"] NDCG[\"NDCG@k\"] MAP[\"MAP\"] MRR[\"MRR@k\"] end CE --> BCE CE --> RE CE --> IRE BCE --> ACC BCE --> F1 RE --> NDCG RE --> MAP RE --> MRR IRE --> NDCG IRE --> MAP IRE --> MRR ``` Sources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27](), [sentence_transformers/evaluation/RerankingEvaluator.py:25](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23]() ## Core Evaluator Classes ### BinaryClassificationEvaluator The `BinaryClassificationEvaluator` class evaluates models on binary classification tasks where pairs of texts are classified as similar (1) or dissimilar (0). It computes accuracy, F1 score, precision, and recall metrics. **Key Methods:** - `__call__(model, output_path, epoch, steps)` - Main evaluation method - `compute_metrices(model)` - Computes classification metrics - `find_best_acc_and_threshold()` - Finds optimal classification threshold - `find_best_f1_and_threshold()` - Finds optimal F1 threshold **Supported Similarity Functions:** - Cosine similarity (`SimilarityFunction.COSINE`) - Dot product (`SimilarityFunction.DOT_PRODUCT`) - Manhattan distance (`SimilarityFunction.MANHATTAN`) - Euclidean distance (`SimilarityFunction.EUCLIDEAN`) Sources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-83](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:223-294]() ### RerankingEvaluator The `RerankingEvaluator` class evaluates models on reranking tasks by computing similarity scores between queries and documents, then measuring ranking quality. **Key Methods:** - `__call__(model, output_path, epoch, steps)` - Main evaluation method - `compute_metrices(model)` - Computes ranking metrics - `compute_metrices_batched(model)` - Batched computation for efficiency - `compute_metrices_individual(model)` - Individual computation for memory efficiency **Metrics Computed:** - Mean Average Precision (MAP) - Mean Reciprocal Rank (MRR@k) - Normalized Discounted Cumulative Gain (NDCG@k) Sources: [sentence_transformers/evaluation/RerankingEvaluator.py:25-87](), [sentence_transformers/evaluation/RerankingEvaluator.py:200-342]() ### InformationRetrievalEvaluator The `InformationRetrievalEvaluator` class evaluates models on information retrieval tasks using query-document pairs and relevance judgments. **Key Methods:** - `__call__(model, output_path, epoch, steps)` - Main evaluation method - `compute_metrices(model, corpus_model, corpus_embeddings)` - Computes IR metrics - `embed_inputs(model, sentences, encode_fn_name)` - Handles encoding for queries/documents **Metrics Computed:** - Accuracy@k, Precision@k, Recall@k - Mean Reciprocal Rank (MRR@k) - Normalized Discounted Cumulative Gain (NDCG@k) - Mean Average Precision (MAP@k) Sources: [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-123](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:292-408]()",
  "**Evaluator Class Hierarchy** ```mermaid classDiagram class SentenceEvaluator { <<abstract>> +__call__(model, output_path, epoch, steps) dict +primary_metric str +prefix_name_to_metrics(metrics, name) dict +store_metrics_in_model_card_data(model, metrics, epoch, steps) } class BinaryClassificationEvaluator { +sentences1 list +sentences2 list +labels list +similarity_fn_names list +compute_metrices(model) dict +find_best_acc_and_threshold(scores, labels, high_score_more_similar) tuple +find_best_f1_and_threshold(scores, labels, high_score_more_similar) tuple } class RerankingEvaluator { +samples list +at_k int +similarity_fct Callable +compute_metrices_batched(model) dict +compute_metrices_individual(model) dict } class InformationRetrievalEvaluator { +queries dict +corpus dict +relevant_docs dict +compute_metrices(model, corpus_model, corpus_embeddings) dict +embed_inputs(model, sentences, encode_fn_name) ndarray } SentenceEvaluator <|-- BinaryClassificationEvaluator SentenceEvaluator <|-- RerankingEvaluator SentenceEvaluator <|-- InformationRetrievalEvaluator ``` Sources: [sentence_transformers/evaluation/SentenceEvaluator.py](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27](), [sentence_transformers/evaluation/RerankingEvaluator.py:25](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23]() ## Evaluation Workflow CrossEncoder evaluators follow a standard evaluation process that involves data preparation, model scoring, metric computation, and result reporting. **Evaluation Process Flow** ```mermaid sequenceDiagram participant Data as \"Test Data\" participant Eval as \"Evaluator\" participant Model as \"CrossEncoder\" participant Metrics as \"Metrics Computer\" participant Output as \"CSV/Results\" Data ->> Eval: \"Load test pairs & labels\" Eval ->> Model: \"encode(sentences1, sentences2)\" Model ->> Eval: \"Return embeddings/scores\" Eval ->> Metrics: \"compute_metrices(scores, labels)\" Metrics ->> Eval: \"Return metric values\" Eval ->> Output: \"Write CSV results\" Eval ->> Model: \"Store in model_card_data\" ``` ### Standard Evaluation Steps 1. **Data Loading**: Load sentence pairs and ground truth labels 2. **Model Encoding**: Generate embeddings or scores for text pairs 3. **Score Computation**: Apply similarity functions or classification 4. **Metric Calculation**: Compute task-specific evaluation metrics 5. **Result Storage**: Write results to CSV files and model metadata ### Primary Metric Selection Each evaluator defines a `primary_metric` property that identifies the main performance measure: | Evaluator | Primary Metric | Description | |-----------|----------------|-------------| | `BinaryClassificationEvaluator` | `{name}_cosine_ap` or `{name}_max_ap` | Average Precision | | `RerankingEvaluator` | `{name}_ndcg@{k}` | NDCG at rank k | | `InformationRetrievalEvaluator` | `{name}_{score_fn}_ndcg@{k}` | NDCG with score function | Sources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:215-218](), [sentence_transformers/evaluation/RerankingEvaluator.py:135](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:272-280]() ## Usage Examples ### Binary Classification Evaluation ```python from sentence_transformers.evaluation import BinaryClassificationEvaluator # Initialize with sentence pairs and binary labels evaluator = BinaryClassificationEvaluator( sentences1=[\"The cat sat on the mat\", \"Hello world\"], sentences2=[\"A cat was sitting on a rug\", \"Goodbye earth\"], labels=[1, 0], # 1 for similar, 0 for dissimilar name=\"similarity_test\" ) # Evaluate model and get metrics results = evaluator(model) print(f\"Accuracy: {results['similarity_test_cosine_accuracy']}\") print(f\"F1 Score: {results['similarity_test_cosine_f1']}\") ``` ### Reranking Evaluation ```python from sentence_transformers.evaluation import RerankingEvaluator",
  "This page covers how to integrate sparse encoder models with search engines and vector databases for semantic search applications. Sparse encoders generate embeddings where most values are zero, enabling efficient storage and search while maintaining semantic understanding capabilities. For information about dense semantic search with `SentenceTransformer` models, see [Semantic Search](#6.1). For general sparse encoder training and usage, see [SparseEncoder Training](#3.2). ## Overview Sparse search integration allows `SparseEncoder` models to work with external search systems that can efficiently handle sparse vector data. The integration supports both manual in-memory search and production-ready vector database solutions. ```mermaid graph TD SparseEncoder[\"SparseEncoder\"] EncodeDoc[\"encode_document()\"] EncodeQuery[\"encode_query()\"] SparseEmbeddings[\"Sparse Embeddings<br/>(COO Tensors)\"] SparseEncoder --> EncodeDoc SparseEncoder --> EncodeQuery EncodeDoc --> SparseEmbeddings EncodeQuery --> SparseEmbeddings SparseEmbeddings --> ManualSearch[\"Manual Search<br/>util.semantic_search()\"] SparseEmbeddings --> VectorDBs[\"Vector Databases\"] VectorDBs --> Qdrant[\"semantic_search_qdrant()\"] VectorDBs --> Elasticsearch[\"semantic_search_elasticsearch()\"] VectorDBs --> OpenSearch[\"semantic_search_opensearch()\"] VectorDBs --> Seismic[\"semantic_search_seismic()\"] VectorDBs --> SpladeIndex[\"SPLADE-index\"] ``` **Architecture Components for Sparse Search Integration** Sources: [sentence_transformers/sparse_encoder/search_engines.py:1-556](), [examples/sparse_encoder/applications/semantic_search/README.md:1-529]() ## Manual Search vs Vector Database Search ### Manual Search Approach Manual search performs similarity computation directly in memory using PyTorch operations. This approach is suitable for small to medium-sized corpora and provides full control over the search process. | Component | Function | Purpose | |-----------|----------|---------| | Document Encoding | `model.encode_document()` | Convert documents to sparse embeddings | | Query Encoding | `model.encode_query()` | Convert queries to sparse embeddings | | Similarity Computation | `util.semantic_search()` | Compute similarity scores between query and corpus | | Results Analysis | `model.intersection()` | Analyze token-level contributions to similarity | ### Vector Database Search Approach Vector database search leverages specialized systems optimized for sparse vector operations, providing better scalability and performance for large corpora. | Search Engine | Function | Index Type | Key Features | |---------------|----------|------------|--------------| | Qdrant | `semantic_search_qdrant()` | Sparse vectors | Native sparse vector support | | Elasticsearch | `semantic_search_elasticsearch()` | rank_features | Elastic stack integration | | OpenSearch | `semantic_search_opensearch()` | neural_sparse | Amazon OpenSearch compatibility | | Seismic | `semantic_search_seismic()` | SeismicIndex | High-performance in-memory search | | SPLADE-index | External library | SciPy sparse matrices | BM25s-based implementation | Sources: [examples/sparse_encoder/applications/semantic_search/README.md:11-132]() ## Search Engine Integration Functions ### Qdrant Integration The `semantic_search_qdrant()` function provides native integration with Qdrant's sparse vector capabilities. ```mermaid graph LR QueryEmb[\"query_embeddings<br/>(COO Tensor)\"] CorpusEmb[\"corpus_embeddings<br/>(COO Tensor)\"] QueryEmb --> QdrantFunc[\"semantic_search_qdrant()\"] CorpusEmb --> QdrantFunc QdrantFunc --> QdrantClient[\"QdrantClient\"] QdrantFunc --> Collection[\"Sparse Collection\"] QdrantFunc --> SparseVector[\"SparseVector Models\"] QdrantClient --> Results[\"Search Results<br/>[{'corpus_id': int, 'score': float}]\"] ``` **Qdrant Integration Data Flow** The integration handles COO sparse tensors directly and creates collections with `SparseVectorParams` configuration. **Key Parameters:** - Input: PyTorch COO sparse tensors - Collection: Auto-generated with timestamp - Indexing: Batch processing with configurable `batch_size` - Search: Native sparse vector queries using `models.SparseVector` Sources: [sentence_transformers/sparse_encoder/search_engines.py:32-158](), [examples/sparse_encoder/applications/semantic_search/semantic_search_qdrant.py:1-64]() ### Elasticsearch Integration The `semantic_search_elasticsearch()` function uses Elasticsearch's `rank_features` field type for sparse vector storage and search.",
  "```mermaid graph LR DecodedEmb[\"query_embeddings_decoded<br/>[[('token', value)]]\"] CorpusDecoded[\"corpus_embeddings_decoded<br/>[[('token', value)]]\"] DecodedEmb --> ESFunc[\"semantic_search_elasticsearch()\"] CorpusDecoded --> ESFunc ESFunc --> ESClient[\"Elasticsearch Client\"] ESFunc --> RankFeatures[\"rank_features Mapping\"] ESFunc --> RankFeatureQuery[\"rank_feature Queries\"] ESClient --> ESResults[\"Search Results<br/>[{'corpus_id': int, '_score': float}]\"] ``` **Elasticsearch Integration Data Flow** **Key Features:** - Input: Decoded embeddings in `[('token', value)]` format - Mapping: Uses `rank_features` field type for sparse vectors - Indexing: Bulk operations with configurable batch size - Search: `rank_feature` queries with `saturation` and `boost` parameters Sources: [sentence_transformers/sparse_encoder/search_engines.py:160-297](), [examples/sparse_encoder/applications/semantic_search/semantic_search_elasticsearch.py:1-68]() ### OpenSearch Integration The `semantic_search_opensearch()` function leverages OpenSearch's `neural_sparse` query capabilities. **Key Differences from Elasticsearch:** - Uses `neural_sparse` query type instead of `rank_feature` - Compatible with Amazon OpenSearch Service - Supports asymmetric sparse encoder architectures Sources: [sentence_transformers/sparse_encoder/search_engines.py:428-556](), [examples/sparse_encoder/applications/semantic_search/semantic_search_opensearch.py:1-87]() ### Seismic Integration The `semantic_search_seismic()` function provides integration with the high-performance Seismic library for in-memory sparse vector search. ```mermaid graph LR DecodedQuery[\"query_embeddings_decoded\"] DecodedCorpus[\"corpus_embeddings_decoded\"] DecodedQuery --> SeismicFunc[\"semantic_search_seismic()\"] DecodedCorpus --> SeismicFunc SeismicFunc --> SeismicDataset[\"SeismicDataset.add_document()\"] SeismicFunc --> SeismicIndex[\"SeismicIndex.build_from_dataset()\"] SeismicFunc --> BatchSearch[\"SeismicIndex.batch_search()\"] BatchSearch --> SeismicResults[\"Sorted Results<br/>[{'corpus_id': int, 'score': float}]\"] ``` **Seismic Integration Architecture** **Performance Features:** - `SeismicDataset` for document management - `SeismicIndex.build_from_dataset()` with configurable index parameters - `batch_search()` with `query_cut` and `heap_factor` optimizations - Order-of-magnitude performance improvements over IVF approaches Sources: [sentence_transformers/sparse_encoder/search_engines.py:299-426](), [examples/sparse_encoder/applications/semantic_search/semantic_search_seismic.py:1-66]() ## Data Format Requirements ### Input Formats by Search Engine | Search Engine | Input Format | Conversion Method | |---------------|--------------|-------------------| | Qdrant | PyTorch COO sparse tensor | `convert_to_sparse_tensor=True` | | Elasticsearch | Decoded embeddings list | `model.decode(embeddings)` | | OpenSearch | Decoded embeddings list | `model.decode(embeddings)` | | Seismic | Decoded embeddings list | `model.decode(embeddings)` | ### Encoding Workflow ```mermaid graph TD TextInput[\"Text Input<br/>['query text', 'document text']\"] TextInput --> EncodeQuery[\"model.encode_query()\"] TextInput --> EncodeDoc[\"model.encode_document()\"] EncodeQuery --> SparseQuery[\"Sparse Query Embeddings\"] EncodeDoc --> SparseDoc[\"Sparse Document Embeddings\"] SparseQuery --> QdrantFormat[\"COO Tensor<br/>(for Qdrant)\"] SparseQuery --> DecodedFormat[\"Decoded Format<br/>(for ES/OpenSearch/Seismic)\"] SparseDoc --> QdrantFormat SparseDoc --> DecodedFormat QdrantFormat --> QdrantSearch[\"semantic_search_qdrant()\"] DecodedFormat --> OtherSearches[\"Other search_* functions\"] ``` **Data Format Conversion Pipeline** ### Sparse Tensor Formats **COO Sparse Tensor (Qdrant):** - Format: PyTorch coordinate format sparse tensor - Indices: `[row_indices, col_indices]` - Values: Sparse embedding values - Advantages: Direct tensor operations, GPU compatibility **Decoded Format (Others):** - Format: `List[List[Tuple[str, float]]]` - Structure: `[[('token1', 0.5), ('token2', 0.3)], ...]` - Advantages: Human-readable, search engine compatible",
  "Sources: [sentence_transformers/sparse_encoder/search_engines.py:67-76](), [examples/sparse_encoder/applications/semantic_search/README.md:53-83]() ## Integration Patterns ### Reusable Index Pattern All search engine integrations support index reuse through the `output_index` parameter: ```mermaid graph TD FirstCall[\"First Function Call\"] CreateIndex[\"Create Index<br/>(corpus_embeddings required)\"] SearchResults1[\"Search Results + Index\"] SecondCall[\"Subsequent Calls\"] ReuseIndex[\"Reuse Existing Index<br/>(corpus_index provided)\"] SearchResults2[\"Search Results Only\"] FirstCall --> CreateIndex CreateIndex --> SearchResults1 SecondCall --> ReuseIndex ReuseIndex --> SearchResults2 SearchResults1 --> IndexStorage[\"Store Index Reference\"] IndexStorage --> SecondCall ``` **Index Reuse Pattern for Production Workflows** ### Error Handling and Validation All integration functions include comprehensive input validation: - Sparse tensor format validation for Qdrant - Decoded embedding format validation for other engines - Client availability checks with helpful error messages - Required dependency import validation Sources: [sentence_transformers/sparse_encoder/search_engines.py:67-76](), [sentence_transformers/sparse_encoder/search_engines.py:204-218]() ## Performance Considerations ### Sparsity Advantages Sparse embeddings provide several performance benefits for search: | Advantage | Description | Impact | |-----------|-------------|---------| | Storage Efficiency | Most dimensions are zero | Reduced memory footprint | | Search Speed | Skip zero-value computations | Faster similarity calculations | | Interpretability | Non-zero dimensions map to tokens | Explainable search results | | Exact Matching | Preserve lexical signals | Hybrid semantic-lexical search | ### Scalability Recommendations - **Small Corpora (< 10K docs):** Manual search with `util.semantic_search()` - **Medium Corpora (10K-1M docs):** Qdrant or Seismic for performance - **Large Corpora (> 1M docs):** Elasticsearch/OpenSearch with distributed setup - **Real-time Applications:** Seismic for lowest latency in-memory search Sources: [examples/sparse_encoder/applications/semantic_search/README.md:127-132](), [examples/sparse_encoder/applications/semantic_search/README.md:388-396]()",
  "inputs = [ Image.open('image1.jpg'), \"Text description 1\", Image.open('image2.jpg'), \"Text description 2\" ] embeddings = model.encode(inputs) ``` Sources: [tests/test_image_embeddings.py:14-31]() ## Integration with SentenceTransformer Ecosystem The `CLIPModel` integrates with the sentence-transformers ecosystem through the modular architecture and supports the same operations as text-only models. **Integration Architecture** ```mermaid graph TB subgraph SentenceTransformer[\"SentenceTransformer Class\"] ENCODE[\"encode() method\"] SIMILARITY[\"similarity() method\"] MODULES[\"_modules list\"] end subgraph CLIPModule[\"CLIPModel Module\"] CLIPMOD[\"CLIPModel(InputModule)\"] TOKENIZE[\"tokenize()\"] FORWARD[\"forward()\"] end subgraph Processing[\"Shared Processing\"] MULTIPROC[\"Multi-processing support\"] NORMALIZE[\"normalize_embeddings\"] TENSOR[\"convert_to_tensor\"] end subgraph Applications[\"Application Support\"] SEARCH[\"Semantic search\"] SIMILARITY_COMP[\"Cosine similarity\"] RETRIEVAL[\"Cross-modal retrieval\"] end SentenceTransformer --> CLIPModule CLIPModule --> Processing Processing --> Applications ``` **Module System Integration** | Component | Role | Implementation | |-----------|------|----------------| | `CLIPModel` | Input processing | Inherits from `InputModule` | | `Pooling` | Optional embedding processing | Can be added after `CLIPModel` | | `Normalize` | L2 normalization | Applied to final embeddings | The `CLIPModel` appears in the module registry and supports the same configuration patterns as other sentence-transformers modules. Sources: [sentence_transformers/models/CLIPModel.py:15](), [sentence_transformers/models/__init__.py:6,40]() ## Model Loading and Configuration CLIP models are loaded through the standard `SentenceTransformer` interface or by constructing `CLIPModel` instances directly. **Configuration Options** | Parameter | Purpose | Default | Example | |-----------|---------|---------|---------| | `model_name` | Base CLIP model | Required | `'openai/clip-vit-base-patch32'` | | `processor_name` | Processor configuration | `model_name` | Custom processor path | | `max_seq_length` | Text sequence limit | From tokenizer | 77 for CLIP models | **Loading Patterns** ```python",
  "This page covers the testing framework, development environment setup, and contribution guidelines for the sentence-transformers library. It provides information about running tests, building documentation, and maintaining code quality standards. For information about training models, see [Training](#3). For details about evaluation methods, see [Evaluation](#4). ## Test Framework Overview The sentence-transformers library uses a comprehensive pytest-based testing framework that validates functionality across all three core model types: SentenceTransformer, SparseEncoder, and CrossEncoder. ```mermaid graph TB subgraph \"Test Structure\" TestRoot[\"tests/\"] MainTests[\"tests/*.py\"] CrossTests[\"tests/cross_encoder/\"] SparseTests[\"tests/sparse_encoder/\"] end subgraph \"Test Categories\" UnitTests[\"Unit Tests\"] IntegrationTests[\"Integration Tests\"] SlowTests[\"Slow Tests (@pytest.mark.slow)\"] ModelTests[\"Pretrained Model Tests\"] TrainingTests[\"Training Tests\"] end subgraph \"Test Configuration\" MainConf[\"tests/conftest.py\"] CrossConf[\"tests/cross_encoder/conftest.py\"] SparseConf[\"tests/sparse_encoder/conftest.py\"] end TestRoot --> MainTests TestRoot --> CrossTests TestRoot --> SparseTests MainConf --> UnitTests MainConf --> IntegrationTests CrossConf --> CrossTests SparseConf --> SparseTests SlowTests --> ModelTests SlowTests --> TrainingTests ``` Sources: [tests/conftest.py:1-115](), [tests/cross_encoder/conftest.py:1-23](), [tests/sparse_encoder/conftest.py:1-46]() ## Test Configuration and Fixtures The testing framework uses pytest fixtures to manage test dependencies and ensure proper isolation between tests. ### Core Test Fixtures | Fixture | Scope | Purpose | Location | |---------|-------|---------|----------| | `stsb_bert_tiny_model` | Function | SentenceTransformer model for testing | [tests/conftest.py:26-28]() | | `splade_bert_tiny_model` | Function | SparseEncoder model for testing | [tests/sparse_encoder/conftest.py:17-19]() | | `reranker_bert_tiny_model` | Function | CrossEncoder model for testing | [tests/cross_encoder/conftest.py:20-22]() | | `cache_dir` | Function | Temporary directory for CI environments | [tests/conftest.py:102-114]() | ### Model Loading Strategy ```mermaid graph LR subgraph \"Session Fixtures\" SessionModel[\"_stsb_bert_tiny_model<br/>(session scope)\"] SessionSparse[\"_splade_bert_tiny_model<br/>(session scope)\"] end subgraph \"Function Fixtures\" FuncModel[\"stsb_bert_tiny_model<br/>(function scope)\"] FuncSparse[\"splade_bert_tiny_model<br/>(function scope)\"] end SessionModel --> |\"deepcopy()\"| FuncModel SessionSparse --> |\"deepcopy()\"| FuncSparse FuncModel --> TestCase1[\"test_encode()\"] FuncModel --> TestCase2[\"test_similarity()\"] FuncSparse --> TestCase3[\"test_sparse_encode()\"] ``` The testing framework uses session-scoped fixtures to load models once per test session, then creates function-scoped copies using `deepcopy()` to ensure test isolation without the overhead of repeatedly loading models. Sources: [tests/conftest.py:19-40](), [tests/sparse_encoder/conftest.py:10-31](), [tests/cross_encoder/conftest.py:15-22]() ## Test Categories ### Performance Validation Tests The `test_pretrained_stsb.py` module validates that pretrained models maintain expected performance on the STSbenchmark dataset: ```python",
  "This document covers the loss functions available for training CrossEncoder models in the sentence-transformers library. CrossEncoder loss functions are specialized for tasks that require joint encoding of text pairs, such as reranking, classification, and learning-to-rank applications. For information about SentenceTransformer loss functions, see [Loss Functions for SentenceTransformer](#3.4). For SparseEncoder loss functions, see [Loss Functions for SparseEncoder](#3.5). ## Overview CrossEncoder loss functions are designed to train models that process text pairs jointly through a single transformer encoder. These loss functions fall into three main categories: - **Learning-to-Rank Losses**: Optimize ranking metrics like NDCG for information retrieval tasks - **Classification Losses**: Handle binary or multi-class classification scenarios - **Regression Losses**: Predict continuous similarity scores between text pairs ## Loss Function Hierarchy The following diagram shows the inheritance and relationship structure of CrossEncoder loss functions: ```mermaid graph TD Module[\"nn.Module\"] subgraph \"Learning-to-Rank Losses\" LambdaLoss[\"LambdaLoss\"] ListNetLoss[\"ListNetLoss\"] PListMLELoss[\"PListMLELoss\"] ListMLELoss[\"ListMLELoss\"] RankNetLoss[\"RankNetLoss\"] end subgraph \"Classification Losses\" BinaryCrossEntropyLoss[\"BinaryCrossEntropyLoss\"] CrossEntropyLoss[\"CrossEntropyLoss\"] MultipleNegativesRankingLoss[\"MultipleNegativesRankingLoss\"] CachedMultipleNegativesRankingLoss[\"CachedMultipleNegativesRankingLoss\"] end subgraph \"Regression Losses\" MSELoss[\"MSELoss\"] MarginMSELoss[\"MarginMSELoss\"] end subgraph \"Weighting Schemes\" BaseWeightingScheme[\"BaseWeightingScheme\"] NoWeightingScheme[\"NoWeightingScheme\"] NDCGLoss1Scheme[\"NDCGLoss1Scheme\"] NDCGLoss2Scheme[\"NDCGLoss2Scheme\"] LambdaRankScheme[\"LambdaRankScheme\"] NDCGLoss2PPScheme[\"NDCGLoss2PPScheme\"] PListMLELambdaWeight[\"PListMLELambdaWeight\"] end Module --> LambdaLoss Module --> ListNetLoss Module --> PListMLELoss Module --> BinaryCrossEntropyLoss Module --> CrossEntropyLoss Module --> MultipleNegativesRankingLoss Module --> CachedMultipleNegativesRankingLoss Module --> MSELoss Module --> MarginMSELoss Module --> BaseWeightingScheme ListMLELoss --> PListMLELoss LambdaLoss --> RankNetLoss BaseWeightingScheme --> NoWeightingScheme BaseWeightingScheme --> NDCGLoss1Scheme BaseWeightingScheme --> NDCGLoss2Scheme BaseWeightingScheme --> LambdaRankScheme BaseWeightingScheme --> NDCGLoss2PPScheme Module --> PListMLELambdaWeight LambdaLoss -.-> BaseWeightingScheme PListMLELoss -.-> PListMLELambdaWeight ``` Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:103-361](), [sentence_transformers/cross_encoder/losses/ListNetLoss.py:10-198](), [sentence_transformers/cross_encoder/losses/PListMLELoss.py:45-295](), [sentence_transformers/cross_encoder/losses/ListMLELoss.py:9-127](), [sentence_transformers/cross_encoder/losses/RankNetLoss.py:11-124](), [docs/package_reference/cross_encoder/losses.md:1-68]() ## Learning-to-Rank Loss Functions Learning-to-rank losses are designed for information retrieval tasks where the goal is to rank documents by relevance for a given query. These losses work with listwise data formats. ### Data Format Requirements All learning-to-rank losses expect the following input format: | Component | Format | Description | |-----------|--------|-------------| | Inputs | `(queries, documents_list)` | List of query strings and list of document lists | | Labels | `[score1, score2, ..., scoreN]` | List of relevance scores per query | | Model Output | 1 label | Single relevance score per query-document pair | ### LambdaLoss Framework The `LambdaLoss` class implements a comprehensive framework for ranking metric optimization with multiple weighting schemes:",
  "```mermaid graph LR subgraph \"Input Processing\" QueryDocs[\"queries + docs_list\"] --> Pairs[\"query-document pairs\"] Labels[\"labels list\"] --> LabelMatrix[\"labels_matrix\"] end subgraph \"Model Processing\" Pairs --> CrossEncoder[\"model.forward()\"] CrossEncoder --> Logits[\"logits\"] Logits --> ActivationFn[\"activation_fn\"] ActivationFn --> LogitsMatrix[\"logits_matrix\"] end subgraph \"LambdaLoss Computation\" LogitsMatrix --> Sorting[\"sort by logits\"] LabelMatrix --> Sorting Sorting --> TrueDiffs[\"true_diffs\"] Sorting --> Gains[\"gain calculation\"] Sorting --> Discounts[\"discount calculation\"] Gains --> WeightingScheme[\"weighting_scheme.forward()\"] Discounts --> WeightingScheme WeightingScheme --> Weights[\"weights\"] TrueDiffs --> ScoreDiffs[\"score differences\"] ScoreDiffs --> WeightedProbas[\"weighted probabilities\"] Weights --> WeightedProbas WeightedProbas --> Loss[\"final loss\"] end ``` The `LambdaLoss` supports five weighting schemes: | Scheme | Class | Purpose | |--------|-------|---------| | No Weighting | `NoWeightingScheme` | Uniform weights (RankNet equivalent) | | NDCG Loss1 | `NDCGLoss1Scheme` | Basic NDCG optimization | | NDCG Loss2 | `NDCGLoss2Scheme` | Improved NDCG with tighter bounds | | LambdaRank | `LambdaRankScheme` | Coarse upper bound optimization | | NDCG Loss2++ | `NDCGLoss2PPScheme` | Hybrid scheme (recommended) | Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:103-361](), [sentence_transformers/cross_encoder/losses/LambdaLoss.py:12-101]() ### ListNet Loss The `ListNetLoss` implements the ListNet ranking algorithm using cross-entropy between predicted and ground truth ranking distributions: ```python",
  "samples = [ { \"query\": \"What is machine learning?\", \"positive\": [\"Machine learning is a subset of AI\"], \"negative\": [\"The weather is nice today\", \"Cats are animals\"] } ] evaluator = RerankingEvaluator(samples=samples, name=\"rerank_test\") results = evaluator(model) print(f\"NDCG@10: {results['rerank_test_ndcg@10']}\") ``` ### Information Retrieval Evaluation ```python from sentence_transformers.evaluation import InformationRetrievalEvaluator # Prepare queries, corpus, and relevance judgments queries = {\"q1\": \"machine learning definition\"} corpus = {\"d1\": \"ML is AI subset\", \"d2\": \"Weather is sunny\"} relevant_docs = {\"q1\": {\"d1\"}} evaluator = InformationRetrievalEvaluator( queries=queries, corpus=corpus, relevant_docs=relevant_docs, name=\"ir_test\" ) results = evaluator(model) print(f\"MAP@100: {results['ir_test_cosine_map@100']}\") ``` Sources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:49-83](), [sentence_transformers/evaluation/RerankingEvaluator.py:48-87](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:54-123](), [tests/test_pretrained_stsb.py:74-79]() ## Training Integration Evaluators integrate with training systems to monitor model performance during training. They are called at specified intervals to compute metrics and store results in CSV files and model metadata. **Training and Evaluation Integration** ```mermaid flowchart TD subgraph training[\"Training System\"] TR[\"Trainer\"] TD[\"Training Data\"] LOSS[\"Loss Functions\"] end subgraph evaluation[\"Evaluation System\"] BCE[\"BinaryClassificationEvaluator\"] RE[\"RerankingEvaluator\"] IRE[\"InformationRetrievalEvaluator\"] CSV[\"CSV Results\"] MCD[\"ModelCardData\"] end subgraph model[\"Model\"] CE[\"CrossEncoder\"] end TD --> TR LOSS --> TR TR --> CE CE --> BCE CE --> RE CE --> IRE BCE --> CSV RE --> CSV IRE --> CSV BCE --> MCD RE --> MCD IRE --> MCD MCD --> CE ``` ### Evaluation During Training Evaluators are called with epoch and step parameters to track training progress: ```python # Called automatically during training results = evaluator(model, output_path=\"./results\", epoch=1, steps=100) # Results are written to CSV files like: # - binary_classification_evaluation_results.csv # - RerankingEvaluator_results_@10.csv # - Information-Retrieval_evaluation_results.csv ``` ### Model Card Integration Evaluation results are automatically stored in the model's metadata via the `store_metrics_in_model_card_data()` method, which updates `model.model_card_data` with performance metrics. Sources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:151-221](), [sentence_transformers/evaluation/RerankingEvaluator.py:137-198](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:211-290]() ## 6. Creating Custom Evaluators You can create custom evaluators for specialized evaluation tasks by: 1. Inheriting from the base evaluator class 2. Implementing the required evaluation methods 3. Defining metrics that are relevant to your task A custom evaluator class typically implements: - An initialization method that accepts test data - An evaluation method that computes scores for the test data - Methods to compute task-specific metrics ## 7. Common Evaluation Metrics Different tasks require different evaluation metrics: | Task Type | Common Metrics | Description | |-----------|----------------|-------------| | Binary Classification | Accuracy, F1, AUC | Measure classification performance | | Ranking | nDCG, MAP, MRR | Assess ranking quality | | Retrieval | Precision@k, Recall@k | Evaluate retrieval effectiveness | | Regression | MSE, Pearson/Spearman correlation | Measure score prediction accuracy | Sources: System architecture diagrams from prompt, tests/test_pretrained_stsb.py (lines 39-46) ## 8. Performance Considerations When evaluating large datasets, consider: - Batch processing: Evaluate models in batches to avoid memory issues - Caching: Cache model outputs to avoid redundant computation - Metrics selection: Choose metrics appropriate for your task and dataset size Efficient evaluation is especially important when working with resource-intensive models or large test sets. ## 9. Comparison with SentenceTransformer Evaluators While both types of evaluators assess model performance, they differ in key ways:",
  "| CrossEncoder Evaluators | SentenceTransformer Evaluators | |------------------------|--------------------------------| | Evaluate pair scoring | Evaluate embedding quality | | Focus on classification/ranking metrics | Focus on similarity and retrieval metrics | | Work with direct text pair inputs | Work with embeddings | | Suited for reranking tasks | Suited for retrieval and similarity tasks | Understanding these differences helps in selecting the appropriate evaluation method for your model type and task.",
  "This document covers implementing semantic search using dense embeddings generated by `SentenceTransformer` models. Semantic search enables finding relevant documents based on meaning rather than exact keyword matching, using vector similarity in high-dimensional embedding spaces. For sparse retrieval approaches using search engines, see [Sparse Search Integration](#6.2). For two-stage systems combining retrieval and reranking, see [Retrieve & Rerank Architecture](#6.3). For measuring text similarity in general, see [Semantic Textual Similarity](#6.4). ## Overview Semantic search with sentence-transformers uses dense vector embeddings to represent both queries and documents in a shared semantic space. Unlike traditional keyword-based search, this approach captures semantic meaning and can find relevant results even when exact terms don't match. ### Semantic Search Flow ```mermaid flowchart TD Query[\"Query Text\"] --> QueryEncode[\"model.encode()\"] Docs[\"Document Collection\"] --> DocEncode[\"model.encode()\"] QueryEncode --> QueryEmb[\"Query Embedding<br/>(dense vector)\"] DocEncode --> DocEmb[\"Document Embeddings<br/>(dense vectors)\"] QueryEmb --> Similarity[\"util.pytorch_cos_sim()\"] DocEmb --> Similarity Similarity --> Results[\"Ranked Results\"] subgraph VectorDB [\"Vector Database Storage\"] DocEmb --> Store[\"Store embeddings\"] Store --> Retrieve[\"Similarity search\"] QueryEmb --> Retrieve Retrieve --> Results end ``` Sources: [docs/pretrained-models/msmarco-v2.md:8-15]() ## Basic Implementation Pattern The fundamental pattern for semantic search involves three steps: encoding the query, encoding the document collection, and computing similarity scores. ### Core Components ```mermaid graph LR subgraph Input [\"Input Layer\"] QueryText[\"Query Text\"] DocText[\"Document Text\"] end subgraph Model [\"SentenceTransformer\"] Encode[\"encode() method\"] end subgraph Similarity [\"Similarity Computation\"] CosSim[\"util.pytorch_cos_sim()\"] SemanticSim[\"util.semantic_search()\"] end subgraph Output [\"Output Layer\"] Scores[\"Similarity Scores\"] RankedResults[\"Ranked Results\"] end QueryText --> Encode DocText --> Encode Encode --> CosSim Encode --> SemanticSim CosSim --> Scores SemanticSim --> RankedResults ``` Sources: [docs/pretrained-models/msmarco-v2.md:8-15]() ### Model Selection The choice of `SentenceTransformer` model significantly impacts search quality. Models trained on information retrieval datasets like MS MARCO provide better performance for search tasks compared to general-purpose models. | Model Type | Use Case | Example | |------------|----------|---------| | MSMARCO-trained | Information retrieval | `msmarco-distilroberta-base-v2` | | General-purpose | Broad semantic similarity | `all-MiniLM-L6-v2` | | Domain-specific | Specialized fields | BioBERT variants | Sources: [docs/pretrained-models/msmarco-v2.md:10](), [docs/pretrained-models/msmarco-v2.md:27-32]() ## Vector Database Integration For large-scale semantic search, vector databases provide efficient storage and retrieval of dense embeddings. The integration pattern involves pre-computing document embeddings and storing them for fast similarity search. ### Vector Database Architecture ```mermaid graph TB subgraph Preprocessing [\"Offline Preprocessing\"] Corpus[\"Document Corpus\"] STModel[\"SentenceTransformer\"] Corpus --> STModel STModel --> Embeddings[\"Dense Embeddings\"] end subgraph VectorDB [\"Vector Database\"] Embeddings --> Store[\"store() / index()\"] Store --> Index[\"Vector Index<br/>(HNSW, IVF, etc.)\"] end subgraph Runtime [\"Query Runtime\"] QueryText[\"Query Text\"] QueryEmbed[\"Query Encoding\"] QueryText --> QueryEmbed QueryEmbed --> Search[\"similarity_search()\"] Index --> Search Search --> Results[\"Top-K Results\"] end subgraph Databases [\"Supported Databases\"] Pinecone[\"Pinecone\"] Weaviate[\"Weaviate\"] Qdrant[\"Qdrant\"] ChromaDB[\"ChromaDB\"] end Store -.-> Pinecone Store -.-> Weaviate Store -.-> Qdrant Store -.-> ChromaDB ``` Sources: Based on integration patterns mentioned in the repository overview ## Performance Considerations Semantic search performance depends on both model quality and computational efficiency. The choice of model affects both retrieval quality and inference speed. ### Model Performance Comparison Based on evaluation against traditional keyword search (BM25), dense embedding models show significant improvements in retrieval quality:",
  "| Approach | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco) | |----------|---------------------|-------------------| | BM25 (Elasticsearch) | 45.46 | 17.29 | | `msmarco-distilroberta-base-v2` | 65.65 | 28.55 | | `msmarco-roberta-base-v2` | 67.18 | 29.17 | | `msmarco-distilbert-base-v2` | 68.35 | 30.77 | The substantial improvement over BM25 demonstrates the effectiveness of semantic search for information retrieval tasks. Sources: [docs/pretrained-models/msmarco-v2.md:25-32]() ### Optimization Strategies For production semantic search systems, several optimization techniques apply: 1. **Model Selection**: Choose appropriately sized models balancing quality and speed 2. **Batch Processing**: Encode multiple documents simultaneously for better throughput 3. **Caching**: Cache frequently accessed embeddings to reduce computation 4. **Quantization**: Use reduced precision embeddings to decrease memory usage 5. **Approximate Search**: Leverage vector database indexing for sub-linear search time ## Implementation Patterns ### Single-Query Search The basic pattern for single-query semantic search involves encoding the query and computing similarity against a collection of pre-computed document embeddings. ```python # Referenced pattern from file model = SentenceTransformer(\"msmarco-distilroberta-base-v2\") query_embedding = model.encode(\"How big is London\") passage_embedding = model.encode(\"London has 9,787,426 inhabitants at the 2011 census\") similarity = util.pytorch_cos_sim(query_embedding, passage_embedding) ``` Sources: [docs/pretrained-models/msmarco-v2.md:10-15]() ### Batch Processing For processing multiple queries or documents, batch encoding provides better performance through parallelization within the model. ### Cross-Lingual Search `SentenceTransformer` models trained on multilingual data enable semantic search across language boundaries, where queries in one language can retrieve relevant documents in another language. ## Integration Examples The MS MARCO models demonstrate effective semantic search for information retrieval tasks. These models are specifically trained on search query-passage pairs, making them well-suited for question-answering and document retrieval applications. Training data characteristics: - Over 500,000 query-passage examples - Complete corpus of 8.8 million passages - Real user search queries from Bing search engine Sources: [docs/pretrained-models/msmarco-v2.md:2-4]()"
]