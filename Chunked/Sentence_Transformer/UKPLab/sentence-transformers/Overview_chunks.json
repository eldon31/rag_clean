[
  {
    "text": "## Purpose and Scope\n\nThe sentence-transformers library is a comprehensive Python framework for accessing, using, and training state-of-the-art embedding and reranker models. It provides three core model types that serve different purposes in natural language processing pipelines: `SentenceTransformer` for dense embeddings, `SparseEncoder` for sparse embeddings, and `CrossEncoder` for pairwise scoring and reranking.\n\nThis document covers the high-level architecture and core concepts of the sentence-transformers library. For specific usage instructions, see [Quickstart Guide](#2.1). For detailed training procedures, see [Training](#3). For performance optimization, see [Advanced Topics](#7).\n\nSources: [README.md:15-21](), [sentence_transformers/__init__.py:27-34]()",
    "metadata": {
      "chunk_id": "143b45214c47-0000",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "filename": "Overview.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Purpose and Scope"
      ],
      "heading_text": "Purpose and Scope",
      "token_count": 157,
      "char_count": 774,
      "start_char": 0,
      "end_char": 774,
      "semantic_score": 0.7,
      "structural_score": 0.7,
      "retrieval_quality": 0.7244827586206896,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.939383",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 157,
      "document_id": "143b45214c47",
      "document_name": "Overview",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "source_filename": "Overview.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "hierarchy_path": "Purpose and Scope",
      "chunk_hash": "5c73b63d3c2771fa",
      "content_digest": "5c73b63d3c2771fa",
      "chunk_length": 774,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "for",
          "and",
          "the",
          "sentence",
          "transformers",
          "training",
          "see",
          "library",
          "core",
          "embeddings",
          "purpose",
          "scope",
          "comprehensive",
          "python",
          "framework",
          "accessing",
          "using",
          "state",
          "art",
          "embedding"
        ],
        "term_weights": [
          {
            "term": "for",
            "tf": 7,
            "weight": 0.079545
          },
          {
            "term": "and",
            "tf": 6,
            "weight": 0.068182
          },
          {
            "term": "the",
            "tf": 4,
            "weight": 0.045455
          },
          {
            "term": "sentence",
            "tf": 3,
            "weight": 0.034091
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.034091
          },
          {
            "term": "training",
            "tf": 3,
            "weight": 0.034091
          },
          {
            "term": "see",
            "tf": 3,
            "weight": 0.034091
          },
          {
            "term": "library",
            "tf": 2,
            "weight": 0.022727
          },
          {
            "term": "core",
            "tf": 2,
            "weight": 0.022727
          },
          {
            "term": "embeddings",
            "tf": 2,
            "weight": 0.022727
          },
          {
            "term": "purpose",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "scope",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "comprehensive",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "framework",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "accessing",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "using",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "state",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "art",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "embedding",
            "tf": 1,
            "weight": 0.011364
          }
        ],
        "unique_terms": 63,
        "total_terms": 88
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Purpose and Scope",
        "and",
        "core",
        "embeddings",
        "for",
        "library",
        "see",
        "sentence",
        "the",
        "training",
        "transformers"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7,
      "retrieval_quality": 0.7244827586206896,
      "overall": 0.7081609195402297
    }
  },
  {
    "text": "## Core Architecture  The sentence-transformers library is built around three fundamental model architectures that can be used independently or in combination for various NLP tasks: ```mermaid graph TB     subgraph \"sentence_transformers Library\"         ST[\"SentenceTransformer<br/>Dense Embeddings\"]         SE[\"SparseEncoder<br/>Sparse Embeddings\"]         CE[\"CrossEncoder<br/>Pairwise Scoring\"]     end          subgraph \"Core Functionality\"         ST --> |\"encode()\"| DenseEmb[\"Dense Vector<br/>Embeddings\"]         SE --> |\"encode_query()<br/>encode_document()\"| SparseEmb[\"Sparse Vector<br/>Embeddings\"]         CE --> |\"predict()<br/>rank()\"| Scores[\"Relevance<br/>Scores\"]     end          subgraph \"Primary Applications\"         DenseEmb --> SemanticSearch[\"Semantic Search\"]         DenseEmb --> Clustering[\"Clustering\"]         SparseEmb --> NeuralLexical[\"Neural Lexical<br/>Search\"]         SparseEmb --> HybridRetrieval[\"Hybrid Retrieval\"]         Scores --> Reranking[\"Reranking\"]         Scores --> Classification[\"Text Classification\"]     end          subgraph \"Training Infrastructure\"         STTrainer[\"SentenceTransformerTrainer\"]         SETrainer[\"SparseEncoderTrainer\"]         CETrainer[\"CrossEncoderTrainer\"]                  STTrainer --> ST         SETrainer --> SE         CETrainer --> CE     end ``` Each model type has corresponding trainer classes and specialized loss functions optimized for their specific use cases. The library provides over 15,000 pretrained models available through Hugging Face Hub. Sources: [sentence_transformers/__init__.py:15-36](), [README.md:19](), [index.rst:12-15]()",
    "metadata": {
      "chunk_id": "143b45214c47-0001",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "filename": "Overview.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Core Architecture"
      ],
      "heading_text": "Core Architecture",
      "token_count": 357,
      "char_count": 1634,
      "start_char": 776,
      "end_char": 2410,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5411909090909091,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.944017",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 357,
      "document_id": "143b45214c47",
      "document_name": "Overview",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "source_filename": "Overview.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "hierarchy_path": "Core Architecture",
      "chunk_hash": "9afe59cf022f8caf",
      "content_digest": "9afe59cf022f8caf",
      "chunk_length": 1634,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "subgraph",
          "embeddings",
          "end",
          "scores",
          "sentence",
          "transformers",
          "library",
          "encode",
          "denseemb",
          "sparseemb",
          "core",
          "the",
          "model",
          "for",
          "dense",
          "sparse",
          "vector",
          "search",
          "clustering",
          "reranking"
        ],
        "term_weights": [
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.02963
          },
          {
            "term": "embeddings",
            "tf": 4,
            "weight": 0.02963
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.02963
          },
          {
            "term": "scores",
            "tf": 4,
            "weight": 0.02963
          },
          {
            "term": "sentence",
            "tf": 3,
            "weight": 0.022222
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.022222
          },
          {
            "term": "library",
            "tf": 3,
            "weight": 0.022222
          },
          {
            "term": "encode",
            "tf": 3,
            "weight": 0.022222
          },
          {
            "term": "denseemb",
            "tf": 3,
            "weight": 0.022222
          },
          {
            "term": "sparseemb",
            "tf": 3,
            "weight": 0.022222
          },
          {
            "term": "core",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "model",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "dense",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "sparse",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "vector",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "clustering",
            "tf": 2,
            "weight": 0.014815
          },
          {
            "term": "reranking",
            "tf": 2,
            "weight": 0.014815
          }
        ],
        "unique_terms": 97,
        "total_terms": 135
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Core Architecture",
        "denseemb",
        "embeddings",
        "encode",
        "end",
        "library",
        "scores",
        "sentence",
        "sparseemb",
        "subgraph",
        "transformers"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5411909090909091,
      "overall": 0.7470636363636363
    }
  },
  {
    "text": "## Model Types",
    "metadata": {
      "chunk_id": "143b45214c47-0002",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "filename": "Overview.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Model Types"
      ],
      "heading_text": "Model Types",
      "token_count": 3,
      "char_count": 14,
      "start_char": 2415,
      "end_char": 2429,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.944512",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 3,
      "document_id": "143b45214c47",
      "document_name": "Overview",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "source_filename": "Overview.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "hierarchy_path": "Model Types",
      "chunk_hash": "fa462c843eb3388e",
      "content_digest": "fa462c843eb3388e",
      "chunk_length": 14,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "model",
          "types"
        ],
        "term_weights": [
          {
            "term": "model",
            "tf": 1,
            "weight": 0.5
          },
          {
            "term": "types",
            "tf": 1,
            "weight": 0.5
          }
        ],
        "unique_terms": 2,
        "total_terms": 2
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Model Types",
        "model",
        "types"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### SentenceTransformer  The `SentenceTransformer` class produces dense vector embeddings where semantically similar texts have similar vector representations. These models use bi-encoder architectures that independently encode each input text. **Key characteristics:** - Output: Dense vectors (typically 384-1024 dimensions) - Use case: Semantic similarity, clustering, dense retrieval - Similarity functions: Cosine similarity, dot product, Euclidean distance - Example models: `all-MiniLM-L6-v2`, `all-mpnet-base-v2`  **Basic usage pattern:** ```python from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = model.encode(sentences) similarities = model.similarity(embeddings, embeddings) ``` Sources: [README.md:56-87](), [sentence_transformers/__init__.py:27]()",
    "metadata": {
      "chunk_id": "143b45214c47-0003",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "filename": "Overview.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "SentenceTransformer"
      ],
      "heading_text": "SentenceTransformer",
      "token_count": 174,
      "char_count": 825,
      "start_char": 2431,
      "end_char": 3256,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7418518518518519,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.946545",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 174,
      "document_id": "143b45214c47",
      "document_name": "Overview",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "source_filename": "Overview.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "hierarchy_path": "SentenceTransformer",
      "chunk_hash": "3f9b3bb8ba6ef7aa",
      "content_digest": "3f9b3bb8ba6ef7aa",
      "chunk_length": 825,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sentencetransformer",
          "embeddings",
          "similarity",
          "dense",
          "all",
          "model",
          "vector",
          "similar",
          "models",
          "use",
          "encode",
          "minilm",
          "sentence",
          "transformers",
          "the",
          "class",
          "produces",
          "where",
          "semantically",
          "texts"
        ],
        "term_weights": [
          {
            "term": "sentencetransformer",
            "tf": 4,
            "weight": 0.047059
          },
          {
            "term": "embeddings",
            "tf": 4,
            "weight": 0.047059
          },
          {
            "term": "similarity",
            "tf": 4,
            "weight": 0.047059
          },
          {
            "term": "dense",
            "tf": 3,
            "weight": 0.035294
          },
          {
            "term": "all",
            "tf": 3,
            "weight": 0.035294
          },
          {
            "term": "model",
            "tf": 3,
            "weight": 0.035294
          },
          {
            "term": "vector",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "similar",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "use",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "encode",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "minilm",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "transformers",
            "tf": 2,
            "weight": 0.023529
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.011765
          },
          {
            "term": "class",
            "tf": 1,
            "weight": 0.011765
          },
          {
            "term": "produces",
            "tf": 1,
            "weight": 0.011765
          },
          {
            "term": "where",
            "tf": 1,
            "weight": 0.011765
          },
          {
            "term": "semantically",
            "tf": 1,
            "weight": 0.011765
          },
          {
            "term": "texts",
            "tf": 1,
            "weight": 0.011765
          }
        ],
        "unique_terms": 62,
        "total_terms": 85
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "SentenceTransformer",
        "all",
        "dense",
        "embeddings",
        "model",
        "models",
        "sentencetransformer",
        "similar",
        "similarity",
        "use",
        "vector"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7418518518518519,
      "overall": 0.8139506172839505
    }
  },
  {
    "text": "## Integration Ecosystem  The sentence-transformers library integrates with a wide ecosystem of tools and platforms:  **Backend Support:** - PyTorch (default) - ONNX Runtime for optimized inference - Intel OpenVINO for CPU optimization  **Vector Databases:** - Pinecone, Weaviate, Qdrant, ChromaDB  **Search Engines:** - Elasticsearch, OpenSearch, Apache Solr  **ML Frameworks:** - Hugging Face ecosystem (transformers, datasets, hub) - LangChain, Haystack, LlamaIndex  **Specialized Libraries:** - BERTopic, KeyBERT, SetFit for domain-specific applications  Sources: [sentence_transformers/__init__.py:10-14](), [README.md:172-189]()",
    "metadata": {
      "chunk_id": "143b45214c47-0007",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "filename": "Overview.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Integration Ecosystem"
      ],
      "heading_text": "Integration Ecosystem",
      "token_count": 156,
      "char_count": 634,
      "start_char": 6281,
      "end_char": 6915,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5435211267605634,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.954283",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 156,
      "document_id": "143b45214c47",
      "document_name": "Overview",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "source_filename": "Overview.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "hierarchy_path": "Integration Ecosystem",
      "chunk_hash": "aed7bf1df5a2b0fd",
      "content_digest": "aed7bf1df5a2b0fd",
      "chunk_length": 634,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "ecosystem",
          "transformers",
          "for",
          "sentence",
          "integration",
          "the",
          "library",
          "integrates",
          "with",
          "wide",
          "tools",
          "and",
          "platforms",
          "backend",
          "support",
          "pytorch",
          "default",
          "onnx",
          "runtime",
          "optimized"
        ],
        "term_weights": [
          {
            "term": "ecosystem",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "integration",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "library",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "integrates",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "wide",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "tools",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "platforms",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "backend",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "support",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "pytorch",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "default",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "onnx",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "runtime",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "optimized",
            "tf": 1,
            "weight": 0.015385
          }
        ],
        "unique_terms": 58,
        "total_terms": 65
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Integration Ecosystem",
        "ecosystem",
        "for",
        "integrates",
        "integration",
        "library",
        "sentence",
        "the",
        "transformers",
        "wide",
        "with"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5435211267605634,
      "overall": 0.7478403755868545
    }
  },
  {
    "text": "## Module Architecture  The library follows a modular design where models are composed of reusable components: ```mermaid graph TB     subgraph \"Core Modules\"         Transformer[\"Transformer<br/>Base encoding\"]         Pooling[\"Pooling<br/>Sequence aggregation\"]         Router[\"Router<br/>Asymmetric routing\"]     end          subgraph \"Specialized Modules\"         MLMTransformer[\"MLMTransformer<br/>Masked language modeling\"]         SpladePooling[\"SpladePooling<br/>Sparse activation\"]         CLIPModel[\"CLIPModel<br/>Vision-text joint encoding\"]     end          subgraph \"Backend Options\"         PyTorchBackend[\"PyTorch Backend\"]         ONNXBackend[\"ONNX Backend\"]         OpenVINOBackend[\"OpenVINO Backend\"]     end          Transformer --> Pooling     MLMTransformer --> SpladePooling          Router --> Transformer     Router --> MLMTransformer     Router --> CLIPModel          PyTorchBackend --> Router     ONNXBackend --> Router     OpenVINOBackend --> Router ``` This modular architecture enables flexible model composition and optimization for different use cases. For detailed information about the module system, see [Module Architecture](#1.2). Sources: [sentence_transformers/__init__.py:10-14](), [pyproject.toml:52-54]()",
    "metadata": {
      "chunk_id": "143b45214c47-0008",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "filename": "Overview.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Module Architecture"
      ],
      "heading_text": "Module Architecture",
      "token_count": 259,
      "char_count": 1245,
      "start_char": 6917,
      "end_char": 8162,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5105882352941177,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.956332",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 259,
      "document_id": "143b45214c47",
      "document_name": "Overview",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "source_filename": "Overview.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Overview.md",
      "hierarchy_path": "Module Architecture",
      "chunk_hash": "d7cff4d5e1b94af8",
      "content_digest": "d7cff4d5e1b94af8",
      "chunk_length": 1245,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "router",
          "transformer",
          "mlmtransformer",
          "backend",
          "module",
          "architecture",
          "subgraph",
          "pooling",
          "end",
          "spladepooling",
          "clipmodel",
          "the",
          "modular",
          "modules",
          "encoding",
          "pytorchbackend",
          "onnxbackend",
          "openvinobackend",
          "for",
          "library"
        ],
        "term_weights": [
          {
            "term": "router",
            "tf": 8,
            "weight": 0.074074
          },
          {
            "term": "transformer",
            "tf": 4,
            "weight": 0.037037
          },
          {
            "term": "mlmtransformer",
            "tf": 4,
            "weight": 0.037037
          },
          {
            "term": "backend",
            "tf": 4,
            "weight": 0.037037
          },
          {
            "term": "module",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "architecture",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "subgraph",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "pooling",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "end",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "spladepooling",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "clipmodel",
            "tf": 3,
            "weight": 0.027778
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "modular",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "modules",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "encoding",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "pytorchbackend",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "onnxbackend",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "openvinobackend",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.018519
          },
          {
            "term": "library",
            "tf": 1,
            "weight": 0.009259
          }
        ],
        "unique_terms": 70,
        "total_terms": 108
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Module Architecture",
        "architecture",
        "backend",
        "end",
        "mlmtransformer",
        "module",
        "pooling",
        "router",
        "spladepooling",
        "subgraph",
        "transformer"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5105882352941177,
      "overall": 0.7368627450980392
    }
  }
]