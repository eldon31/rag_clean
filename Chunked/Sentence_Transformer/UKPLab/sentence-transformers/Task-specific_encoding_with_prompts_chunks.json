[
  {
    "text": "### Sparse Architecture Components ```mermaid graph TB     subgraph \"SparseEncoder Components\"         MLMTransformer[\"MLMTransformer<br/>Token-level predictions\"]         SpladePooling[\"SpladePooling<br/>Sparsification\"]         SparseAutoEncoder[\"SparseAutoEncoder<br/>k-sparse activation\"]         Router[\"Router<br/>Query/Document paths\"]     end          subgraph \"Output Processing\"         ActiveDims[\"max_active_dims<br/>Sparsity control\"]         SparseOutput[\"Sparse COO Tensor<br/>[batch_size, vocab_size]\"]     end          Input[\"Text\"] --> Router     Router --> MLMTransformer     MLMTransformer --> SpladePooling     SpladePooling --> ActiveDims     ActiveDims --> SparseOutput ```",
    "metadata": {
      "chunk_id": "40e16f41e6d0-0001",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "filename": "Task-specific_encoding_with_prompts.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Sparse Architecture Components"
      ],
      "heading_text": "Sparse Architecture Components",
      "token_count": 145,
      "char_count": 696,
      "start_char": 599,
      "end_char": 1295,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5218181818181818,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.562011",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 145,
      "document_id": "40e16f41e6d0",
      "document_name": "Task-specific_encoding_with_prompts",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "source_filename": "Task-specific_encoding_with_prompts.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "hierarchy_path": "Sparse Architecture Components",
      "chunk_hash": "d0820e962cec905d",
      "content_digest": "d0820e962cec905d",
      "chunk_length": 696,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "mlmtransformer",
          "spladepooling",
          "router",
          "sparse",
          "activedims",
          "components",
          "subgraph",
          "sparseautoencoder",
          "end",
          "sparseoutput",
          "size",
          "architecture",
          "mermaid",
          "graph",
          "sparseencoder",
          "token",
          "level",
          "predictions",
          "sparsification",
          "activation"
        ],
        "term_weights": [
          {
            "term": "mlmtransformer",
            "tf": 4,
            "weight": 0.072727
          },
          {
            "term": "spladepooling",
            "tf": 4,
            "weight": 0.072727
          },
          {
            "term": "router",
            "tf": 4,
            "weight": 0.072727
          },
          {
            "term": "sparse",
            "tf": 3,
            "weight": 0.054545
          },
          {
            "term": "activedims",
            "tf": 3,
            "weight": 0.054545
          },
          {
            "term": "components",
            "tf": 2,
            "weight": 0.036364
          },
          {
            "term": "subgraph",
            "tf": 2,
            "weight": 0.036364
          },
          {
            "term": "sparseautoencoder",
            "tf": 2,
            "weight": 0.036364
          },
          {
            "term": "end",
            "tf": 2,
            "weight": 0.036364
          },
          {
            "term": "sparseoutput",
            "tf": 2,
            "weight": 0.036364
          },
          {
            "term": "size",
            "tf": 2,
            "weight": 0.036364
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "mermaid",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "graph",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "sparseencoder",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "token",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "level",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "predictions",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "sparsification",
            "tf": 1,
            "weight": 0.018182
          },
          {
            "term": "activation",
            "tf": 1,
            "weight": 0.018182
          }
        ],
        "unique_terms": 36,
        "total_terms": 55
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Sparse Architecture Components",
        "activedims",
        "components",
        "end",
        "mlmtransformer",
        "router",
        "sparse",
        "sparseautoencoder",
        "sparseoutput",
        "spladepooling",
        "subgraph"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5218181818181818,
      "overall": 0.7406060606060606
    }
  },
  {
    "text": "### Decoding and Interpretation  The `SparseEncoder` provides a `decode()` method to interpret sparse embeddings as weighted vocabulary terms: ```python model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") embeddings = model.encode(\"machine learning\") tokens_weights = model.decode(embeddings, top_k=10)",
    "metadata": {
      "chunk_id": "40e16f41e6d0-0003",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "filename": "Task-specific_encoding_with_prompts.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Decoding and Interpretation"
      ],
      "heading_text": "Decoding and Interpretation",
      "token_count": 70,
      "char_count": 315,
      "start_char": 1681,
      "end_char": 1996,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.56,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.563218",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 70,
      "document_id": "40e16f41e6d0",
      "document_name": "Task-specific_encoding_with_prompts",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "source_filename": "Task-specific_encoding_with_prompts.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "hierarchy_path": "Decoding and Interpretation",
      "chunk_hash": "d3104ba95b2a64ff",
      "content_digest": "d3104ba95b2a64ff",
      "chunk_length": 315,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "embeddings",
          "model",
          "sparseencoder",
          "decode",
          "decoding",
          "and",
          "interpretation",
          "the",
          "provides",
          "method",
          "interpret",
          "sparse",
          "weighted",
          "vocabulary",
          "terms",
          "python",
          "naver",
          "splade",
          "cocondenser",
          "ensembledistil"
        ],
        "term_weights": [
          {
            "term": "embeddings",
            "tf": 3,
            "weight": 0.09375
          },
          {
            "term": "model",
            "tf": 3,
            "weight": 0.09375
          },
          {
            "term": "sparseencoder",
            "tf": 2,
            "weight": 0.0625
          },
          {
            "term": "decode",
            "tf": 2,
            "weight": 0.0625
          },
          {
            "term": "decoding",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "interpretation",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "provides",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "method",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "interpret",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "sparse",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "weighted",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "vocabulary",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "terms",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "naver",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "splade",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "cocondenser",
            "tf": 1,
            "weight": 0.03125
          },
          {
            "term": "ensembledistil",
            "tf": 1,
            "weight": 0.03125
          }
        ],
        "unique_terms": 26,
        "total_terms": 32
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Decoding and Interpretation",
        "and",
        "decode",
        "decoding",
        "embeddings",
        "interpretation",
        "method",
        "model",
        "provides",
        "sparseencoder",
        "the"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.56,
      "overall": 0.7200000000000001
    }
  }
]