[
  {
    "text": "## Overview  CrossEncoder loss functions are designed to train models that process text pairs jointly through a single transformer encoder. These loss functions fall into three main categories:  - **Learning-to-Rank Losses**: Optimize ranking metrics like NDCG for information retrieval tasks - **Classification Losses**: Handle binary or multi-class classification scenarios   - **Regression Losses**: Predict continuous similarity scores between text pairs",
    "metadata": {
      "chunk_id": "d29d645f4c83-0000",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "filename": "Loss_Functions_for_CrossEncoder.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Overview"
      ],
      "heading_text": "Overview",
      "token_count": 81,
      "char_count": 458,
      "start_char": 480,
      "end_char": 938,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5493220338983051,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.701745",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 81,
      "document_id": "d29d645f4c83",
      "document_name": "Loss_Functions_for_CrossEncoder",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "source_filename": "Loss_Functions_for_CrossEncoder.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "hierarchy_path": "Overview",
      "chunk_hash": "8fc81849b0ea35eb",
      "content_digest": "8fc81849b0ea35eb",
      "chunk_length": 458,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "losses",
          "loss",
          "functions",
          "text",
          "pairs",
          "classification",
          "overview",
          "crossencoder",
          "are",
          "designed",
          "train",
          "models",
          "that",
          "process",
          "jointly",
          "through",
          "single",
          "transformer",
          "encoder",
          "these"
        ],
        "term_weights": [
          {
            "term": "losses",
            "tf": 3,
            "weight": 0.055556
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.037037
          },
          {
            "term": "functions",
            "tf": 2,
            "weight": 0.037037
          },
          {
            "term": "text",
            "tf": 2,
            "weight": 0.037037
          },
          {
            "term": "pairs",
            "tf": 2,
            "weight": 0.037037
          },
          {
            "term": "classification",
            "tf": 2,
            "weight": 0.037037
          },
          {
            "term": "overview",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "crossencoder",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "are",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "designed",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "train",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "models",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "process",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "jointly",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "through",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "single",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "transformer",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "encoder",
            "tf": 1,
            "weight": 0.018519
          },
          {
            "term": "these",
            "tf": 1,
            "weight": 0.018519
          }
        ],
        "unique_terms": 47,
        "total_terms": 54
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Overview",
        "are",
        "classification",
        "crossencoder",
        "designed",
        "functions",
        "loss",
        "losses",
        "overview",
        "pairs",
        "text"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5493220338983051,
      "overall": 0.749774011299435
    }
  },
  {
    "text": "## Loss Function Hierarchy  The following diagram shows the inheritance and relationship structure of CrossEncoder loss functions: ```mermaid graph TD     Module[\"nn.Module\"]          subgraph \"Learning-to-Rank Losses\"         LambdaLoss[\"LambdaLoss\"]         ListNetLoss[\"ListNetLoss\"]          PListMLELoss[\"PListMLELoss\"]         ListMLELoss[\"ListMLELoss\"]         RankNetLoss[\"RankNetLoss\"]     end          subgraph \"Classification Losses\"         BinaryCrossEntropyLoss[\"BinaryCrossEntropyLoss\"]         CrossEntropyLoss[\"CrossEntropyLoss\"]         MultipleNegativesRankingLoss[\"MultipleNegativesRankingLoss\"]         CachedMultipleNegativesRankingLoss[\"CachedMultipleNegativesRankingLoss\"]     end          subgraph \"Regression Losses\"         MSELoss[\"MSELoss\"]         MarginMSELoss[\"MarginMSELoss\"]     end          subgraph \"Weighting Schemes\"         BaseWeightingScheme[\"BaseWeightingScheme\"]         NoWeightingScheme[\"NoWeightingScheme\"]         NDCGLoss1Scheme[\"NDCGLoss1Scheme\"]         NDCGLoss2Scheme[\"NDCGLoss2Scheme\"]         LambdaRankScheme[\"LambdaRankScheme\"]         NDCGLoss2PPScheme[\"NDCGLoss2PPScheme\"]         PListMLELambdaWeight[\"PListMLELambdaWeight\"]     end          Module --> LambdaLoss     Module --> ListNetLoss     Module --> PListMLELoss     Module --> BinaryCrossEntropyLoss     Module --> CrossEntropyLoss     Module --> MultipleNegativesRankingLoss     Module --> CachedMultipleNegativesRankingLoss     Module --> MSELoss     Module --> MarginMSELoss     Module --> BaseWeightingScheme          ListMLELoss --> PListMLELoss     LambdaLoss --> RankNetLoss     BaseWeightingScheme --> NoWeightingScheme     BaseWeightingScheme --> NDCGLoss1Scheme     BaseWeightingScheme --> NDCGLoss2Scheme     BaseWeightingScheme --> LambdaRankScheme     BaseWeightingScheme --> NDCGLoss2PPScheme     Module --> PListMLELambdaWeight          LambdaLoss -.-> BaseWeightingScheme     PListMLELoss -.-> PListMLELambdaWeight ``` Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:103-361](), [sentence_transformers/cross_encoder/losses/ListNetLoss.py:10-198](), [sentence_transformers/cross_encoder/losses/PListMLELoss.py:45-295](), [sentence_transformers/cross_encoder/losses/ListMLELoss.py:9-127](), [sentence_transformers/cross_encoder/losses/RankNetLoss.py:11-124](), [docs/package_reference/cross_encoder/losses.md:1-68]()",
    "metadata": {
      "chunk_id": "d29d645f4c83-0001",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "filename": "Loss_Functions_for_CrossEncoder.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Loss Function Hierarchy"
      ],
      "heading_text": "Loss Function Hierarchy",
      "token_count": 576,
      "char_count": 2365,
      "start_char": 940,
      "end_char": 3305,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6804878048780487,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.705022",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 576,
      "document_id": "d29d645f4c83",
      "document_name": "Loss_Functions_for_CrossEncoder",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "source_filename": "Loss_Functions_for_CrossEncoder.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "hierarchy_path": "Loss Function Hierarchy",
      "chunk_hash": "cb5bc93b6beb249a",
      "content_digest": "cb5bc93b6beb249a",
      "chunk_length": 2365,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "module",
          "losses",
          "baseweightingscheme",
          "lambdaloss",
          "plistmleloss",
          "cross",
          "encoder",
          "sentence",
          "transformers",
          "subgraph",
          "listnetloss",
          "listmleloss",
          "ranknetloss",
          "end",
          "plistmlelambdaweight",
          "binarycrossentropyloss",
          "crossentropyloss",
          "multiplenegativesrankingloss",
          "cachedmultiplenegativesrankingloss",
          "mseloss"
        ],
        "term_weights": [
          {
            "term": "module",
            "tf": 13,
            "weight": 0.083871
          },
          {
            "term": "losses",
            "tf": 9,
            "weight": 0.058065
          },
          {
            "term": "baseweightingscheme",
            "tf": 9,
            "weight": 0.058065
          },
          {
            "term": "lambdaloss",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "plistmleloss",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "cross",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "encoder",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "sentence",
            "tf": 5,
            "weight": 0.032258
          },
          {
            "term": "transformers",
            "tf": 5,
            "weight": 0.032258
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "listnetloss",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "listmleloss",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "ranknetloss",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "plistmlelambdaweight",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "binarycrossentropyloss",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "crossentropyloss",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "multiplenegativesrankingloss",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "cachedmultiplenegativesrankingloss",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "mseloss",
            "tf": 3,
            "weight": 0.019355
          }
        ],
        "unique_terms": 57,
        "total_terms": 155
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Loss Function Hierarchy",
        "baseweightingscheme",
        "cross",
        "encoder",
        "lambdaloss",
        "losses",
        "module",
        "plistmleloss",
        "sentence",
        "subgraph",
        "transformers"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6804878048780487,
      "overall": 0.7934959349593494
    }
  },
  {
    "text": "## Learning-to-Rank Loss Functions\n\nLearning-to-rank losses are designed for information retrieval tasks where the goal is to rank documents by relevance for a given query. These losses work with listwise data formats.",
    "metadata": {
      "chunk_id": "d29d645f4c83-0002",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "filename": "Loss_Functions_for_CrossEncoder.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Learning-to-Rank Loss Functions"
      ],
      "heading_text": "Learning-to-Rank Loss Functions",
      "token_count": 42,
      "char_count": 218,
      "start_char": 3309,
      "end_char": 3527,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.561875,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.705713",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 42,
      "document_id": "d29d645f4c83",
      "document_name": "Loss_Functions_for_CrossEncoder",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "source_filename": "Loss_Functions_for_CrossEncoder.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "hierarchy_path": "Learning-to-Rank Loss Functions",
      "chunk_hash": "c93e7276f42d2890",
      "content_digest": "c93e7276f42d2890",
      "chunk_length": 218,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "rank",
          "learning",
          "losses",
          "for",
          "loss",
          "functions",
          "are",
          "designed",
          "information",
          "retrieval",
          "tasks",
          "where",
          "the",
          "goal",
          "documents",
          "relevance",
          "given",
          "query",
          "these",
          "work"
        ],
        "term_weights": [
          {
            "term": "rank",
            "tf": 3,
            "weight": 0.103448
          },
          {
            "term": "learning",
            "tf": 2,
            "weight": 0.068966
          },
          {
            "term": "losses",
            "tf": 2,
            "weight": 0.068966
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.068966
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "are",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "designed",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "information",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "retrieval",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "tasks",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "where",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "goal",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "documents",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "relevance",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "given",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "query",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "these",
            "tf": 1,
            "weight": 0.034483
          },
          {
            "term": "work",
            "tf": 1,
            "weight": 0.034483
          }
        ],
        "unique_terms": 24,
        "total_terms": 29
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Learning-to-Rank Loss Functions",
        "are",
        "designed",
        "for",
        "functions",
        "information",
        "learning",
        "loss",
        "losses",
        "rank",
        "retrieval"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.561875,
      "overall": 0.720625
    }
  },
  {
    "text": "### LambdaLoss Framework  The `LambdaLoss` class implements a comprehensive framework for ranking metric optimization with multiple weighting schemes: ```mermaid graph LR     subgraph \"Input Processing\"         QueryDocs[\"queries + docs_list\"] --> Pairs[\"query-document pairs\"]         Labels[\"labels list\"] --> LabelMatrix[\"labels_matrix\"]     end          subgraph \"Model Processing\"         Pairs --> CrossEncoder[\"model.forward()\"]         CrossEncoder --> Logits[\"logits\"]         Logits --> ActivationFn[\"activation_fn\"]         ActivationFn --> LogitsMatrix[\"logits_matrix\"]     end          subgraph \"LambdaLoss Computation\"         LogitsMatrix --> Sorting[\"sort by logits\"]         LabelMatrix --> Sorting         Sorting --> TrueDiffs[\"true_diffs\"]         Sorting --> Gains[\"gain calculation\"]         Sorting --> Discounts[\"discount calculation\"]                  Gains --> WeightingScheme[\"weighting_scheme.forward()\"]         Discounts --> WeightingScheme         WeightingScheme --> Weights[\"weights\"]                  TrueDiffs --> ScoreDiffs[\"score differences\"]         ScoreDiffs --> WeightedProbas[\"weighted probabilities\"]         Weights --> WeightedProbas         WeightedProbas --> Loss[\"final loss\"]     end ``` The `LambdaLoss` supports five weighting schemes:  | Scheme | Class | Purpose | |--------|-------|---------| | No Weighting | `NoWeightingScheme` | Uniform weights (RankNet equivalent) | | NDCG Loss1 | `NDCGLoss1Scheme` | Basic NDCG optimization | | NDCG Loss2 | `NDCGLoss2Scheme` | Improved NDCG with tighter bounds | | LambdaRank | `LambdaRankScheme` | Coarse upper bound optimization | | NDCG Loss2++ | `NDCGLoss2PPScheme` | Hybrid scheme (recommended) |  Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:103-361](), [sentence_transformers/cross_encoder/losses/LambdaLoss.py:12-101]()",
    "metadata": {
      "chunk_id": "d29d645f4c83-0004",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "filename": "Loss_Functions_for_CrossEncoder.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "LambdaLoss Framework"
      ],
      "heading_text": "LambdaLoss Framework",
      "token_count": 423,
      "char_count": 1843,
      "start_char": 3948,
      "end_char": 5791,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5079650887573964,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.710573",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 423,
      "document_id": "d29d645f4c83",
      "document_name": "Loss_Functions_for_CrossEncoder",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "source_filename": "Loss_Functions_for_CrossEncoder.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "hierarchy_path": "LambdaLoss Framework",
      "chunk_hash": "b598dbb5c5205784",
      "content_digest": "b598dbb5c5205784",
      "chunk_length": 1843,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "lambdaloss",
          "logits",
          "sorting",
          "ndcg",
          "weighting",
          "weights",
          "optimization",
          "subgraph",
          "pairs",
          "labels",
          "end",
          "weightingscheme",
          "scheme",
          "weightedprobas",
          "framework",
          "the",
          "class",
          "with",
          "schemes",
          "processing"
        ],
        "term_weights": [
          {
            "term": "lambdaloss",
            "tf": 6,
            "weight": 0.038217
          },
          {
            "term": "logits",
            "tf": 5,
            "weight": 0.031847
          },
          {
            "term": "sorting",
            "tf": 5,
            "weight": 0.031847
          },
          {
            "term": "ndcg",
            "tf": 5,
            "weight": 0.031847
          },
          {
            "term": "weighting",
            "tf": 4,
            "weight": 0.025478
          },
          {
            "term": "weights",
            "tf": 4,
            "weight": 0.025478
          },
          {
            "term": "optimization",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "subgraph",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "pairs",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "labels",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "end",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "weightingscheme",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "scheme",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "weightedprobas",
            "tf": 3,
            "weight": 0.019108
          },
          {
            "term": "framework",
            "tf": 2,
            "weight": 0.012739
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.012739
          },
          {
            "term": "class",
            "tf": 2,
            "weight": 0.012739
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.012739
          },
          {
            "term": "schemes",
            "tf": 2,
            "weight": 0.012739
          },
          {
            "term": "processing",
            "tf": 2,
            "weight": 0.012739
          }
        ],
        "unique_terms": 92,
        "total_terms": 157
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "LambdaLoss Framework",
        "labels",
        "lambdaloss",
        "logits",
        "ndcg",
        "optimization",
        "pairs",
        "sorting",
        "subgraph",
        "weighting",
        "weights"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5079650887573964,
      "overall": 0.735988362919132
    }
  },
  {
    "text": "### ListNet Loss  The `ListNetLoss` implements the ListNet ranking algorithm using cross-entropy between predicted and ground truth ranking distributions: ```python",
    "metadata": {
      "chunk_id": "d29d645f4c83-0005",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "filename": "Loss_Functions_for_CrossEncoder.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "ListNet Loss"
      ],
      "heading_text": "ListNet Loss",
      "token_count": 31,
      "char_count": 164,
      "start_char": 5795,
      "end_char": 5959,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.545,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.711527",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 31,
      "document_id": "d29d645f4c83",
      "document_name": "Loss_Functions_for_CrossEncoder",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "source_filename": "Loss_Functions_for_CrossEncoder.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Loss_Functions_for_CrossEncoder.md",
      "hierarchy_path": "ListNet Loss",
      "chunk_hash": "8e9316a8e4338aa1",
      "content_digest": "8e9316a8e4338aa1",
      "chunk_length": 164,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "listnet",
          "the",
          "ranking",
          "loss",
          "listnetloss",
          "implements",
          "algorithm",
          "using",
          "cross",
          "entropy",
          "between",
          "predicted",
          "and",
          "ground",
          "truth",
          "distributions",
          "python"
        ],
        "term_weights": [
          {
            "term": "listnet",
            "tf": 2,
            "weight": 0.1
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.1
          },
          {
            "term": "ranking",
            "tf": 2,
            "weight": 0.1
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "listnetloss",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "implements",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "algorithm",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "using",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "cross",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "entropy",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "between",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "predicted",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "ground",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "truth",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "distributions",
            "tf": 1,
            "weight": 0.05
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.05
          }
        ],
        "unique_terms": 17,
        "total_terms": 20
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "ListNet Loss",
        "algorithm",
        "cross",
        "entropy",
        "implements",
        "listnet",
        "listnetloss",
        "loss",
        "ranking",
        "the",
        "using"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.545,
      "overall": 0.7483333333333334
    }
  }
]