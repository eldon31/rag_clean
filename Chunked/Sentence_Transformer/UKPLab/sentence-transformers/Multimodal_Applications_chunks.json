[
  {
    "text": "## CLIPModel Architecture  The `CLIPModel` class provides the core functionality for multimodal applications by implementing the CLIP architecture within the sentence-transformers framework. **CLIPModel Component Architecture** ```mermaid graph TB     subgraph Input[\"Input Processing\"]         IMG[\"PIL.Image objects\"]         TXT[\"Text strings\"]     end          subgraph CLIPModel[\"CLIPModel Class\"]         PROC[\"CLIPProcessor\"]         VMODEL[\"model.vision_model\"]         TMODEL[\"model.text_model\"]         VPROJ[\"model.visual_projection\"]         TPROJ[\"model.text_projection\"]     end          subgraph Processing[\"tokenize() Method\"]         IMGPROC[\"image_processor\"]         TXTPROC[\"tokenizer\"]         INFO[\"image_text_info tracking\"]     end          subgraph Output[\"forward() Output\"]         VEMB[\"Image embeddings\"]         TEMB[\"Text embeddings\"]         UNIFIED[\"sentence_embedding tensor\"]     end          IMG --> Processing     TXT --> Processing     Processing --> CLIPModel     CLIPModel --> Output ``` The `CLIPModel` class inherits from `InputModule` and wraps `transformers.CLIPModel` and `transformers.CLIPProcessor` components. It implements the `tokenize()` and `forward()` methods required by the sentence-transformers module system. Sources: [sentence_transformers/models/CLIPModel.py:15-26](), [sentence_transformers/models/CLIPModel.py:70-92]()",
    "metadata": {
      "chunk_id": "583ff93ed929-0000",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "CLIPModel Architecture"
      ],
      "heading_text": "CLIPModel Architecture",
      "token_count": 301,
      "char_count": 1379,
      "start_char": 726,
      "end_char": 2105,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5124153846153847,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.975225",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 301,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "CLIPModel Architecture",
      "chunk_hash": "38ccf385c9076c6f",
      "content_digest": "38ccf385c9076c6f",
      "chunk_length": 1379,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "clipmodel",
          "the",
          "transformers",
          "model",
          "sentence",
          "processing",
          "text",
          "subgraph",
          "image",
          "end",
          "architecture",
          "class",
          "output",
          "and",
          "input",
          "img",
          "txt",
          "clipprocessor",
          "projection",
          "tokenize"
        ],
        "term_weights": [
          {
            "term": "clipmodel",
            "tf": 11,
            "weight": 0.08209
          },
          {
            "term": "the",
            "tf": 7,
            "weight": 0.052239
          },
          {
            "term": "transformers",
            "tf": 6,
            "weight": 0.044776
          },
          {
            "term": "model",
            "tf": 6,
            "weight": 0.044776
          },
          {
            "term": "sentence",
            "tf": 5,
            "weight": 0.037313
          },
          {
            "term": "processing",
            "tf": 5,
            "weight": 0.037313
          },
          {
            "term": "text",
            "tf": 5,
            "weight": 0.037313
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.029851
          },
          {
            "term": "image",
            "tf": 4,
            "weight": 0.029851
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.029851
          },
          {
            "term": "architecture",
            "tf": 3,
            "weight": 0.022388
          },
          {
            "term": "class",
            "tf": 3,
            "weight": 0.022388
          },
          {
            "term": "output",
            "tf": 3,
            "weight": 0.022388
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.022388
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.014925
          },
          {
            "term": "img",
            "tf": 2,
            "weight": 0.014925
          },
          {
            "term": "txt",
            "tf": 2,
            "weight": 0.014925
          },
          {
            "term": "clipprocessor",
            "tf": 2,
            "weight": 0.014925
          },
          {
            "term": "projection",
            "tf": 2,
            "weight": 0.014925
          },
          {
            "term": "tokenize",
            "tf": 2,
            "weight": 0.014925
          }
        ],
        "unique_terms": 69,
        "total_terms": 134
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "CLIPModel Architecture",
        "clipmodel",
        "end",
        "image",
        "model",
        "processing",
        "sentence",
        "subgraph",
        "text",
        "the",
        "transformers"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5124153846153847,
      "overall": 0.7041384615384615
    }
  },
  {
    "text": "## Input Processing and Tokenization  The CLIP model handles mixed input types through its `tokenize` method, which can process both PIL Images and text strings in the same batch. ```mermaid graph LR     subgraph InputBatch[\"Mixed Input Batch\"]         IMG1[\"PIL.Image\"]         TXT1[\"'A dog in snow'\"]         IMG2[\"PIL.Image\"]          TXT2[\"'A cat on table'\"]     end          subgraph Tokenization[\"tokenize() Method\"]         CLASSIFY[\"Classify Input Types\"]         IMGPROC[\"Image Processing\"]         TXTPROC[\"Text Tokenization\"]         MERGE[\"Merge Features\"]     end          subgraph Features[\"Feature Tensors\"]         PIXELS[\"pixel_values\"]         TOKENS[\"input_ids\"]         MASK[\"attention_mask\"]         INFO[\"image_text_info\"]     end          InputBatch --> CLASSIFY     CLASSIFY --> IMGPROC     CLASSIFY --> TXTPROC     IMGPROC --> PIXELS     TXTPROC --> TOKENS     TXTPROC --> MASK     CLASSIFY --> INFO     MERGE --> Features ``` The `image_text_info` list tracks which inputs are images (0) versus text (1), enabling proper routing during the forward pass. Sources: [sentence_transformers/models/CLIPModel.py:70-92]()",
    "metadata": {
      "chunk_id": "583ff93ed929-0001",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Input Processing and Tokenization"
      ],
      "heading_text": "Input Processing and Tokenization",
      "token_count": 269,
      "char_count": 1140,
      "start_char": 2110,
      "end_char": 3250,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5163157894736842,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.978035",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 269,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "Input Processing and Tokenization",
      "chunk_hash": "1fb8a217650ea5ce",
      "content_digest": "1fb8a217650ea5ce",
      "chunk_length": 1140,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "classify",
          "input",
          "text",
          "image",
          "the",
          "txtproc",
          "info",
          "tokenization",
          "pil",
          "subgraph",
          "end",
          "imgproc",
          "merge",
          "features",
          "mask",
          "processing",
          "and",
          "mixed",
          "types",
          "tokenize"
        ],
        "term_weights": [
          {
            "term": "classify",
            "tf": 6,
            "weight": 0.04878
          },
          {
            "term": "input",
            "tf": 5,
            "weight": 0.04065
          },
          {
            "term": "text",
            "tf": 5,
            "weight": 0.04065
          },
          {
            "term": "image",
            "tf": 5,
            "weight": 0.04065
          },
          {
            "term": "the",
            "tf": 4,
            "weight": 0.03252
          },
          {
            "term": "txtproc",
            "tf": 4,
            "weight": 0.03252
          },
          {
            "term": "info",
            "tf": 4,
            "weight": 0.03252
          },
          {
            "term": "tokenization",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "pil",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "subgraph",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "end",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "imgproc",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "merge",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "features",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "mask",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "processing",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "mixed",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "types",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "tokenize",
            "tf": 2,
            "weight": 0.01626
          }
        ],
        "unique_terms": 69,
        "total_terms": 123
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Input Processing and Tokenization",
        "classify",
        "image",
        "info",
        "input",
        "pil",
        "subgraph",
        "text",
        "the",
        "tokenization",
        "txtproc"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5163157894736842,
      "overall": 0.7387719298245613
    }
  },
  {
    "text": "### Basic Image-Text Similarity ```python from sentence_transformers import SentenceTransformer from PIL import Image  model = SentenceTransformer('clip-ViT-B-32')",
    "metadata": {
      "chunk_id": "583ff93ed929-0004",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Basic Image-Text Similarity"
      ],
      "heading_text": "Basic Image-Text Similarity",
      "token_count": 33,
      "char_count": 163,
      "start_char": 4584,
      "end_char": 4747,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5525,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.982452",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 33,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "Basic Image-Text Similarity",
      "chunk_hash": "b88fee2098f0e0ce",
      "content_digest": "b88fee2098f0e0ce",
      "chunk_length": 163,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "image",
          "from",
          "import",
          "sentencetransformer",
          "basic",
          "text",
          "similarity",
          "python",
          "sentence",
          "transformers",
          "pil",
          "model",
          "clip",
          "vit"
        ],
        "term_weights": [
          {
            "term": "image",
            "tf": 2,
            "weight": 0.111111
          },
          {
            "term": "from",
            "tf": 2,
            "weight": 0.111111
          },
          {
            "term": "import",
            "tf": 2,
            "weight": 0.111111
          },
          {
            "term": "sentencetransformer",
            "tf": 2,
            "weight": 0.111111
          },
          {
            "term": "basic",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "text",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "similarity",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "sentence",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "transformers",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "pil",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "clip",
            "tf": 1,
            "weight": 0.055556
          },
          {
            "term": "vit",
            "tf": 1,
            "weight": 0.055556
          }
        ],
        "unique_terms": 14,
        "total_terms": 18
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Basic Image-Text Similarity",
        "basic",
        "from",
        "image",
        "import",
        "python",
        "sentence",
        "sentencetransformer",
        "similarity",
        "text",
        "transformers"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5525,
      "overall": 0.7508333333333334
    }
  },
  {
    "text": "# Encode image image = Image.open('path/to/image.jpg') img_embedding = model.encode(image)",
    "metadata": {
      "chunk_id": "583ff93ed929-0005",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Encode image"
      ],
      "heading_text": "Encode image",
      "token_count": 20,
      "char_count": 90,
      "start_char": 4750,
      "end_char": 4840,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5233333333333333,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.982688",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 20,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "Encode image",
      "chunk_hash": "e358f017cdf780b4",
      "content_digest": "e358f017cdf780b4",
      "chunk_length": 90,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "image",
          "encode",
          "open",
          "path",
          "jpg",
          "img",
          "embedding",
          "model"
        ],
        "term_weights": [
          {
            "term": "image",
            "tf": 5,
            "weight": 0.384615
          },
          {
            "term": "encode",
            "tf": 2,
            "weight": 0.153846
          },
          {
            "term": "open",
            "tf": 1,
            "weight": 0.076923
          },
          {
            "term": "path",
            "tf": 1,
            "weight": 0.076923
          },
          {
            "term": "jpg",
            "tf": 1,
            "weight": 0.076923
          },
          {
            "term": "img",
            "tf": 1,
            "weight": 0.076923
          },
          {
            "term": "embedding",
            "tf": 1,
            "weight": 0.076923
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.076923
          }
        ],
        "unique_terms": 8,
        "total_terms": 13
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Encode image",
        "embedding",
        "encode",
        "image",
        "img",
        "jpg",
        "model",
        "open",
        "path"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5233333333333333,
      "overall": 0.6744444444444445
    }
  },
  {
    "text": "# Encode text descriptions   texts = [\"A dog in the snow\", \"A cat on a table\"] text_embeddings = model.encode(texts)",
    "metadata": {
      "chunk_id": "583ff93ed929-0006",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Encode text descriptions"
      ],
      "heading_text": "Encode text descriptions",
      "token_count": 29,
      "char_count": 116,
      "start_char": 4842,
      "end_char": 4958,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5742105263157895,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.982913",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 29,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "Encode text descriptions",
      "chunk_hash": "d2008df7b600a760",
      "content_digest": "d2008df7b600a760",
      "chunk_length": 116,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "encode",
          "text",
          "texts",
          "descriptions",
          "dog",
          "the",
          "snow",
          "cat",
          "table",
          "embeddings",
          "model"
        ],
        "term_weights": [
          {
            "term": "encode",
            "tf": 2,
            "weight": 0.142857
          },
          {
            "term": "text",
            "tf": 2,
            "weight": 0.142857
          },
          {
            "term": "texts",
            "tf": 2,
            "weight": 0.142857
          },
          {
            "term": "descriptions",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "dog",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "snow",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "cat",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "table",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.071429
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.071429
          }
        ],
        "unique_terms": 11,
        "total_terms": 14
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Encode text descriptions",
        "cat",
        "descriptions",
        "dog",
        "embeddings",
        "encode",
        "snow",
        "table",
        "text",
        "texts",
        "the"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5742105263157895,
      "overall": 0.6914035087719298
    }
  },
  {
    "text": "# Compute similarities similarities = model.similarity(img_embedding, text_embeddings) ```",
    "metadata": {
      "chunk_id": "583ff93ed929-0007",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Compute similarities"
      ],
      "heading_text": "Compute similarities",
      "token_count": 15,
      "char_count": 90,
      "start_char": 4960,
      "end_char": 5050,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5525,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.983102",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 15,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "Compute similarities",
      "chunk_hash": "5f0131870865573c",
      "content_digest": "5f0131870865573c",
      "chunk_length": 90,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "similarities",
          "compute",
          "model",
          "similarity",
          "img",
          "embedding",
          "text",
          "embeddings"
        ],
        "term_weights": [
          {
            "term": "similarities",
            "tf": 2,
            "weight": 0.222222
          },
          {
            "term": "compute",
            "tf": 1,
            "weight": 0.111111
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.111111
          },
          {
            "term": "similarity",
            "tf": 1,
            "weight": 0.111111
          },
          {
            "term": "img",
            "tf": 1,
            "weight": 0.111111
          },
          {
            "term": "embedding",
            "tf": 1,
            "weight": 0.111111
          },
          {
            "term": "text",
            "tf": 1,
            "weight": 0.111111
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.111111
          }
        ],
        "unique_terms": 8,
        "total_terms": 9
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Compute similarities",
        "compute",
        "embedding",
        "embeddings",
        "img",
        "model",
        "similarities",
        "similarity",
        "text"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5525,
      "overall": 0.6841666666666667
    }
  },
  {
    "text": "### Mixed Batch Processing  The CLIP model can process images and text in the same batch call: ```python",
    "metadata": {
      "chunk_id": "583ff93ed929-0008",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Mixed Batch Processing"
      ],
      "heading_text": "Mixed Batch Processing",
      "token_count": 22,
      "char_count": 104,
      "start_char": 5052,
      "end_char": 5156,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5566666666666666,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:45.983533",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 22,
      "document_id": "583ff93ed929",
      "document_name": "Multimodal_Applications",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_filename": "Multimodal_Applications.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "hierarchy_path": "Mixed Batch Processing",
      "chunk_hash": "61886e23890a0a5a",
      "content_digest": "61886e23890a0a5a",
      "chunk_length": 104,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "batch",
          "the",
          "mixed",
          "processing",
          "clip",
          "model",
          "can",
          "process",
          "images",
          "and",
          "text",
          "same",
          "call",
          "python"
        ],
        "term_weights": [
          {
            "term": "batch",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "mixed",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "processing",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "clip",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "can",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "process",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "images",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "text",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "same",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "call",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.0625
          }
        ],
        "unique_terms": 14,
        "total_terms": 16
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Mixed Batch Processing",
        "and",
        "batch",
        "can",
        "clip",
        "images",
        "mixed",
        "model",
        "process",
        "processing",
        "the"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5566666666666666,
      "overall": 0.7522222222222222
    }
  }
]