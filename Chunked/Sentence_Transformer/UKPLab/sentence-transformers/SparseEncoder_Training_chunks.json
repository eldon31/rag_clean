[
  {
    "text": "## Training System Architecture  The `SparseEncoder` training system consists of specialized components that handle the unique requirements of sparse representation learning, including sparsity regularization and architecture-specific optimizations. ```mermaid graph TB     subgraph \"Training Components\"         SparseEncoderTrainer[\"SparseEncoderTrainer\"]         SparseEncoderTrainingArguments[\"SparseEncoderTrainingArguments\"]         DataCollator[\"DataCollator\"]     end          subgraph \"Model Architectures\"         MLMTransformer[\"MLMTransformer\"]         SpladePooling[\"SpladePooling\"]         SparseAutoEncoder[\"SparseAutoEncoder\"]         SparseStaticEmbedding[\"SparseStaticEmbedding\"]         Router[\"Router\"]     end          subgraph \"Loss Functions\"         SpladeLoss[\"SpladeLoss\"]         CSRLoss[\"CSRLoss\"]         FlopsLoss[\"FlopsLoss\"]         SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"]     end          subgraph \"Evaluators\"         SparseNanoBEIREvaluator[\"SparseNanoBEIREvaluator\"]         SparseInformationRetrievalEvaluator[\"SparseInformationRetrievalEvaluator\"]         SparseEmbeddingSimilarityEvaluator[\"SparseEmbeddingSimilarityEvaluator\"]     end          SparseEncoderTrainer --> MLMTransformer     SparseEncoderTrainer --> SpladePooling     SparseEncoderTrainer --> SparseAutoEncoder     SparseEncoderTrainer --> Router          SparseEncoderTrainingArguments --> SparseEncoderTrainer     DataCollator --> SparseEncoderTrainer          SpladeLoss --> SparseEncoderTrainer     CSRLoss --> SparseEncoderTrainer     FlopsLoss --> SpladeLoss     SparseMultipleNegativesRankingLoss --> SpladeLoss     SparseMultipleNegativesRankingLoss --> CSRLoss          SparseNanoBEIREvaluator --> SparseEncoderTrainer     SparseInformationRetrievalEvaluator --> SparseEncoderTrainer     SparseEmbeddingSimilarityEvaluator --> SparseEncoderTrainer ``` **Sources:** [docs/sparse_encoder/training_overview.md:17-46](), [sentence_transformers/sparse_encoder/__init__.py:1-14](), [sentence_transformers/sparse_encoder/losses/__init__.py:1-29]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0000",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training System Architecture"
      ],
      "heading_text": "Training System Architecture",
      "token_count": 431,
      "char_count": 2085,
      "start_char": 560,
      "end_char": 2645,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5255616822429906,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.442724",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 431,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Training System Architecture",
      "chunk_hash": "219bc2d3b6847a85",
      "content_digest": "219bc2d3b6847a85",
      "chunk_length": 2085,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparseencodertrainer",
          "spladeloss",
          "training",
          "sparse",
          "subgraph",
          "end",
          "csrloss",
          "sparsemultiplenegativesrankingloss",
          "sparseencodertrainingarguments",
          "datacollator",
          "mlmtransformer",
          "spladepooling",
          "sparseautoencoder",
          "router",
          "flopsloss",
          "sparsenanobeirevaluator",
          "sparseinformationretrievalevaluator",
          "sparseembeddingsimilarityevaluator",
          "encoder",
          "system"
        ],
        "term_weights": [
          {
            "term": "sparseencodertrainer",
            "tf": 13,
            "weight": 0.111111
          },
          {
            "term": "spladeloss",
            "tf": 5,
            "weight": 0.042735
          },
          {
            "term": "training",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "sparse",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "csrloss",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "sparsemultiplenegativesrankingloss",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "sparseencodertrainingarguments",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "datacollator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "mlmtransformer",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "spladepooling",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparseautoencoder",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "router",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "flopsloss",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparsenanobeirevaluator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparseinformationretrievalevaluator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparseembeddingsimilarityevaluator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "encoder",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "system",
            "tf": 2,
            "weight": 0.017094
          }
        ],
        "unique_terms": 53,
        "total_terms": 117
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training System Architecture",
        "csrloss",
        "datacollator",
        "end",
        "sparse",
        "sparseencodertrainer",
        "sparseencodertrainingarguments",
        "sparsemultiplenegativesrankingloss",
        "spladeloss",
        "subgraph",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5255616822429906,
      "overall": 0.7418538940809968
    }
  },
  {
    "text": "## Sparse Encoder Architectures\n\nThe training system supports three primary sparse encoder architectures, each requiring different components and training strategies.",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0001",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Sparse Encoder Architectures"
      ],
      "heading_text": "Sparse Encoder Architectures",
      "token_count": 24,
      "char_count": 166,
      "start_char": 2649,
      "end_char": 2815,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.545,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.443788",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 24,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Sparse Encoder Architectures",
      "chunk_hash": "d9d07f8c05898b6b",
      "content_digest": "d9d07f8c05898b6b",
      "chunk_length": 166,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparse",
          "encoder",
          "architectures",
          "training",
          "the",
          "system",
          "supports",
          "three",
          "primary",
          "each",
          "requiring",
          "different",
          "components",
          "and",
          "strategies"
        ],
        "term_weights": [
          {
            "term": "sparse",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "architectures",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "supports",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "three",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "primary",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "each",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "requiring",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "different",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "components",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "strategies",
            "tf": 1,
            "weight": 0.052632
          }
        ],
        "unique_terms": 15,
        "total_terms": 19
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Sparse Encoder Architectures",
        "architectures",
        "each",
        "encoder",
        "primary",
        "sparse",
        "supports",
        "system",
        "the",
        "three",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.545,
      "overall": 0.7483333333333334
    }
  },
  {
    "text": "### SPLADE Architecture  SPLADE models use `MLMTransformer` followed by `SpladePooling` to create sparse lexical representations from masked language model logits. ```mermaid graph LR     Input[\"Text Input\"] --> MLMTransformer[\"MLMTransformer<br/>(BERT/RoBERTa/DistilBERT)\"]     MLMTransformer --> Logits[\"MLM Head Logits<br/>(vocab_size)\"]     Logits --> SpladePooling[\"SpladePooling<br/>(max/sum pooling)\"]     SpladePooling --> Activation[\"ReLU + log1p\"]     Activation --> SparseEmbedding[\"Sparse Embedding<br/>(vocab_size dimensions)\"] ``` **Sources:** [docs/sparse_encoder/training_overview.md:59-98](), [sentence_transformers/sparse_encoder/models/MLMTransformer.py:26-55](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:13-40]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0002",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "SPLADE Architecture"
      ],
      "heading_text": "SPLADE Architecture",
      "token_count": 191,
      "char_count": 753,
      "start_char": 2817,
      "end_char": 3570,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5605882352941176,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.445759",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 191,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "SPLADE Architecture",
      "chunk_hash": "60a0a4b5b91eecb3",
      "content_digest": "60a0a4b5b91eecb3",
      "chunk_length": 753,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "mlmtransformer",
          "spladepooling",
          "sparse",
          "logits",
          "models",
          "encoder",
          "splade",
          "input",
          "vocab",
          "size",
          "activation",
          "sentence",
          "transformers",
          "architecture",
          "use",
          "followed",
          "create",
          "lexical",
          "representations",
          "from"
        ],
        "term_weights": [
          {
            "term": "mlmtransformer",
            "tf": 5,
            "weight": 0.072464
          },
          {
            "term": "spladepooling",
            "tf": 5,
            "weight": 0.072464
          },
          {
            "term": "sparse",
            "tf": 5,
            "weight": 0.072464
          },
          {
            "term": "logits",
            "tf": 4,
            "weight": 0.057971
          },
          {
            "term": "models",
            "tf": 3,
            "weight": 0.043478
          },
          {
            "term": "encoder",
            "tf": 3,
            "weight": 0.043478
          },
          {
            "term": "splade",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "vocab",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "size",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "activation",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "transformers",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "use",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "followed",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "create",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "lexical",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "representations",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "from",
            "tf": 1,
            "weight": 0.014493
          }
        ],
        "unique_terms": 43,
        "total_terms": 69
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "SPLADE Architecture",
        "encoder",
        "input",
        "logits",
        "mlmtransformer",
        "models",
        "size",
        "sparse",
        "splade",
        "spladepooling",
        "vocab"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5605882352941176,
      "overall": 0.7535294117647058
    }
  },
  {
    "text": "### Inference-Free SPLADE Architecture  This architecture uses `Router` to process queries and documents differently, with lightweight `SparseStaticEmbedding` for queries and full SPLADE processing for documents. ```mermaid graph TB     subgraph \"Router Module\"         QueryRoute[\"Query Route\"]         DocumentRoute[\"Document Route\"]     end          QueryInput[\"Query Text\"] --> QueryRoute     DocumentInput[\"Document Text\"] --> DocumentRoute          QueryRoute --> SparseStaticEmbedding[\"SparseStaticEmbedding<br/>(Static Weights)\"]     DocumentRoute --> MLMTransformer[\"MLMTransformer\"]     MLMTransformer --> SpladePooling[\"SpladePooling\"]          SparseStaticEmbedding --> QueryEmbedding[\"Query Sparse Embedding\"]     SpladePooling --> DocumentEmbedding[\"Document Sparse Embedding\"] ``` **Sources:** [docs/sparse_encoder/training_overview.md:99-168](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py:24-62]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0003",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Inference-Free SPLADE Architecture"
      ],
      "heading_text": "Inference-Free SPLADE Architecture",
      "token_count": 199,
      "char_count": 940,
      "start_char": 3574,
      "end_char": 4514,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.515,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.448362",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 199,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Inference-Free SPLADE Architecture",
      "chunk_hash": "4f5608ca57d8054e",
      "content_digest": "4f5608ca57d8054e",
      "chunk_length": 940,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparsestaticembedding",
          "sparse",
          "queryroute",
          "query",
          "documentroute",
          "document",
          "mlmtransformer",
          "spladepooling",
          "splade",
          "architecture",
          "router",
          "queries",
          "and",
          "documents",
          "for",
          "route",
          "text",
          "embedding",
          "encoder",
          "inference"
        ],
        "term_weights": [
          {
            "term": "sparsestaticembedding",
            "tf": 5,
            "weight": 0.064103
          },
          {
            "term": "sparse",
            "tf": 4,
            "weight": 0.051282
          },
          {
            "term": "queryroute",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "documentroute",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "document",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "mlmtransformer",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "spladepooling",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "splade",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "architecture",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "router",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "documents",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "route",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "text",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "inference",
            "tf": 1,
            "weight": 0.012821
          }
        ],
        "unique_terms": 48,
        "total_terms": 78
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Inference-Free SPLADE Architecture",
        "architecture",
        "document",
        "documentroute",
        "mlmtransformer",
        "query",
        "queryroute",
        "sparse",
        "sparsestaticembedding",
        "splade",
        "spladepooling"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.515,
      "overall": 0.7383333333333333
    }
  },
  {
    "text": "### CSR (Contrastive Sparse Representation) Architecture  CSR models apply `SparseAutoEncoder` on top of dense sentence transformer embeddings to create sparse representations. ```mermaid graph LR     Input[\"Text Input\"] --> Transformer[\"Transformer<br/>(BERT/etc)\"]     Transformer --> Pooling[\"Pooling<br/>(mean/cls)\"]     Pooling --> DenseEmbedding[\"Dense Embedding\"]     DenseEmbedding --> SparseAutoEncoder[\"SparseAutoEncoder<br/>(k=256, k_aux=512)\"]     SparseAutoEncoder --> SparseEmbedding[\"Sparse Embedding<br/>(k dimensions)\"] ``` **Sources:** [docs/sparse_encoder/training_overview.md:169-228](), [sentence_transformers/sparse_encoder/models/__init__.py:1-9]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0004",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "CSR (Contrastive Sparse Representation) Architecture"
      ],
      "heading_text": "CSR (Contrastive Sparse Representation) Architecture",
      "token_count": 162,
      "char_count": 671,
      "start_char": 4518,
      "end_char": 5189,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.54625,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.449979",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 162,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "CSR (Contrastive Sparse Representation) Architecture",
      "chunk_hash": "59e2b3cddbe5abd3",
      "content_digest": "59e2b3cddbe5abd3",
      "chunk_length": 671,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparse",
          "sparseautoencoder",
          "transformer",
          "pooling",
          "csr",
          "models",
          "dense",
          "sentence",
          "input",
          "denseembedding",
          "embedding",
          "encoder",
          "contrastive",
          "representation",
          "architecture",
          "apply",
          "top",
          "embeddings",
          "create",
          "representations"
        ],
        "term_weights": [
          {
            "term": "sparse",
            "tf": 5,
            "weight": 0.083333
          },
          {
            "term": "sparseautoencoder",
            "tf": 4,
            "weight": 0.066667
          },
          {
            "term": "transformer",
            "tf": 4,
            "weight": 0.066667
          },
          {
            "term": "pooling",
            "tf": 3,
            "weight": 0.05
          },
          {
            "term": "csr",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "dense",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "denseembedding",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "contrastive",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "representation",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "apply",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "top",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "create",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "representations",
            "tf": 1,
            "weight": 0.016667
          }
        ],
        "unique_terms": 40,
        "total_terms": 60
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "CSR (Contrastive Sparse Representation) Architecture",
        "csr",
        "dense",
        "denseembedding",
        "input",
        "models",
        "pooling",
        "sentence",
        "sparse",
        "sparseautoencoder",
        "transformer"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.54625,
      "overall": 0.7487499999999999
    }
  },
  {
    "text": "### Loss Function Requirements  Sparse encoder training requires specialized loss functions that incorporate sparsity regularization: ```mermaid graph TB     subgraph \"Wrapper Losses\"         SpladeLoss[\"SpladeLoss<br/>(for SPLADE models)\"]         CSRLoss[\"CSRLoss<br/>(for CSR models)\"]     end          subgraph \"Main Loss Functions\"         SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"]         SparseMarginMSELoss[\"SparseMarginMSELoss\"]         SparseDistillKLDivLoss[\"SparseDistillKLDivLoss\"]     end          subgraph \"Regularization\"         FlopsLoss[\"FlopsLoss<br/>(default regularizer)\"]         CustomRegularizer[\"Custom Regularizer\"]     end          subgraph \"Specialized Losses\"         CSRReconstructionLoss[\"CSRReconstructionLoss\"]         SparseMSELoss[\"SparseMSELoss<br/>(standalone distillation)\"]     end          SpladeLoss --> SparseMultipleNegativesRankingLoss     SpladeLoss --> SparseMarginMSELoss     SpladeLoss --> SparseDistillKLDivLoss     SpladeLoss --> FlopsLoss     SpladeLoss --> CustomRegularizer          CSRLoss --> SparseMultipleNegativesRankingLoss     CSRLoss --> CSRReconstructionLoss ``` **Sources:** [docs/sparse_encoder/training_overview.md:346-393](), [docs/sparse_encoder/loss_overview.md:4-28](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-187]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0007",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Loss Function Requirements"
      ],
      "heading_text": "Loss Function Requirements",
      "token_count": 325,
      "char_count": 1337,
      "start_char": 5808,
      "end_char": 7145,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5106578947368421,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.452677",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 325,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Loss Function Requirements",
      "chunk_hash": "e30d18c6b33d33dd",
      "content_digest": "e30d18c6b33d33dd",
      "chunk_length": 1337,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "spladeloss",
          "csrloss",
          "loss",
          "sparse",
          "encoder",
          "subgraph",
          "end",
          "sparsemultiplenegativesrankingloss",
          "losses",
          "sparsemarginmseloss",
          "sparsedistillkldivloss",
          "flopsloss",
          "csrreconstructionloss",
          "training",
          "specialized",
          "functions",
          "regularization",
          "for",
          "models",
          "regularizer"
        ],
        "term_weights": [
          {
            "term": "spladeloss",
            "tf": 7,
            "weight": 0.072917
          },
          {
            "term": "csrloss",
            "tf": 5,
            "weight": 0.052083
          },
          {
            "term": "loss",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "sparse",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "encoder",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "sparsemultiplenegativesrankingloss",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "losses",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "sparsemarginmseloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "sparsedistillkldivloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "flopsloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "csrreconstructionloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "specialized",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "functions",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "regularization",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "regularizer",
            "tf": 2,
            "weight": 0.020833
          }
        ],
        "unique_terms": 47,
        "total_terms": 96
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Loss Function Requirements",
        "csrloss",
        "encoder",
        "end",
        "loss",
        "losses",
        "sparse",
        "sparsemarginmseloss",
        "sparsemultiplenegativesrankingloss",
        "spladeloss",
        "subgraph"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5106578947368421,
      "overall": 0.7368859649122806
    }
  },
  {
    "text": "### Training Arguments  `SparseEncoderTrainingArguments` extends standard training arguments with sparse-specific parameters:  | Parameter | Purpose | Example | |-----------|---------|---------| | `router_mapping` | Maps dataset columns to Router tasks | `{\"question\": \"query\", \"answer\": \"document\"}` | | `learning_rate_mapping` | Sets different learning rates per component | `{\"SparseStaticEmbedding.*\": 1e-3}` | | `batch_sampler` | Controls batch composition | `BatchSamplers.NO_DUPLICATES` | | `prompts` | Task-specific prompts | `{\"query\": \"Represent this query:\"}` |  **Sources:** [docs/sparse_encoder/training_overview.md:394-473](), [docs/sparse_encoder/training_overview.md:149-168]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0008",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training Arguments"
      ],
      "heading_text": "Training Arguments",
      "token_count": 167,
      "char_count": 693,
      "start_char": 7149,
      "end_char": 7842,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.454724",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 167,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Training Arguments",
      "chunk_hash": "3c76f20ce752fa73",
      "content_digest": "3c76f20ce752fa73",
      "chunk_length": 693,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "training",
          "sparse",
          "query",
          "arguments",
          "specific",
          "router",
          "mapping",
          "learning",
          "batch",
          "prompts",
          "docs",
          "encoder",
          "overview",
          "sparseencodertrainingarguments",
          "extends",
          "standard",
          "with",
          "parameters",
          "parameter",
          "purpose"
        ],
        "term_weights": [
          {
            "term": "training",
            "tf": 4,
            "weight": 0.061538
          },
          {
            "term": "sparse",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "arguments",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "specific",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "router",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "mapping",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "learning",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "batch",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "prompts",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "docs",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "overview",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "sparseencodertrainingarguments",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "extends",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "standard",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "parameters",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "parameter",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "purpose",
            "tf": 1,
            "weight": 0.015385
          }
        ],
        "unique_terms": 48,
        "total_terms": 65
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training Arguments",
        "arguments",
        "batch",
        "learning",
        "mapping",
        "prompts",
        "query",
        "router",
        "sparse",
        "specific",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5,
      "overall": 0.7333333333333333
    }
  },
  {
    "text": "## Training Workflow  The complete training workflow integrates all components through the `SparseEncoderTrainer`: ```mermaid sequenceDiagram     participant User     participant SparseEncoderTrainer     participant Model as \"SparseEncoder\"     participant Loss as \"SpladeLoss/CSRLoss\"     participant Evaluator     participant TrainingArgs as \"SparseEncoderTrainingArguments\"          User->>Model: Initialize architecture     User->>Loss: Configure loss function     User->>TrainingArgs: Set training parameters     User->>Evaluator: Setup evaluation     User->>SparseEncoderTrainer: Create trainer          SparseEncoderTrainer->>Model: Forward pass     Model->>Loss: Compute loss + regularization     Loss->>SparseEncoderTrainer: Return loss dict     SparseEncoderTrainer->>Model: Backward pass          alt Evaluation Step         SparseEncoderTrainer->>Evaluator: Run evaluation         Evaluator->>Model: Generate embeddings         Evaluator->>SparseEncoderTrainer: Return metrics     end          SparseEncoderTrainer->>User: Training complete ``` **Sources:** [docs/sparse_encoder/training_overview.md:475-552]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0009",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training Workflow"
      ],
      "heading_text": "Training Workflow",
      "token_count": 228,
      "char_count": 1122,
      "start_char": 7844,
      "end_char": 8966,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.515,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.456832",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 228,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Training Workflow",
      "chunk_hash": "ffd9c42fc35c52f3",
      "content_digest": "ffd9c42fc35c52f3",
      "chunk_length": 1122,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparseencodertrainer",
          "user",
          "loss",
          "participant",
          "model",
          "training",
          "evaluator",
          "evaluation",
          "workflow",
          "the",
          "complete",
          "trainingargs",
          "pass",
          "return",
          "integrates",
          "all",
          "components",
          "through",
          "mermaid",
          "sequencediagram"
        ],
        "term_weights": [
          {
            "term": "sparseencodertrainer",
            "tf": 9,
            "weight": 0.091837
          },
          {
            "term": "user",
            "tf": 7,
            "weight": 0.071429
          },
          {
            "term": "loss",
            "tf": 7,
            "weight": 0.071429
          },
          {
            "term": "participant",
            "tf": 6,
            "weight": 0.061224
          },
          {
            "term": "model",
            "tf": 6,
            "weight": 0.061224
          },
          {
            "term": "training",
            "tf": 5,
            "weight": 0.05102
          },
          {
            "term": "evaluator",
            "tf": 5,
            "weight": 0.05102
          },
          {
            "term": "evaluation",
            "tf": 3,
            "weight": 0.030612
          },
          {
            "term": "workflow",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "complete",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "trainingargs",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "pass",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "return",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "integrates",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "all",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "components",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "through",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "mermaid",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "sequencediagram",
            "tf": 1,
            "weight": 0.010204
          }
        ],
        "unique_terms": 52,
        "total_terms": 98
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training Workflow",
        "evaluation",
        "evaluator",
        "loss",
        "model",
        "participant",
        "sparseencodertrainer",
        "the",
        "training",
        "user",
        "workflow"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.515,
      "overall": 0.7383333333333333
    }
  },
  {
    "text": "### Dataset Format Requirements  Training datasets must match the loss function requirements:  | Loss Function | Input Columns | Label Column | Example | |---------------|---------------|--------------|---------| | `SparseMultipleNegativesRankingLoss` | `(anchor, positive)` or `(anchor, positive, negative)` | None | `[\"query\", \"answer\"]` | | `SparseCoSENTLoss` | `(sentence_A, sentence_B)` | `score` (0-1) | `[\"text1\", \"text2\", \"score\"]` | | `SparseMarginMSELoss` | `(query, positive, negative)` | `margin_scores` | `[\"query\", \"pos\", \"neg\", \"margins\"]` |  **Sources:** [docs/sparse_encoder/training_overview.md:328-344](), [docs/sparse_encoder/loss_overview.md:29-62]()",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0010",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Dataset Format Requirements"
      ],
      "heading_text": "Dataset Format Requirements",
      "token_count": 184,
      "char_count": 671,
      "start_char": 8970,
      "end_char": 9641,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6828571428571429,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.458731",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 184,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Dataset Format Requirements",
      "chunk_hash": "ea1aa474bb38aff3",
      "content_digest": "ea1aa474bb38aff3",
      "chunk_length": 671,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "positive",
          "query",
          "requirements",
          "training",
          "function",
          "anchor",
          "negative",
          "sentence",
          "score",
          "docs",
          "sparse",
          "encoder",
          "overview",
          "dataset",
          "format",
          "datasets",
          "must",
          "match",
          "the"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 3,
            "weight": 0.052632
          },
          {
            "term": "positive",
            "tf": 3,
            "weight": 0.052632
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.052632
          },
          {
            "term": "requirements",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "function",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "anchor",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "negative",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "score",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "docs",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "sparse",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "overview",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "dataset",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "format",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "datasets",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "must",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "match",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.017544
          }
        ],
        "unique_terms": 40,
        "total_terms": 57
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Dataset Format Requirements",
        "anchor",
        "function",
        "loss",
        "negative",
        "positive",
        "query",
        "requirements",
        "score",
        "sentence",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6828571428571429,
      "overall": 0.7942857142857142
    }
  },
  {
    "text": "### Router-Based Training  When using `Router` modules, special configuration is required to map dataset columns to routing paths: ```python",
    "metadata": {
      "chunk_id": "31ed7c569b6a-0012",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Router-Based Training"
      ],
      "heading_text": "Router-Based Training",
      "token_count": 26,
      "char_count": 140,
      "start_char": 9674,
      "end_char": 9814,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5742105263157895,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:50.459471",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 26,
      "document_id": "31ed7c569b6a",
      "document_name": "SparseEncoder_Training",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Router-Based Training",
      "chunk_hash": "7709e5e188a93a77",
      "content_digest": "7709e5e188a93a77",
      "chunk_length": 140,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "router",
          "based",
          "training",
          "when",
          "using",
          "modules",
          "special",
          "configuration",
          "required",
          "map",
          "dataset",
          "columns",
          "routing",
          "paths",
          "python"
        ],
        "term_weights": [
          {
            "term": "router",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "based",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "training",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "when",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "using",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "modules",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "special",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "configuration",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "required",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "map",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "dataset",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "columns",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "routing",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "paths",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.0625
          }
        ],
        "unique_terms": 15,
        "total_terms": 16
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Router-Based Training",
        "based",
        "configuration",
        "map",
        "modules",
        "required",
        "router",
        "special",
        "training",
        "using",
        "when"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5742105263157895,
      "overall": 0.7580701754385964
    }
  }
]