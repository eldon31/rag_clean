[
  {
    "text": "### Dataset Format Requirements  CrossEncoder training datasets must match the chosen loss function requirements. The validation involves three steps:  1. **Input Columns**: All columns except \"label\", \"labels\", \"score\", or \"scores\" are treated as inputs 2. **Label Columns**: If the loss function requires labels, the dataset must have a column named \"label\", \"labels\", \"score\", or \"scores\" 3. **Model Output Compatibility**: The number of model output labels must match the loss function requirements  **Dataset Format Validation Flow** ```mermaid graph TD     InputDataset[Input Dataset]          InputDataset --> CheckColumns{Check Column Names}     CheckColumns --> LabelCols[Extract Label Columns]     CheckColumns --> InputCols[Extract Input Columns]          LabelCols --> ValidateLabels{Validate Labels}     InputCols --> ValidateInputs{Validate Input Count}          ValidateLabels --> CheckModelOutput{Check Model Output}     ValidateInputs --> CheckModelOutput          CheckModelOutput --> Compatible[Compatible Format]     CheckModelOutput --> Error[Format Error]          Compatible --> Training[Begin Training]     Error --> FixDataset[Fix Dataset Format]     FixDataset --> CheckColumns ``` Sources: [docs/cross_encoder/training_overview.md:171-189]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0000",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Dataset Format Requirements"
      ],
      "heading_text": "Dataset Format Requirements",
      "token_count": 259,
      "char_count": 1268,
      "start_char": 119,
      "end_char": 1387,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6922222222222222,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.659737",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 259,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Dataset Format Requirements",
      "chunk_hash": "56fe20b08ef68cbe",
      "content_digest": "56fe20b08ef68cbe",
      "chunk_length": 1268,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "dataset",
          "format",
          "columns",
          "labels",
          "training",
          "input",
          "label",
          "checkcolumns",
          "checkmodeloutput",
          "requirements",
          "must",
          "loss",
          "function",
          "model",
          "output",
          "compatible",
          "error",
          "match",
          "validation"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 6,
            "weight": 0.046875
          },
          {
            "term": "dataset",
            "tf": 5,
            "weight": 0.039062
          },
          {
            "term": "format",
            "tf": 5,
            "weight": 0.039062
          },
          {
            "term": "columns",
            "tf": 5,
            "weight": 0.039062
          },
          {
            "term": "labels",
            "tf": 5,
            "weight": 0.039062
          },
          {
            "term": "training",
            "tf": 4,
            "weight": 0.03125
          },
          {
            "term": "input",
            "tf": 4,
            "weight": 0.03125
          },
          {
            "term": "label",
            "tf": 4,
            "weight": 0.03125
          },
          {
            "term": "checkcolumns",
            "tf": 4,
            "weight": 0.03125
          },
          {
            "term": "checkmodeloutput",
            "tf": 4,
            "weight": 0.03125
          },
          {
            "term": "requirements",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "must",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "loss",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "function",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "model",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "output",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "compatible",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "error",
            "tf": 3,
            "weight": 0.023438
          },
          {
            "term": "match",
            "tf": 2,
            "weight": 0.015625
          },
          {
            "term": "validation",
            "tf": 2,
            "weight": 0.015625
          }
        ],
        "unique_terms": 62,
        "total_terms": 128
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Dataset Format Requirements",
        "checkcolumns",
        "checkmodeloutput",
        "columns",
        "dataset",
        "format",
        "input",
        "label",
        "labels",
        "the",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6922222222222222,
      "overall": 0.7974074074074072
    }
  },
  {
    "text": "### Hard Negatives Mining  CrossEncoder performance often depends on the quality of negative examples. The `mine_hard_negatives` function helps generate challenging negatives: ```python from sentence_transformers.util import mine_hard_negatives  hard_train_dataset = mine_hard_negatives(     train_dataset,     embedding_model,     num_negatives=5,     range_min=10,     range_max=100,     max_score=0.8,     output_format=\"labeled-pair\" ) ``` Sources: [docs/cross_encoder/training_overview.md:204-242]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0002",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Hard Negatives Mining"
      ],
      "heading_text": "Hard Negatives Mining",
      "token_count": 116,
      "char_count": 504,
      "start_char": 2071,
      "end_char": 2575,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7825,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.662353",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 116,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Hard Negatives Mining",
      "chunk_hash": "3940842cbc61098b",
      "content_digest": "3940842cbc61098b",
      "chunk_length": 504,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "negatives",
          "hard",
          "mine",
          "the",
          "train",
          "dataset",
          "range",
          "max",
          "mining",
          "crossencoder",
          "performance",
          "often",
          "depends",
          "quality",
          "negative",
          "examples",
          "function",
          "helps",
          "generate",
          "challenging"
        ],
        "term_weights": [
          {
            "term": "negatives",
            "tf": 6,
            "weight": 0.1
          },
          {
            "term": "hard",
            "tf": 5,
            "weight": 0.083333
          },
          {
            "term": "mine",
            "tf": 3,
            "weight": 0.05
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "train",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "dataset",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "range",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "max",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "mining",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "crossencoder",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "performance",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "often",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "depends",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "quality",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "negative",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "examples",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "function",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "helps",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "generate",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "challenging",
            "tf": 1,
            "weight": 0.016667
          }
        ],
        "unique_terms": 44,
        "total_terms": 60
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Hard Negatives Mining",
        "crossencoder",
        "dataset",
        "hard",
        "max",
        "mine",
        "mining",
        "negatives",
        "range",
        "the",
        "train"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7825,
      "overall": 0.7941666666666666
    }
  },
  {
    "text": "## Training Process Integration  CrossEncoder training integrates with the broader sentence-transformers training infrastructure while maintaining its specialized functionality. **CrossEncoder Training Infrastructure** ```mermaid graph TB     subgraph \"CrossEncoder Specific\"         CEModel[CrossEncoder]         CETrainer[CrossEncoderTrainer]         CEArgs[CrossEncoderTrainingArguments]         CELoss[CrossEncoder Losses]         CEEval[CrossEncoder Evaluators]     end          subgraph \"Shared Infrastructure\"         HFTrainer[transformers.Trainer]         DataCollator[SentenceTransformerDataCollator]         ModelCard[Model Card Callbacks]         Optimizers[Optimizers & Schedulers]     end          subgraph \"External Integration\"         HFHub[Hugging Face Hub]         WandB[Weights & Biases]         TensorBoard[TensorBoard]     end          CETrainer --> HFTrainer     CEArgs --> HFTrainer     DataCollator --> CETrainer     ModelCard --> CETrainer     Optimizers --> CETrainer          CEModel --> CELoss     CELoss --> CETrainer     CEEval --> CETrainer          CETrainer --> HFHub     CETrainer --> WandB     CETrainer --> TensorBoard ``` Sources: [sentence_transformers/trainer.py:59-128](), [docs/cross_encoder/training_overview.md:314-400]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0003",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training Process Integration"
      ],
      "heading_text": "Training Process Integration",
      "token_count": 282,
      "char_count": 1265,
      "start_char": 2579,
      "end_char": 3844,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.501578947368421,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.667353",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 282,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Training Process Integration",
      "chunk_hash": "bd491772ae2bbccc",
      "content_digest": "bd491772ae2bbccc",
      "chunk_length": 1265,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "cetrainer",
          "crossencoder",
          "training",
          "transformers",
          "infrastructure",
          "subgraph",
          "celoss",
          "end",
          "hftrainer",
          "optimizers",
          "tensorboard",
          "integration",
          "sentence",
          "cemodel",
          "ceargs",
          "ceeval",
          "trainer",
          "datacollator",
          "modelcard",
          "hfhub"
        ],
        "term_weights": [
          {
            "term": "cetrainer",
            "tf": 10,
            "weight": 0.098039
          },
          {
            "term": "crossencoder",
            "tf": 6,
            "weight": 0.058824
          },
          {
            "term": "training",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "infrastructure",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "subgraph",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "celoss",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "end",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "hftrainer",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "optimizers",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "tensorboard",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "integration",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "cemodel",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "ceargs",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "ceeval",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "trainer",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "datacollator",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "modelcard",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "hfhub",
            "tf": 2,
            "weight": 0.019608
          }
        ],
        "unique_terms": 58,
        "total_terms": 102
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training Process Integration",
        "celoss",
        "cetrainer",
        "crossencoder",
        "end",
        "hftrainer",
        "infrastructure",
        "optimizers",
        "subgraph",
        "training",
        "transformers"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.501578947368421,
      "overall": 0.7338596491228069
    }
  },
  {
    "text": "### Training Arguments  `CrossEncoderTrainingArguments` extends the standard transformers training arguments with CrossEncoder-specific parameters:  **Key Training Arguments:** - **Performance**: `learning_rate`, `per_device_train_batch_size`, `num_train_epochs`, `gradient_accumulation_steps` - **Optimization**: `fp16`, `bf16`, `optim`, `lr_scheduler_type`, `warmup_ratio` - **Evaluation**: `eval_strategy`, `eval_steps`, `load_best_model_at_end`, `metric_for_best_model` - **Tracking**: `report_to`, `run_name`, `logging_steps`, `push_to_hub`  Sources: [docs/cross_encoder/training_overview.md:320-344]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0004",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training Arguments"
      ],
      "heading_text": "Training Arguments",
      "token_count": 144,
      "char_count": 607,
      "start_char": 3848,
      "end_char": 4455,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7481395348837209,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.668037",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 144,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Training Arguments",
      "chunk_hash": "fc46a09635257338",
      "content_digest": "fc46a09635257338",
      "chunk_length": 607,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "training",
          "arguments",
          "steps",
          "train",
          "eval",
          "best",
          "model",
          "crossencodertrainingarguments",
          "extends",
          "the",
          "standard",
          "transformers",
          "with",
          "crossencoder",
          "specific",
          "parameters",
          "key",
          "performance",
          "learning",
          "rate"
        ],
        "term_weights": [
          {
            "term": "training",
            "tf": 4,
            "weight": 0.059701
          },
          {
            "term": "arguments",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "steps",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "train",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "eval",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "best",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "model",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "crossencodertrainingarguments",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "extends",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "standard",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "transformers",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "crossencoder",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "specific",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "parameters",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "performance",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "learning",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "rate",
            "tf": 1,
            "weight": 0.014925
          }
        ],
        "unique_terms": 56,
        "total_terms": 67
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training Arguments",
        "arguments",
        "best",
        "crossencodertrainingarguments",
        "eval",
        "extends",
        "model",
        "steps",
        "the",
        "train",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7481395348837209,
      "overall": 0.8160465116279069
    }
  },
  {
    "text": "### Evaluation System  CrossEncoder evaluators assess model performance during training with task-specific metrics:  - `BinaryClassificationEvaluator`: For binary classification tasks - `CrossEncoderReranking`: For ranking performance evaluation   - `EmbeddingSimilarityEvaluator`: For similarity scoring tasks - `InformationRetrievalEvaluator`: For retrieval performance  Sources: [docs/cross_encoder/training_overview.md:365-400]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0005",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Evaluation System"
      ],
      "heading_text": "Evaluation System",
      "token_count": 83,
      "char_count": 433,
      "start_char": 4457,
      "end_char": 4890,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5110526315789473,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.668426",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 83,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Evaluation System",
      "chunk_hash": "75db68d2860bcc16",
      "content_digest": "75db68d2860bcc16",
      "chunk_length": 433,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "for",
          "performance",
          "evaluation",
          "training",
          "tasks",
          "system",
          "crossencoder",
          "evaluators",
          "assess",
          "model",
          "during",
          "with",
          "task",
          "specific",
          "metrics",
          "binaryclassificationevaluator",
          "binary",
          "classification",
          "crossencoderreranking",
          "ranking"
        ],
        "term_weights": [
          {
            "term": "for",
            "tf": 4,
            "weight": 0.1
          },
          {
            "term": "performance",
            "tf": 3,
            "weight": 0.075
          },
          {
            "term": "evaluation",
            "tf": 2,
            "weight": 0.05
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.05
          },
          {
            "term": "tasks",
            "tf": 2,
            "weight": 0.05
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "crossencoder",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "evaluators",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "assess",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "during",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "task",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "specific",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "metrics",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "binaryclassificationevaluator",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "binary",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "classification",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "crossencoderreranking",
            "tf": 1,
            "weight": 0.025
          },
          {
            "term": "ranking",
            "tf": 1,
            "weight": 0.025
          }
        ],
        "unique_terms": 32,
        "total_terms": 40
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Evaluation System",
        "assess",
        "crossencoder",
        "evaluation",
        "evaluators",
        "for",
        "model",
        "performance",
        "system",
        "tasks",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5110526315789473,
      "overall": 0.737017543859649
    }
  },
  {
    "text": "## Relationship to Other Training Systems  CrossEncoder training shares infrastructure with other sentence-transformers training systems while maintaining its distinct characteristics. **Training System Relationships** ```mermaid graph TB     subgraph \"Base Training Infrastructure\"         BaseTrainer[SentenceTransformerTrainer]         BaseArgs[SentenceTransformerTrainingArguments]         BaseCollator[SentenceTransformerDataCollator]         BaseCard[SentenceTransformerModelCardCallback]     end          subgraph \"CrossEncoder Training\"         CETrainer[CrossEncoderTrainer]         CEArgs[CrossEncoderTrainingArguments]         CEModel[CrossEncoder]         CELosses[CrossEncoder Losses]         CEEvals[CrossEncoder Evaluators]     end          subgraph \"SparseEncoder Training\"         SETrainer[SparseEncoderTrainer]         SEArgs[SparseEncoderTrainingArguments]         SEModel[SparseEncoder]         SELosses[SparseEncoder Losses]     end          subgraph \"SentenceTransformer Training\"           STModel[SentenceTransformer]         STLosses[SentenceTransformer Losses]         STEvals[SentenceTransformer Evaluators]     end          BaseTrainer --> CETrainer     BaseTrainer --> SETrainer     BaseArgs --> CEArgs     BaseArgs --> SEArgs     BaseCollator --> CETrainer     BaseCollator --> SETrainer     BaseCard --> CETrainer          STModel --> BaseTrainer     CEModel --> CETrainer     SEModel --> SETrainer          STLosses --> BaseTrainer     CELosses --> CETrainer     SELosses --> SETrainer          STEvals --> BaseTrainer     CEEvals --> CETrainer ``` Sources: [sentence_transformers/trainer.py:59-128](), [sentence_transformers/sparse_encoder/trainer.py:31-98]()  The CrossEncoder training system leverages the shared infrastructure while providing specialized components for joint text encoding and ranking tasks, making it suitable for reranking applications and text pair classification scenarios.",
    "metadata": {
      "chunk_id": "2058a897b4b7-0006",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Relationship to Other Training Systems"
      ],
      "heading_text": "Relationship to Other Training Systems",
      "token_count": 401,
      "char_count": 1931,
      "start_char": 4892,
      "end_char": 6823,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5091111888111888,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.674098",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 401,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Relationship to Other Training Systems",
      "chunk_hash": "f6e639b4d2e0e057",
      "content_digest": "f6e639b4d2e0e057",
      "chunk_length": 1931,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "training",
          "cetrainer",
          "crossencoder",
          "basetrainer",
          "setrainer",
          "subgraph",
          "end",
          "sentencetransformer",
          "infrastructure",
          "sentence",
          "transformers",
          "baseargs",
          "basecollator",
          "losses",
          "sparseencoder",
          "other",
          "systems",
          "while",
          "system",
          "basecard"
        ],
        "term_weights": [
          {
            "term": "training",
            "tf": 9,
            "weight": 0.061224
          },
          {
            "term": "cetrainer",
            "tf": 7,
            "weight": 0.047619
          },
          {
            "term": "crossencoder",
            "tf": 6,
            "weight": 0.040816
          },
          {
            "term": "basetrainer",
            "tf": 6,
            "weight": 0.040816
          },
          {
            "term": "setrainer",
            "tf": 5,
            "weight": 0.034014
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.027211
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.027211
          },
          {
            "term": "sentencetransformer",
            "tf": 4,
            "weight": 0.027211
          },
          {
            "term": "infrastructure",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "sentence",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "baseargs",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "basecollator",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "losses",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "sparseencoder",
            "tf": 3,
            "weight": 0.020408
          },
          {
            "term": "other",
            "tf": 2,
            "weight": 0.013605
          },
          {
            "term": "systems",
            "tf": 2,
            "weight": 0.013605
          },
          {
            "term": "while",
            "tf": 2,
            "weight": 0.013605
          },
          {
            "term": "system",
            "tf": 2,
            "weight": 0.013605
          },
          {
            "term": "basecard",
            "tf": 2,
            "weight": 0.013605
          }
        ],
        "unique_terms": 75,
        "total_terms": 147
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Relationship to Other Training Systems",
        "basetrainer",
        "cetrainer",
        "crossencoder",
        "end",
        "infrastructure",
        "sentence",
        "sentencetransformer",
        "setrainer",
        "subgraph",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5091111888111888,
      "overall": 0.7363703962703961
    }
  },
  {
    "text": "# Loss Functions for SentenceTransformer",
    "metadata": {
      "chunk_id": "2058a897b4b7-0007",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Loss Functions for SentenceTransformer"
      ],
      "heading_text": "Loss Functions for SentenceTransformer",
      "token_count": 6,
      "char_count": 40,
      "start_char": 6827,
      "end_char": 6867,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.674691",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 6,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Loss Functions for SentenceTransformer",
      "chunk_hash": "85d4901ab926634f",
      "content_digest": "85d4901ab926634f",
      "chunk_length": 40,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "functions",
          "for",
          "sentencetransformer"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "sentencetransformer",
            "tf": 1,
            "weight": 0.25
          }
        ],
        "unique_terms": 4,
        "total_terms": 4
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Loss Functions for SentenceTransformer",
        "for",
        "functions",
        "loss",
        "sentencetransformer"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "## Introduction  This page documents the loss functions available for training SentenceTransformer models. Loss functions are a critical component that defines the training objective and directly influences the quality and properties of the resulting sentence embeddings. For information about how to use these loss functions in training, see [SentenceTransformer Training](#3.1) and [CrossEncoder Training](#3.2). The SentenceTransformer library offers a diverse set of loss functions, each designed for specific use cases and data formats: ```mermaid graph TD     subgraph \"Overview of Loss Functions\"         ST[SentenceTransformer]         Loss[Loss Functions]         Training[Training Process]                  ST --> Loss         Loss --> Training         Training --> ST     end ``` Sources:  - [sentence_transformers/losses/__init__.py:1-67]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0008",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Introduction"
      ],
      "heading_text": "Introduction",
      "token_count": 163,
      "char_count": 852,
      "start_char": 6872,
      "end_char": 7724,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7051515151515152,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.676660",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 163,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Introduction",
      "chunk_hash": "6f20de80e4895ba5",
      "content_digest": "6f20de80e4895ba5",
      "chunk_length": 852,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "training",
          "functions",
          "the",
          "sentencetransformer",
          "and",
          "for",
          "sentence",
          "use",
          "introduction",
          "this",
          "page",
          "documents",
          "available",
          "models",
          "are",
          "critical",
          "component",
          "that",
          "defines"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 9,
            "weight": 0.102273
          },
          {
            "term": "training",
            "tf": 9,
            "weight": 0.102273
          },
          {
            "term": "functions",
            "tf": 6,
            "weight": 0.068182
          },
          {
            "term": "the",
            "tf": 5,
            "weight": 0.056818
          },
          {
            "term": "sentencetransformer",
            "tf": 4,
            "weight": 0.045455
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.045455
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.034091
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.022727
          },
          {
            "term": "use",
            "tf": 2,
            "weight": 0.022727
          },
          {
            "term": "introduction",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "page",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "documents",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "available",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "models",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "are",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "critical",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "component",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.011364
          },
          {
            "term": "defines",
            "tf": 1,
            "weight": 0.011364
          }
        ],
        "unique_terms": 53,
        "total_terms": 88
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Introduction",
        "and",
        "for",
        "functions",
        "introduction",
        "loss",
        "sentence",
        "sentencetransformer",
        "the",
        "training",
        "use"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7051515151515152,
      "overall": 0.8017171717171716
    }
  },
  {
    "text": "## Ranking-Based Loss Functions\n\nRanking-based loss functions are commonly used for training retrieval models. They focus on learning representations where relevant pairs have higher similarity than irrelevant pairs.",
    "metadata": {
      "chunk_id": "2058a897b4b7-0010",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Ranking-Based Loss Functions"
      ],
      "heading_text": "Ranking-Based Loss Functions",
      "token_count": 34,
      "char_count": 216,
      "start_char": 9787,
      "end_char": 10003,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5578571428571428,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.680517",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 34,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Ranking-Based Loss Functions",
      "chunk_hash": "51702c8a2c43ee77",
      "content_digest": "51702c8a2c43ee77",
      "chunk_length": 216,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "ranking",
          "based",
          "loss",
          "functions",
          "pairs",
          "are",
          "commonly",
          "used",
          "for",
          "training",
          "retrieval",
          "models",
          "they",
          "focus",
          "learning",
          "representations",
          "where",
          "relevant",
          "have",
          "higher"
        ],
        "term_weights": [
          {
            "term": "ranking",
            "tf": 2,
            "weight": 0.071429
          },
          {
            "term": "based",
            "tf": 2,
            "weight": 0.071429
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.071429
          },
          {
            "term": "functions",
            "tf": 2,
            "weight": 0.071429
          },
          {
            "term": "pairs",
            "tf": 2,
            "weight": 0.071429
          },
          {
            "term": "are",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "commonly",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "used",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "training",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "retrieval",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "models",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "they",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "focus",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "learning",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "representations",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "where",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "relevant",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "have",
            "tf": 1,
            "weight": 0.035714
          },
          {
            "term": "higher",
            "tf": 1,
            "weight": 0.035714
          }
        ],
        "unique_terms": 23,
        "total_terms": 28
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Ranking-Based Loss Functions",
        "are",
        "based",
        "commonly",
        "for",
        "functions",
        "loss",
        "pairs",
        "ranking",
        "training",
        "used"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5578571428571428,
      "overall": 0.7192857142857143
    }
  },
  {
    "text": "### MultipleNegativesRankingLoss  This is one of the most widely used loss functions for training sentence embeddings. It treats other samples in the batch as negatives, creating an effective training signal that improves with larger batch sizes. ```mermaid flowchart TD     subgraph \"MultipleNegativesRankingLoss Flow\"         A[\"Anchor Embeddings\"]          P[\"Positive Embeddings\"]         S[\"Similarity Matrix\"]         L[\"Loss Computation\"]                  A --> S         P --> S         S --> L                  L -->|\"Cross Entropy\"| RL[\"Ranking Loss\"]     end ``` **Key Properties**: - Also known as InfoNCE loss, SimCSE loss, or in-batch negatives loss - Performance generally improves with increasing batch size - Requires (anchor, positive) pairs or (anchor, positive, negative) triplets - Each anchor should be most similar to its corresponding positive from all candidates in the batch  Sources: - [sentence_transformers/losses/MultipleNegativesRankingLoss.py:13-132]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0011",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "MultipleNegativesRankingLoss"
      ],
      "heading_text": "MultipleNegativesRankingLoss",
      "token_count": 211,
      "char_count": 984,
      "start_char": 10005,
      "end_char": 10989,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5264406779661017,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.683038",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 211,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "MultipleNegativesRankingLoss",
      "chunk_hash": "62cfef7d1b40fec2",
      "content_digest": "62cfef7d1b40fec2",
      "chunk_length": 984,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "batch",
          "anchor",
          "positive",
          "multiplenegativesrankingloss",
          "the",
          "embeddings",
          "most",
          "training",
          "sentence",
          "negatives",
          "improves",
          "with",
          "this",
          "one",
          "widely",
          "used",
          "functions",
          "for",
          "treats"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 6,
            "weight": 0.065217
          },
          {
            "term": "batch",
            "tf": 5,
            "weight": 0.054348
          },
          {
            "term": "anchor",
            "tf": 4,
            "weight": 0.043478
          },
          {
            "term": "positive",
            "tf": 4,
            "weight": 0.043478
          },
          {
            "term": "multiplenegativesrankingloss",
            "tf": 3,
            "weight": 0.032609
          },
          {
            "term": "the",
            "tf": 3,
            "weight": 0.032609
          },
          {
            "term": "embeddings",
            "tf": 3,
            "weight": 0.032609
          },
          {
            "term": "most",
            "tf": 2,
            "weight": 0.021739
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.021739
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.021739
          },
          {
            "term": "negatives",
            "tf": 2,
            "weight": 0.021739
          },
          {
            "term": "improves",
            "tf": 2,
            "weight": 0.021739
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.021739
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.01087
          },
          {
            "term": "one",
            "tf": 1,
            "weight": 0.01087
          },
          {
            "term": "widely",
            "tf": 1,
            "weight": 0.01087
          },
          {
            "term": "used",
            "tf": 1,
            "weight": 0.01087
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.01087
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.01087
          },
          {
            "term": "treats",
            "tf": 1,
            "weight": 0.01087
          }
        ],
        "unique_terms": 65,
        "total_terms": 92
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "MultipleNegativesRankingLoss",
        "anchor",
        "batch",
        "embeddings",
        "loss",
        "most",
        "multiplenegativesrankingloss",
        "positive",
        "sentence",
        "the",
        "training"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5264406779661017,
      "overall": 0.742146892655367
    }
  },
  {
    "text": "### GISTEmbedLoss  An improved ranking loss that uses a guide model to guide the in-batch negative sample selection, providing a stronger training signal. **Key Properties**: - Uses a teacher/guide model to identify and suppress false negatives - Supports different margin strategies for negative filtering - Better training signal than MultipleNegativesRankingLoss  Sources: - [sentence_transformers/losses/GISTEmbedLoss.py:13-221]() - [sentence_transformers/losses/CachedGISTEmbedLoss.py:63-382]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0013",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "GISTEmbedLoss"
      ],
      "heading_text": "GISTEmbedLoss",
      "token_count": 108,
      "char_count": 499,
      "start_char": 11881,
      "end_char": 12380,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7245454545454545,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.685795",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 108,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "GISTEmbedLoss",
      "chunk_hash": "47ccb112169f0bcc",
      "content_digest": "47ccb112169f0bcc",
      "chunk_length": 499,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "guide",
          "gistembedloss",
          "uses",
          "model",
          "negative",
          "training",
          "signal",
          "sentence",
          "transformers",
          "losses",
          "improved",
          "ranking",
          "loss",
          "that",
          "the",
          "batch",
          "sample",
          "selection",
          "providing",
          "stronger"
        ],
        "term_weights": [
          {
            "term": "guide",
            "tf": 3,
            "weight": 0.057692
          },
          {
            "term": "gistembedloss",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "uses",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "model",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "negative",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "signal",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "transformers",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "losses",
            "tf": 2,
            "weight": 0.038462
          },
          {
            "term": "improved",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "ranking",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "batch",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "sample",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "selection",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "providing",
            "tf": 1,
            "weight": 0.019231
          },
          {
            "term": "stronger",
            "tf": 1,
            "weight": 0.019231
          }
        ],
        "unique_terms": 41,
        "total_terms": 52
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "GISTEmbedLoss",
        "gistembedloss",
        "guide",
        "losses",
        "model",
        "negative",
        "sentence",
        "signal",
        "training",
        "transformers",
        "uses"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7245454545454545,
      "overall": 0.8081818181818181
    }
  },
  {
    "text": "## Contrastive Loss Functions\n\nContrastive losses optimize embeddings so that similar items are closer together and dissimilar items are farther apart in the embedding space.",
    "metadata": {
      "chunk_id": "2058a897b4b7-0014",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Contrastive Loss Functions"
      ],
      "heading_text": "Contrastive Loss Functions",
      "token_count": 31,
      "char_count": 174,
      "start_char": 12383,
      "end_char": 12557,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.554,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.686078",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 31,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Contrastive Loss Functions",
      "chunk_hash": "5154d7599c14e1fa",
      "content_digest": "5154d7599c14e1fa",
      "chunk_length": 174,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "contrastive",
          "items",
          "are",
          "loss",
          "functions",
          "losses",
          "optimize",
          "embeddings",
          "that",
          "similar",
          "closer",
          "together",
          "and",
          "dissimilar",
          "farther",
          "apart",
          "the",
          "embedding",
          "space"
        ],
        "term_weights": [
          {
            "term": "contrastive",
            "tf": 2,
            "weight": 0.090909
          },
          {
            "term": "items",
            "tf": 2,
            "weight": 0.090909
          },
          {
            "term": "are",
            "tf": 2,
            "weight": 0.090909
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "losses",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "optimize",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "similar",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "closer",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "together",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "dissimilar",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "farther",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "apart",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "embedding",
            "tf": 1,
            "weight": 0.045455
          },
          {
            "term": "space",
            "tf": 1,
            "weight": 0.045455
          }
        ],
        "unique_terms": 19,
        "total_terms": 22
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Contrastive Loss Functions",
        "are",
        "contrastive",
        "embeddings",
        "functions",
        "items",
        "loss",
        "losses",
        "optimize",
        "similar",
        "that"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.554,
      "overall": 0.7513333333333333
    }
  },
  {
    "text": "### ContrastiveLoss  The standard contrastive loss function that minimizes distance between positive pairs and maximizes distance between negative pairs. **Key Properties**: - Expects pairs of texts with binary labels (1 for similar, 0 for dissimilar) - Uses a specified distance metric (cosine, euclidean, manhattan) - Includes a margin hyperparameter ```mermaid flowchart TD     subgraph \"ContrastiveLoss Flow\"         Input1[\"Text A\"] --> Embedding1[\"Embedding A\"]         Input2[\"Text B\"] --> Embedding2[\"Embedding B\"]                  Embedding1 --> Distance[\"Distance Calculation\"]         Embedding2 --> Distance                  Distance --> Contrastive[\"Contrastive Loss\"]         Label[\"Label (0 or 1)\"] --> Contrastive     end ``` Sources: - [sentence_transformers/losses/ContrastiveLoss.py:13-120]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0015",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "ContrastiveLoss"
      ],
      "heading_text": "ContrastiveLoss",
      "token_count": 185,
      "char_count": 811,
      "start_char": 12559,
      "end_char": 13370,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5244827586206896,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.688351",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 185,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "ContrastiveLoss",
      "chunk_hash": "974412477551b68d",
      "content_digest": "974412477551b68d",
      "chunk_length": 811,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "distance",
          "contrastive",
          "contrastiveloss",
          "pairs",
          "loss",
          "between",
          "for",
          "text",
          "embedding1",
          "embedding",
          "embedding2",
          "label",
          "the",
          "standard",
          "function",
          "that",
          "minimizes",
          "positive",
          "and",
          "maximizes"
        ],
        "term_weights": [
          {
            "term": "distance",
            "tf": 7,
            "weight": 0.09589
          },
          {
            "term": "contrastive",
            "tf": 4,
            "weight": 0.054795
          },
          {
            "term": "contrastiveloss",
            "tf": 3,
            "weight": 0.041096
          },
          {
            "term": "pairs",
            "tf": 3,
            "weight": 0.041096
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "between",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "text",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "embedding1",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "embedding2",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "label",
            "tf": 2,
            "weight": 0.027397
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "standard",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "function",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "minimizes",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "positive",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.013699
          },
          {
            "term": "maximizes",
            "tf": 1,
            "weight": 0.013699
          }
        ],
        "unique_terms": 52,
        "total_terms": 73
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "ContrastiveLoss",
        "between",
        "contrastive",
        "contrastiveloss",
        "distance",
        "embedding",
        "embedding1",
        "for",
        "loss",
        "pairs",
        "text"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5244827586206896,
      "overall": 0.7414942528735632
    }
  },
  {
    "text": "### CoSENTLoss  An improved contrastive loss that provides a stronger training signal than standard CosineSimilarityLoss. **Key Properties**: - Uses a logsum formulation comparing multiple pairs in the batch - Faster convergence and better performance than CosineSimilarityLoss - Requires sentence pairs with similarity scores  Sources: - [sentence_transformers/losses/CoSENTLoss.py:13-115]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0016",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "CoSENTLoss"
      ],
      "heading_text": "CoSENTLoss",
      "token_count": 80,
      "char_count": 392,
      "start_char": 13375,
      "end_char": 13767,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5508695652173913,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.688759",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 80,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "CoSENTLoss",
      "chunk_hash": "4de75796607c7a9a",
      "content_digest": "4de75796607c7a9a",
      "chunk_length": 392,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "cosentloss",
          "than",
          "cosinesimilarityloss",
          "pairs",
          "sentence",
          "improved",
          "contrastive",
          "loss",
          "that",
          "provides",
          "stronger",
          "training",
          "signal",
          "standard",
          "key",
          "properties",
          "uses",
          "logsum",
          "formulation",
          "comparing"
        ],
        "term_weights": [
          {
            "term": "cosentloss",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "than",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "cosinesimilarityloss",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "pairs",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "improved",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "contrastive",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "provides",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "stronger",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "training",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "signal",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "standard",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "uses",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "logsum",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "formulation",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "comparing",
            "tf": 1,
            "weight": 0.02439
          }
        ],
        "unique_terms": 36,
        "total_terms": 41
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "CoSENTLoss",
        "contrastive",
        "cosentloss",
        "cosinesimilarityloss",
        "improved",
        "loss",
        "pairs",
        "provides",
        "sentence",
        "than",
        "that"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5508695652173913,
      "overall": 0.7502898550724636
    }
  },
  {
    "text": "### ContrastiveTensionLoss  Designed for unsupervised learning, this loss creates positive and negative pairs automatically. **Key Properties**: - Works without explicit labels - Creates a copy of the encoder model to produce embeddings for the first sentence in each pair - Requires using `ContrastiveTensionDataLoader` for proper pair generation  Sources: - [sentence_transformers/losses/ContrastiveTensionLoss.py:17-204]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0017",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "ContrastiveTensionLoss"
      ],
      "heading_text": "ContrastiveTensionLoss",
      "token_count": 90,
      "char_count": 425,
      "start_char": 13770,
      "end_char": 14195,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.542,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.689045",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 90,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "ContrastiveTensionLoss",
      "chunk_hash": "e6da9ca3a58889eb",
      "content_digest": "e6da9ca3a58889eb",
      "chunk_length": 425,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "for",
          "contrastivetensionloss",
          "creates",
          "the",
          "sentence",
          "pair",
          "designed",
          "unsupervised",
          "learning",
          "this",
          "loss",
          "positive",
          "and",
          "negative",
          "pairs",
          "automatically",
          "key",
          "properties",
          "works",
          "without"
        ],
        "term_weights": [
          {
            "term": "for",
            "tf": 3,
            "weight": 0.066667
          },
          {
            "term": "contrastivetensionloss",
            "tf": 2,
            "weight": 0.044444
          },
          {
            "term": "creates",
            "tf": 2,
            "weight": 0.044444
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.044444
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.044444
          },
          {
            "term": "pair",
            "tf": 2,
            "weight": 0.044444
          },
          {
            "term": "designed",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "unsupervised",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "learning",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "positive",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "negative",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "pairs",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "automatically",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "works",
            "tf": 1,
            "weight": 0.022222
          },
          {
            "term": "without",
            "tf": 1,
            "weight": 0.022222
          }
        ],
        "unique_terms": 38,
        "total_terms": 45
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "ContrastiveTensionLoss",
        "contrastivetensionloss",
        "creates",
        "designed",
        "for",
        "learning",
        "pair",
        "sentence",
        "the",
        "this",
        "unsupervised"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.542,
      "overall": 0.7473333333333333
    }
  },
  {
    "text": "## Triplet Loss Functions\n\nTriplet losses use triplets of (anchor, positive, negative) to learn embeddings where the anchor is closer to the positive than to the negative by a certain margin.",
    "metadata": {
      "chunk_id": "2058a897b4b7-0018",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 18,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Triplet Loss Functions"
      ],
      "heading_text": "Triplet Loss Functions",
      "token_count": 40,
      "char_count": 191,
      "start_char": 14198,
      "end_char": 14389,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5416129032258065,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.689295",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 40,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Triplet Loss Functions",
      "chunk_hash": "5737dc9c72c3776d",
      "content_digest": "5737dc9c72c3776d",
      "chunk_length": 191,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "triplet",
          "anchor",
          "positive",
          "negative",
          "loss",
          "functions",
          "losses",
          "use",
          "triplets",
          "learn",
          "embeddings",
          "where",
          "closer",
          "than",
          "certain",
          "margin"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 3,
            "weight": 0.130435
          },
          {
            "term": "triplet",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "anchor",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "positive",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "negative",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "losses",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "use",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "triplets",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "learn",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "where",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "closer",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "than",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "certain",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "margin",
            "tf": 1,
            "weight": 0.043478
          }
        ],
        "unique_terms": 17,
        "total_terms": 23
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Triplet Loss Functions",
        "anchor",
        "functions",
        "loss",
        "losses",
        "negative",
        "positive",
        "the",
        "triplet",
        "triplets",
        "use"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5416129032258065,
      "overall": 0.7472043010752688
    }
  },
  {
    "text": "### TripletLoss  The basic triplet loss function minimizes the distance between anchor and positive while maximizing the distance between anchor and negative. **Key Properties**: - Requires (anchor, positive, negative) triplets - Uses a specified distance metric and margin - Optimizes: max(||anchor - positive|| - ||anchor - negative|| + margin, 0) ```mermaid flowchart TD     subgraph \"TripletLoss Structure\"         A[\"Anchor\"] --> E_A[\"Anchor Embedding\"]         P[\"Positive\"] --> E_P[\"Positive Embedding\"]         N[\"Negative\"] --> E_N[\"Negative Embedding\"]                  E_A --> D_AP[\"Distance(Anchor, Positive)\"]         E_P --> D_AP                  E_A --> D_AN[\"Distance(Anchor, Negative)\"]         E_N --> D_AN                  D_AP --> TL[\"Triplet Loss\"]         D_AN --> TL                  M[\"Margin\"] --> TL     end ``` Sources: - [sentence_transformers/losses/TripletLoss.py:13-112]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0019",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "TripletLoss"
      ],
      "heading_text": "TripletLoss",
      "token_count": 211,
      "char_count": 903,
      "start_char": 14391,
      "end_char": 15294,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5003092783505154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.691248",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 211,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "TripletLoss",
      "chunk_hash": "ae17e9d4132ea13a",
      "content_digest": "ae17e9d4132ea13a",
      "chunk_length": 903,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "anchor",
          "positive",
          "negative",
          "distance",
          "tripletloss",
          "the",
          "and",
          "margin",
          "embedding",
          "triplet",
          "loss",
          "between",
          "basic",
          "function",
          "minimizes",
          "while",
          "maximizing",
          "key",
          "properties",
          "requires"
        ],
        "term_weights": [
          {
            "term": "anchor",
            "tf": 9,
            "weight": 0.126761
          },
          {
            "term": "positive",
            "tf": 6,
            "weight": 0.084507
          },
          {
            "term": "negative",
            "tf": 6,
            "weight": 0.084507
          },
          {
            "term": "distance",
            "tf": 5,
            "weight": 0.070423
          },
          {
            "term": "tripletloss",
            "tf": 3,
            "weight": 0.042254
          },
          {
            "term": "the",
            "tf": 3,
            "weight": 0.042254
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.042254
          },
          {
            "term": "margin",
            "tf": 3,
            "weight": 0.042254
          },
          {
            "term": "embedding",
            "tf": 3,
            "weight": 0.042254
          },
          {
            "term": "triplet",
            "tf": 2,
            "weight": 0.028169
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.028169
          },
          {
            "term": "between",
            "tf": 2,
            "weight": 0.028169
          },
          {
            "term": "basic",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "function",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "minimizes",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "while",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "maximizing",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.014085
          },
          {
            "term": "requires",
            "tf": 1,
            "weight": 0.014085
          }
        ],
        "unique_terms": 36,
        "total_terms": 71
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "TripletLoss",
        "anchor",
        "and",
        "distance",
        "embedding",
        "margin",
        "negative",
        "positive",
        "the",
        "triplet",
        "tripletloss"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5003092783505154,
      "overall": 0.7334364261168383
    }
  },
  {
    "text": "### Batch Triplet Losses  These are more advanced variants of triplet loss that use different strategies for mining triplets within a batch:  1. **BatchHardTripletLoss**: Selects hardest positive and negative samples for each anchor. 2. **BatchSemiHardTripletLoss**: Focuses on semi-hard triplets (not too easy, not too hard). 3. **BatchAllTripletLoss**: Uses all valid triplets in the batch. 4. **BatchHardSoftMarginTripletLoss**: Similar to BatchHardTripletLoss but with a soft margin. **Key Properties**: - Require single sentences with class labels - Create triplets on-the-fly from the batch - Recommend using batches with multiple examples per class - Different mining strategies for different training dynamics  Sources: - [sentence_transformers/losses/BatchHardTripletLoss.py:12-267]() - [sentence_transformers/losses/BatchSemiHardTripletLoss.py:13-188]() - [sentence_transformers/losses/BatchAllTripletLoss.py:13-151]() - [sentence_transformers/losses/BatchHardSoftMarginTripletLoss.py:13-153]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0020",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Batch Triplet Losses"
      ],
      "heading_text": "Batch Triplet Losses",
      "token_count": 246,
      "char_count": 1004,
      "start_char": 15299,
      "end_char": 16303,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5220754716981132,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.693675",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 246,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Batch Triplet Losses",
      "chunk_hash": "2a60e9f05348723c",
      "content_digest": "2a60e9f05348723c",
      "chunk_length": 1004,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "losses",
          "batch",
          "triplets",
          "sentence",
          "transformers",
          "different",
          "for",
          "batchhardtripletloss",
          "the",
          "with",
          "triplet",
          "strategies",
          "mining",
          "batchsemihardtripletloss",
          "hard",
          "not",
          "too",
          "batchalltripletloss",
          "batchhardsoftmargintripletloss",
          "class"
        ],
        "term_weights": [
          {
            "term": "losses",
            "tf": 5,
            "weight": 0.047619
          },
          {
            "term": "batch",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "triplets",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "sentence",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "transformers",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "different",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "batchhardtripletloss",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "the",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "with",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "triplet",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "strategies",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "mining",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "batchsemihardtripletloss",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "hard",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "not",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "too",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "batchalltripletloss",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "batchhardsoftmargintripletloss",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "class",
            "tf": 2,
            "weight": 0.019048
          }
        ],
        "unique_terms": 69,
        "total_terms": 105
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Batch Triplet Losses",
        "batch",
        "batchhardtripletloss",
        "different",
        "for",
        "losses",
        "sentence",
        "the",
        "transformers",
        "triplets",
        "with"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5220754716981132,
      "overall": 0.7073584905660377
    }
  },
  {
    "text": "## MSE-Based Loss Functions\n\nThese loss functions use Mean Squared Error (MSE) to optimize embeddings against a target.",
    "metadata": {
      "chunk_id": "2058a897b4b7-0021",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "MSE-Based Loss Functions"
      ],
      "heading_text": "MSE-Based Loss Functions",
      "token_count": 25,
      "char_count": 119,
      "start_char": 16306,
      "end_char": 16425,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5566666666666666,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.694045",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 25,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "MSE-Based Loss Functions",
      "chunk_hash": "add57076a64e6b2b",
      "content_digest": "add57076a64e6b2b",
      "chunk_length": 119,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "mse",
          "loss",
          "functions",
          "based",
          "these",
          "use",
          "mean",
          "squared",
          "error",
          "optimize",
          "embeddings",
          "against",
          "target"
        ],
        "term_weights": [
          {
            "term": "mse",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "functions",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "based",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "these",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "use",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "mean",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "squared",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "error",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "optimize",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "against",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "target",
            "tf": 1,
            "weight": 0.0625
          }
        ],
        "unique_terms": 13,
        "total_terms": 16
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "MSE-Based Loss Functions",
        "based",
        "error",
        "functions",
        "loss",
        "mean",
        "mse",
        "optimize",
        "squared",
        "these",
        "use"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5566666666666666,
      "overall": 0.7522222222222222
    }
  },
  {
    "text": "### MSELoss  MSE Loss computes the squared error between computed sentence embeddings and target embeddings. **Key Properties**: - Often used for knowledge distillation and multilingual model extension - Requires sentences with corresponding target embeddings - Simple and effective for teacher-student learning ```mermaid flowchart TD     subgraph \"MSELoss Flow\"         Input[\"Input Text\"] --> StudentModel[\"Student Model\"]         StudentModel --> Embedding[\"Student Embedding\"]                  TargetEmb[\"Target Embedding\"] --> MSE[\"MSE Loss\"]         Embedding --> MSE     end ``` Sources: - [sentence_transformers/losses/MSELoss.py:11-98]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0022",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "MSELoss"
      ],
      "heading_text": "MSELoss",
      "token_count": 132,
      "char_count": 647,
      "start_char": 16427,
      "end_char": 17074,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5334782608695652,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.695806",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 132,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "MSELoss",
      "chunk_hash": "de30546d48a5721d",
      "content_digest": "de30546d48a5721d",
      "chunk_length": 647,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "mse",
          "embedding",
          "mseloss",
          "embeddings",
          "and",
          "target",
          "student",
          "loss",
          "sentence",
          "for",
          "model",
          "input",
          "studentmodel",
          "computes",
          "the",
          "squared",
          "error",
          "between",
          "computed",
          "key"
        ],
        "term_weights": [
          {
            "term": "mse",
            "tf": 4,
            "weight": 0.059701
          },
          {
            "term": "embedding",
            "tf": 4,
            "weight": 0.059701
          },
          {
            "term": "mseloss",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "embeddings",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "target",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "student",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "model",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "studentmodel",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "computes",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "squared",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "error",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "between",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "computed",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.014925
          }
        ],
        "unique_terms": 45,
        "total_terms": 67
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "MSELoss",
        "and",
        "embedding",
        "embeddings",
        "for",
        "loss",
        "mse",
        "mseloss",
        "sentence",
        "student",
        "target"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5334782608695652,
      "overall": 0.7444927536231883
    }
  },
  {
    "text": "### MarginMSELoss  An extension of MSE loss that focuses on the margin between pairs of passages for a query. **Key Properties**: - Computes MSE between predicted margins and gold margins - More suitable for ranking tasks - Does not require strict positive/negative distinction - Often used with a teacher model in knowledge distillation  Sources: - [sentence_transformers/losses/MarginMSELoss.py:10-143]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0023",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "MarginMSELoss"
      ],
      "heading_text": "MarginMSELoss",
      "token_count": 85,
      "char_count": 406,
      "start_char": 17079,
      "end_char": 17485,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5364285714285714,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.696171",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 85,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "MarginMSELoss",
      "chunk_hash": "d23e70c5ba6bde76",
      "content_digest": "d23e70c5ba6bde76",
      "chunk_length": 406,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "marginmseloss",
          "mse",
          "between",
          "for",
          "margins",
          "extension",
          "loss",
          "that",
          "focuses",
          "the",
          "margin",
          "pairs",
          "passages",
          "query",
          "key",
          "properties",
          "computes",
          "predicted",
          "and",
          "gold"
        ],
        "term_weights": [
          {
            "term": "marginmseloss",
            "tf": 2,
            "weight": 0.041667
          },
          {
            "term": "mse",
            "tf": 2,
            "weight": 0.041667
          },
          {
            "term": "between",
            "tf": 2,
            "weight": 0.041667
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.041667
          },
          {
            "term": "margins",
            "tf": 2,
            "weight": 0.041667
          },
          {
            "term": "extension",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "that",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "focuses",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "margin",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "pairs",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "passages",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "query",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "computes",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "predicted",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.020833
          },
          {
            "term": "gold",
            "tf": 1,
            "weight": 0.020833
          }
        ],
        "unique_terms": 43,
        "total_terms": 48
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "MarginMSELoss",
        "between",
        "extension",
        "focuses",
        "for",
        "loss",
        "marginmseloss",
        "margins",
        "mse",
        "that",
        "the"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5364285714285714,
      "overall": 0.7454761904761904
    }
  },
  {
    "text": "## Specialized Loss Functions",
    "metadata": {
      "chunk_id": "2058a897b4b7-0025",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Specialized Loss Functions"
      ],
      "heading_text": "Specialized Loss Functions",
      "token_count": 5,
      "char_count": 29,
      "start_char": 17866,
      "end_char": 17895,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.696611",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 5,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Specialized Loss Functions",
      "chunk_hash": "d61f9d78abba3bae",
      "content_digest": "d61f9d78abba3bae",
      "chunk_length": 29,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "specialized",
          "loss",
          "functions"
        ],
        "term_weights": [
          {
            "term": "specialized",
            "tf": 1,
            "weight": 0.333333
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.333333
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.333333
          }
        ],
        "unique_terms": 3,
        "total_terms": 3
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Specialized Loss Functions",
        "functions",
        "loss",
        "specialized"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### MatryoshkaLoss  A loss function modifier that enables training models to produce effective embeddings at multiple dimensions. This allows users to reduce the embedding dimension at inference time without retraining. **Key Properties**: - Trains on multiple embedding dimensions simultaneously - Allows flexible trade-off between quality and dimensionality at inference time - Compatible with other base losses (wraps another loss function) ```mermaid flowchart TD     subgraph \"MatryoshkaLoss Architecture\"         BaseL[\"Base Loss Function\"]         ML[\"MatryoshkaLoss\"]                  Dims[\"Multiple Dimensions\"] --> ML         Weights[\"Dimension Weights\"] --> ML         BaseL --> ML                  ML --> MDim1[\"Train at Dim 768\"]         ML --> MDim2[\"Train at Dim 384\"]         ML --> MDim3[\"Train at Dim 128\"]                  MDim1 --> CombLoss[\"Combined Loss\"]         MDim2 --> CombLoss         MDim3 --> CombLoss     end ``` Sources: - [sentence_transformers/losses/MatryoshkaLoss.py:113-253]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0026",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "MatryoshkaLoss"
      ],
      "heading_text": "MatryoshkaLoss",
      "token_count": 223,
      "char_count": 1013,
      "start_char": 17897,
      "end_char": 18910,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5057894736842106,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.699309",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 223,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "MatryoshkaLoss",
      "chunk_hash": "32b4ac5abaca5f62",
      "content_digest": "32b4ac5abaca5f62",
      "chunk_length": 1013,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "matryoshkaloss",
          "loss",
          "function",
          "multiple",
          "dimensions",
          "train",
          "dim",
          "combloss",
          "allows",
          "embedding",
          "dimension",
          "inference",
          "time",
          "base",
          "losses",
          "basel",
          "weights",
          "mdim1",
          "mdim2",
          "mdim3"
        ],
        "term_weights": [
          {
            "term": "matryoshkaloss",
            "tf": 4,
            "weight": 0.042105
          },
          {
            "term": "loss",
            "tf": 4,
            "weight": 0.042105
          },
          {
            "term": "function",
            "tf": 3,
            "weight": 0.031579
          },
          {
            "term": "multiple",
            "tf": 3,
            "weight": 0.031579
          },
          {
            "term": "dimensions",
            "tf": 3,
            "weight": 0.031579
          },
          {
            "term": "train",
            "tf": 3,
            "weight": 0.031579
          },
          {
            "term": "dim",
            "tf": 3,
            "weight": 0.031579
          },
          {
            "term": "combloss",
            "tf": 3,
            "weight": 0.031579
          },
          {
            "term": "allows",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "dimension",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "inference",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "time",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "base",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "losses",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "basel",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "weights",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "mdim1",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "mdim2",
            "tf": 2,
            "weight": 0.021053
          },
          {
            "term": "mdim3",
            "tf": 2,
            "weight": 0.021053
          }
        ],
        "unique_terms": 65,
        "total_terms": 95
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "MatryoshkaLoss",
        "allows",
        "combloss",
        "dim",
        "dimensions",
        "embedding",
        "function",
        "loss",
        "matryoshkaloss",
        "multiple",
        "train"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5057894736842106,
      "overall": 0.7352631578947367
    }
  },
  {
    "text": "### AdaptiveLayerLoss  Trains model to produce good embeddings with fewer transformer layers, enabling faster inference. **Key Properties**: - Applies loss to intermediate transformer layers - Allows layer reduction at inference time - KL divergence regularization between layer outputs  Sources: - [sentence_transformers/losses/AdaptiveLayerLoss.py:106-274]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0027",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "AdaptiveLayerLoss"
      ],
      "heading_text": "AdaptiveLayerLoss",
      "token_count": 68,
      "char_count": 360,
      "start_char": 18915,
      "end_char": 19275,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5460975609756098,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.699725",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 68,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "AdaptiveLayerLoss",
      "chunk_hash": "d7e466c6ae640b41",
      "content_digest": "d7e466c6ae640b41",
      "chunk_length": 360,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "adaptivelayerloss",
          "transformer",
          "layers",
          "inference",
          "layer",
          "trains",
          "model",
          "produce",
          "good",
          "embeddings",
          "with",
          "fewer",
          "enabling",
          "faster",
          "key",
          "properties",
          "applies",
          "loss",
          "intermediate",
          "allows"
        ],
        "term_weights": [
          {
            "term": "adaptivelayerloss",
            "tf": 2,
            "weight": 0.054054
          },
          {
            "term": "transformer",
            "tf": 2,
            "weight": 0.054054
          },
          {
            "term": "layers",
            "tf": 2,
            "weight": 0.054054
          },
          {
            "term": "inference",
            "tf": 2,
            "weight": 0.054054
          },
          {
            "term": "layer",
            "tf": 2,
            "weight": 0.054054
          },
          {
            "term": "trains",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "produce",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "good",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "fewer",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "enabling",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "faster",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "applies",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "intermediate",
            "tf": 1,
            "weight": 0.027027
          },
          {
            "term": "allows",
            "tf": 1,
            "weight": 0.027027
          }
        ],
        "unique_terms": 32,
        "total_terms": 37
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "AdaptiveLayerLoss",
        "adaptivelayerloss",
        "embeddings",
        "good",
        "inference",
        "layer",
        "layers",
        "model",
        "produce",
        "trains",
        "transformer"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5460975609756098,
      "overall": 0.7153658536585366
    }
  },
  {
    "text": "### Matryoshka2dLoss  Combines MatryoshkaLoss and AdaptiveLayerLoss to enable both dimension and layer reduction. **Key Properties**: - 2D flexibility in both dimensions and layers - Allows for different performance vs. efficiency trade-offs  Sources: - [sentence_transformers/losses/Matryoshka2dLoss.py:13-152]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0028",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Matryoshka2dLoss"
      ],
      "heading_text": "Matryoshka2dLoss",
      "token_count": 77,
      "char_count": 313,
      "start_char": 19278,
      "end_char": 19591,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5458823529411765,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.699974",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 77,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Matryoshka2dLoss",
      "chunk_hash": "42d52817e0b2bb4a",
      "content_digest": "42d52817e0b2bb4a",
      "chunk_length": 313,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "and",
          "matryoshka2dloss",
          "both",
          "combines",
          "matryoshkaloss",
          "adaptivelayerloss",
          "enable",
          "dimension",
          "layer",
          "reduction",
          "key",
          "properties",
          "flexibility",
          "dimensions",
          "layers",
          "allows",
          "for",
          "different",
          "performance",
          "efficiency"
        ],
        "term_weights": [
          {
            "term": "and",
            "tf": 3,
            "weight": 0.096774
          },
          {
            "term": "matryoshka2dloss",
            "tf": 2,
            "weight": 0.064516
          },
          {
            "term": "both",
            "tf": 2,
            "weight": 0.064516
          },
          {
            "term": "combines",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "matryoshkaloss",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "adaptivelayerloss",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "enable",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "dimension",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "layer",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "reduction",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "flexibility",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "dimensions",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "layers",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "allows",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "different",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "performance",
            "tf": 1,
            "weight": 0.032258
          },
          {
            "term": "efficiency",
            "tf": 1,
            "weight": 0.032258
          }
        ],
        "unique_terms": 27,
        "total_terms": 31
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Matryoshka2dLoss",
        "adaptivelayerloss",
        "and",
        "both",
        "combines",
        "dimension",
        "enable",
        "layer",
        "matryoshka2dloss",
        "matryoshkaloss",
        "reduction"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5458823529411765,
      "overall": 0.7152941176470587
    }
  },
  {
    "text": "### DenoisingAutoEncoderLoss  Trains a model to reconstruct original sentences from damaged versions, useful for unsupervised learning. **Key Properties**: - Requires pairs of damaged and original sentences - Creates a decoder component to reconstruct from embeddings - Used in TSDAE (Transformer-based Sequential Denoising Auto-Encoder)  Sources: - [sentence_transformers/losses/DenoisingAutoEncoderLoss.py:15-203]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0029",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "DenoisingAutoEncoderLoss"
      ],
      "heading_text": "DenoisingAutoEncoderLoss",
      "token_count": 88,
      "char_count": 417,
      "start_char": 19594,
      "end_char": 20011,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5247826086956522,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.700250",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 88,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "DenoisingAutoEncoderLoss",
      "chunk_hash": "778d5dbd23568403",
      "content_digest": "778d5dbd23568403",
      "chunk_length": 417,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "denoisingautoencoderloss",
          "reconstruct",
          "original",
          "sentences",
          "from",
          "damaged",
          "trains",
          "model",
          "versions",
          "useful",
          "for",
          "unsupervised",
          "learning",
          "key",
          "properties",
          "requires",
          "pairs",
          "and",
          "creates",
          "decoder"
        ],
        "term_weights": [
          {
            "term": "denoisingautoencoderloss",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "reconstruct",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "original",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "sentences",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "from",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "damaged",
            "tf": 2,
            "weight": 0.04878
          },
          {
            "term": "trains",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "versions",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "useful",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "unsupervised",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "learning",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "key",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "properties",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "requires",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "pairs",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "creates",
            "tf": 1,
            "weight": 0.02439
          },
          {
            "term": "decoder",
            "tf": 1,
            "weight": 0.02439
          }
        ],
        "unique_terms": 35,
        "total_terms": 41
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "DenoisingAutoEncoderLoss",
        "damaged",
        "denoisingautoencoderloss",
        "from",
        "model",
        "original",
        "reconstruct",
        "sentences",
        "trains",
        "useful",
        "versions"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5247826086956522,
      "overall": 0.7415942028985506
    }
  },
  {
    "text": "## Loss Function Selection Guide  The table below provides guidance on which loss function to use based on your data and task:  | Loss Function | Best For | Input Format | Special Requirements | |---------------|----------|--------------|---------------------| | MultipleNegativesRankingLoss | Retrieval, general purpose | (anchor, positive) pairs | Large batch size beneficial | | CoSENTLoss | Semantic similarity | Text pairs with scores | - | | TripletLoss | Clustering, similarity | (anchor, positive, negative) triplets | - | | BatchHardTripletLoss | Classification | Single texts with labels | Multiple examples per class | | MSELoss | Distillation, transfer | Texts with target embeddings | Teacher model | | MatryoshkaLoss | Size-efficient models | Depends on base loss | - | | AdaptiveLayerLoss | Speed-efficient models | Depends on base loss | - | | ContrastiveTensionLoss | Unsupervised learning | Single sentences | Special dataloader | | GISTEmbedLoss | Better negative sampling | Same as MNRL | Guide model |",
    "metadata": {
      "chunk_id": "2058a897b4b7-0031",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 31,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Loss Function Selection Guide"
      ],
      "heading_text": "Loss Function Selection Guide",
      "token_count": 221,
      "char_count": 1022,
      "start_char": 20379,
      "end_char": 21401,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.650377358490566,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.704182",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 221,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Loss Function Selection Guide",
      "chunk_hash": "a79cb57bb5f335ab",
      "content_digest": "a79cb57bb5f335ab",
      "chunk_length": 1022,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "function",
          "with",
          "guide",
          "special",
          "anchor",
          "positive",
          "pairs",
          "size",
          "similarity",
          "negative",
          "single",
          "texts",
          "model",
          "efficient",
          "models",
          "depends",
          "base",
          "selection",
          "the"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 5,
            "weight": 0.050505
          },
          {
            "term": "function",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "with",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "guide",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "special",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "anchor",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "positive",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "pairs",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "size",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "similarity",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "negative",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "single",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "texts",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "model",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "efficient",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "depends",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "base",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "selection",
            "tf": 1,
            "weight": 0.010101
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.010101
          }
        ],
        "unique_terms": 76,
        "total_terms": 99
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Loss Function Selection Guide",
        "anchor",
        "function",
        "guide",
        "loss",
        "pairs",
        "positive",
        "similarity",
        "size",
        "special",
        "with"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.650377358490566,
      "overall": 0.7834591194968553
    }
  },
  {
    "text": "## Implementation Details  All loss functions in SentenceTransformer follow a common pattern:  1. They are subclasses of `torch.nn.Module` 2. They implement a `forward` method that:    - Takes sentence features and optional labels as input    - Computes sentence embeddings using the model    - Calculates and returns the loss value  Many loss functions also provide: - `get_config_dict` method for configuration serialization - `citation` property for academic references - Documentation about input requirements and recommendations  The loss function is typically passed to a `SentenceTransformerTrainer` along with the model and dataset, as shown in this example pattern: ```python from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses from datasets import Dataset  model = SentenceTransformer(\"model_name\") train_dataset = Dataset.from_dict({     \"anchor\": [\"Text A\", \"Text B\"],       \"positive\": [\"Similar to A\", \"Similar to B\"], }) loss = losses.MultipleNegativesRankingLoss(model)  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=train_dataset,     loss=loss, ) trainer.train() ``` Sources: - [sentence_transformers/losses/MultipleNegativesRankingLoss.py:13-132]() - [sentence_transformers/losses/__init__.py:1-67]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0033",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 33,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Implementation Details"
      ],
      "heading_text": "Implementation Details",
      "token_count": 270,
      "char_count": 1284,
      "start_char": 22605,
      "end_char": 23889,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7064285714285714,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.709337",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 270,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Implementation Details",
      "chunk_hash": "29aa53d50bf0b3d9",
      "content_digest": "29aa53d50bf0b3d9",
      "chunk_length": 1284,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "model",
          "dataset",
          "sentence",
          "and",
          "the",
          "losses",
          "train",
          "sentencetransformer",
          "sentencetransformertrainer",
          "from",
          "transformers",
          "functions",
          "pattern",
          "they",
          "method",
          "input",
          "dict",
          "for",
          "import"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 7,
            "weight": 0.053846
          },
          {
            "term": "model",
            "tf": 7,
            "weight": 0.053846
          },
          {
            "term": "dataset",
            "tf": 6,
            "weight": 0.046154
          },
          {
            "term": "sentence",
            "tf": 5,
            "weight": 0.038462
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.030769
          },
          {
            "term": "the",
            "tf": 4,
            "weight": 0.030769
          },
          {
            "term": "losses",
            "tf": 4,
            "weight": 0.030769
          },
          {
            "term": "train",
            "tf": 4,
            "weight": 0.030769
          },
          {
            "term": "sentencetransformer",
            "tf": 3,
            "weight": 0.023077
          },
          {
            "term": "sentencetransformertrainer",
            "tf": 3,
            "weight": 0.023077
          },
          {
            "term": "from",
            "tf": 3,
            "weight": 0.023077
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.023077
          },
          {
            "term": "functions",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "pattern",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "they",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "method",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "dict",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.015385
          },
          {
            "term": "import",
            "tf": 2,
            "weight": 0.015385
          }
        ],
        "unique_terms": 77,
        "total_terms": 130
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Implementation Details",
        "and",
        "dataset",
        "loss",
        "losses",
        "model",
        "sentence",
        "sentencetransformer",
        "sentencetransformertrainer",
        "the",
        "train"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7064285714285714,
      "overall": 0.802142857142857
    }
  },
  {
    "text": "# Loss Functions for SparseEncoder\n\n\n\n\nThis document covers the specialized loss functions designed for training `SparseEncoder` models. These losses are specifically tailored for sparse neural information retrieval models like SPLADE and CSR architectures that require both effectiveness and efficiency through sparsity regularization.\n\nFor dense embedding loss functions, see [Loss Functions for SentenceTransformer](#3.4). For reranking model loss functions, see [Loss Functions for CrossEncoder](#3.6).",
    "metadata": {
      "chunk_id": "2058a897b4b7-0034",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 34,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Loss Functions for SparseEncoder"
      ],
      "heading_text": "Loss Functions for SparseEncoder",
      "token_count": 90,
      "char_count": 506,
      "start_char": 23893,
      "end_char": 24399,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5125806451612903,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.709911",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 90,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Loss Functions for SparseEncoder",
      "chunk_hash": "0eb0751b883e714b",
      "content_digest": "0eb0751b883e714b",
      "chunk_length": 506,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "for",
          "loss",
          "functions",
          "sparseencoder",
          "models",
          "and",
          "see",
          "this",
          "document",
          "covers",
          "the",
          "specialized",
          "designed",
          "training",
          "these",
          "losses",
          "are",
          "specifically",
          "tailored",
          "sparse"
        ],
        "term_weights": [
          {
            "term": "for",
            "tf": 7,
            "weight": 0.114754
          },
          {
            "term": "loss",
            "tf": 6,
            "weight": 0.098361
          },
          {
            "term": "functions",
            "tf": 6,
            "weight": 0.098361
          },
          {
            "term": "sparseencoder",
            "tf": 2,
            "weight": 0.032787
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.032787
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.032787
          },
          {
            "term": "see",
            "tf": 2,
            "weight": 0.032787
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "document",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "covers",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "specialized",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "designed",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "training",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "these",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "losses",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "are",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "specifically",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "tailored",
            "tf": 1,
            "weight": 0.016393
          },
          {
            "term": "sparse",
            "tf": 1,
            "weight": 0.016393
          }
        ],
        "unique_terms": 41,
        "total_terms": 61
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Loss Functions for SparseEncoder",
        "and",
        "covers",
        "document",
        "for",
        "functions",
        "loss",
        "models",
        "see",
        "sparseencoder",
        "this"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5125806451612903,
      "overall": 0.73752688172043
    }
  },
  {
    "text": "## Overview  SparseEncoder loss functions follow a hierarchical architecture where wrapper losses combine base contrastive/similarity losses with regularization terms to control sparsity. The two main wrapper losses are:  - **`SpladeLoss`**: For SPLADE-style models that use MLM heads with pooling - **`CSRLoss`**: For CSR (Contrastive Sparse Representation) models with autoencoder components  These wrapper losses are required for training `SparseEncoder` models, as they provide the necessary sparsity regularization on top of standard contrastive learning objectives.",
    "metadata": {
      "chunk_id": "2058a897b4b7-0035",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 35,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Overview"
      ],
      "heading_text": "Overview",
      "token_count": 110,
      "char_count": 571,
      "start_char": 24401,
      "end_char": 24972,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5365753424657534,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.710171",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 110,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Overview",
      "chunk_hash": "0ef4a91ed1ed23e0",
      "content_digest": "0ef4a91ed1ed23e0",
      "chunk_length": 571,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "losses",
          "wrapper",
          "contrastive",
          "with",
          "for",
          "models",
          "sparseencoder",
          "regularization",
          "sparsity",
          "the",
          "are",
          "overview",
          "loss",
          "functions",
          "follow",
          "hierarchical",
          "architecture",
          "where",
          "combine",
          "base"
        ],
        "term_weights": [
          {
            "term": "losses",
            "tf": 4,
            "weight": 0.059701
          },
          {
            "term": "wrapper",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "contrastive",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "with",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "models",
            "tf": 3,
            "weight": 0.044776
          },
          {
            "term": "sparseencoder",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "regularization",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "sparsity",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "are",
            "tf": 2,
            "weight": 0.029851
          },
          {
            "term": "overview",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "loss",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "functions",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "follow",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "hierarchical",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "where",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "combine",
            "tf": 1,
            "weight": 0.014925
          },
          {
            "term": "base",
            "tf": 1,
            "weight": 0.014925
          }
        ],
        "unique_terms": 49,
        "total_terms": 67
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Overview",
        "contrastive",
        "for",
        "losses",
        "models",
        "regularization",
        "sparseencoder",
        "sparsity",
        "the",
        "with",
        "wrapper"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5365753424657534,
      "overall": 0.7455251141552511
    }
  },
  {
    "text": "### CSRLoss  `CSRLoss` is designed for CSR (Contrastive Sparse Representation) models that use autoencoder components for reconstruction-based training. **Key Components:** - **Base Loss**: Typically `SparseMultipleNegativesRankingLoss` for contrastive learning - **Reconstruction Loss**: `CSRReconstructionLoss` with three components:   - L_k: Reconstruction loss using top-k sparse components   - L_4k: Reconstruction loss using top-4k sparse components     - L_aux: Auxiliary loss for residual information  **Configuration Parameters:** - `beta`: Weight for L_aux component in reconstruction loss - `gamma`: Weight for the main contrastive loss component ```mermaid graph TD     subgraph \"CSRLoss Components\"         CSRMain[\"CSRLoss\"]         MainLoss[\"base_loss<br/>(e.g., SparseMultipleNegativesRankingLoss)\"]         ReconLoss[\"CSRReconstructionLoss\"]                  subgraph \"Reconstruction Components\"             LossK[\"L_k: MSE(x, recons_k)\"]             Loss4K[\"L_4k: MSE(x, recons_4k)\"]             LossAux[\"L_aux: NMSE(recons_aux, residual)\"]         end                  CSRMain --> MainLoss         CSRMain --> ReconLoss         ReconLoss --> LossK         ReconLoss --> Loss4K           ReconLoss --> LossAux     end ``` Sources: [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-215](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:28-127]()",
    "metadata": {
      "chunk_id": "2058a897b4b7-0039",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 39,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "CSRLoss"
      ],
      "heading_text": "CSRLoss",
      "token_count": 328,
      "char_count": 1382,
      "start_char": 28153,
      "end_char": 29535,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5041851239669422,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.720974",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 328,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "CSRLoss",
      "chunk_hash": "16f6d7904fb1c7a9",
      "content_digest": "16f6d7904fb1c7a9",
      "chunk_length": 1382,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "components",
          "csrloss",
          "for",
          "reconstruction",
          "sparse",
          "reconloss",
          "aux",
          "contrastive",
          "csrmain",
          "recons",
          "base",
          "sparsemultiplenegativesrankingloss",
          "csrreconstructionloss",
          "using",
          "top",
          "residual",
          "weight",
          "component",
          "subgraph"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 8,
            "weight": 0.065041
          },
          {
            "term": "components",
            "tf": 7,
            "weight": 0.056911
          },
          {
            "term": "csrloss",
            "tf": 6,
            "weight": 0.04878
          },
          {
            "term": "for",
            "tf": 6,
            "weight": 0.04878
          },
          {
            "term": "reconstruction",
            "tf": 6,
            "weight": 0.04878
          },
          {
            "term": "sparse",
            "tf": 5,
            "weight": 0.04065
          },
          {
            "term": "reconloss",
            "tf": 5,
            "weight": 0.04065
          },
          {
            "term": "aux",
            "tf": 4,
            "weight": 0.03252
          },
          {
            "term": "contrastive",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "csrmain",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "recons",
            "tf": 3,
            "weight": 0.02439
          },
          {
            "term": "base",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "sparsemultiplenegativesrankingloss",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "csrreconstructionloss",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "using",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "top",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "residual",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "weight",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "component",
            "tf": 2,
            "weight": 0.01626
          },
          {
            "term": "subgraph",
            "tf": 2,
            "weight": 0.01626
          }
        ],
        "unique_terms": 59,
        "total_terms": 123
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "CSRLoss",
        "aux",
        "components",
        "contrastive",
        "csrloss",
        "csrmain",
        "for",
        "loss",
        "reconloss",
        "reconstruction",
        "sparse"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5041851239669422,
      "overall": 0.7347283746556473
    }
  },
  {
    "text": "# Typical SPLADE training setup student_model = SparseEncoder(\"distilbert/distilbert-base-uncased\") teacher_model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")  loss = SpladeLoss(     model=student_model,     loss=SparseMarginMSELoss(student_model),     document_regularizer_weight=3e-5,     query_regularizer_weight=5e-5, ) ```",
    "metadata": {
      "chunk_id": "2058a897b4b7-0045",
      "source_file": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "filename": "Model_without_classification_head_will_be_added.md",
      "file_extension": ".md",
      "chunk_index": 45,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Typical SPLADE training setup"
      ],
      "heading_text": "Typical SPLADE training setup",
      "token_count": 90,
      "char_count": 341,
      "start_char": 31979,
      "end_char": 32320,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.56,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:49.725305",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 90,
      "document_id": "2058a897b4b7",
      "document_name": "Model_without_classification_head_will_be_added",
      "source_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "source_filename": "Model_without_classification_head_will_be_added.md",
      "source_directory": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\Sentence_Transformer\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "hierarchy_path": "Typical SPLADE training setup",
      "chunk_hash": "d17ae073f21d5af9",
      "content_digest": "d17ae073f21d5af9",
      "chunk_length": 341,
      "payload_version": "1.3",
      "collection_hints": [
        "sentence_transformer"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "model",
          "student",
          "splade",
          "sparseencoder",
          "distilbert",
          "loss",
          "regularizer",
          "weight",
          "typical",
          "training",
          "setup",
          "base",
          "uncased",
          "teacher",
          "naver",
          "cocondenser",
          "ensembledistil",
          "spladeloss",
          "sparsemarginmseloss",
          "document"
        ],
        "term_weights": [
          {
            "term": "model",
            "tf": 5,
            "weight": 0.151515
          },
          {
            "term": "student",
            "tf": 3,
            "weight": 0.090909
          },
          {
            "term": "splade",
            "tf": 2,
            "weight": 0.060606
          },
          {
            "term": "sparseencoder",
            "tf": 2,
            "weight": 0.060606
          },
          {
            "term": "distilbert",
            "tf": 2,
            "weight": 0.060606
          },
          {
            "term": "loss",
            "tf": 2,
            "weight": 0.060606
          },
          {
            "term": "regularizer",
            "tf": 2,
            "weight": 0.060606
          },
          {
            "term": "weight",
            "tf": 2,
            "weight": 0.060606
          },
          {
            "term": "typical",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "training",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "setup",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "base",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "uncased",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "teacher",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "naver",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "cocondenser",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "ensembledistil",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "spladeloss",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "sparsemarginmseloss",
            "tf": 1,
            "weight": 0.030303
          },
          {
            "term": "document",
            "tf": 1,
            "weight": 0.030303
          }
        ],
        "unique_terms": 21,
        "total_terms": 33
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Typical SPLADE training setup",
        "distilbert",
        "loss",
        "model",
        "regularizer",
        "sparseencoder",
        "splade",
        "student",
        "training",
        "typical",
        "weight"
      ],
      "collection_name": "Sentence_Transformer"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.56,
      "overall": 0.7533333333333333
    }
  }
]