[
  {
    "text": "### Essentials  [Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)  [Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)  [Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)  [Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)  [Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)  [5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)  [Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)",
    "metadata": {
      "chunk_id": "3622726ae776-0000",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Essentials"
      ],
      "heading_text": "Essentials",
      "token_count": 189,
      "char_count": 673,
      "start_char": 679,
      "end_char": 1352,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5053846153846153,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.721642",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 189,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Essentials",
      "chunk_hash": "a64044a3995d82e4",
      "content_digest": "a64044a3995d82e4",
      "chunk_length": 673,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "rag",
          "https",
          "tech",
          "documentation",
          "agentic",
          "with",
          "data",
          "ingestion",
          "beginners",
          "langgraph",
          "discord",
          "multimodal",
          "and",
          "deepseek",
          "n8n",
          "essentials",
          "for",
          "simple",
          "system"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.119048
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "tech",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "documentation",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "agentic",
            "tf": 6,
            "weight": 0.071429
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.059524
          },
          {
            "term": "data",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "ingestion",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "beginners",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "langgraph",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "discord",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "multimodal",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "deepseek",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "n8n",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "essentials",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "simple",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.011905
          }
        ],
        "unique_terms": 31,
        "total_terms": 84
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Essentials",
        "agentic",
        "beginners",
        "data",
        "documentation",
        "https",
        "ingestion",
        "qdrant",
        "rag",
        "tech",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5053846153846153,
      "overall": 0.7017948717948718
    }
  },
  {
    "text": "### Integrations  [Data Management](https://qdrant.tech/documentation/data-management/)  - [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/) - [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/) - [Apache Spark](https://qdrant.tech/documentation/data-management/spark/) - [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/) - [cognee](https://qdrant.tech/documentation/data-management/cognee/) - [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/) - [DLT](https://qdrant.tech/documentation/data-management/dlt/) - [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/) - [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/) - [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)  [Embeddings](https://qdrant.tech/documentation/embeddings/)  - [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/) - [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/) - [Cohere](https://qdrant.tech/documentation/embeddings/cohere/) - [Gemini](https://qdrant.tech/documentation/embeddings/gemini/) - [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/) - [Mistral](https://qdrant.tech/documentation/embeddings/mistral/) - [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/) - [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/) - [Nomic](https://qdrant.tech/documentation/embeddings/nomic/) - [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/) - [Ollama](https://qdrant.tech/documentation/embeddings/ollama/) - [OpenAI](https://qdrant.tech/documentation/embeddings/openai/) - [Prem AI](https://qdrant.tech/documentation/embeddings/premai/) - [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/) - [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/) - [Upstage](https://qdrant.tech/documentation/embeddings/upstage/) - [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)  [Frameworks](https://qdrant.tech/documentation/frameworks/)  - [Autogen](https://qdrant.tech/documentation/frameworks/autogen/) - [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/) - [CamelAI](https://qdrant.tech/documentation/frameworks/camel/) - [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/) - [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/) - [Dagster](https://qdrant.tech/documentation/frameworks/dagster/) - [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/) - [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/) - [Feast](https://qdrant.tech/documentation/frameworks/feast/) - [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/) - [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/) - [Haystack](https://qdrant.tech/documentation/frameworks/haystack/) - [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/) - [Langchain](https://qdrant.tech/documentation/frameworks/langchain/) - [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/) - [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/) - [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/) - [Mastra](https://qdrant.tech/documentation/frameworks/mastra/) - [Mem0](https://qdrant.tech/documentation/frameworks/mem0/) - [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/) - [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/) - [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/) - [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/) - [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/) - [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/) - [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/) - [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/) - [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/) - [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/) - [txtai](https://qdrant.tech/documentation/frameworks/txtai/) - [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/) - [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/) - [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)  [Observability](https://qdrant.tech/documentation/observability/)  - [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/) - [OpenLIT](https://qdrant.tech/documentation/observability/openlit/) - [Datadog](https://qdrant.tech/documentation/observability/datadog/)  [Platforms](https://qdrant.tech/documentation/platforms/)  - [Apify](https://qdrant.tech/documentation/platforms/apify/) - [BuildShip](https://qdrant.tech/documentation/platforms/buildship/) - [Keboola](https://qdrant.tech/documentation/platforms/keboola/) - [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/) - [Make.com](https://qdrant.tech/documentation/platforms/make/) - [N8N](https://qdrant.tech/documentation/platforms/n8n/) - [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/) - [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/) - [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/) - [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/) - [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/) - [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)",
    "metadata": {
      "chunk_id": "3622726ae776-0001",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Integrations"
      ],
      "heading_text": "Integrations",
      "token_count": 1498,
      "char_count": 5661,
      "start_char": 1354,
      "end_char": 7015,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.754907",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1498,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Integrations",
      "chunk_hash": "8a3f2e84e2a75ae9",
      "content_digest": "8a3f2e84e2a75ae9",
      "chunk_length": 5661,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "frameworks",
          "embeddings",
          "platforms",
          "data",
          "management",
          "observability",
          "airbyte",
          "apache",
          "airflow",
          "spark",
          "cocoindex",
          "cognee",
          "confluent",
          "dlt",
          "fluvio",
          "redpanda"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "qdrant",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "tech",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "documentation",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "frameworks",
            "tf": 35,
            "weight": 0.058528
          },
          {
            "term": "embeddings",
            "tf": 21,
            "weight": 0.035117
          },
          {
            "term": "platforms",
            "tf": 14,
            "weight": 0.023411
          },
          {
            "term": "data",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "management",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "observability",
            "tf": 5,
            "weight": 0.008361
          },
          {
            "term": "airbyte",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "apache",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "airflow",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "spark",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cocoindex",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cognee",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "confluent",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "dlt",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "fluvio",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "redpanda",
            "tf": 2,
            "weight": 0.003344
          }
        ],
        "unique_terms": 113,
        "total_terms": 598
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Integrations",
        "data",
        "documentation",
        "embeddings",
        "frameworks",
        "https",
        "management",
        "observability",
        "platforms",
        "qdrant",
        "tech"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "overall": 0.7576923076923077
    }
  },
  {
    "text": "### Examples  [Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)  - [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/) - [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)  [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)  - [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/) - [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/) - [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)  [Build Prototypes](https://qdrant.tech/documentation/examples/)  - [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/) - [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/) - [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/) - [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/) - [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/) - [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/) - [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) - [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/) - [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/) - [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/) - [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/) - [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)  [Practice Datasets](https://qdrant.tech/documentation/datasets/)",
    "metadata": {
      "chunk_id": "3622726ae776-0002",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Examples"
      ],
      "heading_text": "Examples",
      "token_count": 576,
      "char_count": 2347,
      "start_char": 7017,
      "end_char": 9364,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.9076923076923078,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.766445",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 576,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Examples",
      "chunk_hash": "59078d11dd9c9931",
      "content_digest": "59078d11dd9c9931",
      "chunk_length": 2347,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "https",
          "tech",
          "documentation",
          "examples",
          "search",
          "with",
          "rag",
          "data",
          "chatbot",
          "send",
          "system",
          "semantic",
          "and",
          "cohere",
          "precision",
          "reranking",
          "dspy",
          "for",
          "automate"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "tech",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "documentation",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "examples",
            "tf": 14,
            "weight": 0.05
          },
          {
            "term": "search",
            "tf": 10,
            "weight": 0.035714
          },
          {
            "term": "with",
            "tf": 9,
            "weight": 0.032143
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.032143
          },
          {
            "term": "data",
            "tf": 7,
            "weight": 0.025
          },
          {
            "term": "chatbot",
            "tf": 6,
            "weight": 0.021429
          },
          {
            "term": "send",
            "tf": 5,
            "weight": 0.017857
          },
          {
            "term": "system",
            "tf": 5,
            "weight": 0.017857
          },
          {
            "term": "semantic",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "cohere",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "precision",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "reranking",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "dspy",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "automate",
            "tf": 2,
            "weight": 0.007143
          }
        ],
        "unique_terms": 96,
        "total_terms": 280
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Examples",
        "chatbot",
        "data",
        "documentation",
        "examples",
        "https",
        "qdrant",
        "rag",
        "search",
        "tech",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.9076923076923078,
      "overall": 0.8358974358974359
    }
  },
  {
    "text": "### Essentials  [Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)  [Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)  [Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)  [Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)  [Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)  [5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)  [Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)",
    "metadata": {
      "chunk_id": "3622726ae776-0003",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Essentials"
      ],
      "heading_text": "Essentials",
      "token_count": 189,
      "char_count": 673,
      "start_char": 9366,
      "end_char": 10039,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5053846153846153,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.770136",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 189,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Essentials",
      "chunk_hash": "a64044a3995d82e4",
      "content_digest": "a64044a3995d82e4",
      "chunk_length": 673,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "rag",
          "https",
          "tech",
          "documentation",
          "agentic",
          "with",
          "data",
          "ingestion",
          "beginners",
          "langgraph",
          "discord",
          "multimodal",
          "and",
          "deepseek",
          "n8n",
          "essentials",
          "for",
          "simple",
          "system"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.119048
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "tech",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "documentation",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "agentic",
            "tf": 6,
            "weight": 0.071429
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.059524
          },
          {
            "term": "data",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "ingestion",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "beginners",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "langgraph",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "discord",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "multimodal",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "deepseek",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "n8n",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "essentials",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "simple",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.011905
          }
        ],
        "unique_terms": 31,
        "total_terms": 84
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Essentials",
        "agentic",
        "beginners",
        "data",
        "documentation",
        "https",
        "ingestion",
        "qdrant",
        "rag",
        "tech",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5053846153846153,
      "overall": 0.7017948717948718
    }
  },
  {
    "text": "### Integrations  [Data Management](https://qdrant.tech/documentation/data-management/)  - [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/) - [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/) - [Apache Spark](https://qdrant.tech/documentation/data-management/spark/) - [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/) - [cognee](https://qdrant.tech/documentation/data-management/cognee/) - [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/) - [DLT](https://qdrant.tech/documentation/data-management/dlt/) - [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/) - [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/) - [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)  [Embeddings](https://qdrant.tech/documentation/embeddings/)  - [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/) - [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/) - [Cohere](https://qdrant.tech/documentation/embeddings/cohere/) - [Gemini](https://qdrant.tech/documentation/embeddings/gemini/) - [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/) - [Mistral](https://qdrant.tech/documentation/embeddings/mistral/) - [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/) - [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/) - [Nomic](https://qdrant.tech/documentation/embeddings/nomic/) - [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/) - [Ollama](https://qdrant.tech/documentation/embeddings/ollama/) - [OpenAI](https://qdrant.tech/documentation/embeddings/openai/) - [Prem AI](https://qdrant.tech/documentation/embeddings/premai/) - [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/) - [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/) - [Upstage](https://qdrant.tech/documentation/embeddings/upstage/) - [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)  [Frameworks](https://qdrant.tech/documentation/frameworks/)  - [Autogen](https://qdrant.tech/documentation/frameworks/autogen/) - [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/) - [CamelAI](https://qdrant.tech/documentation/frameworks/camel/) - [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/) - [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/) - [Dagster](https://qdrant.tech/documentation/frameworks/dagster/) - [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/) - [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/) - [Feast](https://qdrant.tech/documentation/frameworks/feast/) - [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/) - [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/) - [Haystack](https://qdrant.tech/documentation/frameworks/haystack/) - [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/) - [Langchain](https://qdrant.tech/documentation/frameworks/langchain/) - [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/) - [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/) - [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/) - [Mastra](https://qdrant.tech/documentation/frameworks/mastra/) - [Mem0](https://qdrant.tech/documentation/frameworks/mem0/) - [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/) - [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/) - [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/) - [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/) - [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/) - [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/) - [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/) - [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/) - [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/) - [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/) - [txtai](https://qdrant.tech/documentation/frameworks/txtai/) - [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/) - [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/) - [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)  [Observability](https://qdrant.tech/documentation/observability/)  - [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/) - [OpenLIT](https://qdrant.tech/documentation/observability/openlit/) - [Datadog](https://qdrant.tech/documentation/observability/datadog/)  [Platforms](https://qdrant.tech/documentation/platforms/)  - [Apify](https://qdrant.tech/documentation/platforms/apify/) - [BuildShip](https://qdrant.tech/documentation/platforms/buildship/) - [Keboola](https://qdrant.tech/documentation/platforms/keboola/) - [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/) - [Make.com](https://qdrant.tech/documentation/platforms/make/) - [N8N](https://qdrant.tech/documentation/platforms/n8n/) - [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/) - [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/) - [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/) - [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/) - [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/) - [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)",
    "metadata": {
      "chunk_id": "3622726ae776-0004",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Integrations"
      ],
      "heading_text": "Integrations",
      "token_count": 1498,
      "char_count": 5661,
      "start_char": 10041,
      "end_char": 15702,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.802241",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1498,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Integrations",
      "chunk_hash": "8a3f2e84e2a75ae9",
      "content_digest": "8a3f2e84e2a75ae9",
      "chunk_length": 5661,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "frameworks",
          "embeddings",
          "platforms",
          "data",
          "management",
          "observability",
          "airbyte",
          "apache",
          "airflow",
          "spark",
          "cocoindex",
          "cognee",
          "confluent",
          "dlt",
          "fluvio",
          "redpanda"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "qdrant",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "tech",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "documentation",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "frameworks",
            "tf": 35,
            "weight": 0.058528
          },
          {
            "term": "embeddings",
            "tf": 21,
            "weight": 0.035117
          },
          {
            "term": "platforms",
            "tf": 14,
            "weight": 0.023411
          },
          {
            "term": "data",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "management",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "observability",
            "tf": 5,
            "weight": 0.008361
          },
          {
            "term": "airbyte",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "apache",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "airflow",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "spark",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cocoindex",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cognee",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "confluent",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "dlt",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "fluvio",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "redpanda",
            "tf": 2,
            "weight": 0.003344
          }
        ],
        "unique_terms": 113,
        "total_terms": 598
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Integrations",
        "data",
        "documentation",
        "embeddings",
        "frameworks",
        "https",
        "management",
        "observability",
        "platforms",
        "qdrant",
        "tech"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "overall": 0.7576923076923077
    }
  },
  {
    "text": "### Examples  [Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)  - [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/) - [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)  [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)  - [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/) - [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/) - [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)  [Build Prototypes](https://qdrant.tech/documentation/examples/)  - [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/) - [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/) - [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/) - [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/) - [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/) - [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/) - [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) - [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/) - [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/) - [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/) - [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/) - [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)  [Practice Datasets](https://qdrant.tech/documentation/datasets/)  - [Documentation](https://qdrant.tech/documentation/) - - [Search precision](https://qdrant.tech/documentation/search-precision/) - - Reranking in Semantic Search",
    "metadata": {
      "chunk_id": "3622726ae776-0005",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Examples"
      ],
      "heading_text": "Examples",
      "token_count": 614,
      "char_count": 2511,
      "start_char": 15704,
      "end_char": 18215,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8976744186046512,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.811225",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 614,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Examples",
      "chunk_hash": "ec8b68cbb25d8fef",
      "content_digest": "ec8b68cbb25d8fef",
      "chunk_length": 2511,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "documentation",
          "https",
          "tech",
          "examples",
          "search",
          "with",
          "rag",
          "data",
          "chatbot",
          "precision",
          "semantic",
          "send",
          "system",
          "reranking",
          "and",
          "cohere",
          "dspy",
          "for",
          "automate"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 32,
            "weight": 0.108108
          },
          {
            "term": "documentation",
            "tf": 24,
            "weight": 0.081081
          },
          {
            "term": "https",
            "tf": 23,
            "weight": 0.077703
          },
          {
            "term": "tech",
            "tf": 23,
            "weight": 0.077703
          },
          {
            "term": "examples",
            "tf": 14,
            "weight": 0.047297
          },
          {
            "term": "search",
            "tf": 13,
            "weight": 0.043919
          },
          {
            "term": "with",
            "tf": 9,
            "weight": 0.030405
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.030405
          },
          {
            "term": "data",
            "tf": 7,
            "weight": 0.023649
          },
          {
            "term": "chatbot",
            "tf": 6,
            "weight": 0.02027
          },
          {
            "term": "precision",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "semantic",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "send",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "system",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "reranking",
            "tf": 4,
            "weight": 0.013514
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.013514
          },
          {
            "term": "cohere",
            "tf": 4,
            "weight": 0.013514
          },
          {
            "term": "dspy",
            "tf": 3,
            "weight": 0.010135
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.010135
          },
          {
            "term": "automate",
            "tf": 2,
            "weight": 0.006757
          }
        ],
        "unique_terms": 96,
        "total_terms": 296
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Examples",
        "chatbot",
        "data",
        "documentation",
        "examples",
        "https",
        "qdrant",
        "rag",
        "search",
        "tech",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8976744186046512,
      "overall": 0.8325581395348838
    }
  },
  {
    "text": "# Reranking in RAG with Qdrant Vector Database\n\nIn Retrieval-Augmented Generation (RAG) systems, irrelevant or missing information can throw off your model’s ability to produce accurate, meaningful outputs. One of the best ways to ensure you’re feeding your language model the most relevant, context-rich documents is through reranking. It’s a game-changer.\n\nIn this guide, we’ll dive into using reranking to boost the relevance of search results in Qdrant. We’ll start with an easy use case that leverages the Cohere Rerank model. Then, we’ll take it up a notch by exploring ColBERT for a more advanced approach. By the time you’re done, you’ll know how to implement [hybrid search](https://qdrant.tech/articles/hybrid-search/), fine-tune reranking models, and significantly improve your accuracy.\n\nReady? Let’s jump in.",
    "metadata": {
      "chunk_id": "3622726ae776-0006",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Reranking in RAG with Qdrant Vector Database"
      ],
      "heading_text": "Reranking in RAG with Qdrant Vector Database",
      "token_count": 187,
      "char_count": 821,
      "start_char": 18217,
      "end_char": 19038,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.735,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.812254",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 187,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Reranking in RAG with Qdrant Vector Database",
      "chunk_hash": "87973cb07923413b",
      "content_digest": "87973cb07923413b",
      "chunk_length": 821,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "reranking",
          "qdrant",
          "your",
          "model",
          "you",
          "search",
          "rag",
          "with",
          "hybrid",
          "vector",
          "database",
          "retrieval",
          "augmented",
          "generation",
          "systems",
          "irrelevant",
          "missing",
          "information",
          "can"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 5,
            "weight": 0.048077
          },
          {
            "term": "reranking",
            "tf": 4,
            "weight": 0.038462
          },
          {
            "term": "qdrant",
            "tf": 3,
            "weight": 0.028846
          },
          {
            "term": "your",
            "tf": 3,
            "weight": 0.028846
          },
          {
            "term": "model",
            "tf": 3,
            "weight": 0.028846
          },
          {
            "term": "you",
            "tf": 3,
            "weight": 0.028846
          },
          {
            "term": "search",
            "tf": 3,
            "weight": 0.028846
          },
          {
            "term": "rag",
            "tf": 2,
            "weight": 0.019231
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.019231
          },
          {
            "term": "hybrid",
            "tf": 2,
            "weight": 0.019231
          },
          {
            "term": "vector",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "database",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "retrieval",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "augmented",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "generation",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "systems",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "irrelevant",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "missing",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "information",
            "tf": 1,
            "weight": 0.009615
          },
          {
            "term": "can",
            "tf": 1,
            "weight": 0.009615
          }
        ],
        "unique_terms": 84,
        "total_terms": 104
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Reranking in RAG with Qdrant Vector Database",
        "hybrid",
        "model",
        "qdrant",
        "rag",
        "reranking",
        "search",
        "the",
        "with",
        "you",
        "your"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.735,
      "overall": 0.7783333333333333
    }
  },
  {
    "text": "## Background\n\nIn search systems, two metrics—precision and recall—are the backbone of success. But what do they mean? Precision tells us how many of the retrieved results are actually relevant, while recall measures how well we’ve captured all the relevant results out there. Simply put:\n\nSparse vector searches usually give you high precision because they’re great at finding exact matches. But, here’s the catch—your recall can suffer when relevant documents don’t contain those exact keywords. On the flip side, dense vector searches are fantastic for recall since they grasp the broader, semantic meaning of your query. However, this can lead to lower precision, where you might see results that are only loosely related.\n\nThis is exactly where reranking comes to the rescue. It takes a wide net of documents (giving you high recall) and then refines them by reordering the top candidates based on their relevance scores—boosting precision without losing that broad understanding. Typically, we retain only the top K candidates after reordering to focus on the most relevant results.",
    "metadata": {
      "chunk_id": "3622726ae776-0008",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Background"
      ],
      "heading_text": "Background",
      "token_count": 209,
      "char_count": 1088,
      "start_char": 19194,
      "end_char": 20282,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5123529411764706,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.812898",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 209,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Background",
      "chunk_hash": "8d1894b678505893",
      "content_digest": "8d1894b678505893",
      "chunk_length": 1088,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "precision",
          "recall",
          "are",
          "results",
          "relevant",
          "they",
          "you",
          "and",
          "but",
          "how",
          "vector",
          "searches",
          "high",
          "exact",
          "your",
          "can",
          "documents",
          "this",
          "where"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 10,
            "weight": 0.065789
          },
          {
            "term": "precision",
            "tf": 5,
            "weight": 0.032895
          },
          {
            "term": "recall",
            "tf": 5,
            "weight": 0.032895
          },
          {
            "term": "are",
            "tf": 4,
            "weight": 0.026316
          },
          {
            "term": "results",
            "tf": 4,
            "weight": 0.026316
          },
          {
            "term": "relevant",
            "tf": 4,
            "weight": 0.026316
          },
          {
            "term": "they",
            "tf": 3,
            "weight": 0.019737
          },
          {
            "term": "you",
            "tf": 3,
            "weight": 0.019737
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "but",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "how",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "vector",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "searches",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "high",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "exact",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "your",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "can",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "documents",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "this",
            "tf": 2,
            "weight": 0.013158
          },
          {
            "term": "where",
            "tf": 2,
            "weight": 0.013158
          }
        ],
        "unique_terms": 105,
        "total_terms": 152
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Background",
        "and",
        "are",
        "but",
        "precision",
        "recall",
        "relevant",
        "results",
        "the",
        "they",
        "you"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5123529411764706,
      "overall": 0.7041176470588235
    }
  },
  {
    "text": "## Working\n\nPicture this: You walk into a massive library and ask for a book on “climate change.” The librarian pulls out a dozen books for you—some are scientific papers, others are personal essays, and one’s even a novel. Sure, they’re all relevant, but the first one you get handed is the novel. Not exactly what you were hoping for, right?\n\nNow, imagine a smarter, more intuitive librarian who really gets what you’re after. This one knows exactly which books are most impactful, the most current, and perfectly aligned with what you need. That’s what reranking does for your search results—it doesn’t just grab any relevant document; it smartly reorders them so the best ones land at the top of your list. It’s like having a librarian who knows exactly what you’re looking for before you do!\n\nAn illustration of the rerank model prioritizing better results\n\nTo become that smart, intuitive librarian, your algorithm needs to learn how to understand both your queries and the documents it retrieves. It has to evaluate the relationship between them effectively, so it can give you exactly what you’re looking for.\n\nThe way reranker models operate varies based on their type, which will be discussed later, but in general, they calculate a relevance score for each document-query pair.Unlike embedding models, which squash everything into a single vector upfront, rerankers keep all the important details intact by using the full transformer output to calculate a similarity score. The result? Precision. But, there’s a trade-off—reranking can be slow. Processing millions of documents can take hours, which is why rerankers focus on refining results, not searching through the entire document collection.\n\nRerankers come in different types, each with its own strengths. Let’s break them down:\n\n1. **Cross Encoder Models**: These boost reranking by using a classification system to evaluate pairs of data—like sentences or documents. They spit out a similarity score from 0 to 1, showing how closely the document matches your query. The catch? Cross-encoders need both query and document, so they can’t handle standalone documents or queries by themselves.\n2. **Multi-Vector Rerankers (e.g., ColBERT)**: These models take a more efficient route. They encode your query and the documents separately and only compare them later, reducing the computational load. This means document representations can be precomputed, speeding up retrieval times\n3. **Large Language Models (LLMs) as Rerankers**: This is a newer, smarter way to rerank. LLMs, like GPT, are getting better by the day. With the right instructions, they can prioritize the most relevant documents for you, leveraging their massive understanding of language to deliver even more accurate results.\n\nEach of these rerankers has its own special way of making sure you get the best search results, fast and relevant to what you need.",
    "metadata": {
      "chunk_id": "3622726ae776-0009",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Working"
      ],
      "heading_text": "Working",
      "token_count": 596,
      "char_count": 2892,
      "start_char": 20284,
      "end_char": 23176,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8788209606986901,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.814221",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 596,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Working",
      "chunk_hash": "540e401af8f79c78",
      "content_digest": "540e401af8f79c78",
      "chunk_length": 2892,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "you",
          "and",
          "for",
          "what",
          "they",
          "your",
          "document",
          "documents",
          "can",
          "rerankers",
          "results",
          "models",
          "this",
          "librarian",
          "are",
          "relevant",
          "exactly",
          "which",
          "them"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 22,
            "weight": 0.055276
          },
          {
            "term": "you",
            "tf": 13,
            "weight": 0.032663
          },
          {
            "term": "and",
            "tf": 8,
            "weight": 0.020101
          },
          {
            "term": "for",
            "tf": 8,
            "weight": 0.020101
          },
          {
            "term": "what",
            "tf": 7,
            "weight": 0.017588
          },
          {
            "term": "they",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "your",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "document",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "documents",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "can",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "rerankers",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "results",
            "tf": 5,
            "weight": 0.012563
          },
          {
            "term": "models",
            "tf": 5,
            "weight": 0.012563
          },
          {
            "term": "this",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "librarian",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "are",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "relevant",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "exactly",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "which",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "them",
            "tf": 4,
            "weight": 0.01005
          }
        ],
        "unique_terms": 221,
        "total_terms": 398
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Working",
        "and",
        "can",
        "document",
        "documents",
        "for",
        "the",
        "they",
        "what",
        "you",
        "your"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8788209606986901,
      "overall": 0.8262736535662301
    }
  },
  {
    "text": "# Implementing Vector Search with Reranking\n\nIn this section, you’re going to see how to implement vector search with reranking using Cohere. But first, let’s break it down.",
    "metadata": {
      "chunk_id": "3622726ae776-0011",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Implementing Vector Search with Reranking"
      ],
      "heading_text": "Implementing Vector Search with Reranking",
      "token_count": 40,
      "char_count": 173,
      "start_char": 24534,
      "end_char": 24707,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5364285714285714,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.815276",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 40,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Implementing Vector Search with Reranking",
      "chunk_hash": "54896e2233d9367c",
      "content_digest": "54896e2233d9367c",
      "chunk_length": 173,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "vector",
          "search",
          "with",
          "reranking",
          "implementing",
          "this",
          "section",
          "you",
          "going",
          "see",
          "how",
          "implement",
          "using",
          "cohere",
          "but",
          "first",
          "let",
          "break",
          "down"
        ],
        "term_weights": [
          {
            "term": "vector",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "reranking",
            "tf": 2,
            "weight": 0.086957
          },
          {
            "term": "implementing",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "section",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "you",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "going",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "see",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "how",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "implement",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "using",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "cohere",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "but",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "first",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "let",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "break",
            "tf": 1,
            "weight": 0.043478
          },
          {
            "term": "down",
            "tf": 1,
            "weight": 0.043478
          }
        ],
        "unique_terms": 19,
        "total_terms": 23
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Implementing Vector Search with Reranking",
        "going",
        "implementing",
        "reranking",
        "search",
        "section",
        "see",
        "this",
        "vector",
        "with",
        "you"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5364285714285714,
      "overall": 0.7121428571428571
    }
  },
  {
    "text": "### Ingestion Stage  - **Documents:** This is where it all starts. The system takes in raw data or documents that need to be prepped for search—this is your initial input. - **Embeddings:** Next, these documents are transformed into sparse or dense [embeddings](https://qdrant.tech/documentation/embeddings/), which are basically vector representations. These vectors capture the deep, underlying meaning of the text, allowing your system to perform smart, efficient searches and comparisons based on semantic meaning - **Vector Database:** Once your documents are converted into these embeddings, they get stored in a vector database—essentially the powerhouse behind fast, accurate similarity searches. Here, we’ll see the capabilities of the Qdrant vector database.",
    "metadata": {
      "chunk_id": "3622726ae776-0013",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Ingestion Stage"
      ],
      "heading_text": "Ingestion Stage",
      "token_count": 151,
      "char_count": 768,
      "start_char": 25162,
      "end_char": 25930,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.519245283018868,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.815860",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 151,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Ingestion Stage",
      "chunk_hash": "aa7cfdccbc39b010",
      "content_digest": "aa7cfdccbc39b010",
      "chunk_length": 768,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "documents",
          "embeddings",
          "vector",
          "your",
          "these",
          "are",
          "database",
          "this",
          "system",
          "into",
          "qdrant",
          "meaning",
          "searches",
          "ingestion",
          "stage",
          "where",
          "all",
          "starts",
          "takes"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 6,
            "weight": 0.06383
          },
          {
            "term": "documents",
            "tf": 4,
            "weight": 0.042553
          },
          {
            "term": "embeddings",
            "tf": 4,
            "weight": 0.042553
          },
          {
            "term": "vector",
            "tf": 4,
            "weight": 0.042553
          },
          {
            "term": "your",
            "tf": 3,
            "weight": 0.031915
          },
          {
            "term": "these",
            "tf": 3,
            "weight": 0.031915
          },
          {
            "term": "are",
            "tf": 3,
            "weight": 0.031915
          },
          {
            "term": "database",
            "tf": 3,
            "weight": 0.031915
          },
          {
            "term": "this",
            "tf": 2,
            "weight": 0.021277
          },
          {
            "term": "system",
            "tf": 2,
            "weight": 0.021277
          },
          {
            "term": "into",
            "tf": 2,
            "weight": 0.021277
          },
          {
            "term": "qdrant",
            "tf": 2,
            "weight": 0.021277
          },
          {
            "term": "meaning",
            "tf": 2,
            "weight": 0.021277
          },
          {
            "term": "searches",
            "tf": 2,
            "weight": 0.021277
          },
          {
            "term": "ingestion",
            "tf": 1,
            "weight": 0.010638
          },
          {
            "term": "stage",
            "tf": 1,
            "weight": 0.010638
          },
          {
            "term": "where",
            "tf": 1,
            "weight": 0.010638
          },
          {
            "term": "all",
            "tf": 1,
            "weight": 0.010638
          },
          {
            "term": "starts",
            "tf": 1,
            "weight": 0.010638
          },
          {
            "term": "takes",
            "tf": 1,
            "weight": 0.010638
          }
        ],
        "unique_terms": 66,
        "total_terms": 94
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Ingestion Stage",
        "are",
        "database",
        "documents",
        "embeddings",
        "system",
        "the",
        "these",
        "this",
        "vector",
        "your"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.519245283018868,
      "overall": 0.7397484276729559
    }
  },
  {
    "text": "### Retrieval Stage  - **User’s Query:** Now we enter the retrieval phase. The user submits a query, and it’s time to match that query against the stored documents. - **Embeddings:** Just like with the documents, the user’s query is converted into a sparse or dense embedding. This enables the system to compare the query’s meaning with the meanings of the stored documents. - **Vector Search:** The system searches for the most relevant documents by comparing the query’s embedding to those in the vector database, and it pulls up the closest matches. - **Rerank:** Once the initial results are in, the reranking process kicks in to ensure you get the best results on top. We’ll be using **Cohere’s** rerank-english-v3.0 model, which excels at reordering English language documents to prioritize relevance. It can handle up to 4096 tokens, giving it plenty of context to work with. And if you’re dealing with multi-lingual data, don’t worry—Cohere’s got reranking models for other languages too.",
    "metadata": {
      "chunk_id": "3622726ae776-0014",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Retrieval Stage"
      ],
      "heading_text": "Retrieval Stage",
      "token_count": 223,
      "char_count": 996,
      "start_char": 25932,
      "end_char": 26928,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.505625,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.816899",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 223,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Retrieval Stage",
      "chunk_hash": "0f9ab09a85368c2a",
      "content_digest": "0f9ab09a85368c2a",
      "chunk_length": 996,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "query",
          "documents",
          "with",
          "user",
          "and",
          "retrieval",
          "stored",
          "embedding",
          "system",
          "vector",
          "for",
          "rerank",
          "results",
          "reranking",
          "you",
          "cohere",
          "english",
          "stage",
          "now"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 17,
            "weight": 0.131783
          },
          {
            "term": "query",
            "tf": 6,
            "weight": 0.046512
          },
          {
            "term": "documents",
            "tf": 5,
            "weight": 0.03876
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.031008
          },
          {
            "term": "user",
            "tf": 3,
            "weight": 0.023256
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.023256
          },
          {
            "term": "retrieval",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "stored",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "system",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "vector",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "rerank",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "results",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "reranking",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "you",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "cohere",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "english",
            "tf": 2,
            "weight": 0.015504
          },
          {
            "term": "stage",
            "tf": 1,
            "weight": 0.007752
          },
          {
            "term": "now",
            "tf": 1,
            "weight": 0.007752
          }
        ],
        "unique_terms": 85,
        "total_terms": 129
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Retrieval Stage",
        "and",
        "documents",
        "embedding",
        "query",
        "retrieval",
        "stored",
        "system",
        "the",
        "user",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.505625,
      "overall": 0.7018749999999999
    }
  },
  {
    "text": "## Implementation\n\nNow it’s time to dive into the actual implementation.",
    "metadata": {
      "chunk_id": "3622726ae776-0015",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Implementation"
      ],
      "heading_text": "Implementation",
      "token_count": 14,
      "char_count": 72,
      "start_char": 26930,
      "end_char": 27002,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.817563",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 14,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Implementation",
      "chunk_hash": "b04417c60bddad5e",
      "content_digest": "b04417c60bddad5e",
      "chunk_length": 72,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "implementation",
          "now",
          "time",
          "dive",
          "into",
          "the",
          "actual"
        ],
        "term_weights": [
          {
            "term": "implementation",
            "tf": 2,
            "weight": 0.25
          },
          {
            "term": "now",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "time",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "dive",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "into",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "actual",
            "tf": 1,
            "weight": 0.125
          }
        ],
        "unique_terms": 7,
        "total_terms": 8
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Implementation",
        "actual",
        "dive",
        "implementation",
        "into",
        "now",
        "the",
        "time"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### Setup  To follow along with this tutorial, you’ll need a few key tools::  - Python Client for Qdrant - Cohere  Let’s install everything you need in one go using the Python package manager:: ```jsx pip install qdrant-client cohere ``` ---  Now, let’s bring in all the necessary components in one tidy block: ```jsx from qdrant_client import QdrantClient from qdrant_client.models import Distance, VectorParams, PointStruct import cohere ``` ---  Qdrant is a powerful vector similarity search engine that gives you a production-ready service with an easy-to-use API for storing, searching, and managing data. You can interact with Qdrant through a local or cloud setup, but since we’re working in Colab, let’s go with the cloud setup.",
    "metadata": {
      "chunk_id": "3622726ae776-0016",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setup"
      ],
      "heading_text": "Setup",
      "token_count": 168,
      "char_count": 736,
      "start_char": 27004,
      "end_char": 27740,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7039130434782609,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.820635",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 168,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Setup",
      "chunk_hash": "3e597f8e9738ca0e",
      "content_digest": "3e597f8e9738ca0e",
      "chunk_length": 736,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "with",
          "you",
          "client",
          "setup",
          "cohere",
          "let",
          "the",
          "import",
          "need",
          "python",
          "for",
          "install",
          "one",
          "jsx",
          "from",
          "cloud",
          "follow",
          "along",
          "this"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 6,
            "weight": 0.060606
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.040404
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.040404
          },
          {
            "term": "client",
            "tf": 4,
            "weight": 0.040404
          },
          {
            "term": "setup",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "cohere",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "let",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "the",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "import",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "need",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "python",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "install",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "one",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "jsx",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "from",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "cloud",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "follow",
            "tf": 1,
            "weight": 0.010101
          },
          {
            "term": "along",
            "tf": 1,
            "weight": 0.010101
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.010101
          }
        ],
        "unique_terms": 67,
        "total_terms": 99
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setup",
        "client",
        "cohere",
        "import",
        "let",
        "need",
        "qdrant",
        "setup",
        "the",
        "with",
        "you"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7039130434782609,
      "overall": 0.801304347826087
    }
  },
  {
    "text": "### Creating a Collection  A collection is basically a named group of points (vectors with data) that you can search through. All the vectors in a collection need to have the same size and be compared using one distance metric. Here’s how to create one: ```jsx client.create_collection(     collection_name=\"basic-search-rerank\",     vectors_config=VectorParams(size=1024, distance=Distance.DOT), ) ``` ---  Here, the vector size is set to 1024 to match our dense embeddings, and we’re using dot product as the distance metric—perfect for capturing the similarity between vectors, especially when they’re normalized.",
    "metadata": {
      "chunk_id": "3622726ae776-0020",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Creating a Collection"
      ],
      "heading_text": "Creating a Collection",
      "token_count": 132,
      "char_count": 616,
      "start_char": 28991,
      "end_char": 29607,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.53,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.827629",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 132,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Creating a Collection",
      "chunk_hash": "015c32ee3c3c5b18",
      "content_digest": "015c32ee3c3c5b18",
      "chunk_length": 616,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "collection",
          "the",
          "vectors",
          "distance",
          "size",
          "search",
          "and",
          "using",
          "one",
          "metric",
          "here",
          "create",
          "1024",
          "dot",
          "creating",
          "basically",
          "named",
          "group",
          "points",
          "with"
        ],
        "term_weights": [
          {
            "term": "collection",
            "tf": 5,
            "weight": 0.063291
          },
          {
            "term": "the",
            "tf": 5,
            "weight": 0.063291
          },
          {
            "term": "vectors",
            "tf": 4,
            "weight": 0.050633
          },
          {
            "term": "distance",
            "tf": 4,
            "weight": 0.050633
          },
          {
            "term": "size",
            "tf": 3,
            "weight": 0.037975
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "using",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "one",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "metric",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "here",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "create",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "1024",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "dot",
            "tf": 2,
            "weight": 0.025316
          },
          {
            "term": "creating",
            "tf": 1,
            "weight": 0.012658
          },
          {
            "term": "basically",
            "tf": 1,
            "weight": 0.012658
          },
          {
            "term": "named",
            "tf": 1,
            "weight": 0.012658
          },
          {
            "term": "group",
            "tf": 1,
            "weight": 0.012658
          },
          {
            "term": "points",
            "tf": 1,
            "weight": 0.012658
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.012658
          }
        ],
        "unique_terms": 54,
        "total_terms": 79
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Creating a Collection",
        "and",
        "collection",
        "distance",
        "metric",
        "one",
        "search",
        "size",
        "the",
        "using",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.53,
      "overall": 0.7433333333333332
    }
  },
  {
    "text": "### Documents to Embeddings  Let’s set up some example data. Here’s a query and a few documents for demonstration: ```jsx query = \"What is the purpose of feature scaling in machine learning?\"  documents = [     \"In machine learning, feature scaling is the process of normalizing the range of independent variables or features. The goal is to ensure that all features contribute equally to the model, especially in algorithms like SVM or k-nearest neighbors where distance calculations matter.\",         \"Feature scaling is commonly used in data preprocessing to ensure that features are on the same scale. This is particularly important for gradient descent-based algorithms where features with larger scales could disproportionately impact the cost function.\",         \"In data science, feature extraction is the process of transforming raw data into a set of engineered features that can be used in predictive models. Feature scaling is related but focuses on adjusting the values of these features.\",         \"Unsupervised learning algorithms, such as clustering methods, may benefit from feature scaling as it ensures that features with larger numerical ranges don't dominate the learning process.\",         \"One common data preprocessing technique in data science is feature selection. Unlike feature scaling, feature selection aims to reduce the number of input variables used in a model to avoid overfitting.\",         \"Principal component analysis (PCA) is a dimensionality reduction technique used in data science to reduce the number of variables. PCA works best when data is scaled, as it relies on variance which can be skewed by features on different scales.\",         \"Min-max scaling is a common feature scaling technique that usually transforms features to a fixed range [0, 1]. This method is useful when the distribution of data is not Gaussian.\",         \"Standardization, or z-score normalization, is another technique that transforms features into a mean of 0 and a standard deviation of 1. This method is effective for data that follows a normal distribution.\",         \"Feature scaling is critical when using algorithms that rely on distances, such as k-means clustering, as unscaled features can lead to misleading results.\",         \"Scaling can improve the convergence speed of gradient descent algorithms by preventing issues with different feature scales affecting the cost function's landscape.\",         \"In deep learning, feature scaling helps in stabilizing the learning process, allowing for better performance and faster convergence during training.\",         \"Robust scaling is another method that uses the median and the interquartile range to scale features, making it less sensitive to outliers.\",         \"When working with time series data, feature scaling can help in standardizing the input data, improving model performance across different periods.\",         \"Normalization is often used in image processing to scale pixel values to a range that enhances model performance in computer vision tasks.\",         \"Feature scaling is significant when features have different units of measurement, such as height in centimeters and weight in kilograms.\",         \"In recommendation systems, scaling features such as user ratings can improve the model's ability to find similar users or items.\",         \"Dimensionality reduction techniques, like t-SNE and UMAP, often require feature scaling to visualize high-dimensional data in lower dimensions effectively.\",         \"Outlier detection techniques can also benefit from feature scaling, as they can be influenced by unscaled features that have extreme values.\",         \"Data preprocessing steps, including feature scaling, can significantly impact the performance of machine learning models, making it a crucial part of the modeling pipeline.\",         \"In ensemble methods, like random forests, feature scaling is not strictly necessary, but it can still enhance interpretability and comparison of feature importance.\",         \"Feature scaling should be applied consistently across training and test datasets to avoid data leakage and ensure reliable model evaluation.\",         \"In natural language processing (NLP), scaling can be useful when working with numerical features derived from text data, such as word counts or term frequencies.\",         \"Log transformation is a technique that can be applied to skewed data to stabilize variance and make the data more suitable for scaling.\",         \"Data augmentation techniques in machine learning may also include scaling to ensure consistency across training datasets, especially in computer vision tasks.\" ] ``` ---  We’ll generate embeddings for these documents using Cohere’s embed-english-v3.0 model, which produces 1024-dimensional vectors: ```python model=\"embed-english-v3.0\"  doc_embeddings = co.embed(texts=documents,                           model=model,                           input_type=\"search_document\",                           embedding_types=['float']) ``` ---  This code taps into the power of the Cohere API to generate embeddings for your list of documents. It uses the embed-english-v3.0 model, sets the input type to “search\\_document,” and asks for the embeddings in float format. The result? A set of dense embeddings, each one representing the deep semantic meaning of your documents. These embeddings will be stored in doc\\_embeddings, ready for action.",
    "metadata": {
      "chunk_id": "3622726ae776-0021",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Documents to Embeddings"
      ],
      "heading_text": "Documents to Embeddings",
      "token_count": 978,
      "char_count": 5430,
      "start_char": 29611,
      "end_char": 35041,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8441767068273094,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.832355",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 978,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Documents to Embeddings",
      "chunk_hash": "e429520249125a2d",
      "content_digest": "e429520249125a2d",
      "chunk_length": 5430,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "scaling",
          "feature",
          "data",
          "features",
          "that",
          "can",
          "and",
          "model",
          "for",
          "embeddings",
          "learning",
          "documents",
          "when",
          "algorithms",
          "used",
          "with",
          "such",
          "technique",
          "machine"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 30,
            "weight": 0.04878
          },
          {
            "term": "scaling",
            "tf": 23,
            "weight": 0.037398
          },
          {
            "term": "feature",
            "tf": 21,
            "weight": 0.034146
          },
          {
            "term": "data",
            "tf": 19,
            "weight": 0.030894
          },
          {
            "term": "features",
            "tf": 16,
            "weight": 0.026016
          },
          {
            "term": "that",
            "tf": 12,
            "weight": 0.019512
          },
          {
            "term": "can",
            "tf": 12,
            "weight": 0.019512
          },
          {
            "term": "and",
            "tf": 11,
            "weight": 0.017886
          },
          {
            "term": "model",
            "tf": 11,
            "weight": 0.017886
          },
          {
            "term": "for",
            "tf": 9,
            "weight": 0.014634
          },
          {
            "term": "embeddings",
            "tf": 8,
            "weight": 0.013008
          },
          {
            "term": "learning",
            "tf": 8,
            "weight": 0.013008
          },
          {
            "term": "documents",
            "tf": 7,
            "weight": 0.011382
          },
          {
            "term": "when",
            "tf": 6,
            "weight": 0.009756
          },
          {
            "term": "algorithms",
            "tf": 5,
            "weight": 0.00813
          },
          {
            "term": "used",
            "tf": 5,
            "weight": 0.00813
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.00813
          },
          {
            "term": "such",
            "tf": 5,
            "weight": 0.00813
          },
          {
            "term": "technique",
            "tf": 5,
            "weight": 0.00813
          },
          {
            "term": "machine",
            "tf": 4,
            "weight": 0.006504
          }
        ],
        "unique_terms": 297,
        "total_terms": 615
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Documents to Embeddings",
        "and",
        "can",
        "data",
        "feature",
        "features",
        "for",
        "model",
        "scaling",
        "that",
        "the"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8441767068273094,
      "overall": 0.8480589022757696
    }
  },
  {
    "text": "### Retrieval  The first few steps here mirror what we did during ingestion—just like before, we need to convert the query into an embedding: ```python query_embeddings = co.embed(texts=[query],                           model=model,                           input_type=\"search_query\",                           embedding_types=['float']) ``` ---  After that, we’ll move on to retrieve results using vector search and apply reranking on the results. This two-stage process is super efficient because we’re grabbing a small set of the most relevant documents first, which is much faster than reranking a huge dataset.",
    "metadata": {
      "chunk_id": "3622726ae776-0024",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Retrieval"
      ],
      "heading_text": "Retrieval",
      "token_count": 115,
      "char_count": 617,
      "start_char": 36421,
      "end_char": 37038,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7549350649350649,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.842863",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 115,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Retrieval",
      "chunk_hash": "5d6ff268f20a6452",
      "content_digest": "5d6ff268f20a6452",
      "chunk_length": 617,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "query",
          "first",
          "embedding",
          "model",
          "search",
          "results",
          "reranking",
          "retrieval",
          "few",
          "steps",
          "here",
          "mirror",
          "what",
          "did",
          "during",
          "ingestion",
          "just",
          "like",
          "before"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 4,
            "weight": 0.057143
          },
          {
            "term": "query",
            "tf": 4,
            "weight": 0.057143
          },
          {
            "term": "first",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "model",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "results",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "reranking",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "retrieval",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "few",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "steps",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "here",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "mirror",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "what",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "did",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "during",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "ingestion",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "just",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "like",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "before",
            "tf": 1,
            "weight": 0.014286
          }
        ],
        "unique_terms": 58,
        "total_terms": 70
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Retrieval",
        "embedding",
        "few",
        "first",
        "model",
        "query",
        "reranking",
        "results",
        "retrieval",
        "search",
        "the"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7549350649350649,
      "overall": 0.8183116883116882
    }
  },
  {
    "text": "### Vector Search  This snippet grabs the top 10 most relevant points from your Qdrant collection using the query embedding. ```python search_result = client.query_points(     collection_name=\"basic-search-rerank\", query=query_embeddings.embeddings.float_[0], limit=10 ).points ``` ---  Here’s how it works: we use the query\\_points method to search within the “basic-search-rerank” collection. It compares the query embedding (the first embedding in query\\_embeddings) against all the document embeddings, pulling up the 10 closest matches. The matching points get stored in search\\_result. And here’s a sneak peek at what you’ll get from the vector search:  | **ID** | **Document**                                                                                   | **Score** | | ------ | ---------------------------------------------------------------------------------------------- | --------- | | 0      | In machine learning, feature scaling is the process of normalizing the range of independent…   | 0.71      | | 10     | In deep learning, feature scaling helps stabilize the learning process, allowing for…          | 0.69      | | 1      | Feature scaling is commonly used in data preprocessing to ensure that features are on the…     | 0.68      | | 23     | Data augmentation techniques in machine learning may also include scaling to ensure…           | 0.64      | | 3      | Unsupervised learning algorithms, such as clustering methods, may benefit from feature…        | 0.64      | | 12     | When working with time series data, feature scaling can help standardize the input…            | 0.62      | | 19     | In ensemble methods, like random forests, feature scaling is not strictly necessary…           | 0.61      | | 21     | In natural language processing (NLP), scaling can be useful when working with numerical…       | 0.61      | | 20     | Feature scaling should be applied consistently across training and test datasets…              | 0.61      | | 18     | Data preprocessing steps, including feature scaling, can significantly impact the performance… | 0.61      |  From the looks of it, the data pulled up is highly relevant to your query. Now, with this solid base of results, it’s time to refine them further with reranking.",
    "metadata": {
      "chunk_id": "3622726ae776-0025",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Vector Search"
      ],
      "heading_text": "Vector Search",
      "token_count": 484,
      "char_count": 2262,
      "start_char": 37042,
      "end_char": 39304,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7181299363057325,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.852561",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 484,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Vector Search",
      "chunk_hash": "2851d704bd7bf6b2",
      "content_digest": "2851d704bd7bf6b2",
      "chunk_length": 2262,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "scaling",
          "query",
          "feature",
          "search",
          "points",
          "learning",
          "data",
          "from",
          "embeddings",
          "with",
          "collection",
          "embedding",
          "can",
          "vector",
          "this",
          "relevant",
          "your",
          "result",
          "basic"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 18,
            "weight": 0.082569
          },
          {
            "term": "scaling",
            "tf": 9,
            "weight": 0.041284
          },
          {
            "term": "query",
            "tf": 8,
            "weight": 0.036697
          },
          {
            "term": "feature",
            "tf": 8,
            "weight": 0.036697
          },
          {
            "term": "search",
            "tf": 7,
            "weight": 0.03211
          },
          {
            "term": "points",
            "tf": 5,
            "weight": 0.022936
          },
          {
            "term": "learning",
            "tf": 5,
            "weight": 0.022936
          },
          {
            "term": "data",
            "tf": 5,
            "weight": 0.022936
          },
          {
            "term": "from",
            "tf": 4,
            "weight": 0.018349
          },
          {
            "term": "embeddings",
            "tf": 4,
            "weight": 0.018349
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.018349
          },
          {
            "term": "collection",
            "tf": 3,
            "weight": 0.013761
          },
          {
            "term": "embedding",
            "tf": 3,
            "weight": 0.013761
          },
          {
            "term": "can",
            "tf": 3,
            "weight": 0.013761
          },
          {
            "term": "vector",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "this",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "relevant",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "your",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "result",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "basic",
            "tf": 2,
            "weight": 0.009174
          }
        ],
        "unique_terms": 126,
        "total_terms": 218
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Vector Search",
        "data",
        "embeddings",
        "feature",
        "from",
        "learning",
        "points",
        "query",
        "scaling",
        "search",
        "the"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7181299363057325,
      "overall": 0.7727099787685775
    }
  },
  {
    "text": "### Rerank  This code takes the documents from the search results and reranks them based on your query, making sure you get the most relevant ones right at the top. First, we pull out the documents from the search results. Then we use Cohere’s rerank model to refine these results: ```python document_list = [point.payload['document'] for point in search_result]  rerank_results = co.rerank(     model=\"rerank-english-v3.0\",     query=query,     documents=document_list,     top_n=5, ) ``` ---  What’s happening here? In the first line, we’re building a list of documents by grabbing the ‘document’ field from each search result point. Then, we pass this list, along with the original query, to Cohere’s rerank method. Using the **rerank-english-v3.0** model, it reshuffles the documents and gives you back the top 5, ranked by their relevance to the query. Here’s the reranked result table, with the new order and their relevance scores:  | **Index** | **Document**                                                                                                                    | **Relevance Score** | | --------- | ------------------------------------------------------------------------------------------------------------------------------- | ------------------- | | 0         | In machine learning, feature scaling is the process of normalizing the range of independent variables or features.              | 0.99995166          | | 1         | Feature scaling is commonly used in data preprocessing to ensure that features are on the same scale.                           | 0.99929035          | | 10        | In deep learning, feature scaling helps stabilize the learning process, allowing for better performance and faster convergence. | 0.998675            | | 23        | Data augmentation techniques in machine learning may also include scaling to ensure consistency across training datasets.       | 0.998043            | | 3         | Unsupervised learning algorithms, such as clustering methods, may benefit from feature scaling.                                 | 0.9979967           |  As you can see, the reranking did its job. Positions for documents 10 and 1 got swapped, showing that the reranker has fine-tuned the results to give you the most relevant content at the top.",
    "metadata": {
      "chunk_id": "3622726ae776-0026",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Rerank"
      ],
      "heading_text": "Rerank",
      "token_count": 461,
      "char_count": 2294,
      "start_char": 39309,
      "end_char": 41603,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5209086956521739,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.861942",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 461,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Rerank",
      "chunk_hash": "52f75b14762ff85b",
      "content_digest": "52f75b14762ff85b",
      "chunk_length": 2294,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "rerank",
          "documents",
          "results",
          "and",
          "query",
          "document",
          "learning",
          "scaling",
          "from",
          "search",
          "you",
          "top",
          "list",
          "feature",
          "model",
          "point",
          "for",
          "result",
          "relevance"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 24,
            "weight": 0.102564
          },
          {
            "term": "rerank",
            "tf": 7,
            "weight": 0.029915
          },
          {
            "term": "documents",
            "tf": 6,
            "weight": 0.025641
          },
          {
            "term": "results",
            "tf": 5,
            "weight": 0.021368
          },
          {
            "term": "and",
            "tf": 5,
            "weight": 0.021368
          },
          {
            "term": "query",
            "tf": 5,
            "weight": 0.021368
          },
          {
            "term": "document",
            "tf": 5,
            "weight": 0.021368
          },
          {
            "term": "learning",
            "tf": 5,
            "weight": 0.021368
          },
          {
            "term": "scaling",
            "tf": 5,
            "weight": 0.021368
          },
          {
            "term": "from",
            "tf": 4,
            "weight": 0.017094
          },
          {
            "term": "search",
            "tf": 4,
            "weight": 0.017094
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.017094
          },
          {
            "term": "top",
            "tf": 4,
            "weight": 0.017094
          },
          {
            "term": "list",
            "tf": 4,
            "weight": 0.017094
          },
          {
            "term": "feature",
            "tf": 4,
            "weight": 0.017094
          },
          {
            "term": "model",
            "tf": 3,
            "weight": 0.012821
          },
          {
            "term": "point",
            "tf": 3,
            "weight": 0.012821
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.012821
          },
          {
            "term": "result",
            "tf": 3,
            "weight": 0.012821
          },
          {
            "term": "relevance",
            "tf": 3,
            "weight": 0.012821
          }
        ],
        "unique_terms": 131,
        "total_terms": 234
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Rerank",
        "and",
        "document",
        "documents",
        "from",
        "learning",
        "query",
        "rerank",
        "results",
        "scaling",
        "the"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5209086956521739,
      "overall": 0.7403028985507246
    }
  },
  {
    "text": "## Conclusion\n\nReranking is a powerful way to boost the relevance and precision of search results in RAG systems. By combining Qdrant’s vector search capabilities with tools like Cohere’s Rerank model or ColBERT, you can refine search outputs, ensuring the most relevant information rises to the top.\n\nThis guide demonstrated how reranking enhances precision without sacrificing recall, delivering sharper, context-rich results. With these tools, you’re equipped to create search systems that provide meaningful and impactful user experiences. Start implementing reranking to take your applications to the next level!",
    "metadata": {
      "chunk_id": "3622726ae776-0027",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Conclusion"
      ],
      "heading_text": "Conclusion",
      "token_count": 114,
      "char_count": 617,
      "start_char": 41609,
      "end_char": 42226,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7388636363636364,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.863495",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 114,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Conclusion",
      "chunk_hash": "e28d4e73cf116e54",
      "content_digest": "e28d4e73cf116e54",
      "chunk_length": 617,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "search",
          "reranking",
          "and",
          "precision",
          "results",
          "systems",
          "with",
          "tools",
          "you",
          "conclusion",
          "powerful",
          "way",
          "boost",
          "relevance",
          "rag",
          "combining",
          "qdrant",
          "vector",
          "capabilities"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 4,
            "weight": 0.051948
          },
          {
            "term": "search",
            "tf": 4,
            "weight": 0.051948
          },
          {
            "term": "reranking",
            "tf": 3,
            "weight": 0.038961
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "precision",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "results",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "systems",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "tools",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "you",
            "tf": 2,
            "weight": 0.025974
          },
          {
            "term": "conclusion",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "powerful",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "way",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "boost",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "relevance",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "rag",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "combining",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "qdrant",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "vector",
            "tf": 1,
            "weight": 0.012987
          },
          {
            "term": "capabilities",
            "tf": 1,
            "weight": 0.012987
          }
        ],
        "unique_terms": 62,
        "total_terms": 77
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Conclusion",
        "and",
        "precision",
        "reranking",
        "results",
        "search",
        "systems",
        "the",
        "tools",
        "with",
        "you"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7388636363636364,
      "overall": 0.8129545454545454
    }
  },
  {
    "text": "##### Was this page useful? Yes No  Thank you for your feedback! 🙏  We are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/search-precision/reranking-semantic-search.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue. On this page:  - [Reranking in RAG with Qdrant Vector Database](#reranking-in-rag-with-qdrant-vector-database.md)  - [Understanding Reranking](#understanding-reranking.md)    - [Background](#background.md)   - [Working](#working.md)   - [Importance](#importance.md)  - [Implementing Vector Search with Reranking](#implementing-vector-search-with-reranking.md)    - [Overview](#overview.md)      - [Ingestion Stage](#ingestion-stage.md)     - [Retrieval Stage](#retrieval-stage.md)    - [Implementation](#implementation.md)      - [Setup](#setup.md)     - [**Steps to Set Up Qdrant Cloud:**](#steps-to-set-up-qdrant-cloud.md)     - [Ingestion](#ingestion.md)     - [There are three key parts to ingestion: Creating a Collection, Converting Documents to Embeddings, and Upserting the Data. Let’s break it down.](#there-are-three-key-parts-to-ingestion-creating-a-collection-converting-documents-to-embeddings-and-upserting-the-data-lets-break-it-down.md)     - [Creating a Collection](#creating-a-collection.md)     - [Documents to Embeddings](#documents-to-embeddings.md)     - [Upsert Data](#upsert-data.md)     - [Now your embeddings are all set in Qdrant, ready to power your search.](#now-your-embeddings-are-all-set-in-qdrant-ready-to-power-your-search.md)     - [Retrieval](#retrieval.md)     - [Vector Search](#vector-search.md)     - [Rerank](#rerank.md)    - [Conclusion](#conclusion.md)  * [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/search-precision/reranking-semantic-search.md) * [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)",
    "metadata": {
      "chunk_id": "3622726ae776-0028",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Was this page useful?"
      ],
      "heading_text": "Was this page useful?",
      "token_count": 546,
      "char_count": 1981,
      "start_char": 42228,
      "end_char": 44209,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.9083333333333334,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T16:07:37.868962",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 546,
      "document_id": "3622726ae776",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Was this page useful?",
      "chunk_hash": "bf2ddc2be6b9cf8f",
      "content_digest": "bf2ddc2be6b9cf8f",
      "chunk_length": 1981,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1536,
      "matryoshka_dimension": 1536,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "search",
          "reranking",
          "page",
          "github",
          "landing",
          "vector",
          "ingestion",
          "embeddings",
          "your",
          "are",
          "https",
          "com",
          "with",
          "stage",
          "retrieval",
          "set",
          "creating",
          "collection",
          "documents"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 12,
            "weight": 0.0553
          },
          {
            "term": "search",
            "tf": 10,
            "weight": 0.046083
          },
          {
            "term": "reranking",
            "tf": 8,
            "weight": 0.036866
          },
          {
            "term": "page",
            "tf": 7,
            "weight": 0.032258
          },
          {
            "term": "github",
            "tf": 7,
            "weight": 0.032258
          },
          {
            "term": "landing",
            "tf": 6,
            "weight": 0.02765
          },
          {
            "term": "vector",
            "tf": 6,
            "weight": 0.02765
          },
          {
            "term": "ingestion",
            "tf": 6,
            "weight": 0.02765
          },
          {
            "term": "embeddings",
            "tf": 6,
            "weight": 0.02765
          },
          {
            "term": "your",
            "tf": 5,
            "weight": 0.023041
          },
          {
            "term": "are",
            "tf": 5,
            "weight": 0.023041
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "com",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "stage",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "retrieval",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "set",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "creating",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "collection",
            "tf": 4,
            "weight": 0.018433
          },
          {
            "term": "documents",
            "tf": 4,
            "weight": 0.018433
          }
        ],
        "unique_terms": 76,
        "total_terms": 217
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Was this page useful?",
        "embeddings",
        "github",
        "ingestion",
        "landing",
        "page",
        "qdrant",
        "reranking",
        "search",
        "vector",
        "your"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.9083333333333334,
      "overall": 0.8027777777777777
    }
  }
]