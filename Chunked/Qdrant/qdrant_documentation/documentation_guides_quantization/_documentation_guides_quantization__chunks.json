[
  {
    "text": "### User Manual  [Concepts](https://qdrant.tech/documentation/concepts/)  - [Collections](https://qdrant.tech/documentation/concepts/collections/) - [Points](https://qdrant.tech/documentation/concepts/points/) - [Vectors](https://qdrant.tech/documentation/concepts/vectors/) - [Payload](https://qdrant.tech/documentation/concepts/payload/) - [Search](https://qdrant.tech/documentation/concepts/search/) - [Explore](https://qdrant.tech/documentation/concepts/explore/) - [Hybrid Queries](https://qdrant.tech/documentation/concepts/hybrid-queries/) - [Filtering](https://qdrant.tech/documentation/concepts/filtering/) - [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/) - [Storage](https://qdrant.tech/documentation/concepts/storage/) - [Indexing](https://qdrant.tech/documentation/concepts/indexing/) - [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/)  [Guides](https://qdrant.tech/documentation/guides/installation/)  - [Installation](https://qdrant.tech/documentation/guides/installation/) - [Administration](https://qdrant.tech/documentation/guides/administration/) - [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/) - [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/) - [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/) - [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) - [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/) - [Quantization](https://qdrant.tech/documentation/guides/quantization/) - [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/) - [Configuration](https://qdrant.tech/documentation/guides/configuration/) - [Security](https://qdrant.tech/documentation/guides/security/) - [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/) - [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/)",
    "metadata": {
      "chunk_id": "6453778d0049-0001",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "User Manual"
      ],
      "heading_text": "User Manual",
      "token_count": 485,
      "char_count": 1968,
      "start_char": 1038,
      "end_char": 3006,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.337627",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 485,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "User Manual",
      "chunk_hash": "aa99483e94fddd20",
      "content_digest": "aa99483e94fddd20",
      "chunk_length": 1968,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "guides",
          "concepts",
          "installation",
          "collections",
          "points",
          "vectors",
          "payload",
          "search",
          "explore",
          "hybrid",
          "queries",
          "filtering",
          "optimizer",
          "storage",
          "indexing",
          "snapshots"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "qdrant",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "tech",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "documentation",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "guides",
            "tf": 15,
            "weight": 0.072816
          },
          {
            "term": "concepts",
            "tf": 14,
            "weight": 0.067961
          },
          {
            "term": "installation",
            "tf": 3,
            "weight": 0.014563
          },
          {
            "term": "collections",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "points",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "payload",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "explore",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "hybrid",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "filtering",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "optimizer",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "storage",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "indexing",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "snapshots",
            "tf": 2,
            "weight": 0.009709
          }
        ],
        "unique_terms": 45,
        "total_terms": 206
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "User Manual",
        "collections",
        "concepts",
        "documentation",
        "guides",
        "https",
        "installation",
        "points",
        "qdrant",
        "tech",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "overall": 0.7776666666666666
    }
  },
  {
    "text": "### Tutorials  [Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/)  - [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/) - [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/) - [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/) - [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/)  [Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/)  - [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/) - [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/) - [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/) - [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/) - [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/)  [Using the Database](https://qdrant.tech/documentation/database-tutorials/)  - [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/) - [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/) - [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/) - [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/) - [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/) - [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/) - [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/)",
    "metadata": {
      "chunk_id": "6453778d0049-0003",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Tutorials"
      ],
      "heading_text": "Tutorials",
      "token_count": 459,
      "char_count": 1988,
      "start_char": 3688,
      "end_char": 5676,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.344995",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 459,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Tutorials",
      "chunk_hash": "f480f0283a04b78a",
      "content_digest": "f480f0283a04b78a",
      "chunk_length": 1988,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "tutorials",
          "https",
          "tech",
          "documentation",
          "search",
          "database",
          "advanced",
          "beginner",
          "with",
          "hybrid",
          "retrieval",
          "build",
          "scale",
          "neural",
          "fastembed",
          "quality",
          "multivector",
          "representations",
          "using"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.098655
          },
          {
            "term": "tutorials",
            "tf": 20,
            "weight": 0.089686
          },
          {
            "term": "https",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "tech",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "documentation",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "search",
            "tf": 14,
            "weight": 0.06278
          },
          {
            "term": "database",
            "tf": 9,
            "weight": 0.040359
          },
          {
            "term": "advanced",
            "tf": 7,
            "weight": 0.03139
          },
          {
            "term": "beginner",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "hybrid",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "retrieval",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "build",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "scale",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "neural",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "fastembed",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "quality",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "multivector",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "representations",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "using",
            "tf": 2,
            "weight": 0.008969
          }
        ],
        "unique_terms": 64,
        "total_terms": 223
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Tutorials",
        "advanced",
        "beginner",
        "database",
        "documentation",
        "https",
        "qdrant",
        "search",
        "tech",
        "tutorials",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "overall": 0.7826697594501718
    }
  },
  {
    "text": "### Support  [FAQ](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)  - [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/) - [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/)  [Release Notes](https://github.com/qdrant/qdrant/releases)",
    "metadata": {
      "chunk_id": "6453778d0049-0004",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Support"
      ],
      "heading_text": "Support",
      "token_count": 83,
      "char_count": 311,
      "start_char": 5678,
      "end_char": 5989,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5627272727272727,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.346977",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 83,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Support",
      "chunk_hash": "f059a5deb61e367d",
      "content_digest": "f059a5deb61e367d",
      "chunk_length": 311,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "faq",
          "https",
          "tech",
          "documentation",
          "fundamentals",
          "database",
          "optimization",
          "support",
          "release",
          "notes",
          "github",
          "com",
          "releases"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 8,
            "weight": 0.228571
          },
          {
            "term": "faq",
            "tf": 4,
            "weight": 0.114286
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.114286
          },
          {
            "term": "tech",
            "tf": 3,
            "weight": 0.085714
          },
          {
            "term": "documentation",
            "tf": 3,
            "weight": 0.085714
          },
          {
            "term": "fundamentals",
            "tf": 3,
            "weight": 0.085714
          },
          {
            "term": "database",
            "tf": 2,
            "weight": 0.057143
          },
          {
            "term": "optimization",
            "tf": 2,
            "weight": 0.057143
          },
          {
            "term": "support",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "release",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "notes",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "github",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "com",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "releases",
            "tf": 1,
            "weight": 0.028571
          }
        ],
        "unique_terms": 14,
        "total_terms": 35
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Support",
        "database",
        "documentation",
        "faq",
        "fundamentals",
        "https",
        "optimization",
        "qdrant",
        "release",
        "support",
        "tech"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5627272727272727,
      "overall": 0.7209090909090908
    }
  },
  {
    "text": "### User Manual  [Concepts](https://qdrant.tech/documentation/concepts/)  - [Collections](https://qdrant.tech/documentation/concepts/collections/) - [Points](https://qdrant.tech/documentation/concepts/points/) - [Vectors](https://qdrant.tech/documentation/concepts/vectors/) - [Payload](https://qdrant.tech/documentation/concepts/payload/) - [Search](https://qdrant.tech/documentation/concepts/search/) - [Explore](https://qdrant.tech/documentation/concepts/explore/) - [Hybrid Queries](https://qdrant.tech/documentation/concepts/hybrid-queries/) - [Filtering](https://qdrant.tech/documentation/concepts/filtering/) - [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/) - [Storage](https://qdrant.tech/documentation/concepts/storage/) - [Indexing](https://qdrant.tech/documentation/concepts/indexing/) - [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/)  [Guides](https://qdrant.tech/documentation/guides/installation/)  - [Installation](https://qdrant.tech/documentation/guides/installation/) - [Administration](https://qdrant.tech/documentation/guides/administration/) - [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/) - [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/) - [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/) - [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) - [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/) - [Quantization](https://qdrant.tech/documentation/guides/quantization/) - [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/) - [Configuration](https://qdrant.tech/documentation/guides/configuration/) - [Security](https://qdrant.tech/documentation/guides/security/) - [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/) - [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/)",
    "metadata": {
      "chunk_id": "6453778d0049-0006",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "User Manual"
      ],
      "heading_text": "User Manual",
      "token_count": 485,
      "char_count": 1968,
      "start_char": 6366,
      "end_char": 8334,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.357383",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 485,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "User Manual",
      "chunk_hash": "aa99483e94fddd20",
      "content_digest": "aa99483e94fddd20",
      "chunk_length": 1968,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "guides",
          "concepts",
          "installation",
          "collections",
          "points",
          "vectors",
          "payload",
          "search",
          "explore",
          "hybrid",
          "queries",
          "filtering",
          "optimizer",
          "storage",
          "indexing",
          "snapshots"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "qdrant",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "tech",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "documentation",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "guides",
            "tf": 15,
            "weight": 0.072816
          },
          {
            "term": "concepts",
            "tf": 14,
            "weight": 0.067961
          },
          {
            "term": "installation",
            "tf": 3,
            "weight": 0.014563
          },
          {
            "term": "collections",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "points",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "payload",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "explore",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "hybrid",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "filtering",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "optimizer",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "storage",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "indexing",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "snapshots",
            "tf": 2,
            "weight": 0.009709
          }
        ],
        "unique_terms": 45,
        "total_terms": 206
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "User Manual",
        "collections",
        "concepts",
        "documentation",
        "guides",
        "https",
        "installation",
        "points",
        "qdrant",
        "tech",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "overall": 0.7776666666666666
    }
  },
  {
    "text": "### Tutorials  [Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/)  - [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/) - [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/) - [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/) - [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/)  [Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/)  - [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/) - [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/) - [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/) - [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/) - [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/)  [Using the Database](https://qdrant.tech/documentation/database-tutorials/)  - [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/) - [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/) - [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/) - [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/) - [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/) - [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/) - [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/)",
    "metadata": {
      "chunk_id": "6453778d0049-0008",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Tutorials"
      ],
      "heading_text": "Tutorials",
      "token_count": 459,
      "char_count": 1988,
      "start_char": 9016,
      "end_char": 11004,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.368468",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 459,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Tutorials",
      "chunk_hash": "f480f0283a04b78a",
      "content_digest": "f480f0283a04b78a",
      "chunk_length": 1988,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "tutorials",
          "https",
          "tech",
          "documentation",
          "search",
          "database",
          "advanced",
          "beginner",
          "with",
          "hybrid",
          "retrieval",
          "build",
          "scale",
          "neural",
          "fastembed",
          "quality",
          "multivector",
          "representations",
          "using"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.098655
          },
          {
            "term": "tutorials",
            "tf": 20,
            "weight": 0.089686
          },
          {
            "term": "https",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "tech",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "documentation",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "search",
            "tf": 14,
            "weight": 0.06278
          },
          {
            "term": "database",
            "tf": 9,
            "weight": 0.040359
          },
          {
            "term": "advanced",
            "tf": 7,
            "weight": 0.03139
          },
          {
            "term": "beginner",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "hybrid",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "retrieval",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "build",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "scale",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "neural",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "fastembed",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "quality",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "multivector",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "representations",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "using",
            "tf": 2,
            "weight": 0.008969
          }
        ],
        "unique_terms": 64,
        "total_terms": 223
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Tutorials",
        "advanced",
        "beginner",
        "database",
        "documentation",
        "https",
        "qdrant",
        "search",
        "tech",
        "tutorials",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "overall": 0.7826697594501718
    }
  },
  {
    "text": "### Support  [FAQ](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)  - [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/) - [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/)  [Release Notes](https://github.com/qdrant/qdrant/releases)  - [Documentation](https://qdrant.tech/documentation/) - - [Guides](https://qdrant.tech/documentation/guides/) - - Quantization",
    "metadata": {
      "chunk_id": "6453778d0049-0009",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Support"
      ],
      "heading_text": "Support",
      "token_count": 117,
      "char_count": 439,
      "start_char": 11006,
      "end_char": 11445,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6952631578947369,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.371274",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 117,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Support",
      "chunk_hash": "ff177125622d8d98",
      "content_digest": "ff177125622d8d98",
      "chunk_length": 439,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "https",
          "documentation",
          "tech",
          "faq",
          "fundamentals",
          "database",
          "optimization",
          "guides",
          "support",
          "release",
          "notes",
          "github",
          "com",
          "releases",
          "quantization"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.212766
          },
          {
            "term": "https",
            "tf": 6,
            "weight": 0.12766
          },
          {
            "term": "documentation",
            "tf": 6,
            "weight": 0.12766
          },
          {
            "term": "tech",
            "tf": 5,
            "weight": 0.106383
          },
          {
            "term": "faq",
            "tf": 4,
            "weight": 0.085106
          },
          {
            "term": "fundamentals",
            "tf": 3,
            "weight": 0.06383
          },
          {
            "term": "database",
            "tf": 2,
            "weight": 0.042553
          },
          {
            "term": "optimization",
            "tf": 2,
            "weight": 0.042553
          },
          {
            "term": "guides",
            "tf": 2,
            "weight": 0.042553
          },
          {
            "term": "support",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "release",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "notes",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "github",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "com",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "releases",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "quantization",
            "tf": 1,
            "weight": 0.021277
          }
        ],
        "unique_terms": 16,
        "total_terms": 47
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Support",
        "database",
        "documentation",
        "faq",
        "fundamentals",
        "guides",
        "https",
        "optimization",
        "qdrant",
        "support",
        "tech"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6952631578947369,
      "overall": 0.7650877192982456
    }
  },
  {
    "text": "## Scalar Quantization\n\n*Available as of v1.1.0*\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8. In other words, Qdrant performs `float32 -> uint8` conversion for each vector component. Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process. Qdrant uses a special SIMD CPU instruction to perform fast vector comparison. This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.\n\nThe main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality. However, this error is usually negligible, and tends to be less significant for high-dimensional vectors. In our experiments, we found that the error introduced by scalar quantization is usually less than 1%.\n\nHowever, this value depends on the data and the quantization parameters. Please refer to the [Quantization Tips](#quantization-tips.md) section for more information on how to optimize the quantization parameters for your use case.",
    "metadata": {
      "chunk_id": "6453778d0049-0011",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Scalar Quantization"
      ],
      "heading_text": "Scalar Quantization",
      "token_count": 312,
      "char_count": 1513,
      "start_char": 12697,
      "end_char": 14210,
      "semantic_score": 0.7,
      "structural_score": 0.9999999999999999,
      "retrieval_quality": 0.6591172413793104,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.374530",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 312,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Scalar Quantization",
      "chunk_hash": "f8b066880b27f298",
      "content_digest": "f8b066880b27f298",
      "chunk_length": 1513,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "quantization",
          "scalar",
          "vector",
          "for",
          "that",
          "qdrant",
          "this",
          "search",
          "uint8",
          "conversion",
          "error",
          "vectors",
          "reducing",
          "number",
          "bits",
          "used",
          "represent",
          "each",
          "component"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 17,
            "weight": 0.093923
          },
          {
            "term": "quantization",
            "tf": 10,
            "weight": 0.055249
          },
          {
            "term": "scalar",
            "tf": 6,
            "weight": 0.033149
          },
          {
            "term": "vector",
            "tf": 6,
            "weight": 0.033149
          },
          {
            "term": "for",
            "tf": 5,
            "weight": 0.027624
          },
          {
            "term": "that",
            "tf": 4,
            "weight": 0.022099
          },
          {
            "term": "qdrant",
            "tf": 4,
            "weight": 0.022099
          },
          {
            "term": "this",
            "tf": 4,
            "weight": 0.022099
          },
          {
            "term": "search",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "uint8",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "conversion",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "error",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "reducing",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "number",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "bits",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "used",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "represent",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "each",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "component",
            "tf": 2,
            "weight": 0.01105
          }
        ],
        "unique_terms": 103,
        "total_terms": 181
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Scalar Quantization",
        "for",
        "qdrant",
        "quantization",
        "scalar",
        "search",
        "that",
        "the",
        "this",
        "uint8",
        "vector"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.9999999999999999,
      "retrieval_quality": 0.6591172413793104,
      "overall": 0.7863724137931034
    }
  },
  {
    "text": "### 1.5-Bit and 2-Bit Quantization  *Available as of v1.15.0*  **Binary quantization** storage can use **2 and 1.5 bits** per dimension, improving precision for smaller vectors. One-bit compression resulted in significant data loss and precision drops for vectors smaller than a thousand dimensions, often requiring expensive rescoring. 2-bit quantization offers 16X compression compared to 32X with one bit, improving performance for smaller vector dimensions. The 1.5-bit quantization compression offers 24X compression and intermediate accuracy. A major limitation of binary quantization is poor handling of values close to zero. 2-bit quantization addresses this by explicitly representing zeros using an efficient scoring mechanism. In the case of 1.5-bit quantization, the zero-bit is shared between two values, balancing the efficiency of binary quantization with the accuracy improvements of 2-bit quantization, especially when 2-bit BQ requires too much memory. In order to build 2-bit representation, Qdrant computes values distribution and then assigns bit values to 3 possible buckets:  - `-1` - 00 - `0` - 01 - `1` - 11  1.5-bit quantization is similar, but merges buckets of pairs of elements into a binary triptets  2-bit quantization  See how to set up 1.5-bit and 2-bit quantization in the [following section](#set-up-bit-depth.md).",
    "metadata": {
      "chunk_id": "6453778d0049-0014",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "1.5-Bit and 2-Bit Quantization"
      ],
      "heading_text": "1.5-Bit and 2-Bit Quantization",
      "token_count": 314,
      "char_count": 1349,
      "start_char": 16695,
      "end_char": 18044,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6895846153846154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.379757",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 314,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "1.5-Bit and 2-Bit Quantization",
      "chunk_hash": "6ccd865bd4530126",
      "content_digest": "6ccd865bd4530126",
      "chunk_length": 1349,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "bit",
          "quantization",
          "and",
          "the",
          "binary",
          "compression",
          "values",
          "for",
          "smaller",
          "improving",
          "precision",
          "vectors",
          "one",
          "dimensions",
          "offers",
          "with",
          "accuracy",
          "zero",
          "buckets",
          "set"
        ],
        "term_weights": [
          {
            "term": "bit",
            "tf": 18,
            "weight": 0.116129
          },
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.077419
          },
          {
            "term": "and",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "the",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "binary",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "compression",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "values",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "smaller",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "improving",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "precision",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "one",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "dimensions",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "offers",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "accuracy",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "zero",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "buckets",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "set",
            "tf": 2,
            "weight": 0.012903
          }
        ],
        "unique_terms": 93,
        "total_terms": 155
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "1.5-Bit and 2-Bit Quantization",
        "and",
        "binary",
        "bit",
        "compression",
        "for",
        "improving",
        "quantization",
        "smaller",
        "the",
        "values"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6895846153846154,
      "overall": 0.7631948717948718
    }
  },
  {
    "text": "### Asymmetric Quantization\n\n*Available as of v1.15.0*\n\nThe **Asymmetric Quantization** technique allows qdrant to use different vector encoding algorithm for stored vectors and for queries. Particularly interesting combination is a Binary stored vectors and Scalar quantized queries.\n\nAsymmetric quantization\n\nThis approach maintains storage size and RAM usage similar to binary quantization while offering improved precision. It is beneficial for memory-constrained deployments, or where the bottleneck is disk I/O rather than CPU. This is particularly useful for indexing millions of vectors as it improves precision without sacrificing much because the limitation in such scenarios is disk speed, not CPU. This approach requires less rescoring for the same quality output.\n\nSee how to set up Asymmetric Quantization quantization in the [following section](#set-up-asymmetric-quantization.md)",
    "metadata": {
      "chunk_id": "6453778d0049-0015",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Asymmetric Quantization"
      ],
      "heading_text": "Asymmetric Quantization",
      "token_count": 168,
      "char_count": 895,
      "start_char": 18048,
      "end_char": 18943,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.6974999999999999,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.381185",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 168,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Asymmetric Quantization",
      "chunk_hash": "3131cf846cd4513a",
      "content_digest": "3131cf846cd4513a",
      "chunk_length": 895,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "asymmetric",
          "the",
          "for",
          "vectors",
          "and",
          "this",
          "stored",
          "queries",
          "particularly",
          "binary",
          "approach",
          "precision",
          "disk",
          "cpu",
          "set",
          "available",
          "technique",
          "allows",
          "qdrant"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 7,
            "weight": 0.068627
          },
          {
            "term": "asymmetric",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "the",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "for",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "vectors",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "this",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "stored",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "particularly",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "binary",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "approach",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "precision",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "disk",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "cpu",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "set",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "available",
            "tf": 1,
            "weight": 0.009804
          },
          {
            "term": "technique",
            "tf": 1,
            "weight": 0.009804
          },
          {
            "term": "allows",
            "tf": 1,
            "weight": 0.009804
          },
          {
            "term": "qdrant",
            "tf": 1,
            "weight": 0.009804
          }
        ],
        "unique_terms": 69,
        "total_terms": 102
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Asymmetric Quantization",
        "and",
        "asymmetric",
        "for",
        "particularly",
        "quantization",
        "queries",
        "stored",
        "the",
        "this",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.6974999999999999,
      "overall": 0.7324999999999999
    }
  },
  {
    "text": "## Product Quantization\n\n*Available as of v1.2.0*\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into chunks and quantizing each segment individually. Each chunk is approximated by a centroid index that represents the original vector component. The positions of the centroids are determined through the utilization of a clustering algorithm such as k-means. For now, Qdrant uses only 256 centroids, so each centroid index can be represented by a single byte.\n\nProduct quantization can compress by a more prominent factor than a scalar one. But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization. Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\nPlease refer to the [Quantization Tips](#quantization-tips.md) section for more information on how to optimize the quantization parameters for your use case.",
    "metadata": {
      "chunk_id": "6453778d0049-0016",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Product Quantization"
      ],
      "heading_text": "Product Quantization",
      "token_count": 201,
      "char_count": 1011,
      "start_char": 18945,
      "end_char": 19956,
      "semantic_score": 0.7,
      "structural_score": 0.9999999999999999,
      "retrieval_quality": 0.684701986754967,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.382378",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 201,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Product Quantization",
      "chunk_hash": "b459f6516339668f",
      "content_digest": "b459f6516339668f",
      "chunk_length": 1011,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "the",
          "product",
          "for",
          "each",
          "are",
          "vectors",
          "centroid",
          "index",
          "centroids",
          "only",
          "can",
          "more",
          "than",
          "scalar",
          "use",
          "tips",
          "available",
          "method",
          "compressing"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 9,
            "weight": 0.07563
          },
          {
            "term": "the",
            "tf": 6,
            "weight": 0.05042
          },
          {
            "term": "product",
            "tf": 5,
            "weight": 0.042017
          },
          {
            "term": "for",
            "tf": 4,
            "weight": 0.033613
          },
          {
            "term": "each",
            "tf": 3,
            "weight": 0.02521
          },
          {
            "term": "are",
            "tf": 3,
            "weight": 0.02521
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "centroid",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "index",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "centroids",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "only",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "can",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "more",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "than",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "scalar",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "use",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "tips",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "available",
            "tf": 1,
            "weight": 0.008403
          },
          {
            "term": "method",
            "tf": 1,
            "weight": 0.008403
          },
          {
            "term": "compressing",
            "tf": 1,
            "weight": 0.008403
          }
        ],
        "unique_terms": 84,
        "total_terms": 119
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Product Quantization",
        "are",
        "centroid",
        "centroids",
        "each",
        "for",
        "index",
        "product",
        "quantization",
        "the",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.9999999999999999,
      "retrieval_quality": 0.684701986754967,
      "overall": 0.7949006622516556
    }
  },
  {
    "text": "## How to choose the right quantization method  Here is a brief table of the pros and cons of each quantization method:  | Quantization method | Accuracy   | Speed     | Compression | | ------------------- | ---------- | --------- | ----------- | | Scalar              | 0.99       | up to x2  | 4           | | Product             | 0.7        | 0.5       | up to 64    | | Binary (1 bit)      | 0.95\\*     | up to x40 | 32          | | Binary (1.5 bit)    | 0.95\\*\\*   | up to x30 | 24          | | Binary (2 bit)      | 0.95\\*\\*\\* | up to x20 | 16          |  - `*` - for compatible models with high-dimensional vectors (approx. 1536+ dimensions)  - `**` - for compatible models with medium-dimensional vectors (approx. 1024-1536 dimensions)  - `***` - for compatible models with low-dimensional vectors (approx. 768-1024 dimensions)  - **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only. - If you are planning to use binary quantization with low or medium-dimensional vectors (approx. 512-1024 dimensions), it is recommended to use 1.5-bit or 2-bit quantization as well as asymmetric quantization feature. - **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable. - **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.",
    "metadata": {
      "chunk_id": "6453778d0049-0017",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "How to choose the right quantization method"
      ],
      "heading_text": "How to choose the right quantization method",
      "token_count": 416,
      "char_count": 1707,
      "start_char": 19958,
      "end_char": 21665,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6653904059040591,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "table_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.384169",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 416,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "How to choose the right quantization method",
      "chunk_hash": "eb09fdf54f800e4a",
      "content_digest": "eb09fdf54f800e4a",
      "chunk_length": 1707,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "the",
          "binary",
          "method",
          "and",
          "bit",
          "with",
          "models",
          "dimensional",
          "vectors",
          "approx",
          "dimensions",
          "recommended",
          "accuracy",
          "speed",
          "compression",
          "scalar",
          "for",
          "compatible",
          "1024"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.074534
          },
          {
            "term": "the",
            "tf": 8,
            "weight": 0.049689
          },
          {
            "term": "binary",
            "tf": 6,
            "weight": 0.037267
          },
          {
            "term": "method",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "and",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "bit",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "models",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "dimensional",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "vectors",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "approx",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "dimensions",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "recommended",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "accuracy",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "speed",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "compression",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "scalar",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "compatible",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "1024",
            "tf": 3,
            "weight": 0.018634
          }
        ],
        "unique_terms": 80,
        "total_terms": 161
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "How to choose the right quantization method",
        "and",
        "binary",
        "bit",
        "dimensional",
        "method",
        "models",
        "quantization",
        "the",
        "vectors",
        "with"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6653904059040591,
      "overall": 0.755130135301353
    }
  },
  {
    "text": "### Setting up Scalar Quantization  To enable scalar quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. When enabling scalar quantization on an existing collection, use a PATCH request or the corresponding `update_collection` method and omit the vector configuration, as its already defined. ```http PUT /collections/{collection_name} {     \"vectors\": {       \"size\": 768,       \"distance\": \"Cosine\"     },     \"quantization_config\": {         \"scalar\": {             \"type\": \"int8\",             \"quantile\": 0.99,             \"always_ram\": true         }     } } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.create_collection(     collection_name=\"{collection_name}\",     vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),     quantization_config=models.ScalarQuantization(         scalar=models.ScalarQuantizationConfig(             type=models.ScalarType.INT8,             quantile=0.99,             always_ram=True,         ),     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.createCollection(\"{collection_name}\", {   vectors: {     size: 768,     distance: \"Cosine\",   },   quantization_config: {     scalar: {       type: \"int8\",       quantile: 0.99,       always_ram: true,     },   }, }); ``` ```rust use qdrant_client::qdrant::{     CreateCollectionBuilder, Distance, QuantizationType, ScalarQuantizationBuilder,     VectorParamsBuilder, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;  client     .create_collection(         CreateCollectionBuilder::new(\"{collection_name}\")             .vectors_config(VectorParamsBuilder::new(768, Distance::Cosine))             .quantization_config(                 ScalarQuantizationBuilder::default()                     .r#type(QuantizationType::Int8.into())                     .quantile(0.99)                     .always_ram(true),             ),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client     .createCollectionAsync(         CreateCollection.newBuilder()             .setCollectionName(\"{collection_name}\")             .setVectorsConfig(                 VectorsConfig.newBuilder()                     .setParams(                         VectorParams.newBuilder()                             .setSize(768)                             .setDistance(Distance.Cosine)                             .build())                     .build())             .setQuantizationConfig(                 QuantizationConfig.newBuilder()                     .setScalar(                         ScalarQuantization.newBuilder()                             .setType(QuantizationType.Int8)                             .setQuantile(0.99f)                             .setAlwaysRam(true)                             .build())                     .build())             .build())     .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.CreateCollectionAsync(  collectionName: \"{collection_name}\",  vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },  quantizationConfig: new QuantizationConfig  {   Scalar = new ScalarQuantization   {    Type = QuantizationType.Int8,    Quantile = 0.99f,    AlwaysRam = true   }  } ); ``` ```go import ( \t\"context\"  \t\"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{ \tHost: \"localhost\", \tPort: 6334, })  client.CreateCollection(context.Background(), &qdrant.CreateCollection{ \tCollectionName: \"{collection_name}\", \tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{ \t\tSize:     768, \t\tDistance: qdrant.Distance_Cosine, \t}), \tQuantizationConfig: qdrant.NewQuantizationScalar( \t\t&qdrant.ScalarQuantization{             Type:      qdrant.QuantizationType_Int8, \t\t\tQuantile:  qdrant.PtrOf(float32(0.99)), \t\t\tAlwaysRam: qdrant.PtrOf(true), \t\t}, \t), }) ``` There are 3 parameters that you can specify in the `quantization_config` section:  `type` - the type of the quantized vector components. Currently, Qdrant supports only `int8`. `quantile` - the quantile of the quantized vector components. The quantile is used to calculate the quantization bounds. For instance, if you specify `0.99` as the quantile, 1% of extreme values will be excluded from the quantization bounds. Using quantiles lower than `1.0` might be useful if there are outliers in your vector components. This parameter only affects the resulting precision and not the memory footprint. It might be worth tuning this parameter if you experience a significant decrease in search quality. `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.",
    "metadata": {
      "chunk_id": "6453778d0049-0019",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setting up Scalar Quantization"
      ],
      "heading_text": "Setting up Scalar Quantization",
      "token_count": 1324,
      "char_count": 5736,
      "start_char": 22239,
      "end_char": 27975,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6786016949152542,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.411927",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1324,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Setting up Scalar Quantization",
      "chunk_hash": "bac07ee54b1ac89f",
      "content_digest": "bac07ee54b1ac89f",
      "chunk_length": 5736,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "the",
          "collection",
          "quantization",
          "distance",
          "import",
          "quantile",
          "config",
          "vectors",
          "ram",
          "collections",
          "name",
          "type",
          "int8",
          "true",
          "qdrantclient",
          "new",
          "grpc",
          "scalar"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 32,
            "weight": 0.062378
          },
          {
            "term": "client",
            "tf": 28,
            "weight": 0.054581
          },
          {
            "term": "the",
            "tf": 19,
            "weight": 0.037037
          },
          {
            "term": "collection",
            "tf": 13,
            "weight": 0.025341
          },
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.023392
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.023392
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.023392
          },
          {
            "term": "quantile",
            "tf": 10,
            "weight": 0.019493
          },
          {
            "term": "config",
            "tf": 9,
            "weight": 0.017544
          },
          {
            "term": "vectors",
            "tf": 9,
            "weight": 0.017544
          },
          {
            "term": "ram",
            "tf": 9,
            "weight": 0.017544
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "type",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "int8",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "true",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "new",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.015595
          },
          {
            "term": "scalar",
            "tf": 7,
            "weight": 0.013645
          }
        ],
        "unique_terms": 166,
        "total_terms": 513
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setting up Scalar Quantization",
        "client",
        "collection",
        "config",
        "distance",
        "import",
        "qdrant",
        "quantile",
        "quantization",
        "the",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6786016949152542,
      "overall": 0.7262005649717514
    }
  },
  {
    "text": "### Setting up Binary Quantization  To enable binary quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. When enabling binary quantization on an existing collection, use a PATCH request or the corresponding `update_collection` method and omit the vector configuration, as its already defined. ```http PUT /collections/{collection_name} {     \"vectors\": {       \"size\": 1536,       \"distance\": \"Cosine\"     },     \"quantization_config\": {         \"binary\": {             \"always_ram\": true         }     } } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.create_collection(     collection_name=\"{collection_name}\",     vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),     quantization_config=models.BinaryQuantization(         binary=models.BinaryQuantizationConfig(             always_ram=True,         ),     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.createCollection(\"{collection_name}\", {   vectors: {     size: 1536,     distance: \"Cosine\",   },   quantization_config: {     binary: {       always_ram: true,     },   }, }); ``` ```rust use qdrant_client::qdrant::{     BinaryQuantizationBuilder, CreateCollectionBuilder, Distance, VectorParamsBuilder, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;  client     .create_collection(         CreateCollectionBuilder::new(\"{collection_name}\")             .vectors_config(VectorParamsBuilder::new(1536, Distance::Cosine))             .quantization_config(BinaryQuantizationBuilder::new(true)),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.BinaryQuantization; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client     .createCollectionAsync(         CreateCollection.newBuilder()             .setCollectionName(\"{collection_name}\")             .setVectorsConfig(                 VectorsConfig.newBuilder()                     .setParams(                         VectorParams.newBuilder()                             .setSize(1536)                             .setDistance(Distance.Cosine)                             .build())                     .build())             .setQuantizationConfig(                 QuantizationConfig.newBuilder()                     .setBinary(BinaryQuantization.newBuilder().setAlwaysRam(true).build())                     .build())             .build())     .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.CreateCollectionAsync(  collectionName: \"{collection_name}\",  vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine },  quantizationConfig: new QuantizationConfig  {   Binary = new BinaryQuantization { AlwaysRam = true }  } ); ``` ```go import ( \t\"context\"  \t\"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{ \tHost: \"localhost\", \tPort: 6334, })  client.CreateCollection(context.Background(), &qdrant.CreateCollection{ \tCollectionName: \"{collection_name}\", \tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{ \t\tSize:     1536, \t\tDistance: qdrant.Distance_Cosine, \t}), \tQuantizationConfig: qdrant.NewQuantizationBinary( \t\t&qdrant.BinaryQuantization{ \t\t\tAlwaysRam: qdrant.PtrOf(true), \t\t}, \t), }) ``` `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.",
    "metadata": {
      "chunk_id": "6453778d0049-0020",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setting up Binary Quantization"
      ],
      "heading_text": "Setting up Binary Quantization",
      "token_count": 992,
      "char_count": 4283,
      "start_char": 27990,
      "end_char": 32273,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6894259818731118,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.433052",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 992,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Setting up Binary Quantization",
      "chunk_hash": "8966869d32368afa",
      "content_digest": "8966869d32368afa",
      "chunk_length": 4283,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "collection",
          "distance",
          "import",
          "quantization",
          "vectors",
          "new",
          "the",
          "config",
          "name",
          "ram",
          "true",
          "qdrantclient",
          "binary",
          "collections",
          "1536",
          "cosine",
          "build",
          "grpc"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 28,
            "weight": 0.072917
          },
          {
            "term": "client",
            "tf": 27,
            "weight": 0.070312
          },
          {
            "term": "collection",
            "tf": 13,
            "weight": 0.033854
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.03125
          },
          {
            "term": "import",
            "tf": 11,
            "weight": 0.028646
          },
          {
            "term": "quantization",
            "tf": 9,
            "weight": 0.023438
          },
          {
            "term": "vectors",
            "tf": 9,
            "weight": 0.023438
          },
          {
            "term": "new",
            "tf": 9,
            "weight": 0.023438
          },
          {
            "term": "the",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "ram",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "true",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "binary",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "collections",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "1536",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "cosine",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "grpc",
            "tf": 7,
            "weight": 0.018229
          }
        ],
        "unique_terms": 123,
        "total_terms": 384
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setting up Binary Quantization",
        "client",
        "collection",
        "config",
        "distance",
        "import",
        "new",
        "qdrant",
        "quantization",
        "the",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6894259818731118,
      "overall": 0.7298086606243706
    }
  },
  {
    "text": "#### Set up bit depth  To enable 2bit or 1.5bit quantization, you need to specify `encoding` parameter in the `quantization_config` section of the collection configuration. Available values are `two_bits` and `one_and_half_bits`. ```http PUT /collections/{collection_name} {     \"vectors\": {       \"size\": 1536,       \"distance\": \"Cosine\"     },     \"quantization_config\": {         \"binary\": {             \"encoding\": \"two_bits\",             \"always_ram\": true         }     } } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.create_collection(     collection_name=\"{collection_name}\",     vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),     quantization_config=models.BinaryQuantization(         binary=models.BinaryQuantizationConfig(             encoding=models.BinaryQuantizationEncoding.TWO_BITS,             always_ram=True,         ),     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.createCollection(\"{collection_name}\", {   vectors: {     size: 1536,     distance: \"Cosine\",   },   quantization_config: {     binary: {       encoding: \"two_bits\",       always_ram: true,     },   }, }); ``` ```rust use qdrant_client::qdrant::{     BinaryQuantizationBuilder,     CreateCollectionBuilder,     Distance,     VectorParamsBuilder,     BinaryQuantizationEncoding, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;  client     .create_collection(         CreateCollectionBuilder::new(\"{collection_name}\")             .vectors_config(VectorParamsBuilder::new(1536, Distance::Cosine))             .quantization_config(BinaryQuantizationBuilder::new(true)                 .encoding(BinaryQuantizationEncoding::TwoBits)             ),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.BinaryQuantization; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; import io.qdrant.client.grpc.Collections.BinaryQuantizationEncoding;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client     .createCollectionAsync(         CreateCollection.newBuilder()             .setCollectionName(\"{collection_name}\")             .setVectorsConfig(                 VectorsConfig.newBuilder()                     .setParams(                         VectorParams.newBuilder()                             .setSize(1536)                             .setDistance(Distance.Cosine)                             .build())                     .build())             .setQuantizationConfig(                 QuantizationConfig.newBuilder()                     .setBinary(BinaryQuantization                         .newBuilder()                         .setEncoding(BinaryQuantizationEncoding.TwoBits)                         .setAlwaysRam(true)                         .build())                     .build())             .build())     .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.CreateCollectionAsync(   collectionName: \"{collection_name}\",   vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine },   quantizationConfig: new QuantizationConfig   {     Binary = new BinaryQuantization {       Encoding = BinaryQuantizationEncoding.TwoBits,       AlwaysRam = true     }   } ); ``` ```go import (     \"context\"      \"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{     Host: \"localhost\",     Port: 6334, })  client.CreateCollection(context.Background(), &qdrant.CreateCollection{     CollectionName: \"{collection_name}\",     VectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{         Size:     1536,         Distance: qdrant.Distance_Cosine,     }),     QuantizationConfig: qdrant.NewQuantizationBinary(         &qdrant.BinaryQuantization{             Encoding: qdrant.BinaryQuantizationEncoding_TwoBits.Enum(),             AlwaysRam: qdrant.PtrOf(true),         },     ), }) ```",
    "metadata": {
      "chunk_id": "6453778d0049-0021",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Set up bit depth"
      ],
      "heading_text": "Set up bit depth",
      "token_count": 999,
      "char_count": 4459,
      "start_char": 32285,
      "end_char": 36744,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6988888888888889,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.449724",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 999,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Set up bit depth",
      "chunk_hash": "89eb4656e9628e0e",
      "content_digest": "89eb4656e9628e0e",
      "chunk_length": 4459,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "distance",
          "import",
          "collection",
          "new",
          "config",
          "collections",
          "name",
          "qdrantclient",
          "grpc",
          "encoding",
          "1536",
          "cosine",
          "true",
          "binaryquantizationencoding",
          "build",
          "quantization",
          "models",
          "localhost"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.083799
          },
          {
            "term": "client",
            "tf": 28,
            "weight": 0.078212
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.03352
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.03352
          },
          {
            "term": "collection",
            "tf": 11,
            "weight": 0.030726
          },
          {
            "term": "new",
            "tf": 9,
            "weight": 0.02514
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "encoding",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "1536",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "cosine",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "binaryquantizationencoding",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "quantization",
            "tf": 6,
            "weight": 0.01676
          },
          {
            "term": "models",
            "tf": 6,
            "weight": 0.01676
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.01676
          }
        ],
        "unique_terms": 103,
        "total_terms": 358
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Set up bit depth",
        "client",
        "collection",
        "collections",
        "config",
        "distance",
        "import",
        "name",
        "new",
        "qdrant",
        "qdrantclient"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6988888888888889,
      "overall": 0.7329629629629629
    }
  },
  {
    "text": "#### Set up asymmetric quantization  To enable asymmetric quantization, you need to specify `query_encoding` parameter in the `quantization_config` section of the collection configuration. Available values are:  - `default` and `binary` - use regular binary quantization for the query. - `scalar8bits` - use 8bit quantization for the query. - `scalar4bits` - use 4bit quantization for the query. ```http PUT /collections/{collection_name} {     \"vectors\": {       \"size\": 1536,       \"distance\": \"Cosine\"     },     \"quantization_config\": {         \"binary\": {             \"query_encoding\": \"scalar8bits\",             \"always_ram\": true         }     } } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.create_collection(     collection_name=\"{collection_name}\",     vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),     quantization_config=models.BinaryQuantization(         binary=models.BinaryQuantizationConfig(             query_encoding=models.BinaryQuantizationQueryEncoding.SCALAR8BITS,             always_ram=True,         ),     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.createCollection(\"{collection_name}\", {   vectors: {     size: 1536,     distance: \"Cosine\",   },   quantization_config: {     binary: {       query_encoding: \"scalar8bits\",       always_ram: true,     },   }, }); ``` ```rust use qdrant_client::qdrant::{     BinaryQuantizationBuilder,     CreateCollectionBuilder,     Distance,     VectorParamsBuilder,     BinaryQuantizationQueryEncoding, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;  client     .create_collection(         CreateCollectionBuilder::new(\"{collection_name}\")             .vectors_config(VectorParamsBuilder::new(1536, Distance::Cosine))             .quantization_config(                 BinaryQuantizationBuilder::new(true)                     .query_encoding(BinaryQuantizationQueryEncoding::scalar8bits())             ),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.BinaryQuantization; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; import io.qdrant.client.grpc.Collections.BinaryQuantizationQueryEncoding;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client     .createCollectionAsync(         CreateCollection.newBuilder()             .setCollectionName(\"{collection_name}\")             .setVectorsConfig(                 VectorsConfig.newBuilder()                     .setParams(                         VectorParams.newBuilder()                             .setSize(1536)                             .setDistance(Distance.Cosine)                             .build())                     .build())             .setQuantizationConfig(                 QuantizationConfig.newBuilder()                     .setBinary(BinaryQuantization.newBuilder()                         .setQueryEncoding(BinaryQuantizationQueryEncoding                             .newBuilder()                             .setSetting(BinaryQuantizationQueryEncoding.Setting.Scalar8Bits)                             .build())                         .setAlwaysRam(true)                         .build())                     .build())             .build())     .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.CreateCollectionAsync(   collectionName: \"{collection_name}\",   vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine },   quantizationConfig: new QuantizationConfig   {     Binary = new BinaryQuantization {       QueryEncoding = new BinaryQuantizationQueryEncoding       {         Setting = BinaryQuantizationQueryEncoding.Types.Setting.Scalar8Bits,       },       AlwaysRam = true     }   } ); ``` ```go import (     \"context\"      \"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{     Host: \"localhost\",     Port: 6334, })  client.CreateCollection(context.Background(), &qdrant.CreateCollection{     CollectionName: \"{collection_name}\",     VectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{         Size:     1536,         Distance: qdrant.Distance_Cosine,     }),     QuantizationConfig: qdrant.NewQuantizationBinary(         &qdrant.BinaryQuantization{             QueryEncoding: qdrant.NewBinaryQuantizationQueryEncodingSetting(BinaryQuantizationQueryEncoding_Scalar8Bits),             AlwaysRam: qdrant.PtrOf(true),         },     ), }) ```",
    "metadata": {
      "chunk_id": "6453778d0049-0022",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Set up asymmetric quantization"
      ],
      "heading_text": "Set up asymmetric quantization",
      "token_count": 1099,
      "char_count": 5003,
      "start_char": 36753,
      "end_char": 41756,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6865131578947368,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.467690",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1099,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Set up asymmetric quantization",
      "chunk_hash": "decc2bd9fb937c0d",
      "content_digest": "decc2bd9fb937c0d",
      "chunk_length": 5003,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "distance",
          "import",
          "collection",
          "quantization",
          "new",
          "binaryquantizationqueryencoding",
          "query",
          "config",
          "scalar8bits",
          "collections",
          "name",
          "qdrantclient",
          "build",
          "grpc",
          "1536",
          "cosine",
          "true",
          "newbuilder"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.07772
          },
          {
            "term": "client",
            "tf": 28,
            "weight": 0.072539
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.031088
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.031088
          },
          {
            "term": "collection",
            "tf": 11,
            "weight": 0.028497
          },
          {
            "term": "quantization",
            "tf": 10,
            "weight": 0.025907
          },
          {
            "term": "new",
            "tf": 10,
            "weight": 0.025907
          },
          {
            "term": "binaryquantizationqueryencoding",
            "tf": 9,
            "weight": 0.023316
          },
          {
            "term": "query",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "scalar8bits",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "build",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.020725
          },
          {
            "term": "1536",
            "tf": 7,
            "weight": 0.018135
          },
          {
            "term": "cosine",
            "tf": 7,
            "weight": 0.018135
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.018135
          },
          {
            "term": "newbuilder",
            "tf": 7,
            "weight": 0.018135
          }
        ],
        "unique_terms": 107,
        "total_terms": 386
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Set up asymmetric quantization",
        "binaryquantizationqueryencoding",
        "client",
        "collection",
        "config",
        "distance",
        "import",
        "new",
        "qdrant",
        "quantization",
        "query"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6865131578947368,
      "overall": 0.7288377192982457
    }
  },
  {
    "text": "### Setting up Product Quantization  To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. When enabling product quantization on an existing collection, use a PATCH request or the corresponding `update_collection` method and omit the vector configuration, as its already defined. ```http PUT /collections/{collection_name} {     \"vectors\": {       \"size\": 768,       \"distance\": \"Cosine\"     },     \"quantization_config\": {         \"product\": {             \"compression\": \"x16\",             \"always_ram\": true         }     } } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.create_collection(     collection_name=\"{collection_name}\",     vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),     quantization_config=models.ProductQuantization(         product=models.ProductQuantizationConfig(             compression=models.CompressionRatio.X16,             always_ram=True,         ),     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.createCollection(\"{collection_name}\", {   vectors: {     size: 768,     distance: \"Cosine\",   },   quantization_config: {     product: {       compression: \"x16\",       always_ram: true,     },   }, }); ``` ```rust use qdrant_client::qdrant::{     CompressionRatio, CreateCollectionBuilder, Distance, ProductQuantizationBuilder,     VectorParamsBuilder, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;  client     .create_collection(         CreateCollectionBuilder::new(\"{collection_name}\")             .vectors_config(VectorParamsBuilder::new(768, Distance::Cosine))             .quantization_config(                 ProductQuantizationBuilder::new(CompressionRatio::X16.into()).always_ram(true),             ),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CompressionRatio; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.ProductQuantization; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client     .createCollectionAsync(         CreateCollection.newBuilder()             .setCollectionName(\"{collection_name}\")             .setVectorsConfig(                 VectorsConfig.newBuilder()                     .setParams(                         VectorParams.newBuilder()                             .setSize(768)                             .setDistance(Distance.Cosine)                             .build())                     .build())             .setQuantizationConfig(                 QuantizationConfig.newBuilder()                     .setProduct(                         ProductQuantization.newBuilder()                             .setCompression(CompressionRatio.x16)                             .setAlwaysRam(true)                             .build())                     .build())             .build())     .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.CreateCollectionAsync(  collectionName: \"{collection_name}\",  vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },  quantizationConfig: new QuantizationConfig  {   Product = new ProductQuantization { Compression = CompressionRatio.X16, AlwaysRam = true }  } ); ``` ```go import ( \t\"context\"  \t\"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{ \tHost: \"localhost\", \tPort: 6334, })  client.CreateCollection(context.Background(), &qdrant.CreateCollection{ \tCollectionName: \"{collection_name}\", \tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{ \t\tSize:     768, \t\tDistance: qdrant.Distance_Cosine, \t}), \tQuantizationConfig: qdrant.NewQuantizationProduct( \t\t&qdrant.ProductQuantization{ \t\t\tCompression: qdrant.CompressionRatio_x16, \t\t\tAlwaysRam:   qdrant.PtrOf(true), \t\t}, \t), }) ``` There are two parameters that you can specify in the `quantization_config` section:  `compression` - compression ratio. Compression ratio represents the size of the quantized vector in bytes divided by the size of the original vector in bytes. In this case, the quantized vector will be 16 times smaller than the original vector. `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.",
    "metadata": {
      "chunk_id": "6453778d0049-0023",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setting up Product Quantization"
      ],
      "heading_text": "Setting up Product Quantization",
      "token_count": 1130,
      "char_count": 5070,
      "start_char": 41765,
      "end_char": 46835,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6829081632653061,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.482252",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1130,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Setting up Product Quantization",
      "chunk_hash": "e68ded27658a0da3",
      "content_digest": "e68ded27658a0da3",
      "chunk_length": 5070,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "the",
          "collection",
          "distance",
          "import",
          "quantization",
          "config",
          "new",
          "collections",
          "name",
          "vectors",
          "compression",
          "ram",
          "true",
          "qdrantclient",
          "grpc",
          "product",
          "size",
          "768"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.066667
          },
          {
            "term": "client",
            "tf": 28,
            "weight": 0.062222
          },
          {
            "term": "the",
            "tf": 15,
            "weight": 0.033333
          },
          {
            "term": "collection",
            "tf": 13,
            "weight": 0.028889
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.026667
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.026667
          },
          {
            "term": "quantization",
            "tf": 10,
            "weight": 0.022222
          },
          {
            "term": "config",
            "tf": 9,
            "weight": 0.02
          },
          {
            "term": "new",
            "tf": 9,
            "weight": 0.02
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "vectors",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "compression",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "ram",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "true",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.017778
          },
          {
            "term": "product",
            "tf": 7,
            "weight": 0.015556
          },
          {
            "term": "size",
            "tf": 7,
            "weight": 0.015556
          },
          {
            "term": "768",
            "tf": 7,
            "weight": 0.015556
          }
        ],
        "unique_terms": 139,
        "total_terms": 450
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setting up Product Quantization",
        "client",
        "collection",
        "collections",
        "config",
        "distance",
        "import",
        "new",
        "qdrant",
        "quantization",
        "the"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6829081632653061,
      "overall": 0.7276360544217687
    }
  },
  {
    "text": "### Disabling Quantization  To disable quantization in an existing collection, you can do the following: ```http PATCH /collections/{collection_name} {     \"quantization_config\": \"Disabled\" } ``` ```bash curl -X PATCH http://localhost:6333/collections/{collection_name} \\   -H 'Content-Type: application/json' \\   --data-raw '{     \"quantization_config\": \"Disabled\"   }' ``` ```python client.update_collection(     collection_name=\"{collection_name}\",     quantization_config=models.Disabled.DISABLED, ) ``` ```typescript client.updateCollection(\"{collection_name}\", {     quantization_config: 'Disabled' }); ``` ```rust use qdrant_client::qdrant::{Disabled, UpdateCollectionBuilder};  client     .update_collection(UpdateCollectionBuilder::new(\"{collection_name}\").quantization_config(Disabled {}))     .await?; ``` ```java import io.qdrant.client.grpc.Collections.Disabled; import io.qdrant.client.grpc.Collections.QuantizationConfigDiff; import io.qdrant.client.grpc.Collections.UpdateCollection;  client.updateCollectionAsync(     UpdateCollection.newBuilder()         .setCollectionName(\"{collection_name}\")         .setQuantizationConfig(             QuantizationConfigDiff.newBuilder()                 .setDisabled(Disabled.getDefaultInstance())                 .build())         .build()); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.UpdateCollectionAsync( \tcollectionName: \"{collection_name}\", \tquantizationConfig: new QuantizationConfigDiff { Disabled = new Disabled() } ); ``` ```go import ( \t\"context\"  \t\"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{ \tHost: \"localhost\", \tPort: 6334, })  client.UpdateCollection(context.Background(), &qdrant.UpdateCollection{ \tCollectionName:     \"{collection_name}\", \tQuantizationConfig: qdrant.NewQuantizationDiffDisabled(), }) ```",
    "metadata": {
      "chunk_id": "6453778d0049-0024",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Disabling Quantization"
      ],
      "heading_text": "Disabling Quantization",
      "token_count": 435,
      "char_count": 1902,
      "start_char": 46847,
      "end_char": 48749,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.561452380952381,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.489286",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 435,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Disabling Quantization",
      "chunk_hash": "0ba4130abe4b72a7",
      "content_digest": "0ba4130abe4b72a7",
      "chunk_length": 1902,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "collection",
          "disabled",
          "name",
          "quantization",
          "config",
          "collections",
          "updatecollection",
          "new",
          "import",
          "grpc",
          "localhost",
          "quantizationconfigdiff",
          "http",
          "patch",
          "update",
          "updatecollectionbuilder",
          "await",
          "updatecollectionasync"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 15,
            "weight": 0.091463
          },
          {
            "term": "qdrant",
            "tf": 13,
            "weight": 0.079268
          },
          {
            "term": "collection",
            "tf": 12,
            "weight": 0.073171
          },
          {
            "term": "disabled",
            "tf": 11,
            "weight": 0.067073
          },
          {
            "term": "name",
            "tf": 9,
            "weight": 0.054878
          },
          {
            "term": "quantization",
            "tf": 7,
            "weight": 0.042683
          },
          {
            "term": "config",
            "tf": 6,
            "weight": 0.036585
          },
          {
            "term": "collections",
            "tf": 5,
            "weight": 0.030488
          },
          {
            "term": "updatecollection",
            "tf": 5,
            "weight": 0.030488
          },
          {
            "term": "new",
            "tf": 4,
            "weight": 0.02439
          },
          {
            "term": "import",
            "tf": 4,
            "weight": 0.02439
          },
          {
            "term": "grpc",
            "tf": 4,
            "weight": 0.02439
          },
          {
            "term": "localhost",
            "tf": 3,
            "weight": 0.018293
          },
          {
            "term": "quantizationconfigdiff",
            "tf": 3,
            "weight": 0.018293
          },
          {
            "term": "http",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "patch",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "update",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "updatecollectionbuilder",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "await",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "updatecollectionasync",
            "tf": 2,
            "weight": 0.012195
          }
        ],
        "unique_terms": 64,
        "total_terms": 164
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Disabling Quantization",
        "client",
        "collection",
        "collections",
        "config",
        "disabled",
        "name",
        "new",
        "qdrant",
        "quantization",
        "updatecollection"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.561452380952381,
      "overall": 0.6871507936507936
    }
  },
  {
    "text": "### Searching with Quantization  Once you have configured quantization for a collection, you dont need to do anything extra to search with quantization. Qdrant will automatically use quantized vectors if they are available. However, there are a few options that you can use to control the search process: ```http POST /collections/{collection_name}/points/query {     \"query\": [0.2, 0.1, 0.9, 0.7],     \"params\": {         \"quantization\": {             \"ignore\": false,             \"rescore\": true,             \"oversampling\": 2.0         }     },     \"limit\": 10 } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.query_points(     collection_name=\"{collection_name}\",     query=[0.2, 0.1, 0.9, 0.7],     search_params=models.SearchParams(         quantization=models.QuantizationSearchParams(             ignore=False,             rescore=True,             oversampling=2.0,         )     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.query(\"{collection_name}\", {     query: [0.2, 0.1, 0.9, 0.7],     params: {         quantization: {             ignore: false,             rescore: true,             oversampling: 2.0,         },     },     limit: 10, }); ``` ```rust use qdrant_client::qdrant::{     QuantizationSearchParamsBuilder, QueryPointsBuilder, SearchParamsBuilder, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;      client     .query(         QueryPointsBuilder::new(\"{collection_name}\")             .query(vec![0.2, 0.1, 0.9, 0.7])             .limit(10)             .params(                 SearchParamsBuilder::default().quantization(                     QuantizationSearchParamsBuilder::default()                         .ignore(false)                         .rescore(true)                         .oversampling(2.0),                 ),             ),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Points.QuantizationSearchParams; import io.qdrant.client.grpc.Points.QueryPoints; import io.qdrant.client.grpc.Points.SearchParams;  import static io.qdrant.client.QueryFactory.nearest;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client.queryAsync(         QueryPoints.newBuilder()                 .setCollectionName(\"{collection_name}\")                 .setQuery(nearest(0.2f, 0.1f, 0.9f, 0.7f))                 .setParams(                         SearchParams.newBuilder()                                 .setQuantization(                                         QuantizationSearchParams.newBuilder()                                                 .setIgnore(false)                                                 .setRescore(true)                                                 .setOversampling(2.0)                                                 .build())                                 .build())                 .setLimit(10)                 .build())         .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.QueryAsync( \tcollectionName: \"{collection_name}\", \tquery: new float[] { 0.2f, 0.1f, 0.9f, 0.7f }, \tsearchParams: new SearchParams \t{ \t\tQuantization = new QuantizationSearchParams \t\t{ \t\t\tIgnore = false, \t\t\tRescore = true, \t\t\tOversampling = 2.0 \t\t} \t}, \tlimit: 10 ); ``` ```go import ( \t\"context\"  \t\"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{ \tHost: \"localhost\", \tPort: 6334, })  client.Query(context.Background(), &qdrant.QueryPoints{ \tCollectionName: \"{collection_name}\", \tQuery:          qdrant.NewQuery(0.2, 0.1, 0.9, 0.7), \tParams: &qdrant.SearchParams{ \t\tQuantization: &qdrant.QuantizationSearchParams{ \t\t\tIgnore:       qdrant.PtrOf(false), \t\t\tRescore:      qdrant.PtrOf(true), \t\t\tOversampling: qdrant.PtrOf(2.0), \t\t}, \t}, }) ``` `ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available. `rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. This can improve the search quality, but may slightly decrease the search speed, compared to the search without rescore. It is recommended to disable rescore only if the original vectors are stored on a slow storage (e.g. HDD or network storage). By default, rescore is enabled. **Available as of v1.3.0**  `oversampling` - Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.",
    "metadata": {
      "chunk_id": "6453778d0049-0025",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Searching with Quantization"
      ],
      "heading_text": "Searching with Quantization",
      "token_count": 1273,
      "char_count": 5086,
      "start_char": 48759,
      "end_char": 53845,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8745575221238939,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.501096",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1273,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Searching with Quantization",
      "chunk_hash": "bbc44da8656b2b59",
      "content_digest": "bbc44da8656b2b59",
      "chunk_length": 5086,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "query",
          "search",
          "the",
          "rescore",
          "quantization",
          "collection",
          "vectors",
          "oversampling",
          "import",
          "name",
          "ignore",
          "false",
          "qdrantclient",
          "true",
          "new",
          "localhost",
          "searchparams",
          "using"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 29,
            "weight": 0.066514
          },
          {
            "term": "client",
            "tf": 25,
            "weight": 0.057339
          },
          {
            "term": "query",
            "tf": 12,
            "weight": 0.027523
          },
          {
            "term": "search",
            "tf": 10,
            "weight": 0.022936
          },
          {
            "term": "the",
            "tf": 10,
            "weight": 0.022936
          },
          {
            "term": "rescore",
            "tf": 10,
            "weight": 0.022936
          },
          {
            "term": "quantization",
            "tf": 9,
            "weight": 0.020642
          },
          {
            "term": "collection",
            "tf": 9,
            "weight": 0.020642
          },
          {
            "term": "vectors",
            "tf": 9,
            "weight": 0.020642
          },
          {
            "term": "oversampling",
            "tf": 9,
            "weight": 0.020642
          },
          {
            "term": "import",
            "tf": 9,
            "weight": 0.020642
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.018349
          },
          {
            "term": "ignore",
            "tf": 8,
            "weight": 0.018349
          },
          {
            "term": "false",
            "tf": 8,
            "weight": 0.018349
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.018349
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.016055
          },
          {
            "term": "new",
            "tf": 7,
            "weight": 0.016055
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.013761
          },
          {
            "term": "searchparams",
            "tf": 6,
            "weight": 0.013761
          },
          {
            "term": "using",
            "tf": 6,
            "weight": 0.013761
          }
        ],
        "unique_terms": 153,
        "total_terms": 436
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Searching with Quantization",
        "client",
        "collection",
        "oversampling",
        "qdrant",
        "quantization",
        "query",
        "rescore",
        "search",
        "the",
        "vectors"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8745575221238939,
      "overall": 0.7915191740412979
    }
  },
  {
    "text": "#### Accuracy tuning  In this section, we will discuss how to tune the search precision. The fastest way to understand the impact of quantization on the search quality is to compare the search results with and without quantization. In order to disable quantization, you can set `ignore` to `true` in the search request: ```http POST /collections/{collection_name}/points/query {     \"query\": [0.2, 0.1, 0.9, 0.7],     \"params\": {         \"quantization\": {             \"ignore\": true         }     },     \"limit\": 10 } ``` ```python from qdrant_client import QdrantClient, models  client = QdrantClient(url=\"http://localhost:6333\")  client.query_points(     collection_name=\"{collection_name}\",     query=[0.2, 0.1, 0.9, 0.7],     search_params=models.SearchParams(         quantization=models.QuantizationSearchParams(             ignore=True,         )     ), ) ``` ```typescript import { QdrantClient } from \"@qdrant/js-client-rest\";  const client = new QdrantClient({ host: \"localhost\", port: 6333 });  client.query(\"{collection_name}\", {     query: [0.2, 0.1, 0.9, 0.7],     params: {         quantization: {             ignore: true,         },     }, }); ``` ```rust use qdrant_client::qdrant::{     QuantizationSearchParamsBuilder, QueryPointsBuilder, SearchParamsBuilder, }; use qdrant_client::Qdrant;  let client = Qdrant::from_url(\"http://localhost:6334\").build()?;  client     .query(         QueryPointsBuilder::new(\"{collection_name}\")             .query(vec![0.2, 0.1, 0.9, 0.7])             .limit(3)             .params(                 SearchParamsBuilder::default()                     .quantization(QuantizationSearchParamsBuilder::default().ignore(true)),             ),     )     .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Points.QuantizationSearchParams; import io.qdrant.client.grpc.Points.QueryPoints; import io.qdrant.client.grpc.Points.SearchParams;  import static io.qdrant.client.QueryFactory.nearest;  QdrantClient client =     new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());  client.queryAsync(         QueryPoints.newBuilder()                 .setCollectionName(\"{collection_name}\")                 .setQuery(nearest(0.2f, 0.1f, 0.9f, 0.7f))                 .setParams(                         SearchParams.newBuilder()                                 .setQuantization(                                         QuantizationSearchParams.newBuilder().setIgnore(true).build())                                 .build())                 .setLimit(10)                 .build())         .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc;  var client = new QdrantClient(\"localhost\", 6334);  await client.QueryAsync( \tcollectionName: \"{collection_name}\", \tquery: new float[] { 0.2f, 0.1f, 0.9f, 0.7f }, \tsearchParams: new SearchParams \t{ \t\tQuantization = new QuantizationSearchParams { Ignore = true } \t}, \tlimit: 10 ); ``` ```go import ( \t\"context\"  \t\"github.com/qdrant/go-client/qdrant\" )  client, err := qdrant.NewClient(&qdrant.Config{ \tHost: \"localhost\", \tPort: 6334, })  client.Query(context.Background(), &qdrant.QueryPoints{ \tCollectionName: \"{collection_name}\", \tQuery:          qdrant.NewQuery(0.2, 0.1, 0.9, 0.7), \tParams: &qdrant.SearchParams{ \t\tQuantization: &qdrant.QuantizationSearchParams{ \t\t\tIgnore: qdrant.PtrOf(false), \t\t}, \t}, }) ``` - **Adjust the quantile parameter**: The quantile parameter in scalar quantization determines the quantization bounds. By setting it to a value lower than 1.0, you can exclude extreme values (outliers) from the quantization bounds. For example, if you set the quantile to 0.99, 1% of the extreme values will be excluded. By adjusting the quantile, you find an optimal value that will provide the best search quality for your collection. - **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.",
    "metadata": {
      "chunk_id": "6453778d0049-0027",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Accuracy tuning"
      ],
      "heading_text": "Accuracy tuning",
      "token_count": 1055,
      "char_count": 4077,
      "start_char": 53880,
      "end_char": 57957,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8809651474530831,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.514790",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 1055,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Accuracy tuning",
      "chunk_hash": "91fd7942d86f989e",
      "content_digest": "91fd7942d86f989e",
      "chunk_length": 4077,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "the",
          "quantization",
          "query",
          "collection",
          "import",
          "search",
          "name",
          "qdrantclient",
          "ignore",
          "true",
          "new",
          "localhost",
          "searchparams",
          "points",
          "params",
          "quantizationsearchparams",
          "build",
          "you"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 25,
            "weight": 0.06812
          },
          {
            "term": "client",
            "tf": 25,
            "weight": 0.06812
          },
          {
            "term": "the",
            "tf": 17,
            "weight": 0.046322
          },
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.032698
          },
          {
            "term": "query",
            "tf": 11,
            "weight": 0.029973
          },
          {
            "term": "collection",
            "tf": 9,
            "weight": 0.024523
          },
          {
            "term": "import",
            "tf": 9,
            "weight": 0.024523
          },
          {
            "term": "search",
            "tf": 8,
            "weight": 0.021798
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.021798
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.021798
          },
          {
            "term": "ignore",
            "tf": 7,
            "weight": 0.019074
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.019074
          },
          {
            "term": "new",
            "tf": 7,
            "weight": 0.019074
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.016349
          },
          {
            "term": "searchparams",
            "tf": 6,
            "weight": 0.016349
          },
          {
            "term": "points",
            "tf": 5,
            "weight": 0.013624
          },
          {
            "term": "params",
            "tf": 5,
            "weight": 0.013624
          },
          {
            "term": "quantizationsearchparams",
            "tf": 5,
            "weight": 0.013624
          },
          {
            "term": "build",
            "tf": 5,
            "weight": 0.013624
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.010899
          }
        ],
        "unique_terms": 136,
        "total_terms": 367
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Accuracy tuning",
        "client",
        "collection",
        "import",
        "name",
        "qdrant",
        "qdrantclient",
        "quantization",
        "query",
        "search",
        "the"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8809651474530831,
      "overall": 0.826988382484361
    }
  },
  {
    "text": "##### Was this page useful? Yes No  Thank you for your feedback!   We are sorry to hear that.  You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/guides/quantization.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue. On this page:  - [Quantization](#quantization.md)    - [Scalar Quantization](#scalar-quantization.md)    - [Binary Quantization](#binary-quantization.md)      - [Binary Quantization as Hamming Distance](#binary-quantization-as-hamming-distance.md)     - [1.5-Bit and 2-Bit Quantization](#15-bit-and-2-bit-quantization.md)     - [Asymmetric Quantization](#asymmetric-quantization.md)    - [Product Quantization](#product-quantization.md)    - [How to choose the right quantization method](#how-to-choose-the-right-quantization-method.md)    - [Setting up Quantization in Qdrant](#setting-up-quantization-in-qdrant.md)      - [Setting up Scalar Quantization](#setting-up-scalar-quantization.md)     - [Setting up Binary Quantization](#setting-up-binary-quantization.md)     - [Setting up Product Quantization](#setting-up-product-quantization.md)     - [Disabling Quantization](#disabling-quantization.md)     - [Searching with Quantization](#searching-with-quantization.md)    - [Quantization tips](#quantization-tips.md)     -  * [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/guides/quantization.md) * [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)",
    "metadata": {
      "chunk_id": "6453778d0049-0029",
      "source_file": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Was this page useful?"
      ],
      "heading_text": "Was this page useful?",
      "token_count": 420,
      "char_count": 1566,
      "start_char": 70996,
      "end_char": 72562,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7371111111111112,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "code_block",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1024,
      "processing_timestamp": "2025-10-20T18:30:32.562720",
      "model_aware_chunking": true,
      "within_token_limit": true,
      "estimated_tokens": 420,
      "document_id": "6453778d0049",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\Qdrant\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Was this page useful?",
      "chunk_hash": "51143f4e13e2f752",
      "content_digest": "51143f4e13e2f752",
      "chunk_length": 1566,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant"
      ],
      "target_model": "jina-code-embeddings-1.5b",
      "chunker_version": "v5_unified",
      "chunk_size_tokens": 26214,
      "chunk_overlap_tokens": 2621,
      "chunk_size_chars": 104856,
      "chunk_overlap_chars": 10484,
      "safety_margin": 0.8,
      "model_hf_id": "jinaai/jina-code-embeddings-1.5b",
      "model_max_tokens": 32768,
      "model_vector_dim": 1024,
      "recommended_batch_size": 16,
      "backend": "pytorch",
      "memory_efficient": true,
      "query_prefix": "Encode this code snippet for semantic retrieval: ",
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "qdrant",
          "setting",
          "page",
          "github",
          "landing",
          "binary",
          "https",
          "com",
          "choose",
          "scalar",
          "bit",
          "product",
          "this",
          "you",
          "edit",
          "tree",
          "master",
          "content",
          "documentation"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 32,
            "weight": 0.201258
          },
          {
            "term": "qdrant",
            "tf": 8,
            "weight": 0.050314
          },
          {
            "term": "setting",
            "tf": 8,
            "weight": 0.050314
          },
          {
            "term": "page",
            "tf": 7,
            "weight": 0.044025
          },
          {
            "term": "github",
            "tf": 7,
            "weight": 0.044025
          },
          {
            "term": "landing",
            "tf": 6,
            "weight": 0.037736
          },
          {
            "term": "binary",
            "tf": 6,
            "weight": 0.037736
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "com",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "choose",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "scalar",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "bit",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "product",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "this",
            "tf": 3,
            "weight": 0.018868
          },
          {
            "term": "you",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "edit",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "tree",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "master",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "content",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "documentation",
            "tf": 2,
            "weight": 0.012579
          }
        ],
        "unique_terms": 49,
        "total_terms": 159
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Was this page useful?",
        "binary",
        "choose",
        "com",
        "github",
        "https",
        "landing",
        "page",
        "qdrant",
        "quantization",
        "setting"
      ],
      "collection_name": "Qdrant"
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7371111111111112,
      "overall": 0.7790370370370371
    }
  }
]