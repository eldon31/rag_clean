[
  {
    "text": "## Training System Architecture\n\nThe `SparseEncoder` training system consists of specialized components that handle the unique requirements of sparse representation learning, including sparsity regularization and architecture-specific optimizations.\n\n```mermaid\ngraph TB\n    subgraph \"Training Components\"\n        SparseEncoderTrainer[\"SparseEncoderTrainer\"]\n        SparseEncoderTrainingArguments[\"SparseEncoderTrainingArguments\"]\n        DataCollator[\"DataCollator\"]\n    end\n    \n    subgraph \"Model Architectures\"\n        MLMTransformer[\"MLMTransformer\"]\n        SpladePooling[\"SpladePooling\"]\n        SparseAutoEncoder[\"SparseAutoEncoder\"]\n        SparseStaticEmbedding[\"SparseStaticEmbedding\"]\n        Router[\"Router\"]\n    end\n    \n    subgraph \"Loss Functions\"\n        SpladeLoss[\"SpladeLoss\"]\n        CSRLoss[\"CSRLoss\"]\n        FlopsLoss[\"FlopsLoss\"]\n        SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"]\n    end\n    \n    subgraph \"Evaluators\"\n        SparseNanoBEIREvaluator[\"SparseNanoBEIREvaluator\"]\n        SparseInformationRetrievalEvaluator[\"SparseInformationRetrievalEvaluator\"]\n        SparseEmbeddingSimilarityEvaluator[\"SparseEmbeddingSimilarityEvaluator\"]\n    end\n    \n    SparseEncoderTrainer --> MLMTransformer\n    SparseEncoderTrainer --> SpladePooling\n    SparseEncoderTrainer --> SparseAutoEncoder\n    SparseEncoderTrainer --> Router\n    \n    SparseEncoderTrainingArguments --> SparseEncoderTrainer\n    DataCollator --> SparseEncoderTrainer\n    \n    SpladeLoss --> SparseEncoderTrainer\n    CSRLoss --> SparseEncoderTrainer\n    FlopsLoss --> SpladeLoss\n    SparseMultipleNegativesRankingLoss --> SpladeLoss\n    SparseMultipleNegativesRankingLoss --> CSRLoss\n    \n    SparseNanoBEIREvaluator --> SparseEncoderTrainer\n    SparseInformationRetrievalEvaluator --> SparseEncoderTrainer\n    SparseEmbeddingSimilarityEvaluator --> SparseEncoderTrainer\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:17-46](), [sentence_transformers/sparse_encoder/__init__.py:1-14](), [sentence_transformers/sparse_encoder/losses/__init__.py:1-29]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0000",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training System Architecture"
      ],
      "heading_text": "Training System Architecture",
      "token_count": 452,
      "char_count": 2087,
      "start_char": 560,
      "end_char": 2647,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5318616822429906,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.763734",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Training System Architecture",
      "chunk_hash": "17db00c4dc34dd2c",
      "content_digest": "17db00c4dc34dd2c",
      "chunk_length": 2087,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparseencodertrainer",
          "spladeloss",
          "training",
          "sparse",
          "subgraph",
          "end",
          "csrloss",
          "sparsemultiplenegativesrankingloss",
          "sparseencodertrainingarguments",
          "datacollator",
          "mlmtransformer",
          "spladepooling",
          "sparseautoencoder",
          "router",
          "flopsloss",
          "sparsenanobeirevaluator",
          "sparseinformationretrievalevaluator",
          "sparseembeddingsimilarityevaluator",
          "encoder",
          "system"
        ],
        "term_weights": [
          {
            "term": "sparseencodertrainer",
            "tf": 13,
            "weight": 0.111111
          },
          {
            "term": "spladeloss",
            "tf": 5,
            "weight": 0.042735
          },
          {
            "term": "training",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "sparse",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "csrloss",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "sparsemultiplenegativesrankingloss",
            "tf": 4,
            "weight": 0.034188
          },
          {
            "term": "sparseencodertrainingarguments",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "datacollator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "mlmtransformer",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "spladepooling",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparseautoencoder",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "router",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "flopsloss",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparsenanobeirevaluator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparseinformationretrievalevaluator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "sparseembeddingsimilarityevaluator",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "encoder",
            "tf": 3,
            "weight": 0.025641
          },
          {
            "term": "system",
            "tf": 2,
            "weight": 0.017094
          }
        ],
        "unique_terms": 53,
        "total_terms": 117
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training System Architecture",
        "csrloss",
        "datacollator",
        "end",
        "sparse",
        "sparseencodertrainer",
        "sparseencodertrainingarguments",
        "sparsemultiplenegativesrankingloss",
        "spladeloss",
        "subgraph",
        "training"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5318616822429906,
      "overall": 0.7106205607476634
    }
  },
  {
    "text": "## Sparse Encoder Architectures\n\nThe training system supports three primary sparse encoder architectures, each requiring different components and training strategies.",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0001",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Sparse Encoder Architectures"
      ],
      "heading_text": "Sparse Encoder Architectures",
      "token_count": 24,
      "char_count": 166,
      "start_char": 2649,
      "end_char": 2815,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.545,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.763903",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Sparse Encoder Architectures",
      "chunk_hash": "d9d07f8c05898b6b",
      "content_digest": "d9d07f8c05898b6b",
      "chunk_length": 166,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparse",
          "encoder",
          "architectures",
          "training",
          "the",
          "system",
          "supports",
          "three",
          "primary",
          "each",
          "requiring",
          "different",
          "components",
          "and",
          "strategies"
        ],
        "term_weights": [
          {
            "term": "sparse",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "architectures",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.105263
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "supports",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "three",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "primary",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "each",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "requiring",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "different",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "components",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.052632
          },
          {
            "term": "strategies",
            "tf": 1,
            "weight": 0.052632
          }
        ],
        "unique_terms": 15,
        "total_terms": 19
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Sparse Encoder Architectures",
        "architectures",
        "each",
        "encoder",
        "primary",
        "sparse",
        "supports",
        "system",
        "the",
        "three",
        "training"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.545,
      "overall": 0.7483333333333334
    }
  },
  {
    "text": "### SPLADE Architecture\n\nSPLADE models use `MLMTransformer` followed by `SpladePooling` to create sparse lexical representations from masked language model logits.\n\n```mermaid\ngraph LR\n    Input[\"Text Input\"] --> MLMTransformer[\"MLMTransformer<br/>(BERT/RoBERTa/DistilBERT)\"]\n    MLMTransformer --> Logits[\"MLM Head Logits<br/>(vocab_size)\"]\n    Logits --> SpladePooling[\"SpladePooling<br/>(max/sum pooling)\"]\n    SpladePooling --> Activation[\"ReLU + log1p\"]\n    Activation --> SparseEmbedding[\"Sparse Embedding<br/>(vocab_size dimensions)\"]\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:59-98](), [sentence_transformers/sparse_encoder/models/MLMTransformer.py:26-55](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:13-40]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0002",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "SPLADE Architecture"
      ],
      "heading_text": "SPLADE Architecture",
      "token_count": 195,
      "char_count": 755,
      "start_char": 2817,
      "end_char": 3572,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5605882352941176,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.764369",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "SPLADE Architecture",
      "chunk_hash": "02149f5b346bcd9a",
      "content_digest": "02149f5b346bcd9a",
      "chunk_length": 755,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "mlmtransformer",
          "spladepooling",
          "sparse",
          "logits",
          "models",
          "encoder",
          "splade",
          "input",
          "vocab",
          "size",
          "activation",
          "sentence",
          "transformers",
          "architecture",
          "use",
          "followed",
          "create",
          "lexical",
          "representations",
          "from"
        ],
        "term_weights": [
          {
            "term": "mlmtransformer",
            "tf": 5,
            "weight": 0.072464
          },
          {
            "term": "spladepooling",
            "tf": 5,
            "weight": 0.072464
          },
          {
            "term": "sparse",
            "tf": 5,
            "weight": 0.072464
          },
          {
            "term": "logits",
            "tf": 4,
            "weight": 0.057971
          },
          {
            "term": "models",
            "tf": 3,
            "weight": 0.043478
          },
          {
            "term": "encoder",
            "tf": 3,
            "weight": 0.043478
          },
          {
            "term": "splade",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "vocab",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "size",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "activation",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "transformers",
            "tf": 2,
            "weight": 0.028986
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "use",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "followed",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "create",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "lexical",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "representations",
            "tf": 1,
            "weight": 0.014493
          },
          {
            "term": "from",
            "tf": 1,
            "weight": 0.014493
          }
        ],
        "unique_terms": 43,
        "total_terms": 69
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "SPLADE Architecture",
        "encoder",
        "input",
        "logits",
        "mlmtransformer",
        "models",
        "size",
        "sparse",
        "splade",
        "spladepooling",
        "vocab"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5605882352941176,
      "overall": 0.7201960784313725
    }
  },
  {
    "text": "### Inference-Free SPLADE Architecture\n\nThis architecture uses `Router` to process queries and documents differently, with lightweight `SparseStaticEmbedding` for queries and full SPLADE processing for documents.\n\n```mermaid\ngraph TB\n    subgraph \"Router Module\"\n        QueryRoute[\"Query Route\"]\n        DocumentRoute[\"Document Route\"]\n    end\n    \n    QueryInput[\"Query Text\"] --> QueryRoute\n    DocumentInput[\"Document Text\"] --> DocumentRoute\n    \n    QueryRoute --> SparseStaticEmbedding[\"SparseStaticEmbedding<br/>(Static Weights)\"]\n    DocumentRoute --> MLMTransformer[\"MLMTransformer\"]\n    MLMTransformer --> SpladePooling[\"SpladePooling\"]\n    \n    SparseStaticEmbedding --> QueryEmbedding[\"Query Sparse Embedding\"]\n    SpladePooling --> DocumentEmbedding[\"Document Sparse Embedding\"]\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:99-168](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py:24-62]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0003",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Inference-Free SPLADE Architecture"
      ],
      "heading_text": "Inference-Free SPLADE Architecture",
      "token_count": 206,
      "char_count": 942,
      "start_char": 3574,
      "end_char": 4516,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.515,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.764821",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Inference-Free SPLADE Architecture",
      "chunk_hash": "3d5527a4deaa645a",
      "content_digest": "3d5527a4deaa645a",
      "chunk_length": 942,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparsestaticembedding",
          "sparse",
          "queryroute",
          "query",
          "documentroute",
          "document",
          "mlmtransformer",
          "spladepooling",
          "splade",
          "architecture",
          "router",
          "queries",
          "and",
          "documents",
          "for",
          "route",
          "text",
          "embedding",
          "encoder",
          "inference"
        ],
        "term_weights": [
          {
            "term": "sparsestaticembedding",
            "tf": 5,
            "weight": 0.064103
          },
          {
            "term": "sparse",
            "tf": 4,
            "weight": 0.051282
          },
          {
            "term": "queryroute",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "documentroute",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "document",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "mlmtransformer",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "spladepooling",
            "tf": 3,
            "weight": 0.038462
          },
          {
            "term": "splade",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "architecture",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "router",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "documents",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "route",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "text",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.025641
          },
          {
            "term": "inference",
            "tf": 1,
            "weight": 0.012821
          }
        ],
        "unique_terms": 48,
        "total_terms": 78
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Inference-Free SPLADE Architecture",
        "architecture",
        "document",
        "documentroute",
        "mlmtransformer",
        "query",
        "queryroute",
        "sparse",
        "sparsestaticembedding",
        "splade",
        "spladepooling"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.515,
      "overall": 0.705
    }
  },
  {
    "text": "### CSR (Contrastive Sparse Representation) Architecture\n\nCSR models apply `SparseAutoEncoder` on top of dense sentence transformer embeddings to create sparse representations.\n\n```mermaid\ngraph LR\n    Input[\"Text Input\"] --> Transformer[\"Transformer<br/>(BERT/etc)\"]\n    Transformer --> Pooling[\"Pooling<br/>(mean/cls)\"]\n    Pooling --> DenseEmbedding[\"Dense Embedding\"]\n    DenseEmbedding --> SparseAutoEncoder[\"SparseAutoEncoder<br/>(k=256, k_aux=512)\"]\n    SparseAutoEncoder --> SparseEmbedding[\"Sparse Embedding<br/>(k dimensions)\"]\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:169-228](), [sentence_transformers/sparse_encoder/models/__init__.py:1-9]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0004",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "CSR (Contrastive Sparse Representation) Architecture"
      ],
      "heading_text": "CSR (Contrastive Sparse Representation) Architecture",
      "token_count": 165,
      "char_count": 673,
      "start_char": 4518,
      "end_char": 5191,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.54625,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.765237",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "CSR (Contrastive Sparse Representation) Architecture",
      "chunk_hash": "5e0429722f8e3808",
      "content_digest": "5e0429722f8e3808",
      "chunk_length": 673,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparse",
          "sparseautoencoder",
          "transformer",
          "pooling",
          "csr",
          "models",
          "dense",
          "sentence",
          "input",
          "denseembedding",
          "embedding",
          "encoder",
          "contrastive",
          "representation",
          "architecture",
          "apply",
          "top",
          "embeddings",
          "create",
          "representations"
        ],
        "term_weights": [
          {
            "term": "sparse",
            "tf": 5,
            "weight": 0.083333
          },
          {
            "term": "sparseautoencoder",
            "tf": 4,
            "weight": 0.066667
          },
          {
            "term": "transformer",
            "tf": 4,
            "weight": 0.066667
          },
          {
            "term": "pooling",
            "tf": 3,
            "weight": 0.05
          },
          {
            "term": "csr",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "dense",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "input",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "denseembedding",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "embedding",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.033333
          },
          {
            "term": "contrastive",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "representation",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "apply",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "top",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "embeddings",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "create",
            "tf": 1,
            "weight": 0.016667
          },
          {
            "term": "representations",
            "tf": 1,
            "weight": 0.016667
          }
        ],
        "unique_terms": 40,
        "total_terms": 60
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "CSR (Contrastive Sparse Representation) Architecture",
        "csr",
        "dense",
        "denseembedding",
        "input",
        "models",
        "pooling",
        "sentence",
        "sparse",
        "sparseautoencoder",
        "transformer"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.54625,
      "overall": 0.7154166666666666
    }
  },
  {
    "text": "### Loss Function Requirements\n\nSparse encoder training requires specialized loss functions that incorporate sparsity regularization:\n\n```mermaid\ngraph TB\n    subgraph \"Wrapper Losses\"\n        SpladeLoss[\"SpladeLoss<br/>(for SPLADE models)\"]\n        CSRLoss[\"CSRLoss<br/>(for CSR models)\"]\n    end\n    \n    subgraph \"Main Loss Functions\"\n        SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"]\n        SparseMarginMSELoss[\"SparseMarginMSELoss\"]\n        SparseDistillKLDivLoss[\"SparseDistillKLDivLoss\"]\n    end\n    \n    subgraph \"Regularization\"\n        FlopsLoss[\"FlopsLoss<br/>(default regularizer)\"]\n        CustomRegularizer[\"Custom Regularizer\"]\n    end\n    \n    subgraph \"Specialized Losses\"\n        CSRReconstructionLoss[\"CSRReconstructionLoss\"]\n        SparseMSELoss[\"SparseMSELoss<br/>(standalone distillation)\"]\n    end\n    \n    SpladeLoss --> SparseMultipleNegativesRankingLoss\n    SpladeLoss --> SparseMarginMSELoss\n    SpladeLoss --> SparseDistillKLDivLoss\n    SpladeLoss --> FlopsLoss\n    SpladeLoss --> CustomRegularizer\n    \n    CSRLoss --> SparseMultipleNegativesRankingLoss\n    CSRLoss --> CSRReconstructionLoss\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:346-393](), [docs/sparse_encoder/loss_overview.md:4-28](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-187]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0007",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Loss Function Requirements"
      ],
      "heading_text": "Loss Function Requirements",
      "token_count": 339,
      "char_count": 1339,
      "start_char": 5808,
      "end_char": 7147,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5148578947368421,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.766547",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Loss Function Requirements",
      "chunk_hash": "7a46d866266b175f",
      "content_digest": "7a46d866266b175f",
      "chunk_length": 1339,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "spladeloss",
          "csrloss",
          "loss",
          "sparse",
          "encoder",
          "subgraph",
          "end",
          "sparsemultiplenegativesrankingloss",
          "losses",
          "sparsemarginmseloss",
          "sparsedistillkldivloss",
          "flopsloss",
          "csrreconstructionloss",
          "training",
          "specialized",
          "functions",
          "regularization",
          "for",
          "models",
          "regularizer"
        ],
        "term_weights": [
          {
            "term": "spladeloss",
            "tf": 7,
            "weight": 0.072917
          },
          {
            "term": "csrloss",
            "tf": 5,
            "weight": 0.052083
          },
          {
            "term": "loss",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "sparse",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "encoder",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "subgraph",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "end",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "sparsemultiplenegativesrankingloss",
            "tf": 4,
            "weight": 0.041667
          },
          {
            "term": "losses",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "sparsemarginmseloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "sparsedistillkldivloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "flopsloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "csrreconstructionloss",
            "tf": 3,
            "weight": 0.03125
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "specialized",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "functions",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "regularization",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.020833
          },
          {
            "term": "regularizer",
            "tf": 2,
            "weight": 0.020833
          }
        ],
        "unique_terms": 47,
        "total_terms": 96
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Loss Function Requirements",
        "csrloss",
        "encoder",
        "end",
        "loss",
        "losses",
        "sparse",
        "sparsemarginmseloss",
        "sparsemultiplenegativesrankingloss",
        "spladeloss",
        "subgraph"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5148578947368421,
      "overall": 0.7049526315789473
    }
  },
  {
    "text": "### Training Arguments  `SparseEncoderTrainingArguments` extends standard training arguments with sparse-specific parameters:  | Parameter | Purpose | Example | |-----------|---------|---------| | `router_mapping` | Maps dataset columns to Router tasks | `{\"question\": \"query\", \"answer\": \"document\"}` | | `learning_rate_mapping` | Sets different learning rates per component | `{\"SparseStaticEmbedding.*\": 1e-3}` | | `batch_sampler` | Controls batch composition | `BatchSamplers.NO_DUPLICATES` | | `prompts` | Task-specific prompts | `{\"query\": \"Represent this query:\"}` |  **Sources:** [docs/sparse_encoder/training_overview.md:394-473](), [docs/sparse_encoder/training_overview.md:149-168]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0008",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training Arguments"
      ],
      "heading_text": "Training Arguments",
      "token_count": 167,
      "char_count": 693,
      "start_char": 7149,
      "end_char": 7842,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "table_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.766887",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Training Arguments",
      "chunk_hash": "3c76f20ce752fa73",
      "content_digest": "3c76f20ce752fa73",
      "chunk_length": 693,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "training",
          "sparse",
          "query",
          "arguments",
          "specific",
          "router",
          "mapping",
          "learning",
          "batch",
          "prompts",
          "docs",
          "encoder",
          "overview",
          "sparseencodertrainingarguments",
          "extends",
          "standard",
          "with",
          "parameters",
          "parameter",
          "purpose"
        ],
        "term_weights": [
          {
            "term": "training",
            "tf": 4,
            "weight": 0.061538
          },
          {
            "term": "sparse",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.046154
          },
          {
            "term": "arguments",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "specific",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "router",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "mapping",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "learning",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "batch",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "prompts",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "docs",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "overview",
            "tf": 2,
            "weight": 0.030769
          },
          {
            "term": "sparseencodertrainingarguments",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "extends",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "standard",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "parameters",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "parameter",
            "tf": 1,
            "weight": 0.015385
          },
          {
            "term": "purpose",
            "tf": 1,
            "weight": 0.015385
          }
        ],
        "unique_terms": 48,
        "total_terms": 65
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training Arguments",
        "arguments",
        "batch",
        "learning",
        "mapping",
        "prompts",
        "query",
        "router",
        "sparse",
        "specific",
        "training"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5,
      "overall": 0.7333333333333333
    }
  },
  {
    "text": "## Training Workflow\n\nThe complete training workflow integrates all components through the `SparseEncoderTrainer`:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant SparseEncoderTrainer\n    participant Model as \"SparseEncoder\"\n    participant Loss as \"SpladeLoss/CSRLoss\"\n    participant Evaluator\n    participant TrainingArgs as \"SparseEncoderTrainingArguments\"\n    \n    User->>Model: Initialize architecture\n    User->>Loss: Configure loss function\n    User->>TrainingArgs: Set training parameters\n    User->>Evaluator: Setup evaluation\n    User->>SparseEncoderTrainer: Create trainer\n    \n    SparseEncoderTrainer->>Model: Forward pass\n    Model->>Loss: Compute loss + regularization\n    Loss->>SparseEncoderTrainer: Return loss dict\n    SparseEncoderTrainer->>Model: Backward pass\n    \n    alt Evaluation Step\n        SparseEncoderTrainer->>Evaluator: Run evaluation\n        Evaluator->>Model: Generate embeddings\n        Evaluator->>SparseEncoderTrainer: Return metrics\n    end\n    \n    SparseEncoderTrainer->>User: Training complete\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:475-552]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0009",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Training Workflow"
      ],
      "heading_text": "Training Workflow",
      "token_count": 251,
      "char_count": 1124,
      "start_char": 7844,
      "end_char": 8968,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.515,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.767721",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Training Workflow",
      "chunk_hash": "25837c803eb46160",
      "content_digest": "25837c803eb46160",
      "chunk_length": 1124,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "sparseencodertrainer",
          "user",
          "loss",
          "participant",
          "model",
          "training",
          "evaluator",
          "evaluation",
          "workflow",
          "the",
          "complete",
          "trainingargs",
          "pass",
          "return",
          "integrates",
          "all",
          "components",
          "through",
          "mermaid",
          "sequencediagram"
        ],
        "term_weights": [
          {
            "term": "sparseencodertrainer",
            "tf": 9,
            "weight": 0.091837
          },
          {
            "term": "user",
            "tf": 7,
            "weight": 0.071429
          },
          {
            "term": "loss",
            "tf": 7,
            "weight": 0.071429
          },
          {
            "term": "participant",
            "tf": 6,
            "weight": 0.061224
          },
          {
            "term": "model",
            "tf": 6,
            "weight": 0.061224
          },
          {
            "term": "training",
            "tf": 5,
            "weight": 0.05102
          },
          {
            "term": "evaluator",
            "tf": 5,
            "weight": 0.05102
          },
          {
            "term": "evaluation",
            "tf": 3,
            "weight": 0.030612
          },
          {
            "term": "workflow",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "complete",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "trainingargs",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "pass",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "return",
            "tf": 2,
            "weight": 0.020408
          },
          {
            "term": "integrates",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "all",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "components",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "through",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "mermaid",
            "tf": 1,
            "weight": 0.010204
          },
          {
            "term": "sequencediagram",
            "tf": 1,
            "weight": 0.010204
          }
        ],
        "unique_terms": 52,
        "total_terms": 98
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Training Workflow",
        "evaluation",
        "evaluator",
        "loss",
        "model",
        "participant",
        "sparseencodertrainer",
        "the",
        "training",
        "user",
        "workflow"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.515,
      "overall": 0.705
    }
  },
  {
    "text": "### Dataset Format Requirements  Training datasets must match the loss function requirements:  | Loss Function | Input Columns | Label Column | Example | |---------------|---------------|--------------|---------| | `SparseMultipleNegativesRankingLoss` | `(anchor, positive)` or `(anchor, positive, negative)` | None | `[\"query\", \"answer\"]` | | `SparseCoSENTLoss` | `(sentence_A, sentence_B)` | `score` (0-1) | `[\"text1\", \"text2\", \"score\"]` | | `SparseMarginMSELoss` | `(query, positive, negative)` | `margin_scores` | `[\"query\", \"pos\", \"neg\", \"margins\"]` |  **Sources:** [docs/sparse_encoder/training_overview.md:328-344](), [docs/sparse_encoder/loss_overview.md:29-62]()",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0010",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Dataset Format Requirements"
      ],
      "heading_text": "Dataset Format Requirements",
      "token_count": 184,
      "char_count": 671,
      "start_char": 8970,
      "end_char": 9641,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6828571428571429,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "table_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.768042",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Dataset Format Requirements",
      "chunk_hash": "ea1aa474bb38aff3",
      "content_digest": "ea1aa474bb38aff3",
      "chunk_length": 671,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "loss",
          "positive",
          "query",
          "requirements",
          "training",
          "function",
          "anchor",
          "negative",
          "sentence",
          "score",
          "docs",
          "sparse",
          "encoder",
          "overview",
          "dataset",
          "format",
          "datasets",
          "must",
          "match",
          "the"
        ],
        "term_weights": [
          {
            "term": "loss",
            "tf": 3,
            "weight": 0.052632
          },
          {
            "term": "positive",
            "tf": 3,
            "weight": 0.052632
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.052632
          },
          {
            "term": "requirements",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "training",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "function",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "anchor",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "negative",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "sentence",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "score",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "docs",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "sparse",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "encoder",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "overview",
            "tf": 2,
            "weight": 0.035088
          },
          {
            "term": "dataset",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "format",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "datasets",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "must",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "match",
            "tf": 1,
            "weight": 0.017544
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.017544
          }
        ],
        "unique_terms": 40,
        "total_terms": 57
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Dataset Format Requirements",
        "anchor",
        "function",
        "loss",
        "negative",
        "positive",
        "query",
        "requirements",
        "score",
        "sentence",
        "training"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6828571428571429,
      "overall": 0.7942857142857142
    }
  },
  {
    "text": "### Router-Based Training\n\nWhen using `Router` modules, special configuration is required to map dataset columns to routing paths:\n\n```python",
    "metadata": {
      "chunk_id": "cd19be4e1d1f-0012",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "filename": "SparseEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Router-Based Training"
      ],
      "heading_text": "Router-Based Training",
      "token_count": 26,
      "char_count": 141,
      "start_char": 9674,
      "end_char": 9815,
      "semantic_score": 0.8,
      "structural_score": 0.7,
      "retrieval_quality": 0.5742105263157895,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:29.768347",
      "document_id": "cd19be4e1d1f",
      "document_name": "SparseEncoder_Training",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "source_filename": "SparseEncoder_Training.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers",
      "relative_path": "Docs\\sentence_transformers_docs\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "hierarchy_path": "Router-Based Training",
      "chunk_hash": "84461e1f09023ba6",
      "content_digest": "84461e1f09023ba6",
      "chunk_length": 141,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "router",
          "based",
          "training",
          "when",
          "using",
          "modules",
          "special",
          "configuration",
          "required",
          "map",
          "dataset",
          "columns",
          "routing",
          "paths",
          "python"
        ],
        "term_weights": [
          {
            "term": "router",
            "tf": 2,
            "weight": 0.125
          },
          {
            "term": "based",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "training",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "when",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "using",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "modules",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "special",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "configuration",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "required",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "map",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "dataset",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "columns",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "routing",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "paths",
            "tf": 1,
            "weight": 0.0625
          },
          {
            "term": "python",
            "tf": 1,
            "weight": 0.0625
          }
        ],
        "unique_terms": 15,
        "total_terms": 16
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Router-Based Training",
        "based",
        "configuration",
        "map",
        "modules",
        "required",
        "router",
        "special",
        "training",
        "using",
        "when"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.7,
      "retrieval_quality": 0.5742105263157895,
      "overall": 0.6914035087719298
    }
  }
]