[
  {
    "text": "### User Manual  [Concepts](https://qdrant.tech/documentation/concepts/)  - [Collections](https://qdrant.tech/documentation/concepts/collections/) - [Points](https://qdrant.tech/documentation/concepts/points/) - [Vectors](https://qdrant.tech/documentation/concepts/vectors/) - [Payload](https://qdrant.tech/documentation/concepts/payload/) - [Search](https://qdrant.tech/documentation/concepts/search/) - [Explore](https://qdrant.tech/documentation/concepts/explore/) - [Hybrid Queries](https://qdrant.tech/documentation/concepts/hybrid-queries/) - [Filtering](https://qdrant.tech/documentation/concepts/filtering/) - [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/) - [Storage](https://qdrant.tech/documentation/concepts/storage/) - [Indexing](https://qdrant.tech/documentation/concepts/indexing/) - [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/)  [Guides](https://qdrant.tech/documentation/guides/installation/)  - [Installation](https://qdrant.tech/documentation/guides/installation/) - [Administration](https://qdrant.tech/documentation/guides/administration/) - [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/) - [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/) - [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/) - [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) - [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/) - [Quantization](https://qdrant.tech/documentation/guides/quantization/) - [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/) - [Configuration](https://qdrant.tech/documentation/guides/configuration/) - [Security](https://qdrant.tech/documentation/guides/security/) - [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/) - [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/)",
    "metadata": {
      "chunk_id": "7d1033e04950-0001",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "User Manual"
      ],
      "heading_text": "User Manual",
      "token_count": 485,
      "char_count": 1968,
      "start_char": 1038,
      "end_char": 3006,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.262784",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "User Manual",
      "chunk_hash": "aa99483e94fddd20",
      "content_digest": "aa99483e94fddd20",
      "chunk_length": 1968,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "guides",
          "concepts",
          "installation",
          "collections",
          "points",
          "vectors",
          "payload",
          "search",
          "explore",
          "hybrid",
          "queries",
          "filtering",
          "optimizer",
          "storage",
          "indexing",
          "snapshots"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "qdrant",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "tech",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "documentation",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "guides",
            "tf": 15,
            "weight": 0.072816
          },
          {
            "term": "concepts",
            "tf": 14,
            "weight": 0.067961
          },
          {
            "term": "installation",
            "tf": 3,
            "weight": 0.014563
          },
          {
            "term": "collections",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "points",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "payload",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "explore",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "hybrid",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "filtering",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "optimizer",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "storage",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "indexing",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "snapshots",
            "tf": 2,
            "weight": 0.009709
          }
        ],
        "unique_terms": 45,
        "total_terms": 206
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "User Manual",
        "collections",
        "concepts",
        "documentation",
        "guides",
        "https",
        "installation",
        "points",
        "qdrant",
        "tech",
        "vectors"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "overall": 0.7776666666666666
    }
  },
  {
    "text": "### Tutorials  [Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/)  - [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/) - [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/) - [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/) - [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/)  [Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/)  - [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/) - [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/) - [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/) - [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/) - [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/)  [Using the Database](https://qdrant.tech/documentation/database-tutorials/)  - [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/) - [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/) - [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/) - [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/) - [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/) - [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/) - [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/)",
    "metadata": {
      "chunk_id": "7d1033e04950-0003",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Tutorials"
      ],
      "heading_text": "Tutorials",
      "token_count": 459,
      "char_count": 1988,
      "start_char": 3688,
      "end_char": 5676,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.264038",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Tutorials",
      "chunk_hash": "f480f0283a04b78a",
      "content_digest": "f480f0283a04b78a",
      "chunk_length": 1988,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "tutorials",
          "https",
          "tech",
          "documentation",
          "search",
          "database",
          "advanced",
          "beginner",
          "with",
          "hybrid",
          "retrieval",
          "build",
          "scale",
          "neural",
          "fastembed",
          "quality",
          "multivector",
          "representations",
          "using"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.098655
          },
          {
            "term": "tutorials",
            "tf": 20,
            "weight": 0.089686
          },
          {
            "term": "https",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "tech",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "documentation",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "search",
            "tf": 14,
            "weight": 0.06278
          },
          {
            "term": "database",
            "tf": 9,
            "weight": 0.040359
          },
          {
            "term": "advanced",
            "tf": 7,
            "weight": 0.03139
          },
          {
            "term": "beginner",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "hybrid",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "retrieval",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "build",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "scale",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "neural",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "fastembed",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "quality",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "multivector",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "representations",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "using",
            "tf": 2,
            "weight": 0.008969
          }
        ],
        "unique_terms": 64,
        "total_terms": 223
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Tutorials",
        "advanced",
        "beginner",
        "database",
        "documentation",
        "https",
        "qdrant",
        "search",
        "tech",
        "tutorials",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "overall": 0.7826697594501718
    }
  },
  {
    "text": "### Support  [FAQ](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)  - [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/) - [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/)  [Release Notes](https://github.com/qdrant/qdrant/releases)",
    "metadata": {
      "chunk_id": "7d1033e04950-0004",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Support"
      ],
      "heading_text": "Support",
      "token_count": 83,
      "char_count": 311,
      "start_char": 5678,
      "end_char": 5989,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5627272727272727,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.264444",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Support",
      "chunk_hash": "f059a5deb61e367d",
      "content_digest": "f059a5deb61e367d",
      "chunk_length": 311,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "faq",
          "https",
          "tech",
          "documentation",
          "fundamentals",
          "database",
          "optimization",
          "support",
          "release",
          "notes",
          "github",
          "com",
          "releases"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 8,
            "weight": 0.228571
          },
          {
            "term": "faq",
            "tf": 4,
            "weight": 0.114286
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.114286
          },
          {
            "term": "tech",
            "tf": 3,
            "weight": 0.085714
          },
          {
            "term": "documentation",
            "tf": 3,
            "weight": 0.085714
          },
          {
            "term": "fundamentals",
            "tf": 3,
            "weight": 0.085714
          },
          {
            "term": "database",
            "tf": 2,
            "weight": 0.057143
          },
          {
            "term": "optimization",
            "tf": 2,
            "weight": 0.057143
          },
          {
            "term": "support",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "release",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "notes",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "github",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "com",
            "tf": 1,
            "weight": 0.028571
          },
          {
            "term": "releases",
            "tf": 1,
            "weight": 0.028571
          }
        ],
        "unique_terms": 14,
        "total_terms": 35
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Support",
        "database",
        "documentation",
        "faq",
        "fundamentals",
        "https",
        "optimization",
        "qdrant",
        "release",
        "support",
        "tech"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5627272727272727,
      "overall": 0.7209090909090908
    }
  },
  {
    "text": "### User Manual  [Concepts](https://qdrant.tech/documentation/concepts/)  - [Collections](https://qdrant.tech/documentation/concepts/collections/) - [Points](https://qdrant.tech/documentation/concepts/points/) - [Vectors](https://qdrant.tech/documentation/concepts/vectors/) - [Payload](https://qdrant.tech/documentation/concepts/payload/) - [Search](https://qdrant.tech/documentation/concepts/search/) - [Explore](https://qdrant.tech/documentation/concepts/explore/) - [Hybrid Queries](https://qdrant.tech/documentation/concepts/hybrid-queries/) - [Filtering](https://qdrant.tech/documentation/concepts/filtering/) - [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/) - [Storage](https://qdrant.tech/documentation/concepts/storage/) - [Indexing](https://qdrant.tech/documentation/concepts/indexing/) - [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/)  [Guides](https://qdrant.tech/documentation/guides/installation/)  - [Installation](https://qdrant.tech/documentation/guides/installation/) - [Administration](https://qdrant.tech/documentation/guides/administration/) - [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/) - [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/) - [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/) - [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) - [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/) - [Quantization](https://qdrant.tech/documentation/guides/quantization/) - [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/) - [Configuration](https://qdrant.tech/documentation/guides/configuration/) - [Security](https://qdrant.tech/documentation/guides/security/) - [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/) - [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/)",
    "metadata": {
      "chunk_id": "7d1033e04950-0006",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "User Manual"
      ],
      "heading_text": "User Manual",
      "token_count": 485,
      "char_count": 1968,
      "start_char": 6366,
      "end_char": 8334,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.265331",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "User Manual",
      "chunk_hash": "aa99483e94fddd20",
      "content_digest": "aa99483e94fddd20",
      "chunk_length": 1968,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "guides",
          "concepts",
          "installation",
          "collections",
          "points",
          "vectors",
          "payload",
          "search",
          "explore",
          "hybrid",
          "queries",
          "filtering",
          "optimizer",
          "storage",
          "indexing",
          "snapshots"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "qdrant",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "tech",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "documentation",
            "tf": 27,
            "weight": 0.131068
          },
          {
            "term": "guides",
            "tf": 15,
            "weight": 0.072816
          },
          {
            "term": "concepts",
            "tf": 14,
            "weight": 0.067961
          },
          {
            "term": "installation",
            "tf": 3,
            "weight": 0.014563
          },
          {
            "term": "collections",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "points",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "payload",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "search",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "explore",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "hybrid",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "filtering",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "optimizer",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "storage",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "indexing",
            "tf": 2,
            "weight": 0.009709
          },
          {
            "term": "snapshots",
            "tf": 2,
            "weight": 0.009709
          }
        ],
        "unique_terms": 45,
        "total_terms": 206
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "User Manual",
        "collections",
        "concepts",
        "documentation",
        "guides",
        "https",
        "installation",
        "points",
        "qdrant",
        "tech",
        "vectors"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.733,
      "overall": 0.7776666666666666
    }
  },
  {
    "text": "### Tutorials  [Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/)  - [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/) - [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/) - [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/) - [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/)  [Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/)  - [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/) - [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/) - [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/) - [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/) - [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/)  [Using the Database](https://qdrant.tech/documentation/database-tutorials/)  - [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/) - [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/) - [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/) - [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/) - [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/) - [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/) - [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/)",
    "metadata": {
      "chunk_id": "7d1033e04950-0008",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Tutorials"
      ],
      "heading_text": "Tutorials",
      "token_count": 459,
      "char_count": 1988,
      "start_char": 9016,
      "end_char": 11004,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.266511",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Tutorials",
      "chunk_hash": "f480f0283a04b78a",
      "content_digest": "f480f0283a04b78a",
      "chunk_length": 1988,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "tutorials",
          "https",
          "tech",
          "documentation",
          "search",
          "database",
          "advanced",
          "beginner",
          "with",
          "hybrid",
          "retrieval",
          "build",
          "scale",
          "neural",
          "fastembed",
          "quality",
          "multivector",
          "representations",
          "using"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.098655
          },
          {
            "term": "tutorials",
            "tf": 20,
            "weight": 0.089686
          },
          {
            "term": "https",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "tech",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "documentation",
            "tf": 19,
            "weight": 0.085202
          },
          {
            "term": "search",
            "tf": 14,
            "weight": 0.06278
          },
          {
            "term": "database",
            "tf": 9,
            "weight": 0.040359
          },
          {
            "term": "advanced",
            "tf": 7,
            "weight": 0.03139
          },
          {
            "term": "beginner",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.022422
          },
          {
            "term": "hybrid",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "retrieval",
            "tf": 4,
            "weight": 0.017937
          },
          {
            "term": "build",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "scale",
            "tf": 3,
            "weight": 0.013453
          },
          {
            "term": "neural",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "fastembed",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "quality",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "multivector",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "representations",
            "tf": 2,
            "weight": 0.008969
          },
          {
            "term": "using",
            "tf": 2,
            "weight": 0.008969
          }
        ],
        "unique_terms": 64,
        "total_terms": 223
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Tutorials",
        "advanced",
        "beginner",
        "database",
        "documentation",
        "https",
        "qdrant",
        "search",
        "tech",
        "tutorials",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7480092783505154,
      "overall": 0.7826697594501718
    }
  },
  {
    "text": "### Support  [FAQ](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)  - [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/) - [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/)  [Release Notes](https://github.com/qdrant/qdrant/releases)  - [Documentation](https://qdrant.tech/documentation/) - - [Guides](https://qdrant.tech/documentation/guides/) - - Quantization",
    "metadata": {
      "chunk_id": "7d1033e04950-0009",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Support"
      ],
      "heading_text": "Support",
      "token_count": 117,
      "char_count": 439,
      "start_char": 11006,
      "end_char": 11445,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6952631578947369,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.266935",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Support",
      "chunk_hash": "ff177125622d8d98",
      "content_digest": "ff177125622d8d98",
      "chunk_length": 439,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "https",
          "documentation",
          "tech",
          "faq",
          "fundamentals",
          "database",
          "optimization",
          "guides",
          "support",
          "release",
          "notes",
          "github",
          "com",
          "releases",
          "quantization"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.212766
          },
          {
            "term": "https",
            "tf": 6,
            "weight": 0.12766
          },
          {
            "term": "documentation",
            "tf": 6,
            "weight": 0.12766
          },
          {
            "term": "tech",
            "tf": 5,
            "weight": 0.106383
          },
          {
            "term": "faq",
            "tf": 4,
            "weight": 0.085106
          },
          {
            "term": "fundamentals",
            "tf": 3,
            "weight": 0.06383
          },
          {
            "term": "database",
            "tf": 2,
            "weight": 0.042553
          },
          {
            "term": "optimization",
            "tf": 2,
            "weight": 0.042553
          },
          {
            "term": "guides",
            "tf": 2,
            "weight": 0.042553
          },
          {
            "term": "support",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "release",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "notes",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "github",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "com",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "releases",
            "tf": 1,
            "weight": 0.021277
          },
          {
            "term": "quantization",
            "tf": 1,
            "weight": 0.021277
          }
        ],
        "unique_terms": 16,
        "total_terms": 47
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Support",
        "database",
        "documentation",
        "faq",
        "fundamentals",
        "guides",
        "https",
        "optimization",
        "qdrant",
        "support",
        "tech"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6952631578947369,
      "overall": 0.7650877192982456
    }
  },
  {
    "text": "## Scalar Quantization\n\n*Available as of v1.1.0*\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8. In other words, Qdrant performs `float32 -> uint8` conversion for each vector component. Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process. Qdrant uses a special SIMD CPU instruction to perform fast vector comparison. This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.\n\nThe main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality. However, this error is usually negligible, and tends to be less significant for high-dimensional vectors. In our experiments, we found that the error introduced by scalar quantization is usually less than 1%.\n\nHowever, this value depends on the data and the quantization parameters. Please refer to the [Quantization Tips](#quantization-tips.md) section for more information on how to optimize the quantization parameters for your use case.",
    "metadata": {
      "chunk_id": "7d1033e04950-0011",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Scalar Quantization"
      ],
      "heading_text": "Scalar Quantization",
      "token_count": 312,
      "char_count": 1513,
      "start_char": 12697,
      "end_char": 14210,
      "semantic_score": 0.7,
      "structural_score": 0.9999999999999999,
      "retrieval_quality": 0.6591172413793104,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.268010",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Scalar Quantization",
      "chunk_hash": "f8b066880b27f298",
      "content_digest": "f8b066880b27f298",
      "chunk_length": 1513,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "quantization",
          "scalar",
          "vector",
          "for",
          "that",
          "qdrant",
          "this",
          "search",
          "uint8",
          "conversion",
          "error",
          "vectors",
          "reducing",
          "number",
          "bits",
          "used",
          "represent",
          "each",
          "component"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 17,
            "weight": 0.093923
          },
          {
            "term": "quantization",
            "tf": 10,
            "weight": 0.055249
          },
          {
            "term": "scalar",
            "tf": 6,
            "weight": 0.033149
          },
          {
            "term": "vector",
            "tf": 6,
            "weight": 0.033149
          },
          {
            "term": "for",
            "tf": 5,
            "weight": 0.027624
          },
          {
            "term": "that",
            "tf": 4,
            "weight": 0.022099
          },
          {
            "term": "qdrant",
            "tf": 4,
            "weight": 0.022099
          },
          {
            "term": "this",
            "tf": 4,
            "weight": 0.022099
          },
          {
            "term": "search",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "uint8",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "conversion",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "error",
            "tf": 3,
            "weight": 0.016575
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "reducing",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "number",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "bits",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "used",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "represent",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "each",
            "tf": 2,
            "weight": 0.01105
          },
          {
            "term": "component",
            "tf": 2,
            "weight": 0.01105
          }
        ],
        "unique_terms": 103,
        "total_terms": 181
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Scalar Quantization",
        "for",
        "qdrant",
        "quantization",
        "scalar",
        "search",
        "that",
        "the",
        "this",
        "uint8",
        "vector"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.9999999999999999,
      "retrieval_quality": 0.6591172413793104,
      "overall": 0.7863724137931034
    }
  },
  {
    "text": "### 1.5-Bit and 2-Bit Quantization  *Available as of v1.15.0*  **Binary quantization** storage can use **2 and 1.5 bits** per dimension, improving precision for smaller vectors. One-bit compression resulted in significant data loss and precision drops for vectors smaller than a thousand dimensions, often requiring expensive rescoring. 2-bit quantization offers 16X compression compared to 32X with one bit, improving performance for smaller vector dimensions. The 1.5-bit quantization compression offers 24X compression and intermediate accuracy. A major limitation of binary quantization is poor handling of values close to zero. 2-bit quantization addresses this by explicitly representing zeros using an efficient scoring mechanism. In the case of 1.5-bit quantization, the zero-bit is shared between two values, balancing the efficiency of binary quantization with the accuracy improvements of 2-bit quantization, especially when 2-bit BQ requires too much memory. In order to build 2-bit representation, Qdrant computes values distribution and then assigns bit values to 3 possible buckets:  - `-1` - 00 - `0` - 01 - `1` - 11  1.5-bit quantization is similar, but merges buckets of pairs of elements into a binary triptets  2-bit quantization  See how to set up 1.5-bit and 2-bit quantization in the [following section](#set-up-bit-depth.md).",
    "metadata": {
      "chunk_id": "7d1033e04950-0014",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "1.5-Bit and 2-Bit Quantization"
      ],
      "heading_text": "1.5-Bit and 2-Bit Quantization",
      "token_count": 314,
      "char_count": 1349,
      "start_char": 16695,
      "end_char": 18044,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6895846153846154,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.269869",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "1.5-Bit and 2-Bit Quantization",
      "chunk_hash": "6ccd865bd4530126",
      "content_digest": "6ccd865bd4530126",
      "chunk_length": 1349,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "bit",
          "quantization",
          "and",
          "the",
          "binary",
          "compression",
          "values",
          "for",
          "smaller",
          "improving",
          "precision",
          "vectors",
          "one",
          "dimensions",
          "offers",
          "with",
          "accuracy",
          "zero",
          "buckets",
          "set"
        ],
        "term_weights": [
          {
            "term": "bit",
            "tf": 18,
            "weight": 0.116129
          },
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.077419
          },
          {
            "term": "and",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "the",
            "tf": 6,
            "weight": 0.03871
          },
          {
            "term": "binary",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "compression",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "values",
            "tf": 4,
            "weight": 0.025806
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "smaller",
            "tf": 3,
            "weight": 0.019355
          },
          {
            "term": "improving",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "precision",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "one",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "dimensions",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "offers",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "accuracy",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "zero",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "buckets",
            "tf": 2,
            "weight": 0.012903
          },
          {
            "term": "set",
            "tf": 2,
            "weight": 0.012903
          }
        ],
        "unique_terms": 93,
        "total_terms": 155
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "1.5-Bit and 2-Bit Quantization",
        "and",
        "binary",
        "bit",
        "compression",
        "for",
        "improving",
        "quantization",
        "smaller",
        "the",
        "values"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6895846153846154,
      "overall": 0.7631948717948718
    }
  },
  {
    "text": "### Asymmetric Quantization\n\n*Available as of v1.15.0*\n\nThe **Asymmetric Quantization** technique allows qdrant to use different vector encoding algorithm for stored vectors and for queries. Particularly interesting combination is a Binary stored vectors and Scalar quantized queries.\n\nAsymmetric quantization\n\nThis approach maintains storage size and RAM usage similar to binary quantization while offering improved precision. It is beneficial for memory-constrained deployments, or where the bottleneck is disk I/O rather than CPU. This is particularly useful for indexing millions of vectors as it improves precision without sacrificing much because the limitation in such scenarios is disk speed, not CPU. This approach requires less rescoring for the same quality output.\n\nSee how to set up Asymmetric Quantization quantization in the [following section](#set-up-asymmetric-quantization.md)",
    "metadata": {
      "chunk_id": "7d1033e04950-0015",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Asymmetric Quantization"
      ],
      "heading_text": "Asymmetric Quantization",
      "token_count": 168,
      "char_count": 895,
      "start_char": 18048,
      "end_char": 18943,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.6974999999999999,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.270392",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Asymmetric Quantization",
      "chunk_hash": "3131cf846cd4513a",
      "content_digest": "3131cf846cd4513a",
      "chunk_length": 895,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "asymmetric",
          "the",
          "for",
          "vectors",
          "and",
          "this",
          "stored",
          "queries",
          "particularly",
          "binary",
          "approach",
          "precision",
          "disk",
          "cpu",
          "set",
          "available",
          "technique",
          "allows",
          "qdrant"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 7,
            "weight": 0.068627
          },
          {
            "term": "asymmetric",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "the",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "for",
            "tf": 5,
            "weight": 0.04902
          },
          {
            "term": "vectors",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "this",
            "tf": 3,
            "weight": 0.029412
          },
          {
            "term": "stored",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "queries",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "particularly",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "binary",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "approach",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "precision",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "disk",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "cpu",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "set",
            "tf": 2,
            "weight": 0.019608
          },
          {
            "term": "available",
            "tf": 1,
            "weight": 0.009804
          },
          {
            "term": "technique",
            "tf": 1,
            "weight": 0.009804
          },
          {
            "term": "allows",
            "tf": 1,
            "weight": 0.009804
          },
          {
            "term": "qdrant",
            "tf": 1,
            "weight": 0.009804
          }
        ],
        "unique_terms": 69,
        "total_terms": 102
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Asymmetric Quantization",
        "and",
        "asymmetric",
        "for",
        "particularly",
        "quantization",
        "queries",
        "stored",
        "the",
        "this",
        "vectors"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.6974999999999999,
      "overall": 0.7324999999999999
    }
  },
  {
    "text": "## Product Quantization\n\n*Available as of v1.2.0*\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into chunks and quantizing each segment individually. Each chunk is approximated by a centroid index that represents the original vector component. The positions of the centroids are determined through the utilization of a clustering algorithm such as k-means. For now, Qdrant uses only 256 centroids, so each centroid index can be represented by a single byte.\n\nProduct quantization can compress by a more prominent factor than a scalar one. But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization. Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\nPlease refer to the [Quantization Tips](#quantization-tips.md) section for more information on how to optimize the quantization parameters for your use case.",
    "metadata": {
      "chunk_id": "7d1033e04950-0016",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Product Quantization"
      ],
      "heading_text": "Product Quantization",
      "token_count": 201,
      "char_count": 1011,
      "start_char": 18945,
      "end_char": 19956,
      "semantic_score": 0.7,
      "structural_score": 0.9999999999999999,
      "retrieval_quality": 0.684701986754967,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.270796",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Product Quantization",
      "chunk_hash": "b459f6516339668f",
      "content_digest": "b459f6516339668f",
      "chunk_length": 1011,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "the",
          "product",
          "for",
          "each",
          "are",
          "vectors",
          "centroid",
          "index",
          "centroids",
          "only",
          "can",
          "more",
          "than",
          "scalar",
          "use",
          "tips",
          "available",
          "method",
          "compressing"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 9,
            "weight": 0.07563
          },
          {
            "term": "the",
            "tf": 6,
            "weight": 0.05042
          },
          {
            "term": "product",
            "tf": 5,
            "weight": 0.042017
          },
          {
            "term": "for",
            "tf": 4,
            "weight": 0.033613
          },
          {
            "term": "each",
            "tf": 3,
            "weight": 0.02521
          },
          {
            "term": "are",
            "tf": 3,
            "weight": 0.02521
          },
          {
            "term": "vectors",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "centroid",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "index",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "centroids",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "only",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "can",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "more",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "than",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "scalar",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "use",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "tips",
            "tf": 2,
            "weight": 0.016807
          },
          {
            "term": "available",
            "tf": 1,
            "weight": 0.008403
          },
          {
            "term": "method",
            "tf": 1,
            "weight": 0.008403
          },
          {
            "term": "compressing",
            "tf": 1,
            "weight": 0.008403
          }
        ],
        "unique_terms": 84,
        "total_terms": 119
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Product Quantization",
        "are",
        "centroid",
        "centroids",
        "each",
        "for",
        "index",
        "product",
        "quantization",
        "the",
        "vectors"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.9999999999999999,
      "retrieval_quality": 0.684701986754967,
      "overall": 0.7949006622516556
    }
  },
  {
    "text": "## How to choose the right quantization method  Here is a brief table of the pros and cons of each quantization method:  | Quantization method | Accuracy   | Speed     | Compression | | ------------------- | ---------- | --------- | ----------- | | Scalar              | 0.99       | up to x2  | 4           | | Product             | 0.7        | 0.5       | up to 64    | | Binary (1 bit)      | 0.95\\*     | up to x40 | 32          | | Binary (1.5 bit)    | 0.95\\*\\*   | up to x30 | 24          | | Binary (2 bit)      | 0.95\\*\\*\\* | up to x20 | 16          |  - `*` - for compatible models with high-dimensional vectors (approx. 1536+ dimensions)  - `**` - for compatible models with medium-dimensional vectors (approx. 1024-1536 dimensions)  - `***` - for compatible models with low-dimensional vectors (approx. 768-1024 dimensions)  - **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only. - If you are planning to use binary quantization with low or medium-dimensional vectors (approx. 512-1024 dimensions), it is recommended to use 1.5-bit or 2-bit quantization as well as asymmetric quantization feature. - **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable. - **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.",
    "metadata": {
      "chunk_id": "7d1033e04950-0017",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "How to choose the right quantization method"
      ],
      "heading_text": "How to choose the right quantization method",
      "token_count": 416,
      "char_count": 1707,
      "start_char": 19958,
      "end_char": 21665,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6653904059040591,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "table_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.271443",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "How to choose the right quantization method",
      "chunk_hash": "eb09fdf54f800e4a",
      "content_digest": "eb09fdf54f800e4a",
      "chunk_length": 1707,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "the",
          "binary",
          "method",
          "and",
          "bit",
          "with",
          "models",
          "dimensional",
          "vectors",
          "approx",
          "dimensions",
          "recommended",
          "accuracy",
          "speed",
          "compression",
          "scalar",
          "for",
          "compatible",
          "1024"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.074534
          },
          {
            "term": "the",
            "tf": 8,
            "weight": 0.049689
          },
          {
            "term": "binary",
            "tf": 6,
            "weight": 0.037267
          },
          {
            "term": "method",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "and",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "bit",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.031056
          },
          {
            "term": "models",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "dimensional",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "vectors",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "approx",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "dimensions",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "recommended",
            "tf": 4,
            "weight": 0.024845
          },
          {
            "term": "accuracy",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "speed",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "compression",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "scalar",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "compatible",
            "tf": 3,
            "weight": 0.018634
          },
          {
            "term": "1024",
            "tf": 3,
            "weight": 0.018634
          }
        ],
        "unique_terms": 80,
        "total_terms": 161
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "How to choose the right quantization method",
        "and",
        "binary",
        "bit",
        "dimensional",
        "method",
        "models",
        "quantization",
        "the",
        "vectors",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6653904059040591,
      "overall": 0.755130135301353
    }
  },
  {
    "text": "### Setting up Scalar Quantization\n\nTo enable scalar quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.\n\nWhen enabling scalar quantization on an existing collection, use a PATCH request or the corresponding `update_collection` method and omit the vector configuration, as it’s already defined.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\"\n    },\n    \"quantization_config\": {\n        \"scalar\": {\n            \"type\": \"int8\",\n            \"quantile\": 0.99,\n            \"always_ram\": true\n        }\n    }\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n    quantization_config=models.ScalarQuantization(\n        scalar=models.ScalarQuantizationConfig(\n            type=models.ScalarType.INT8,\n            quantile=0.99,\n            always_ram=True,\n        ),\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 768,\n    distance: \"Cosine\",\n  },\n  quantization_config: {\n    scalar: {\n      type: \"int8\",\n      quantile: 0.99,\n      always_ram: true,\n    },\n  },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    CreateCollectionBuilder, Distance, QuantizationType, ScalarQuantizationBuilder,\n    VectorParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(768, Distance::Cosine))\n            .quantization_config(\n                ScalarQuantizationBuilder::default()\n                    .r#type(QuantizationType::Int8.into())\n                    .quantile(0.99)\n                    .always_ram(true),\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\nimport io.qdrant.client.grpc.Collections.QuantizationType;\nimport io.qdrant.client.grpc.Collections.ScalarQuantization;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(768)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setQuantizationConfig(\n                QuantizationConfig.newBuilder()\n                    .setScalar(\n                        ScalarQuantization.newBuilder()\n                            .setType(QuantizationType.Int8)\n                            .setQuantile(0.99f)\n                            .setAlwaysRam(true)\n                            .build())\n                    .build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n collectionName: \"{collection_name}\",\n vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n quantizationConfig: new QuantizationConfig\n {\n  Scalar = new ScalarQuantization\n  {\n   Type = QuantizationType.Int8,\n   Quantile = 0.99f,\n   AlwaysRam = true\n  }\n }\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})",
    "metadata": {
      "chunk_id": "7d1033e04950-0019",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setting up Scalar Quantization"
      ],
      "heading_text": "Setting up Scalar Quantization",
      "token_count": 952,
      "char_count": 4239,
      "start_char": 22239,
      "end_char": 26478,
      "semantic_score": 0.6,
      "structural_score": 0.7,
      "retrieval_quality": 0.7032258064516128,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.275369",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Setting up Scalar Quantization",
      "chunk_hash": "1134c291a4045521",
      "content_digest": "1134c291a4045521",
      "chunk_length": 4239,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "collection",
          "import",
          "distance",
          "quantization",
          "config",
          "collections",
          "qdrantclient",
          "new",
          "grpc",
          "scalar",
          "name",
          "build",
          "768",
          "cosine",
          "int8",
          "true",
          "models",
          "localhost"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 27,
            "weight": 0.078717
          },
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.06414
          },
          {
            "term": "collection",
            "tf": 12,
            "weight": 0.034985
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.034985
          },
          {
            "term": "distance",
            "tf": 10,
            "weight": 0.029155
          },
          {
            "term": "quantization",
            "tf": 9,
            "weight": 0.026239
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.023324
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.023324
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.023324
          },
          {
            "term": "new",
            "tf": 8,
            "weight": 0.023324
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.023324
          },
          {
            "term": "scalar",
            "tf": 7,
            "weight": 0.020408
          },
          {
            "term": "name",
            "tf": 7,
            "weight": 0.020408
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.020408
          },
          {
            "term": "768",
            "tf": 6,
            "weight": 0.017493
          },
          {
            "term": "cosine",
            "tf": 6,
            "weight": 0.017493
          },
          {
            "term": "int8",
            "tf": 6,
            "weight": 0.017493
          },
          {
            "term": "true",
            "tf": 6,
            "weight": 0.017493
          },
          {
            "term": "models",
            "tf": 6,
            "weight": 0.017493
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.017493
          }
        ],
        "unique_terms": 105,
        "total_terms": 343
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setting up Scalar Quantization",
        "client",
        "collection",
        "collections",
        "config",
        "distance",
        "import",
        "new",
        "qdrant",
        "qdrantclient",
        "quantization"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7,
      "retrieval_quality": 0.7032258064516128,
      "overall": 0.6677419354838708
    }
  },
  {
    "text": "### Setting up Binary Quantization\n\nTo enable binary quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.\n\nWhen enabling binary quantization on an existing collection, use a PATCH request or the corresponding `update_collection` method and omit the vector configuration, as it’s already defined.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 1536,\n      \"distance\": \"Cosine\"\n    },\n    \"quantization_config\": {\n        \"binary\": {\n            \"always_ram\": true\n        }\n    }\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n    quantization_config=models.BinaryQuantization(\n        binary=models.BinaryQuantizationConfig(\n            always_ram=True,\n        ),\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 1536,\n    distance: \"Cosine\",\n  },\n  quantization_config: {\n    binary: {\n      always_ram: true,\n    },\n  },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    BinaryQuantizationBuilder, CreateCollectionBuilder, Distance, VectorParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(1536, Distance::Cosine))\n            .quantization_config(BinaryQuantizationBuilder::new(true)),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(1536)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setQuantizationConfig(\n                QuantizationConfig.newBuilder()\n                    .setBinary(BinaryQuantization.newBuilder().setAlwaysRam(true).build())\n                    .build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n collectionName: \"{collection_name}\",\n vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine },\n quantizationConfig: new QuantizationConfig\n {\n  Binary = new BinaryQuantization { AlwaysRam = true }\n }\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n\tCollectionName: \"{collection_name}\",\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n\t\tSize:     1536,\n\t\tDistance: qdrant.Distance_Cosine,\n\t}),\n\tQuantizationConfig: qdrant.NewQuantizationBinary(\n\t\t&qdrant.BinaryQuantization{\n\t\t\tAlwaysRam: qdrant.PtrOf(true),\n\t\t},\n\t),\n})\n```\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.",
    "metadata": {
      "chunk_id": "7d1033e04950-0021",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setting up Binary Quantization"
      ],
      "heading_text": "Setting up Binary Quantization",
      "token_count": 984,
      "char_count": 4293,
      "start_char": 27990,
      "end_char": 32283,
      "semantic_score": 0.6,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6894259818731118,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.276963",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Setting up Binary Quantization",
      "chunk_hash": "4f83ab96c33ed91c",
      "content_digest": "4f83ab96c33ed91c",
      "chunk_length": 4293,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "collection",
          "distance",
          "import",
          "quantization",
          "vectors",
          "new",
          "the",
          "config",
          "name",
          "ram",
          "true",
          "qdrantclient",
          "binary",
          "collections",
          "1536",
          "cosine",
          "build",
          "grpc"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 28,
            "weight": 0.072917
          },
          {
            "term": "client",
            "tf": 27,
            "weight": 0.070312
          },
          {
            "term": "collection",
            "tf": 13,
            "weight": 0.033854
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.03125
          },
          {
            "term": "import",
            "tf": 11,
            "weight": 0.028646
          },
          {
            "term": "quantization",
            "tf": 9,
            "weight": 0.023438
          },
          {
            "term": "vectors",
            "tf": 9,
            "weight": 0.023438
          },
          {
            "term": "new",
            "tf": 9,
            "weight": 0.023438
          },
          {
            "term": "the",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "ram",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "true",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.020833
          },
          {
            "term": "binary",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "collections",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "1536",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "cosine",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.018229
          },
          {
            "term": "grpc",
            "tf": 7,
            "weight": 0.018229
          }
        ],
        "unique_terms": 123,
        "total_terms": 384
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setting up Binary Quantization",
        "client",
        "collection",
        "config",
        "distance",
        "import",
        "new",
        "qdrant",
        "quantization",
        "the",
        "vectors"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6894259818731118,
      "overall": 0.7298086606243706
    }
  },
  {
    "text": "#### Set up bit depth\n\nTo enable 2bit or 1.5bit quantization, you need to specify `encoding` parameter in the `quantization_config` section of the collection configuration. Available values are `two_bits` and `one_and_half_bits`.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 1536,\n      \"distance\": \"Cosine\"\n    },\n    \"quantization_config\": {\n        \"binary\": {\n            \"encoding\": \"two_bits\",\n            \"always_ram\": true\n        }\n    }\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n    quantization_config=models.BinaryQuantization(\n        binary=models.BinaryQuantizationConfig(\n            encoding=models.BinaryQuantizationEncoding.TWO_BITS,\n            always_ram=True,\n        ),\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 1536,\n    distance: \"Cosine\",\n  },\n  quantization_config: {\n    binary: {\n      encoding: \"two_bits\",\n      always_ram: true,\n    },\n  },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    BinaryQuantizationBuilder,\n    CreateCollectionBuilder,\n    Distance,\n    VectorParamsBuilder,\n    BinaryQuantizationEncoding,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(1536, Distance::Cosine))\n            .quantization_config(BinaryQuantizationBuilder::new(true)\n                .encoding(BinaryQuantizationEncoding::TwoBits)\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\nimport io.qdrant.client.grpc.Collections.BinaryQuantizationEncoding;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(1536)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setQuantizationConfig(\n                QuantizationConfig.newBuilder()\n                    .setBinary(BinaryQuantization\n                        .newBuilder()\n                        .setEncoding(BinaryQuantizationEncoding.TwoBits)\n                        .setAlwaysRam(true)\n                        .build())\n                    .build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n  collectionName: \"{collection_name}\",\n  vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine },\n  quantizationConfig: new QuantizationConfig\n  {\n    Binary = new BinaryQuantization {\n      Encoding = BinaryQuantizationEncoding.TwoBits,\n      AlwaysRam = true\n    }\n  }\n);\n```\n\n```go\nimport (\n    \"context\"\n\n    \"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n    Host: \"localhost\",\n    Port: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n    CollectionName: \"{collection_name}\",\n    VectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n        Size:     1536,\n        Distance: qdrant.Distance_Cosine,\n    }),\n    QuantizationConfig: qdrant.NewQuantizationBinary(\n        &qdrant.BinaryQuantization{\n            Encoding: qdrant.BinaryQuantizationEncoding_TwoBits.Enum(),\n            AlwaysRam: qdrant.PtrOf(true),\n        },\n    ),\n})\n```",
    "metadata": {
      "chunk_id": "7d1033e04950-0022",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Set up bit depth"
      ],
      "heading_text": "Set up bit depth",
      "token_count": 1006,
      "char_count": 4466,
      "start_char": 32285,
      "end_char": 36751,
      "semantic_score": 0.6,
      "structural_score": 0.7,
      "retrieval_quality": 0.6988888888888889,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.278484",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Set up bit depth",
      "chunk_hash": "42b1ae989e17871c",
      "content_digest": "42b1ae989e17871c",
      "chunk_length": 4466,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "distance",
          "import",
          "collection",
          "new",
          "config",
          "collections",
          "name",
          "qdrantclient",
          "grpc",
          "encoding",
          "1536",
          "cosine",
          "true",
          "binaryquantizationencoding",
          "build",
          "quantization",
          "models",
          "localhost"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.083799
          },
          {
            "term": "client",
            "tf": 28,
            "weight": 0.078212
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.03352
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.03352
          },
          {
            "term": "collection",
            "tf": 11,
            "weight": 0.030726
          },
          {
            "term": "new",
            "tf": 9,
            "weight": 0.02514
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.022346
          },
          {
            "term": "encoding",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "1536",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "cosine",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "binaryquantizationencoding",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.019553
          },
          {
            "term": "quantization",
            "tf": 6,
            "weight": 0.01676
          },
          {
            "term": "models",
            "tf": 6,
            "weight": 0.01676
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.01676
          }
        ],
        "unique_terms": 103,
        "total_terms": 358
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Set up bit depth",
        "client",
        "collection",
        "collections",
        "config",
        "distance",
        "import",
        "name",
        "new",
        "qdrant",
        "qdrantclient"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7,
      "retrieval_quality": 0.6988888888888889,
      "overall": 0.6662962962962963
    }
  },
  {
    "text": "#### Set up asymmetric quantization\n\nTo enable asymmetric quantization, you need to specify `query_encoding` parameter in the `quantization_config` section of the collection configuration. Available values are:\n\n- `default` and `binary` - use regular binary quantization for the query.\n- `scalar8bits` - use 8bit quantization for the query.\n- `scalar4bits` - use 4bit quantization for the query.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 1536,\n      \"distance\": \"Cosine\"\n    },\n    \"quantization_config\": {\n        \"binary\": {\n            \"query_encoding\": \"scalar8bits\",\n            \"always_ram\": true\n        }\n    }\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n    quantization_config=models.BinaryQuantization(\n        binary=models.BinaryQuantizationConfig(\n            query_encoding=models.BinaryQuantizationQueryEncoding.SCALAR8BITS,\n            always_ram=True,\n        ),\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 1536,\n    distance: \"Cosine\",\n  },\n  quantization_config: {\n    binary: {\n      query_encoding: \"scalar8bits\",\n      always_ram: true,\n    },\n  },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    BinaryQuantizationBuilder,\n    CreateCollectionBuilder,\n    Distance,\n    VectorParamsBuilder,\n    BinaryQuantizationQueryEncoding,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(1536, Distance::Cosine))\n            .quantization_config(\n                BinaryQuantizationBuilder::new(true)\n                    .query_encoding(BinaryQuantizationQueryEncoding::scalar8bits())\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\nimport io.qdrant.client.grpc.Collections.BinaryQuantizationQueryEncoding;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(1536)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setQuantizationConfig(\n                QuantizationConfig.newBuilder()\n                    .setBinary(BinaryQuantization.newBuilder()\n                        .setQueryEncoding(BinaryQuantizationQueryEncoding\n                            .newBuilder()\n                            .setSetting(BinaryQuantizationQueryEncoding.Setting.Scalar8Bits)\n                            .build())\n                        .setAlwaysRam(true)\n                        .build())\n                    .build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n  collectionName: \"{collection_name}\",\n  vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine },\n  quantizationConfig: new QuantizationConfig\n  {\n    Binary = new BinaryQuantization {\n      QueryEncoding = new BinaryQuantizationQueryEncoding\n      {\n        Setting = BinaryQuantizationQueryEncoding.Types.Setting.Scalar8Bits,\n      },\n      AlwaysRam = true\n    }\n  }\n);\n```\n\n```go\nimport (\n    \"context\"\n\n    \"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n    Host: \"localhost\",\n    Port: 6334,\n})",
    "metadata": {
      "chunk_id": "7d1033e04950-0023",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Set up asymmetric quantization"
      ],
      "heading_text": "Set up asymmetric quantization",
      "token_count": 983,
      "char_count": 4477,
      "start_char": 36753,
      "end_char": 41230,
      "semantic_score": 0.6,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.6893617021276596,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.280723",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Set up asymmetric quantization",
      "chunk_hash": "a3633f238e1a222d",
      "content_digest": "a3633f238e1a222d",
      "chunk_length": 4477,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "import",
          "quantization",
          "collection",
          "distance",
          "new",
          "query",
          "config",
          "collections",
          "qdrantclient",
          "binaryquantizationqueryencoding",
          "build",
          "grpc",
          "scalar8bits",
          "name",
          "newbuilder",
          "binary",
          "1536",
          "cosine"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 27,
            "weight": 0.076705
          },
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.0625
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.034091
          },
          {
            "term": "quantization",
            "tf": 10,
            "weight": 0.028409
          },
          {
            "term": "collection",
            "tf": 10,
            "weight": 0.028409
          },
          {
            "term": "distance",
            "tf": 10,
            "weight": 0.028409
          },
          {
            "term": "new",
            "tf": 10,
            "weight": 0.028409
          },
          {
            "term": "query",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "config",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "binaryquantizationqueryencoding",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "build",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.022727
          },
          {
            "term": "scalar8bits",
            "tf": 7,
            "weight": 0.019886
          },
          {
            "term": "name",
            "tf": 7,
            "weight": 0.019886
          },
          {
            "term": "newbuilder",
            "tf": 7,
            "weight": 0.019886
          },
          {
            "term": "binary",
            "tf": 6,
            "weight": 0.017045
          },
          {
            "term": "1536",
            "tf": 6,
            "weight": 0.017045
          },
          {
            "term": "cosine",
            "tf": 6,
            "weight": 0.017045
          }
        ],
        "unique_terms": 102,
        "total_terms": 352
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": true,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Set up asymmetric quantization",
        "client",
        "collection",
        "collections",
        "config",
        "distance",
        "import",
        "new",
        "qdrant",
        "quantization",
        "query"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.6893617021276596,
      "overall": 0.6964539007092198
    }
  },
  {
    "text": "### Setting up Product Quantization\n\nTo enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.\n\nWhen enabling product quantization on an existing collection, use a PATCH request or the corresponding `update_collection` method and omit the vector configuration, as it’s already defined.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\"\n    },\n    \"quantization_config\": {\n        \"product\": {\n            \"compression\": \"x16\",\n            \"always_ram\": true\n        }\n    }\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n    quantization_config=models.ProductQuantization(\n        product=models.ProductQuantizationConfig(\n            compression=models.CompressionRatio.X16,\n            always_ram=True,\n        ),\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 768,\n    distance: \"Cosine\",\n  },\n  quantization_config: {\n    product: {\n      compression: \"x16\",\n      always_ram: true,\n    },\n  },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    CompressionRatio, CreateCollectionBuilder, Distance, ProductQuantizationBuilder,\n    VectorParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(768, Distance::Cosine))\n            .quantization_config(\n                ProductQuantizationBuilder::new(CompressionRatio::X16.into()).always_ram(true),\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CompressionRatio;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.ProductQuantization;\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(768)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setQuantizationConfig(\n                QuantizationConfig.newBuilder()\n                    .setProduct(\n                        ProductQuantization.newBuilder()\n                            .setCompression(CompressionRatio.x16)\n                            .setAlwaysRam(true)\n                            .build())\n                    .build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n collectionName: \"{collection_name}\",\n vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n quantizationConfig: new QuantizationConfig\n {\n  Product = new ProductQuantization { Compression = CompressionRatio.X16, AlwaysRam = true }\n }\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n\tCollectionName: \"{collection_name}\",\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n\t\tSize:     768,\n\t\tDistance: qdrant.Distance_Cosine,\n\t}),\n\tQuantizationConfig: qdrant.NewQuantizationProduct(\n\t\t&qdrant.ProductQuantization{\n\t\t\tCompression: qdrant.CompressionRatio_x16,\n\t\t\tAlwaysRam:   qdrant.PtrOf(true),\n\t\t},\n\t),\n})\n```\n\nThere are two parameters that you can specify in the `quantization_config` section:",
    "metadata": {
      "chunk_id": "7d1033e04950-0025",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setting up Product Quantization"
      ],
      "heading_text": "Setting up Product Quantization",
      "token_count": 1005,
      "char_count": 4536,
      "start_char": 41765,
      "end_char": 46301,
      "semantic_score": 0.6,
      "structural_score": 0.7,
      "retrieval_quality": 0.6943521594684385,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.283889",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Setting up Product Quantization",
      "chunk_hash": "4a425b2afbe8e173",
      "content_digest": "4a425b2afbe8e173",
      "chunk_length": 4536,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "client",
          "collection",
          "distance",
          "import",
          "quantization",
          "config",
          "new",
          "collections",
          "name",
          "qdrantclient",
          "grpc",
          "product",
          "768",
          "cosine",
          "x16",
          "true",
          "compressionratio",
          "build",
          "the"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.079156
          },
          {
            "term": "client",
            "tf": 28,
            "weight": 0.073879
          },
          {
            "term": "collection",
            "tf": 13,
            "weight": 0.034301
          },
          {
            "term": "distance",
            "tf": 12,
            "weight": 0.031662
          },
          {
            "term": "import",
            "tf": 12,
            "weight": 0.031662
          },
          {
            "term": "quantization",
            "tf": 10,
            "weight": 0.026385
          },
          {
            "term": "config",
            "tf": 9,
            "weight": 0.023747
          },
          {
            "term": "new",
            "tf": 9,
            "weight": 0.023747
          },
          {
            "term": "collections",
            "tf": 8,
            "weight": 0.021108
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.021108
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.021108
          },
          {
            "term": "grpc",
            "tf": 8,
            "weight": 0.021108
          },
          {
            "term": "product",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "768",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "cosine",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "x16",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "compressionratio",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.01847
          },
          {
            "term": "the",
            "tf": 6,
            "weight": 0.015831
          }
        ],
        "unique_terms": 109,
        "total_terms": 379
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setting up Product Quantization",
        "client",
        "collection",
        "collections",
        "config",
        "distance",
        "import",
        "name",
        "new",
        "qdrant",
        "quantization"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7,
      "retrieval_quality": 0.6943521594684385,
      "overall": 0.6647840531561461
    }
  },
  {
    "text": "### Disabling Quantization\n\nTo disable quantization in an existing collection, you can do the following:\n\n```http\nPATCH /collections/{collection_name}\n{\n    \"quantization_config\": \"Disabled\"\n}\n```\n\n```bash\ncurl -X PATCH http://localhost:6333/collections/{collection_name} \\\n  -H 'Content-Type: application/json' \\\n  --data-raw '{\n    \"quantization_config\": \"Disabled\"\n  }'\n```\n\n```python\nclient.update_collection(\n    collection_name=\"{collection_name}\",\n    quantization_config=models.Disabled.DISABLED,\n)\n```\n\n```typescript\nclient.updateCollection(\"{collection_name}\", {\n    quantization_config: 'Disabled'\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{Disabled, UpdateCollectionBuilder};\n\nclient\n    .update_collection(UpdateCollectionBuilder::new(\"{collection_name}\").quantization_config(Disabled {}))\n    .await?;\n```\n\n```java\nimport io.qdrant.client.grpc.Collections.Disabled;\nimport io.qdrant.client.grpc.Collections.QuantizationConfigDiff;\nimport io.qdrant.client.grpc.Collections.UpdateCollection;\n\nclient.updateCollectionAsync(\n    UpdateCollection.newBuilder()\n        .setCollectionName(\"{collection_name}\")\n        .setQuantizationConfig(\n            QuantizationConfigDiff.newBuilder()\n                .setDisabled(Disabled.getDefaultInstance())\n                .build())\n        .build());\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.UpdateCollectionAsync(\n\tcollectionName: \"{collection_name}\",\n\tquantizationConfig: new QuantizationConfigDiff { Disabled = new Disabled() }\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.UpdateCollection(context.Background(), &qdrant.UpdateCollection{\n\tCollectionName:     \"{collection_name}\",\n\tQuantizationConfig: qdrant.NewQuantizationDiffDisabled(),\n})\n```",
    "metadata": {
      "chunk_id": "7d1033e04950-0027",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Disabling Quantization"
      ],
      "heading_text": "Disabling Quantization",
      "token_count": 437,
      "char_count": 1910,
      "start_char": 46847,
      "end_char": 48757,
      "semantic_score": 0.6,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.562052380952381,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.284713",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Disabling Quantization",
      "chunk_hash": "d41c9c8c3d430490",
      "content_digest": "d41c9c8c3d430490",
      "chunk_length": 1910,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "collection",
          "disabled",
          "name",
          "quantization",
          "config",
          "collections",
          "updatecollection",
          "new",
          "import",
          "grpc",
          "localhost",
          "quantizationconfigdiff",
          "http",
          "patch",
          "update",
          "updatecollectionbuilder",
          "await",
          "updatecollectionasync"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 15,
            "weight": 0.091463
          },
          {
            "term": "qdrant",
            "tf": 13,
            "weight": 0.079268
          },
          {
            "term": "collection",
            "tf": 12,
            "weight": 0.073171
          },
          {
            "term": "disabled",
            "tf": 11,
            "weight": 0.067073
          },
          {
            "term": "name",
            "tf": 9,
            "weight": 0.054878
          },
          {
            "term": "quantization",
            "tf": 7,
            "weight": 0.042683
          },
          {
            "term": "config",
            "tf": 6,
            "weight": 0.036585
          },
          {
            "term": "collections",
            "tf": 5,
            "weight": 0.030488
          },
          {
            "term": "updatecollection",
            "tf": 5,
            "weight": 0.030488
          },
          {
            "term": "new",
            "tf": 4,
            "weight": 0.02439
          },
          {
            "term": "import",
            "tf": 4,
            "weight": 0.02439
          },
          {
            "term": "grpc",
            "tf": 4,
            "weight": 0.02439
          },
          {
            "term": "localhost",
            "tf": 3,
            "weight": 0.018293
          },
          {
            "term": "quantizationconfigdiff",
            "tf": 3,
            "weight": 0.018293
          },
          {
            "term": "http",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "patch",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "update",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "updatecollectionbuilder",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "await",
            "tf": 2,
            "weight": 0.012195
          },
          {
            "term": "updatecollectionasync",
            "tf": 2,
            "weight": 0.012195
          }
        ],
        "unique_terms": 64,
        "total_terms": 164
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Disabling Quantization",
        "client",
        "collection",
        "collections",
        "config",
        "disabled",
        "name",
        "new",
        "qdrant",
        "quantization",
        "updatecollection"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.562052380952381,
      "overall": 0.6540174603174603
    }
  },
  {
    "text": "### Searching with Quantization\n\nOnce you have configured quantization for a collection, you don’t need to do anything extra to search with quantization. Qdrant will automatically use quantized vectors if they are available.\n\nHowever, there are a few options that you can use to control the search process:\n\n```http\nPOST /collections/{collection_name}/points/query\n{\n    \"query\": [0.2, 0.1, 0.9, 0.7],\n    \"params\": {\n        \"quantization\": {\n            \"ignore\": false,\n            \"rescore\": true,\n            \"oversampling\": 2.0\n        }\n    },\n    \"limit\": 10\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.query_points(\n    collection_name=\"{collection_name}\",\n    query=[0.2, 0.1, 0.9, 0.7],\n    search_params=models.SearchParams(\n        quantization=models.QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.query(\"{collection_name}\", {\n    query: [0.2, 0.1, 0.9, 0.7],\n    params: {\n        quantization: {\n            ignore: false,\n            rescore: true,\n            oversampling: 2.0,\n        },\n    },\n    limit: 10,\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    QuantizationSearchParamsBuilder, QueryPointsBuilder, SearchParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n    \nclient\n    .query(\n        QueryPointsBuilder::new(\"{collection_name}\")\n            .query(vec![0.2, 0.1, 0.9, 0.7])\n            .limit(10)\n            .params(\n                SearchParamsBuilder::default().quantization(\n                    QuantizationSearchParamsBuilder::default()\n                        .ignore(false)\n                        .rescore(true)\n                        .oversampling(2.0),\n                ),\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Points.QuantizationSearchParams;\nimport io.qdrant.client.grpc.Points.QueryPoints;\nimport io.qdrant.client.grpc.Points.SearchParams;\n\nimport static io.qdrant.client.QueryFactory.nearest;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient.queryAsync(\n        QueryPoints.newBuilder()\n                .setCollectionName(\"{collection_name}\")\n                .setQuery(nearest(0.2f, 0.1f, 0.9f, 0.7f))\n                .setParams(\n                        SearchParams.newBuilder()\n                                .setQuantization(\n                                        QuantizationSearchParams.newBuilder()\n                                                .setIgnore(false)\n                                                .setRescore(true)\n                                                .setOversampling(2.0)\n                                                .build())\n                                .build())\n                .setLimit(10)\n                .build())\n        .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.QueryAsync(\n\tcollectionName: \"{collection_name}\",\n\tquery: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },\n\tsearchParams: new SearchParams\n\t{\n\t\tQuantization = new QuantizationSearchParams\n\t\t{\n\t\t\tIgnore = false,\n\t\t\tRescore = true,\n\t\t\tOversampling = 2.0\n\t\t}\n\t},\n\tlimit: 10\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})",
    "metadata": {
      "chunk_id": "7d1033e04950-0028",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Searching with Quantization"
      ],
      "heading_text": "Searching with Quantization",
      "token_count": 926,
      "char_count": 3738,
      "start_char": 48759,
      "end_char": 52497,
      "semantic_score": 0.6,
      "structural_score": 0.7,
      "retrieval_quality": 0.6956043956043956,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.288667",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Searching with Quantization",
      "chunk_hash": "50ed2373696f79cc",
      "content_digest": "50ed2373696f79cc",
      "chunk_length": 3738,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "query",
          "import",
          "quantization",
          "collection",
          "qdrantclient",
          "name",
          "false",
          "new",
          "true",
          "localhost",
          "points",
          "ignore",
          "rescore",
          "oversampling",
          "searchparams",
          "build",
          "use",
          "params"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 24,
            "weight": 0.086022
          },
          {
            "term": "qdrant",
            "tf": 20,
            "weight": 0.071685
          },
          {
            "term": "query",
            "tf": 9,
            "weight": 0.032258
          },
          {
            "term": "import",
            "tf": 9,
            "weight": 0.032258
          },
          {
            "term": "quantization",
            "tf": 8,
            "weight": 0.028674
          },
          {
            "term": "collection",
            "tf": 8,
            "weight": 0.028674
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.028674
          },
          {
            "term": "name",
            "tf": 7,
            "weight": 0.02509
          },
          {
            "term": "false",
            "tf": 7,
            "weight": 0.02509
          },
          {
            "term": "new",
            "tf": 7,
            "weight": 0.02509
          },
          {
            "term": "true",
            "tf": 6,
            "weight": 0.021505
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.021505
          },
          {
            "term": "points",
            "tf": 5,
            "weight": 0.017921
          },
          {
            "term": "ignore",
            "tf": 5,
            "weight": 0.017921
          },
          {
            "term": "rescore",
            "tf": 5,
            "weight": 0.017921
          },
          {
            "term": "oversampling",
            "tf": 5,
            "weight": 0.017921
          },
          {
            "term": "searchparams",
            "tf": 5,
            "weight": 0.017921
          },
          {
            "term": "build",
            "tf": 5,
            "weight": 0.017921
          },
          {
            "term": "use",
            "tf": 4,
            "weight": 0.014337
          },
          {
            "term": "params",
            "tf": 4,
            "weight": 0.014337
          }
        ],
        "unique_terms": 101,
        "total_terms": 279
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Searching with Quantization",
        "client",
        "collection",
        "false",
        "import",
        "name",
        "new",
        "qdrant",
        "qdrantclient",
        "quantization",
        "query"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7,
      "retrieval_quality": 0.6956043956043956,
      "overall": 0.6652014652014652
    }
  },
  {
    "text": "#### Accuracy tuning\n\nIn this section, we will discuss how to tune the search precision. The fastest way to understand the impact of quantization on the search quality is to compare the search results with and without quantization.\n\nIn order to disable quantization, you can set `ignore` to `true` in the search request:\n\n```http\nPOST /collections/{collection_name}/points/query\n{\n    \"query\": [0.2, 0.1, 0.9, 0.7],\n    \"params\": {\n        \"quantization\": {\n            \"ignore\": true\n        }\n    },\n    \"limit\": 10\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.query_points(\n    collection_name=\"{collection_name}\",\n    query=[0.2, 0.1, 0.9, 0.7],\n    search_params=models.SearchParams(\n        quantization=models.QuantizationSearchParams(\n            ignore=True,\n        )\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.query(\"{collection_name}\", {\n    query: [0.2, 0.1, 0.9, 0.7],\n    params: {\n        quantization: {\n            ignore: true,\n        },\n    },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    QuantizationSearchParamsBuilder, QueryPointsBuilder, SearchParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .query(\n        QueryPointsBuilder::new(\"{collection_name}\")\n            .query(vec![0.2, 0.1, 0.9, 0.7])\n            .limit(3)\n            .params(\n                SearchParamsBuilder::default()\n                    .quantization(QuantizationSearchParamsBuilder::default().ignore(true)),\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Points.QuantizationSearchParams;\nimport io.qdrant.client.grpc.Points.QueryPoints;\nimport io.qdrant.client.grpc.Points.SearchParams;\n\nimport static io.qdrant.client.QueryFactory.nearest;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient.queryAsync(\n        QueryPoints.newBuilder()\n                .setCollectionName(\"{collection_name}\")\n                .setQuery(nearest(0.2f, 0.1f, 0.9f, 0.7f))\n                .setParams(\n                        SearchParams.newBuilder()\n                                .setQuantization(\n                                        QuantizationSearchParams.newBuilder().setIgnore(true).build())\n                                .build())\n                .setLimit(10)\n                .build())\n        .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.QueryAsync(\n\tcollectionName: \"{collection_name}\",\n\tquery: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },\n\tsearchParams: new SearchParams\n\t{\n\t\tQuantization = new QuantizationSearchParams { Ignore = true }\n\t},\n\tlimit: 10\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.Query(context.Background(), &qdrant.QueryPoints{\n\tCollectionName: \"{collection_name}\",\n\tQuery:          qdrant.NewQuery(0.2, 0.1, 0.9, 0.7),\n\tParams: &qdrant.SearchParams{\n\t\tQuantization: &qdrant.QuantizationSearchParams{\n\t\t\tIgnore: qdrant.PtrOf(false),\n\t\t},\n\t},\n})\n```\n\n- **Adjust the quantile parameter**: The quantile parameter in scalar quantization determines the quantization bounds. By setting it to a value lower than 1.0, you can exclude extreme values (outliers) from the quantization bounds. For example, if you set the quantile to 0.99, 1% of the extreme values will be excluded. By adjusting the quantile, you find an optimal value that will provide the best search quality for your collection.",
    "metadata": {
      "chunk_id": "7d1033e04950-0031",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 31,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Accuracy tuning"
      ],
      "heading_text": "Accuracy tuning",
      "token_count": 1003,
      "char_count": 3858,
      "start_char": 53880,
      "end_char": 57738,
      "semantic_score": 0.6,
      "structural_score": 0.9999999999999999,
      "retrieval_quality": 0.8821114369501466,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.290961",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Accuracy tuning",
      "chunk_hash": "df9b61ec26da9022",
      "content_digest": "df9b61ec26da9022",
      "chunk_length": 3858,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "the",
          "quantization",
          "query",
          "collection",
          "import",
          "name",
          "qdrantclient",
          "ignore",
          "true",
          "new",
          "search",
          "localhost",
          "searchparams",
          "points",
          "params",
          "quantizationsearchparams",
          "build",
          "you"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 25,
            "weight": 0.074184
          },
          {
            "term": "qdrant",
            "tf": 24,
            "weight": 0.071217
          },
          {
            "term": "the",
            "tf": 14,
            "weight": 0.041543
          },
          {
            "term": "quantization",
            "tf": 12,
            "weight": 0.035608
          },
          {
            "term": "query",
            "tf": 11,
            "weight": 0.032641
          },
          {
            "term": "collection",
            "tf": 9,
            "weight": 0.026706
          },
          {
            "term": "import",
            "tf": 9,
            "weight": 0.026706
          },
          {
            "term": "name",
            "tf": 8,
            "weight": 0.023739
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.023739
          },
          {
            "term": "ignore",
            "tf": 7,
            "weight": 0.020772
          },
          {
            "term": "true",
            "tf": 7,
            "weight": 0.020772
          },
          {
            "term": "new",
            "tf": 7,
            "weight": 0.020772
          },
          {
            "term": "search",
            "tf": 6,
            "weight": 0.017804
          },
          {
            "term": "localhost",
            "tf": 6,
            "weight": 0.017804
          },
          {
            "term": "searchparams",
            "tf": 6,
            "weight": 0.017804
          },
          {
            "term": "points",
            "tf": 5,
            "weight": 0.014837
          },
          {
            "term": "params",
            "tf": 5,
            "weight": 0.014837
          },
          {
            "term": "quantizationsearchparams",
            "tf": 5,
            "weight": 0.014837
          },
          {
            "term": "build",
            "tf": 5,
            "weight": 0.014837
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.011869
          }
        ],
        "unique_terms": 123,
        "total_terms": 337
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": true,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Accuracy tuning",
        "client",
        "collection",
        "ignore",
        "import",
        "name",
        "qdrant",
        "qdrantclient",
        "quantization",
        "query",
        "the"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.9999999999999999,
      "retrieval_quality": 0.8821114369501466,
      "overall": 0.8273704789833821
    }
  },
  {
    "text": "#### Memory and speed tuning\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:\n\n- **All in RAM** - all vector, original and quantized, are loaded and kept in RAM. This is the fastest mode, but requires a lot of RAM. Enabled by default.\n\n- **Original on Disk, quantized in RAM** - this is a hybrid mode, allows to obtain a good balance between speed and memory usage. Recommended scenario if you are aiming to shrink the memory footprint while keeping the search speed.\n\nThis mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n        \"size\": 768,\n        \"distance\": \"Cosine\",\n        \"on_disk\": true\n    },\n    \"quantization_config\": {\n        \"scalar\": {\n            \"type\": \"int8\",\n            \"always_ram\": true\n        }\n    }\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE, on_disk=True),\n    quantization_config=models.ScalarQuantization(\n        scalar=models.ScalarQuantizationConfig(\n            type=models.ScalarType.INT8,\n            always_ram=True,\n        ),\n    ),\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 768,\n    distance: \"Cosine\",\n    on_disk: true,\n  },\n  quantization_config: {\n    scalar: {\n      type: \"int8\",\n      always_ram: true,\n    },\n  },\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    CreateCollectionBuilder, Distance, QuantizationType, ScalarQuantizationBuilder,\n    VectorParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(768, Distance::Cosine))\n            .quantization_config(\n                ScalarQuantizationBuilder::default()\n                    .r#type(QuantizationType::Int8.into())\n                    .always_ram(true),\n            ),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.OptimizersConfigDiff;\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\nimport io.qdrant.client.grpc.Collections.QuantizationType;\nimport io.qdrant.client.grpc.Collections.ScalarQuantization;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(768)\n                            .setDistance(Distance.Cosine)\n                            .setOnDisk(true)\n                            .build())\n                    .build())\n            .setQuantizationConfig(\n                QuantizationConfig.newBuilder()\n                    .setScalar(\n                        ScalarQuantization.newBuilder()\n                            .setType(QuantizationType.Int8)\n                            .setAlwaysRam(true)\n                            .build())\n                    .build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n\tcollectionName: \"{collection_name}\",\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine, OnDisk = true },\n\tquantizationConfig: new QuantizationConfig\n\t{\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\t}\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)",
    "metadata": {
      "chunk_id": "7d1033e04950-0033",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 33,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Memory and speed tuning"
      ],
      "heading_text": "Memory and speed tuning",
      "token_count": 1000,
      "char_count": 4494,
      "start_char": 57969,
      "end_char": 62463,
      "semantic_score": 0.6,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.892485549132948,
      "chunking_strategy": "hybrid_adaptive_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.298744",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Memory and speed tuning",
      "chunk_hash": "249e59e5157e5a5d",
      "content_digest": "249e59e5157e5a5d",
      "chunk_length": 4494,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "client",
          "qdrant",
          "import",
          "true",
          "collection",
          "distance",
          "ram",
          "collections",
          "grpc",
          "qdrantclient",
          "new",
          "the",
          "config",
          "name",
          "build",
          "quantization",
          "768",
          "cosine",
          "int8",
          "models"
        ],
        "term_weights": [
          {
            "term": "client",
            "tf": 27,
            "weight": 0.06801
          },
          {
            "term": "qdrant",
            "tf": 22,
            "weight": 0.055416
          },
          {
            "term": "import",
            "tf": 13,
            "weight": 0.032746
          },
          {
            "term": "true",
            "tf": 12,
            "weight": 0.030227
          },
          {
            "term": "collection",
            "tf": 10,
            "weight": 0.025189
          },
          {
            "term": "distance",
            "tf": 10,
            "weight": 0.025189
          },
          {
            "term": "ram",
            "tf": 9,
            "weight": 0.02267
          },
          {
            "term": "collections",
            "tf": 9,
            "weight": 0.02267
          },
          {
            "term": "grpc",
            "tf": 9,
            "weight": 0.02267
          },
          {
            "term": "qdrantclient",
            "tf": 8,
            "weight": 0.020151
          },
          {
            "term": "new",
            "tf": 8,
            "weight": 0.020151
          },
          {
            "term": "the",
            "tf": 7,
            "weight": 0.017632
          },
          {
            "term": "config",
            "tf": 7,
            "weight": 0.017632
          },
          {
            "term": "name",
            "tf": 7,
            "weight": 0.017632
          },
          {
            "term": "build",
            "tf": 7,
            "weight": 0.017632
          },
          {
            "term": "quantization",
            "tf": 6,
            "weight": 0.015113
          },
          {
            "term": "768",
            "tf": 6,
            "weight": 0.015113
          },
          {
            "term": "cosine",
            "tf": 6,
            "weight": 0.015113
          },
          {
            "term": "int8",
            "tf": 6,
            "weight": 0.015113
          },
          {
            "term": "models",
            "tf": 6,
            "weight": 0.015113
          }
        ],
        "unique_terms": 132,
        "total_terms": 397
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": true,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Memory and speed tuning",
        "client",
        "collection",
        "collections",
        "distance",
        "grpc",
        "import",
        "qdrant",
        "qdrantclient",
        "ram",
        "true"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.892485549132948,
      "overall": 0.7641618497109827
    }
  },
  {
    "text": "##### Was this page useful? Yes No  Thank you for your feedback! 🙏  We are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/guides/quantization.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue. On this page:  - [Quantization](#quantization.md)    - [Scalar Quantization](#scalar-quantization.md)    - [Binary Quantization](#binary-quantization.md)      - [Binary Quantization as Hamming Distance](#binary-quantization-as-hamming-distance.md)     - [1.5-Bit and 2-Bit Quantization](#15-bit-and-2-bit-quantization.md)     - [Asymmetric Quantization](#asymmetric-quantization.md)    - [Product Quantization](#product-quantization.md)    - [How to choose the right quantization method](#how-to-choose-the-right-quantization-method.md)    - [Setting up Quantization in Qdrant](#setting-up-quantization-in-qdrant.md)      - [Setting up Scalar Quantization](#setting-up-scalar-quantization.md)     - [Setting up Binary Quantization](#setting-up-binary-quantization.md)     - [Setting up Product Quantization](#setting-up-product-quantization.md)     - [Disabling Quantization](#disabling-quantization.md)     - [Searching with Quantization](#searching-with-quantization.md)    - [Quantization tips](#quantization-tips.md)     -  * [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/guides/quantization.md) * [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)",
    "metadata": {
      "chunk_id": "7d1033e04950-0037",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "filename": "_documentation_guides_quantization_.md",
      "file_extension": ".md",
      "chunk_index": 37,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Was this page useful?"
      ],
      "heading_text": "Was this page useful?",
      "token_count": 420,
      "char_count": 1566,
      "start_char": 70996,
      "end_char": 72562,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7371111111111112,
      "chunking_strategy": "hybrid_adaptive_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:25.300774",
      "document_id": "7d1033e04950",
      "document_name": "_documentation_guides_quantization_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "source_filename": "_documentation_guides_quantization_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_quantization\\_documentation_guides_quantization_.md",
      "hierarchy_path": "Was this page useful?",
      "chunk_hash": "51143f4e13e2f752",
      "content_digest": "51143f4e13e2f752",
      "chunk_length": 1566,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "quantization",
          "qdrant",
          "setting",
          "page",
          "github",
          "landing",
          "binary",
          "https",
          "com",
          "choose",
          "scalar",
          "bit",
          "product",
          "this",
          "you",
          "edit",
          "tree",
          "master",
          "content",
          "documentation"
        ],
        "term_weights": [
          {
            "term": "quantization",
            "tf": 32,
            "weight": 0.201258
          },
          {
            "term": "qdrant",
            "tf": 8,
            "weight": 0.050314
          },
          {
            "term": "setting",
            "tf": 8,
            "weight": 0.050314
          },
          {
            "term": "page",
            "tf": 7,
            "weight": 0.044025
          },
          {
            "term": "github",
            "tf": 7,
            "weight": 0.044025
          },
          {
            "term": "landing",
            "tf": 6,
            "weight": 0.037736
          },
          {
            "term": "binary",
            "tf": 6,
            "weight": 0.037736
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "com",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "choose",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "scalar",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "bit",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "product",
            "tf": 4,
            "weight": 0.025157
          },
          {
            "term": "this",
            "tf": 3,
            "weight": 0.018868
          },
          {
            "term": "you",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "edit",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "tree",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "master",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "content",
            "tf": 2,
            "weight": 0.012579
          },
          {
            "term": "documentation",
            "tf": 2,
            "weight": 0.012579
          }
        ],
        "unique_terms": 49,
        "total_terms": 159
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Was this page useful?",
        "binary",
        "choose",
        "com",
        "github",
        "https",
        "landing",
        "page",
        "qdrant",
        "quantization",
        "setting"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7371111111111112,
      "overall": 0.7790370370370371
    }
  }
]