[
  {
    "text": "### Examples  [Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)  - [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/) - [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)  [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)  - [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/) - [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/) - [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)  [Build Prototypes](https://qdrant.tech/documentation/examples/)  - [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/) - [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/) - [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/) - [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/) - [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/) - [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/) - [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) - [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/) - [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/) - [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/) - [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/) - [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)  [Practice Datasets](https://qdrant.tech/documentation/datasets/)",
    "metadata": {
      "chunk_id": "647c93a7ff48-0002",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Examples"
      ],
      "heading_text": "Examples",
      "token_count": 576,
      "char_count": 2347,
      "start_char": 0,
      "end_char": 2347,
      "semantic_score": 0.5320299863815308,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.9076923076923078,
      "chunking_strategy": "hierarchical_balanced",
      "content_type": "hierarchical_section",
      "embedding_model": "nomic-ai/CodeRankEmbed",
      "processing_timestamp": "2025-10-19T15:29:33.010991",
      "document_id": "647c93a7ff48",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Examples",
      "chunk_hash": "59078d11dd9c9931",
      "content_digest": "59078d11dd9c9931",
      "chunk_length": 2347,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "https",
          "tech",
          "documentation",
          "examples",
          "search",
          "with",
          "rag",
          "data",
          "chatbot",
          "send",
          "system",
          "semantic",
          "and",
          "cohere",
          "precision",
          "reranking",
          "dspy",
          "for",
          "automate"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "tech",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "documentation",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "examples",
            "tf": 14,
            "weight": 0.05
          },
          {
            "term": "search",
            "tf": 10,
            "weight": 0.035714
          },
          {
            "term": "with",
            "tf": 9,
            "weight": 0.032143
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.032143
          },
          {
            "term": "data",
            "tf": 7,
            "weight": 0.025
          },
          {
            "term": "chatbot",
            "tf": 6,
            "weight": 0.021429
          },
          {
            "term": "send",
            "tf": 5,
            "weight": 0.017857
          },
          {
            "term": "system",
            "tf": 5,
            "weight": 0.017857
          },
          {
            "term": "semantic",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "cohere",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "precision",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "reranking",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "dspy",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "automate",
            "tf": 2,
            "weight": 0.007143
          }
        ],
        "unique_terms": 96,
        "total_terms": 280
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Examples",
        "chatbot",
        "data",
        "documentation",
        "examples",
        "https",
        "qdrant",
        "rag",
        "search",
        "tech",
        "with"
      ],
      "quality_fallback": true,
      "quality_notes": "Promoted via relaxed thresholds"
    },
    "advanced_scores": {
      "semantic": 0.5320299863815308,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.9076923076923078,
      "overall": 0.779907431357946
    }
  },
  {
    "text": "### Examples  [Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)  - [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/) - [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)  [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)  - [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/) - [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/) - [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)  [Build Prototypes](https://qdrant.tech/documentation/examples/)  - [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/) - [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/) - [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/) - [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/) - [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/) - [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/) - [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) - [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/) - [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/) - [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/) - [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/) - [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)  [Practice Datasets](https://qdrant.tech/documentation/datasets/)  - [Documentation](https://qdrant.tech/documentation/) - - [Search precision](https://qdrant.tech/documentation/search-precision/) - - Reranking in Semantic Search",
    "metadata": {
      "chunk_id": "647c93a7ff48-0005",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Examples"
      ],
      "heading_text": "Examples",
      "token_count": 614,
      "char_count": 2511,
      "start_char": 0,
      "end_char": 2511,
      "semantic_score": 0.5320299863815308,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8976744186046512,
      "chunking_strategy": "hierarchical_balanced",
      "content_type": "hierarchical_section",
      "embedding_model": "nomic-ai/CodeRankEmbed",
      "processing_timestamp": "2025-10-19T15:29:33.023798",
      "document_id": "647c93a7ff48",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Examples",
      "chunk_hash": "ec8b68cbb25d8fef",
      "content_digest": "ec8b68cbb25d8fef",
      "chunk_length": 2511,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "documentation",
          "https",
          "tech",
          "examples",
          "search",
          "with",
          "rag",
          "data",
          "chatbot",
          "precision",
          "semantic",
          "send",
          "system",
          "reranking",
          "and",
          "cohere",
          "dspy",
          "for",
          "automate"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 32,
            "weight": 0.108108
          },
          {
            "term": "documentation",
            "tf": 24,
            "weight": 0.081081
          },
          {
            "term": "https",
            "tf": 23,
            "weight": 0.077703
          },
          {
            "term": "tech",
            "tf": 23,
            "weight": 0.077703
          },
          {
            "term": "examples",
            "tf": 14,
            "weight": 0.047297
          },
          {
            "term": "search",
            "tf": 13,
            "weight": 0.043919
          },
          {
            "term": "with",
            "tf": 9,
            "weight": 0.030405
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.030405
          },
          {
            "term": "data",
            "tf": 7,
            "weight": 0.023649
          },
          {
            "term": "chatbot",
            "tf": 6,
            "weight": 0.02027
          },
          {
            "term": "precision",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "semantic",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "send",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "system",
            "tf": 5,
            "weight": 0.016892
          },
          {
            "term": "reranking",
            "tf": 4,
            "weight": 0.013514
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.013514
          },
          {
            "term": "cohere",
            "tf": 4,
            "weight": 0.013514
          },
          {
            "term": "dspy",
            "tf": 3,
            "weight": 0.010135
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.010135
          },
          {
            "term": "automate",
            "tf": 2,
            "weight": 0.006757
          }
        ],
        "unique_terms": 96,
        "total_terms": 296
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Examples",
        "chatbot",
        "data",
        "documentation",
        "examples",
        "https",
        "qdrant",
        "rag",
        "search",
        "tech",
        "with"
      ],
      "quality_fallback": true,
      "quality_notes": "Promoted via relaxed thresholds"
    },
    "advanced_scores": {
      "semantic": 0.5320299863815308,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8976744186046512,
      "overall": 0.776568134995394
    }
  },
  {
    "text": "## Implementation  Now it’s time to dive into the actual implementation.",
    "metadata": {
      "chunk_id": "647c93a7ff48-0015",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Implementation"
      ],
      "heading_text": "Implementation",
      "token_count": 14,
      "char_count": 72,
      "start_char": 0,
      "end_char": 72,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced",
      "content_type": "hierarchical_section",
      "embedding_model": "nomic-ai/CodeRankEmbed",
      "processing_timestamp": "2025-10-19T15:29:33.034582",
      "document_id": "647c93a7ff48",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Implementation",
      "chunk_hash": "44f9fbfaa8c45dcb",
      "content_digest": "44f9fbfaa8c45dcb",
      "chunk_length": 72,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "implementation",
          "now",
          "time",
          "dive",
          "into",
          "the",
          "actual"
        ],
        "term_weights": [
          {
            "term": "implementation",
            "tf": 2,
            "weight": 0.25
          },
          {
            "term": "now",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "time",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "dive",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "into",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.125
          },
          {
            "term": "actual",
            "tf": 1,
            "weight": 0.125
          }
        ],
        "unique_terms": 7,
        "total_terms": 8
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Implementation",
        "actual",
        "dive",
        "implementation",
        "into",
        "now",
        "the",
        "time"
      ],
      "quality_fallback": true,
      "quality_notes": "Promoted via relaxed thresholds"
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "## Working  Picture this: You walk into a massive library and ask for a book on “climate change.” The librarian pulls out a dozen books for you—some are scientific papers, others are personal essays, and one’s even a novel. Sure, they’re all relevant, but the first one you get handed is the novel. Not exactly what you were hoping for, right? Now, imagine a smarter, more intuitive librarian who really gets what you’re after. This one knows exactly which books are most impactful, the most current, and perfectly aligned with what you need. That’s what reranking does for your search results—it doesn’t just grab any relevant document; it smartly reorders them so the best ones land at the top of your list. It’s like having a librarian who knows exactly what you’re looking for before you do! An illustration of the rerank model prioritizing better results  To become that smart, intuitive librarian, your algorithm needs to learn how to understand both your queries and the documents it retrieves. It has to evaluate the relationship between them effectively, so it can give you exactly what you’re looking for. The way reranker models operate varies based on their type, which will be discussed later, but in general, they calculate a relevance score for each document-query pair.Unlike embedding models, which squash everything into a single vector upfront, rerankers keep all the important details intact by using the full transformer output to calculate a similarity score. The result? Precision. But, there’s a trade-off—reranking can be slow. Processing millions of documents can take hours, which is why rerankers focus on refining results, not searching through the entire document collection. Rerankers come in different types, each with its own strengths. Let’s break them down:  1. **Cross Encoder Models**: These boost reranking by using a classification system to evaluate pairs of data—like sentences or documents. They spit out a similarity score from 0 to 1, showing how closely the document matches your query. The catch? Cross-encoders need both query and document, so they can’t handle standalone documents or queries by themselves. 2. **Multi-Vector Rerankers (e.g., ColBERT)**: These models take a more efficient route. They encode your query and the documents separately and only compare them later, reducing the computational load. This means document representations can be precomputed, speeding up retrieval times 3. **Large Language Models (LLMs) as Rerankers**: This is a newer, smarter way to rerank. LLMs, like GPT, are getting better by the day. With the right instructions, they can prioritize the most relevant documents for you, leveraging their massive understanding of language to deliver even more accurate results. Each of these rerankers has its own special way of making sure you get the best search results, fast and relevant to what you need.",
    "metadata": {
      "chunk_id": "647c93a7ff48-0009",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Working"
      ],
      "heading_text": "Working",
      "token_count": 599,
      "char_count": 2887,
      "start_char": 0,
      "end_char": 2887,
      "semantic_score": 0.46868714690208435,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8788209606986901,
      "chunking_strategy": "hierarchical_balanced",
      "content_type": "hierarchical_section",
      "embedding_model": "nomic-ai/CodeRankEmbed",
      "processing_timestamp": "2025-10-19T15:29:33.028952",
      "document_id": "647c93a7ff48",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Working",
      "chunk_hash": "140d58dc3de75ed3",
      "content_digest": "140d58dc3de75ed3",
      "chunk_length": 2887,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "you",
          "and",
          "for",
          "what",
          "they",
          "your",
          "document",
          "documents",
          "can",
          "rerankers",
          "results",
          "models",
          "this",
          "librarian",
          "are",
          "relevant",
          "exactly",
          "which",
          "them"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 22,
            "weight": 0.055276
          },
          {
            "term": "you",
            "tf": 13,
            "weight": 0.032663
          },
          {
            "term": "and",
            "tf": 8,
            "weight": 0.020101
          },
          {
            "term": "for",
            "tf": 8,
            "weight": 0.020101
          },
          {
            "term": "what",
            "tf": 7,
            "weight": 0.017588
          },
          {
            "term": "they",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "your",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "document",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "documents",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "can",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "rerankers",
            "tf": 6,
            "weight": 0.015075
          },
          {
            "term": "results",
            "tf": 5,
            "weight": 0.012563
          },
          {
            "term": "models",
            "tf": 5,
            "weight": 0.012563
          },
          {
            "term": "this",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "librarian",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "are",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "relevant",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "exactly",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "which",
            "tf": 4,
            "weight": 0.01005
          },
          {
            "term": "them",
            "tf": 4,
            "weight": 0.01005
          }
        ],
        "unique_terms": 221,
        "total_terms": 398
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Working",
        "and",
        "can",
        "document",
        "documents",
        "for",
        "the",
        "they",
        "what",
        "you",
        "your"
      ],
      "quality_fallback": true,
      "quality_notes": "Promoted via relaxed thresholds"
    },
    "advanced_scores": {
      "semantic": 0.46868714690208435,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8788209606986901,
      "overall": 0.7491693692002581
    }
  },
  {
    "text": "### Setup  To follow along with this tutorial, you’ll need a few key tools::  - Python Client for Qdrant - Cohere  Let’s install everything you need in one go using the Python package manager:: ```jsx pip install qdrant-client cohere ``` ---  Now, let’s bring in all the necessary components in one tidy block: ```jsx from qdrant_client import QdrantClient from qdrant_client.models import Distance, VectorParams, PointStruct import cohere ``` ---  Qdrant is a powerful vector similarity search engine that gives you a production-ready service with an easy-to-use API for storing, searching, and managing data. You can interact with Qdrant through a local or cloud setup, but since we’re working in Colab, let’s go with the cloud setup.",
    "metadata": {
      "chunk_id": "647c93a7ff48-0016",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Setup"
      ],
      "heading_text": "Setup",
      "token_count": 168,
      "char_count": 736,
      "start_char": 0,
      "end_char": 736,
      "semantic_score": 0.5945252776145935,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7039130434782609,
      "chunking_strategy": "hierarchical_balanced",
      "content_type": "hierarchical_section",
      "embedding_model": "nomic-ai/CodeRankEmbed",
      "processing_timestamp": "2025-10-19T15:29:33.035257",
      "document_id": "647c93a7ff48",
      "document_name": "_documentation_search-precision_reranking-semantic-search_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "source_filename": "_documentation_search-precision_reranking-semantic-search_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_reranking-semantic-search\\_documentation_search-precision_reranking-semantic-search_.md",
      "hierarchy_path": "Setup",
      "chunk_hash": "3e597f8e9738ca0e",
      "content_digest": "3e597f8e9738ca0e",
      "chunk_length": 736,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "with",
          "you",
          "client",
          "setup",
          "cohere",
          "let",
          "the",
          "import",
          "need",
          "python",
          "for",
          "install",
          "one",
          "jsx",
          "from",
          "cloud",
          "follow",
          "along",
          "this"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 6,
            "weight": 0.060606
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.040404
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.040404
          },
          {
            "term": "client",
            "tf": 4,
            "weight": 0.040404
          },
          {
            "term": "setup",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "cohere",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "let",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "the",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "import",
            "tf": 3,
            "weight": 0.030303
          },
          {
            "term": "need",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "python",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "install",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "one",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "jsx",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "from",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "cloud",
            "tf": 2,
            "weight": 0.020202
          },
          {
            "term": "follow",
            "tf": 1,
            "weight": 0.010101
          },
          {
            "term": "along",
            "tf": 1,
            "weight": 0.010101
          },
          {
            "term": "this",
            "tf": 1,
            "weight": 0.010101
          }
        ],
        "unique_terms": 67,
        "total_terms": 99
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Setup",
        "client",
        "cohere",
        "import",
        "let",
        "need",
        "qdrant",
        "setup",
        "the",
        "with",
        "you"
      ],
      "quality_fallback": true,
      "quality_notes": "Promoted via relaxed thresholds"
    },
    "advanced_scores": {
      "semantic": 0.5945252776145935,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7039130434782609,
      "overall": 0.7328127736976181
    }
  }
]