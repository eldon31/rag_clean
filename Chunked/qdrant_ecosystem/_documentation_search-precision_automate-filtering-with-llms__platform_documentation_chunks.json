[
  {
    "text": "### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)",
    "metadata": {
      "chunk_id": "9b70691be664-0000",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Essentials"
      ],
      "heading_text": "Essentials",
      "token_count": 189,
      "char_count": 673,
      "start_char": 679,
      "end_char": 1352,
      "semantic_score": 0.7,
      "structural_score": 0.7,
      "retrieval_quality": 0.5053846153846153,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.604501",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Essentials",
      "chunk_hash": "b49cc869c6854f5f",
      "content_digest": "b49cc869c6854f5f",
      "chunk_length": 673,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "rag",
          "https",
          "tech",
          "documentation",
          "agentic",
          "with",
          "data",
          "ingestion",
          "beginners",
          "langgraph",
          "discord",
          "multimodal",
          "and",
          "deepseek",
          "n8n",
          "essentials",
          "for",
          "simple",
          "system"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.119048
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "tech",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "documentation",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "agentic",
            "tf": 6,
            "weight": 0.071429
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.059524
          },
          {
            "term": "data",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "ingestion",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "beginners",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "langgraph",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "discord",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "multimodal",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "deepseek",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "n8n",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "essentials",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "simple",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.011905
          }
        ],
        "unique_terms": 31,
        "total_terms": 84
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Essentials",
        "agentic",
        "beginners",
        "data",
        "documentation",
        "https",
        "ingestion",
        "qdrant",
        "rag",
        "tech",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7,
      "retrieval_quality": 0.5053846153846153,
      "overall": 0.6351282051282051
    }
  },
  {
    "text": "### Integrations  [Data Management](https://qdrant.tech/documentation/data-management/)  - [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/) - [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/) - [Apache Spark](https://qdrant.tech/documentation/data-management/spark/) - [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/) - [cognee](https://qdrant.tech/documentation/data-management/cognee/) - [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/) - [DLT](https://qdrant.tech/documentation/data-management/dlt/) - [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/) - [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/) - [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)  [Embeddings](https://qdrant.tech/documentation/embeddings/)  - [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/) - [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/) - [Cohere](https://qdrant.tech/documentation/embeddings/cohere/) - [Gemini](https://qdrant.tech/documentation/embeddings/gemini/) - [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/) - [Mistral](https://qdrant.tech/documentation/embeddings/mistral/) - [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/) - [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/) - [Nomic](https://qdrant.tech/documentation/embeddings/nomic/) - [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/) - [Ollama](https://qdrant.tech/documentation/embeddings/ollama/) - [OpenAI](https://qdrant.tech/documentation/embeddings/openai/) - [Prem AI](https://qdrant.tech/documentation/embeddings/premai/) - [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/) - [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/) - [Upstage](https://qdrant.tech/documentation/embeddings/upstage/) - [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)  [Frameworks](https://qdrant.tech/documentation/frameworks/)  - [Autogen](https://qdrant.tech/documentation/frameworks/autogen/) - [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/) - [CamelAI](https://qdrant.tech/documentation/frameworks/camel/) - [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/) - [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/) - [Dagster](https://qdrant.tech/documentation/frameworks/dagster/) - [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/) - [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/) - [Feast](https://qdrant.tech/documentation/frameworks/feast/) - [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/) - [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/) - [Haystack](https://qdrant.tech/documentation/frameworks/haystack/) - [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/) - [Langchain](https://qdrant.tech/documentation/frameworks/langchain/) - [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/) - [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/) - [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/) - [Mastra](https://qdrant.tech/documentation/frameworks/mastra/) - [Mem0](https://qdrant.tech/documentation/frameworks/mem0/) - [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/) - [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/) - [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/) - [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/) - [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/) - [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/) - [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/) - [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/) - [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/) - [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/) - [txtai](https://qdrant.tech/documentation/frameworks/txtai/) - [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/) - [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/) - [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)  [Observability](https://qdrant.tech/documentation/observability/)  - [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/) - [OpenLIT](https://qdrant.tech/documentation/observability/openlit/) - [Datadog](https://qdrant.tech/documentation/observability/datadog/)  [Platforms](https://qdrant.tech/documentation/platforms/)  - [Apify](https://qdrant.tech/documentation/platforms/apify/) - [BuildShip](https://qdrant.tech/documentation/platforms/buildship/) - [Keboola](https://qdrant.tech/documentation/platforms/keboola/) - [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/) - [Make.com](https://qdrant.tech/documentation/platforms/make/) - [N8N](https://qdrant.tech/documentation/platforms/n8n/) - [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/) - [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/) - [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/) - [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/) - [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/) - [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)",
    "metadata": {
      "chunk_id": "9b70691be664-0001",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Integrations"
      ],
      "heading_text": "Integrations",
      "token_count": 1498,
      "char_count": 5661,
      "start_char": 1354,
      "end_char": 7015,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "chunking_strategy": "hierarchical_context_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.606459",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Integrations",
      "chunk_hash": "8a3f2e84e2a75ae9",
      "content_digest": "8a3f2e84e2a75ae9",
      "chunk_length": 5661,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "frameworks",
          "embeddings",
          "platforms",
          "data",
          "management",
          "observability",
          "airbyte",
          "apache",
          "airflow",
          "spark",
          "cocoindex",
          "cognee",
          "confluent",
          "dlt",
          "fluvio",
          "redpanda"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "qdrant",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "tech",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "documentation",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "frameworks",
            "tf": 35,
            "weight": 0.058528
          },
          {
            "term": "embeddings",
            "tf": 21,
            "weight": 0.035117
          },
          {
            "term": "platforms",
            "tf": 14,
            "weight": 0.023411
          },
          {
            "term": "data",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "management",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "observability",
            "tf": 5,
            "weight": 0.008361
          },
          {
            "term": "airbyte",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "apache",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "airflow",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "spark",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cocoindex",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cognee",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "confluent",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "dlt",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "fluvio",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "redpanda",
            "tf": 2,
            "weight": 0.003344
          }
        ],
        "unique_terms": 113,
        "total_terms": 598
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Integrations",
        "data",
        "documentation",
        "embeddings",
        "frameworks",
        "https",
        "management",
        "observability",
        "platforms",
        "qdrant",
        "tech"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "overall": 0.7576923076923077
    }
  },
  {
    "text": "### Examples  [Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)  - [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/) - [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)  [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)  - [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/) - [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/) - [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)  [Build Prototypes](https://qdrant.tech/documentation/examples/)  - [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/) - [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/) - [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/) - [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/) - [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/) - [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/) - [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) - [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/) - [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/) - [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/) - [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/) - [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)  [Practice Datasets](https://qdrant.tech/documentation/datasets/)",
    "metadata": {
      "chunk_id": "9b70691be664-0002",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Examples"
      ],
      "heading_text": "Examples",
      "token_count": 576,
      "char_count": 2347,
      "start_char": 7017,
      "end_char": 9364,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.9076923076923078,
      "chunking_strategy": "hierarchical_context_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.608032",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Examples",
      "chunk_hash": "59078d11dd9c9931",
      "content_digest": "59078d11dd9c9931",
      "chunk_length": 2347,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "https",
          "tech",
          "documentation",
          "examples",
          "search",
          "with",
          "rag",
          "data",
          "chatbot",
          "send",
          "system",
          "semantic",
          "and",
          "cohere",
          "precision",
          "reranking",
          "dspy",
          "for",
          "automate"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 30,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "tech",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "documentation",
            "tf": 21,
            "weight": 0.075
          },
          {
            "term": "examples",
            "tf": 14,
            "weight": 0.05
          },
          {
            "term": "search",
            "tf": 10,
            "weight": 0.035714
          },
          {
            "term": "with",
            "tf": 9,
            "weight": 0.032143
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.032143
          },
          {
            "term": "data",
            "tf": 7,
            "weight": 0.025
          },
          {
            "term": "chatbot",
            "tf": 6,
            "weight": 0.021429
          },
          {
            "term": "send",
            "tf": 5,
            "weight": 0.017857
          },
          {
            "term": "system",
            "tf": 5,
            "weight": 0.017857
          },
          {
            "term": "semantic",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "cohere",
            "tf": 4,
            "weight": 0.014286
          },
          {
            "term": "precision",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "reranking",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "dspy",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.010714
          },
          {
            "term": "automate",
            "tf": 2,
            "weight": 0.007143
          }
        ],
        "unique_terms": 96,
        "total_terms": 280
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Examples",
        "chatbot",
        "data",
        "documentation",
        "examples",
        "https",
        "qdrant",
        "rag",
        "search",
        "tech",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.9076923076923078,
      "overall": 0.8358974358974359
    }
  },
  {
    "text": "### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)",
    "metadata": {
      "chunk_id": "9b70691be664-0003",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Essentials"
      ],
      "heading_text": "Essentials",
      "token_count": 189,
      "char_count": 673,
      "start_char": 9366,
      "end_char": 10039,
      "semantic_score": 0.7,
      "structural_score": 0.7,
      "retrieval_quality": 0.5053846153846153,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.608588",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Essentials",
      "chunk_hash": "b49cc869c6854f5f",
      "content_digest": "b49cc869c6854f5f",
      "chunk_length": 673,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "rag",
          "https",
          "tech",
          "documentation",
          "agentic",
          "with",
          "data",
          "ingestion",
          "beginners",
          "langgraph",
          "discord",
          "multimodal",
          "and",
          "deepseek",
          "n8n",
          "essentials",
          "for",
          "simple",
          "system"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.119048
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.107143
          },
          {
            "term": "https",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "tech",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "documentation",
            "tf": 7,
            "weight": 0.083333
          },
          {
            "term": "agentic",
            "tf": 6,
            "weight": 0.071429
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.059524
          },
          {
            "term": "data",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "ingestion",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "beginners",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "langgraph",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "discord",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "multimodal",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "deepseek",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "n8n",
            "tf": 2,
            "weight": 0.02381
          },
          {
            "term": "essentials",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "for",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "simple",
            "tf": 1,
            "weight": 0.011905
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.011905
          }
        ],
        "unique_terms": 31,
        "total_terms": 84
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Essentials",
        "agentic",
        "beginners",
        "data",
        "documentation",
        "https",
        "ingestion",
        "qdrant",
        "rag",
        "tech",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7,
      "retrieval_quality": 0.5053846153846153,
      "overall": 0.6351282051282051
    }
  },
  {
    "text": "### Integrations  [Data Management](https://qdrant.tech/documentation/data-management/)  - [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/) - [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/) - [Apache Spark](https://qdrant.tech/documentation/data-management/spark/) - [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/) - [cognee](https://qdrant.tech/documentation/data-management/cognee/) - [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/) - [DLT](https://qdrant.tech/documentation/data-management/dlt/) - [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/) - [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/) - [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)  [Embeddings](https://qdrant.tech/documentation/embeddings/)  - [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/) - [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/) - [Cohere](https://qdrant.tech/documentation/embeddings/cohere/) - [Gemini](https://qdrant.tech/documentation/embeddings/gemini/) - [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/) - [Mistral](https://qdrant.tech/documentation/embeddings/mistral/) - [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/) - [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/) - [Nomic](https://qdrant.tech/documentation/embeddings/nomic/) - [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/) - [Ollama](https://qdrant.tech/documentation/embeddings/ollama/) - [OpenAI](https://qdrant.tech/documentation/embeddings/openai/) - [Prem AI](https://qdrant.tech/documentation/embeddings/premai/) - [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/) - [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/) - [Upstage](https://qdrant.tech/documentation/embeddings/upstage/) - [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)  [Frameworks](https://qdrant.tech/documentation/frameworks/)  - [Autogen](https://qdrant.tech/documentation/frameworks/autogen/) - [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/) - [CamelAI](https://qdrant.tech/documentation/frameworks/camel/) - [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/) - [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/) - [Dagster](https://qdrant.tech/documentation/frameworks/dagster/) - [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/) - [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/) - [Feast](https://qdrant.tech/documentation/frameworks/feast/) - [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/) - [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/) - [Haystack](https://qdrant.tech/documentation/frameworks/haystack/) - [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/) - [Langchain](https://qdrant.tech/documentation/frameworks/langchain/) - [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/) - [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/) - [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/) - [Mastra](https://qdrant.tech/documentation/frameworks/mastra/) - [Mem0](https://qdrant.tech/documentation/frameworks/mem0/) - [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/) - [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/) - [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/) - [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/) - [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/) - [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/) - [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/) - [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/) - [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/) - [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/) - [txtai](https://qdrant.tech/documentation/frameworks/txtai/) - [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/) - [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/) - [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)  [Observability](https://qdrant.tech/documentation/observability/)  - [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/) - [OpenLIT](https://qdrant.tech/documentation/observability/openlit/) - [Datadog](https://qdrant.tech/documentation/observability/datadog/)  [Platforms](https://qdrant.tech/documentation/platforms/)  - [Apify](https://qdrant.tech/documentation/platforms/apify/) - [BuildShip](https://qdrant.tech/documentation/platforms/buildship/) - [Keboola](https://qdrant.tech/documentation/platforms/keboola/) - [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/) - [Make.com](https://qdrant.tech/documentation/platforms/make/) - [N8N](https://qdrant.tech/documentation/platforms/n8n/) - [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/) - [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/) - [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/) - [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/) - [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/) - [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)",
    "metadata": {
      "chunk_id": "9b70691be664-0004",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Integrations"
      ],
      "heading_text": "Integrations",
      "token_count": 1498,
      "char_count": 5661,
      "start_char": 10041,
      "end_char": 15702,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "chunking_strategy": "hierarchical_context_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.610533",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Integrations",
      "chunk_hash": "8a3f2e84e2a75ae9",
      "content_digest": "8a3f2e84e2a75ae9",
      "chunk_length": 5661,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "https",
          "qdrant",
          "tech",
          "documentation",
          "frameworks",
          "embeddings",
          "platforms",
          "data",
          "management",
          "observability",
          "airbyte",
          "apache",
          "airflow",
          "spark",
          "cocoindex",
          "cognee",
          "confluent",
          "dlt",
          "fluvio",
          "redpanda"
        ],
        "term_weights": [
          {
            "term": "https",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "qdrant",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "tech",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "documentation",
            "tf": 80,
            "weight": 0.133779
          },
          {
            "term": "frameworks",
            "tf": 35,
            "weight": 0.058528
          },
          {
            "term": "embeddings",
            "tf": 21,
            "weight": 0.035117
          },
          {
            "term": "platforms",
            "tf": 14,
            "weight": 0.023411
          },
          {
            "term": "data",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "management",
            "tf": 12,
            "weight": 0.020067
          },
          {
            "term": "observability",
            "tf": 5,
            "weight": 0.008361
          },
          {
            "term": "airbyte",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "apache",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "airflow",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "spark",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cocoindex",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "cognee",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "confluent",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "dlt",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "fluvio",
            "tf": 2,
            "weight": 0.003344
          },
          {
            "term": "redpanda",
            "tf": 2,
            "weight": 0.003344
          }
        ],
        "unique_terms": 113,
        "total_terms": 598
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Integrations",
        "data",
        "documentation",
        "embeddings",
        "frameworks",
        "https",
        "management",
        "observability",
        "platforms",
        "qdrant",
        "tech"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6730769230769231,
      "overall": 0.7576923076923077
    }
  },
  {
    "text": "### Examples  [Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)  - [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/) - [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)  [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)  - [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/) - [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/) - [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)  [Build Prototypes](https://qdrant.tech/documentation/examples/)  - [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/) - [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/) - [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/) - [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/) - [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/) - [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/) - [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/) - [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/) - [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/) - [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/) - [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/) - [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)  [Practice Datasets](https://qdrant.tech/documentation/datasets/)  - [Documentation](https://qdrant.tech/documentation/) - - [Search precision](https://qdrant.tech/documentation/search-precision/) - - Automate filtering with LLMs",
    "metadata": {
      "chunk_id": "9b70691be664-0005",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Examples"
      ],
      "heading_text": "Examples",
      "token_count": 615,
      "char_count": 2511,
      "start_char": 15704,
      "end_char": 18215,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8976744186046512,
      "chunking_strategy": "hierarchical_context_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.612150",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Examples",
      "chunk_hash": "30a9ae7610627fd3",
      "content_digest": "30a9ae7610627fd3",
      "chunk_length": 2511,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "documentation",
          "https",
          "tech",
          "examples",
          "search",
          "with",
          "rag",
          "data",
          "chatbot",
          "precision",
          "send",
          "system",
          "semantic",
          "and",
          "cohere",
          "reranking",
          "automate",
          "filtering",
          "llms"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 32,
            "weight": 0.107744
          },
          {
            "term": "documentation",
            "tf": 24,
            "weight": 0.080808
          },
          {
            "term": "https",
            "tf": 23,
            "weight": 0.077441
          },
          {
            "term": "tech",
            "tf": 23,
            "weight": 0.077441
          },
          {
            "term": "examples",
            "tf": 14,
            "weight": 0.047138
          },
          {
            "term": "search",
            "tf": 12,
            "weight": 0.040404
          },
          {
            "term": "with",
            "tf": 10,
            "weight": 0.03367
          },
          {
            "term": "rag",
            "tf": 9,
            "weight": 0.030303
          },
          {
            "term": "data",
            "tf": 7,
            "weight": 0.023569
          },
          {
            "term": "chatbot",
            "tf": 6,
            "weight": 0.020202
          },
          {
            "term": "precision",
            "tf": 5,
            "weight": 0.016835
          },
          {
            "term": "send",
            "tf": 5,
            "weight": 0.016835
          },
          {
            "term": "system",
            "tf": 5,
            "weight": 0.016835
          },
          {
            "term": "semantic",
            "tf": 4,
            "weight": 0.013468
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.013468
          },
          {
            "term": "cohere",
            "tf": 4,
            "weight": 0.013468
          },
          {
            "term": "reranking",
            "tf": 3,
            "weight": 0.010101
          },
          {
            "term": "automate",
            "tf": 3,
            "weight": 0.010101
          },
          {
            "term": "filtering",
            "tf": 3,
            "weight": 0.010101
          },
          {
            "term": "llms",
            "tf": 3,
            "weight": 0.010101
          }
        ],
        "unique_terms": 96,
        "total_terms": 297
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Examples",
        "chatbot",
        "data",
        "documentation",
        "examples",
        "https",
        "qdrant",
        "rag",
        "search",
        "tech",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8976744186046512,
      "overall": 0.8325581395348838
    }
  },
  {
    "text": "# Automate filtering with LLMs\n\nOur [complete guide to filtering in vector search](https://qdrant.tech/articles/vector-search-filtering/) describes why filtering is important, and how to implement it with Qdrant. However, applying filters is easier when you build an application with a traditional interface. Your UI may contain a form with checkboxes, sliders, and other elements that users can use to set their criteria. But what if you want to build a RAG-powered application with just the conversational interface, or even voice commands? In this case, you need to automate the filtering process!\n\nLLMs seem to be particularly good at this task. They can understand natural language and generate structured output based on it. In this tutorial, we’ll show you how to use LLMs to automate filtering in your vector search application.",
    "metadata": {
      "chunk_id": "9b70691be664-0006",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Automate filtering with LLMs"
      ],
      "heading_text": "Automate filtering with LLMs",
      "token_count": 169,
      "char_count": 836,
      "start_char": 18217,
      "end_char": 19053,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.695511811023622,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.612914",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Automate filtering with LLMs",
      "chunk_hash": "f6877389ea9b32e9",
      "content_digest": "f6877389ea9b32e9",
      "chunk_length": 836,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "filtering",
          "with",
          "you",
          "automate",
          "llms",
          "vector",
          "search",
          "and",
          "application",
          "this",
          "qdrant",
          "how",
          "build",
          "interface",
          "your",
          "can",
          "use",
          "the",
          "our",
          "complete"
        ],
        "term_weights": [
          {
            "term": "filtering",
            "tf": 6,
            "weight": 0.056075
          },
          {
            "term": "with",
            "tf": 5,
            "weight": 0.046729
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.037383
          },
          {
            "term": "automate",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "llms",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "vector",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "search",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "application",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "this",
            "tf": 3,
            "weight": 0.028037
          },
          {
            "term": "qdrant",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "how",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "build",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "interface",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "your",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "can",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "use",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.018692
          },
          {
            "term": "our",
            "tf": 1,
            "weight": 0.009346
          },
          {
            "term": "complete",
            "tf": 1,
            "weight": 0.009346
          }
        ],
        "unique_terms": 73,
        "total_terms": 107
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Automate filtering with LLMs",
        "and",
        "application",
        "automate",
        "filtering",
        "llms",
        "search",
        "this",
        "vector",
        "with",
        "you"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.695511811023622,
      "overall": 0.7651706036745406
    }
  },
  {
    "text": "## Few notes on Qdrant filters\n\nQdrant Python SDK defines the models using [Pydantic](https://docs.pydantic.dev/latest/). This library is de facto standard for data validation and serialization in Python. It allows you to define the structure of your data using Python type hints. For example, our `Filter` model is defined as follows:\n\n```python\nclass Filter(BaseModel, extra=\"forbid\"):\n    should: Optional[Union[List[\"Condition\"], \"Condition\"]] = Field(\n        default=None, description=\"At least one of those conditions should match\"\n    )\n    min_should: Optional[\"MinShould\"] = Field(\n        default=None, description=\"At least minimum amount of given conditions should match\"\n    )\n    must: Optional[Union[List[\"Condition\"], \"Condition\"]] = Field(default=None, description=\"All conditions must match\")\n    must_not: Optional[Union[List[\"Condition\"], \"Condition\"]] = Field(\n        default=None, description=\"All conditions must NOT match\"\n    )\n```\n\nQdrant filters may be nested, and you can express even the most complex conditions using the `must`, `should`, and `must_not` notation.",
    "metadata": {
      "chunk_id": "9b70691be664-0007",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Few notes on Qdrant filters"
      ],
      "heading_text": "Few notes on Qdrant filters",
      "token_count": 241,
      "char_count": 1095,
      "start_char": 19055,
      "end_char": 20150,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6931496062992125,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.613458",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Few notes on Qdrant filters",
      "chunk_hash": "b2e8a20b1195a589",
      "content_digest": "b2e8a20b1195a589",
      "chunk_length": 1095,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "condition",
          "must",
          "should",
          "conditions",
          "python",
          "the",
          "optional",
          "field",
          "default",
          "none",
          "description",
          "match",
          "qdrant",
          "using",
          "and",
          "union",
          "list",
          "not",
          "filters",
          "pydantic"
        ],
        "term_weights": [
          {
            "term": "condition",
            "tf": 6,
            "weight": 0.045113
          },
          {
            "term": "must",
            "tf": 6,
            "weight": 0.045113
          },
          {
            "term": "should",
            "tf": 5,
            "weight": 0.037594
          },
          {
            "term": "conditions",
            "tf": 5,
            "weight": 0.037594
          },
          {
            "term": "python",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "the",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "optional",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "field",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "default",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "none",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "description",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "match",
            "tf": 4,
            "weight": 0.030075
          },
          {
            "term": "qdrant",
            "tf": 3,
            "weight": 0.022556
          },
          {
            "term": "using",
            "tf": 3,
            "weight": 0.022556
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.022556
          },
          {
            "term": "union",
            "tf": 3,
            "weight": 0.022556
          },
          {
            "term": "list",
            "tf": 3,
            "weight": 0.022556
          },
          {
            "term": "not",
            "tf": 3,
            "weight": 0.022556
          },
          {
            "term": "filters",
            "tf": 2,
            "weight": 0.015038
          },
          {
            "term": "pydantic",
            "tf": 2,
            "weight": 0.015038
          }
        ],
        "unique_terms": 71,
        "total_terms": 133
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Few notes on Qdrant filters",
        "condition",
        "conditions",
        "default",
        "field",
        "must",
        "none",
        "optional",
        "python",
        "should",
        "the"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6931496062992125,
      "overall": 0.7977165354330708
    }
  },
  {
    "text": "## Structured output from LLMs\n\nIt isn’t an uncommon practice to use LLMs to generate structured output. It is primarily useful if their output is intended for further processing by a different application. For example, you can use LLMs to generate SQL queries, JSON objects, and most importantly, Qdrant filters. Pydantic got adopted by the LLM ecosystem quite well, so there is plenty of libraries which uses Pydantic models to define the structure of the output for the Language Models.\n\nOne of the interesting projects in this area is [Instructor](https://python.useinstructor.com/) that allows you to play with different LLM providers and restrict their output to a specific structure. Let’s install the library and already choose a provider we’ll use in this tutorial:\n\n```shell\npip install \"instructor[anthropic]\"\n```\n\nAnthropic is not the only option out there, as Instructor supports many other providers including OpenAI, Ollama, Llama, Gemini, Vertex AI, Groq, Litellm and others. You can choose the one that fits your needs the best, or the one you already use in your RAG.",
    "metadata": {
      "chunk_id": "9b70691be664-0008",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Structured output from LLMs"
      ],
      "heading_text": "Structured output from LLMs",
      "token_count": 240,
      "char_count": 1085,
      "start_char": 20152,
      "end_char": 21237,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6882456140350878,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.613880",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Structured output from LLMs",
      "chunk_hash": "26303c529d5784f0",
      "content_digest": "26303c529d5784f0",
      "chunk_length": 1085,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "output",
          "use",
          "you",
          "and",
          "llms",
          "for",
          "one",
          "instructor",
          "structured",
          "generate",
          "their",
          "different",
          "can",
          "pydantic",
          "llm",
          "there",
          "models",
          "structure",
          "this"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 10,
            "weight": 0.06993
          },
          {
            "term": "output",
            "tf": 5,
            "weight": 0.034965
          },
          {
            "term": "use",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "and",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "llms",
            "tf": 3,
            "weight": 0.020979
          },
          {
            "term": "for",
            "tf": 3,
            "weight": 0.020979
          },
          {
            "term": "one",
            "tf": 3,
            "weight": 0.020979
          },
          {
            "term": "instructor",
            "tf": 3,
            "weight": 0.020979
          },
          {
            "term": "structured",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "generate",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "their",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "different",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "can",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "pydantic",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "llm",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "there",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "structure",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "this",
            "tf": 2,
            "weight": 0.013986
          }
        ],
        "unique_terms": 95,
        "total_terms": 143
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Structured output from LLMs",
        "and",
        "for",
        "instructor",
        "llms",
        "one",
        "output",
        "structured",
        "the",
        "use",
        "you"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6882456140350878,
      "overall": 0.7627485380116958
    }
  },
  {
    "text": "## Using Instructor to generate Qdrant filters\n\nInstructor has some helper methods to decorate the LLM APIs, so you can interact with them as if you were using their normal SDKs. In case of Anthropic, you just pass an instance of `Anthropic` class to the `from_anthropic` function:\n\n```python\nimport instructor\nfrom anthropic import Anthropic\n\nanthropic_client = instructor.from_anthropic(\n    client=Anthropic(\n        api_key=\"YOUR_API_KEY\",\n    )\n)\n```\n\nA decorated client slightly modifies the original API, so you can pass the `response_model` parameter to the `.messages.create` method. This parameter should be a Pydantic model that defines the structure of the output. In case of Qdrant filters, it should be a `Filter` model:\n\n```python\nfrom qdrant_client import models\n\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"red T-shirt\"\n        }\n    ],\n)\n```\n\nThe output of this code will be a Pydantic model that represents a Qdrant filter. Surprisingly, there is no need to pass additional instructions to already figure out that the user wants to filter by the color and the type of the product. Here is how the output looks like:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=[\n        FieldCondition(\n            key=\"color\", \n            match=MatchValue(value=\"red\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"type\", \n            match=MatchValue(value=\"t-shirt\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        )\n    ], \n    must_not=None\n)\n```\n\nObviously, giving the model complete freedom to generate the filter may lead to unexpected results, or no results at all. Your collection probably has payloads with a specific structure, so it doesn’t make sense to use anything else. Moreover, **it’s considered a good practice to filter by the fields that have been indexed**. That’s why it makes sense to automatically determine the indexed fields and restrict the output to them.",
    "metadata": {
      "chunk_id": "9b70691be664-0009",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Using Instructor to generate Qdrant filters"
      ],
      "heading_text": "Using Instructor to generate Qdrant filters",
      "token_count": 525,
      "char_count": 2363,
      "start_char": 21239,
      "end_char": 23602,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6898181818181819,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.614832",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Using Instructor to generate Qdrant filters",
      "chunk_hash": "e1a22d3527e386ad",
      "content_digest": "e1a22d3527e386ad",
      "chunk_length": 2363,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "none",
          "anthropic",
          "filter",
          "model",
          "geo",
          "qdrant",
          "client",
          "that",
          "instructor",
          "you",
          "from",
          "key",
          "should",
          "output",
          "pass",
          "python",
          "import",
          "api",
          "messages"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 18,
            "weight": 0.067416
          },
          {
            "term": "none",
            "tf": 13,
            "weight": 0.048689
          },
          {
            "term": "anthropic",
            "tf": 9,
            "weight": 0.033708
          },
          {
            "term": "filter",
            "tf": 8,
            "weight": 0.029963
          },
          {
            "term": "model",
            "tf": 7,
            "weight": 0.026217
          },
          {
            "term": "geo",
            "tf": 6,
            "weight": 0.022472
          },
          {
            "term": "qdrant",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "client",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "that",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "instructor",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "you",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "from",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "key",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "should",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "output",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "pass",
            "tf": 3,
            "weight": 0.011236
          },
          {
            "term": "python",
            "tf": 3,
            "weight": 0.011236
          },
          {
            "term": "import",
            "tf": 3,
            "weight": 0.011236
          },
          {
            "term": "api",
            "tf": 3,
            "weight": 0.011236
          },
          {
            "term": "messages",
            "tf": 3,
            "weight": 0.011236
          }
        ],
        "unique_terms": 134,
        "total_terms": 267
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Using Instructor to generate Qdrant filters",
        "anthropic",
        "client",
        "filter",
        "geo",
        "instructor",
        "model",
        "none",
        "qdrant",
        "that",
        "the"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6898181818181819,
      "overall": 0.7966060606060607
    }
  },
  {
    "text": "### Restricting the available fields\n\nQdrant collection info contains a list of the indexes created on a particular collection. You can use this information to automatically determine the fields that can be used for filtering. Here is how you can do it:\n\n```python\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\"http://localhost:6333\")\ncollection_info = client.get_collection(collection_name=\"test_filter\")\nindexes = collection_info.payload_schema\nprint(indexes)\n```\n\nOutput:\n\n```python\n{\n    \"city.location\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.GEO,\n        ...\n    ),\n    \"city.name\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.KEYWORD,\n        ...\n    ),\n    \"color\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.KEYWORD,\n        ...\n    ),\n    \"fabric\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.KEYWORD,\n        ...\n    ),\n    \"price\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.FLOAT,\n        ...\n    ),\n}\n```\n\nOur LLM should know the names of the fields it can use, but also their type, as e.g., range filtering only makes sense for numerical fields, and geo filtering on non-geo fields won’t yield anything meaningful. You can pass this information as a part of the prompt to the LLM, so let’s encode it as a string:\n\n```python\nformatted_indexes = \"\\n\".join([\n    f\"- {index_name} - {index.data_type.name}\"\n    for index_name, index in indexes.items()\n])\nprint(formatted_indexes)\n```\n\nOutput:\n\n```text\n- fabric - KEYWORD\n- city.name - KEYWORD\n- color - KEYWORD\n- price - FLOAT\n- city.location - GEO\n```\n\n**It’s a good idea to cache the list of the available fields and their types**, as they are not supposed to change often. Our interactions with the LLM should be slightly different now:\n\n```python\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>color is red</query>\"\n                f\"<indexes>\\n{formatted_indexes}\\n</indexes>\"\n            )\n        }\n    ],\n)\n```\n\nOutput:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=FieldCondition(\n        key=\"color\", \n        match=MatchValue(value=\"red\"), \n        range=None, \n        geo_bounding_box=None, \n        geo_radius=None, \n        geo_polygon=None, \n        values_count=None\n    ), \n    must_not=None\n)\n```\n\nThe same query, restricted to the available fields, now generates better criteria, as it doesn’t try to filter by the fields that don’t exist in the collection.",
    "metadata": {
      "chunk_id": "9b70691be664-0010",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Restricting the available fields"
      ],
      "heading_text": "Restricting the available fields",
      "token_count": 619,
      "char_count": 2632,
      "start_char": 23604,
      "end_char": 26236,
      "semantic_score": 0.7,
      "structural_score": 0.9999999999999999,
      "retrieval_quality": 0.6833922261484099,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.615886",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Restricting the available fields",
      "chunk_hash": "5b15ec776ff66dee",
      "content_digest": "5b15ec776ff66dee",
      "chunk_length": 2632,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "indexes",
          "fields",
          "none",
          "collection",
          "type",
          "geo",
          "name",
          "data",
          "keyword",
          "can",
          "python",
          "filter",
          "payloadindexinfo",
          "payloadschematype",
          "client",
          "city",
          "color",
          "should",
          "index"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 14,
            "weight": 0.049645
          },
          {
            "term": "indexes",
            "tf": 9,
            "weight": 0.031915
          },
          {
            "term": "fields",
            "tf": 8,
            "weight": 0.028369
          },
          {
            "term": "none",
            "tf": 8,
            "weight": 0.028369
          },
          {
            "term": "collection",
            "tf": 7,
            "weight": 0.024823
          },
          {
            "term": "type",
            "tf": 7,
            "weight": 0.024823
          },
          {
            "term": "geo",
            "tf": 7,
            "weight": 0.024823
          },
          {
            "term": "name",
            "tf": 6,
            "weight": 0.021277
          },
          {
            "term": "data",
            "tf": 6,
            "weight": 0.021277
          },
          {
            "term": "keyword",
            "tf": 6,
            "weight": 0.021277
          },
          {
            "term": "can",
            "tf": 5,
            "weight": 0.01773
          },
          {
            "term": "python",
            "tf": 5,
            "weight": 0.01773
          },
          {
            "term": "filter",
            "tf": 5,
            "weight": 0.01773
          },
          {
            "term": "payloadindexinfo",
            "tf": 5,
            "weight": 0.01773
          },
          {
            "term": "payloadschematype",
            "tf": 5,
            "weight": 0.01773
          },
          {
            "term": "client",
            "tf": 4,
            "weight": 0.014184
          },
          {
            "term": "city",
            "tf": 4,
            "weight": 0.014184
          },
          {
            "term": "color",
            "tf": 4,
            "weight": 0.014184
          },
          {
            "term": "should",
            "tf": 4,
            "weight": 0.014184
          },
          {
            "term": "index",
            "tf": 4,
            "weight": 0.014184
          }
        ],
        "unique_terms": 138,
        "total_terms": 282
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": true,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Restricting the available fields",
        "collection",
        "data",
        "fields",
        "geo",
        "indexes",
        "keyword",
        "name",
        "none",
        "the",
        "type"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.9999999999999999,
      "retrieval_quality": 0.6833922261484099,
      "overall": 0.7944640753828032
    }
  },
  {
    "text": "### Testing the LLM output\n\nAlthough the LLMs are quite powerful, they are not perfect. If you plan to automate filtering, it makes sense to run some tests to see how well they perform. Especially edge cases, like queries that cannot be expressed as filters. Let’s see how the LLM will handle the following query:\n\n```python\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>fruit salad with no more than 100 calories</query>\"\n                f\"<indexes>\\n{formatted_indexes}\\n</indexes>\"\n            )\n        }\n    ],\n)\n```\n\nOutput:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=FieldCondition(\n        key=\"price\", \n        match=None, \n        range=Range(lt=None, gt=None, gte=None, lte=100.0), \n        geo_bounding_box=None, \n        geo_radius=None, \n        geo_polygon=None, \n        values_count=None\n    ), \n    must_not=None\n)\n```\n\nSurprisingly, the LLM extracted the calorie information from the query and generated a filter based on the price field. It somehow extracts any numerical information from the query and tries to match it with the available fields.\n\nGenerally, giving model some more guidance on how to interpret the query may lead to better results. Adding a system prompt that defines the rules for the query interpretation may help the model to do a better job. Here is how you can do it:\n\n```python\nSYSTEM_PROMPT = \"\"\"\nYou are extracting filters from a text query. Please follow the following rules:\n1. Query is provided in the form of a text enclosed in <query> tags.\n2. Available indexes are put at the end of the text in the form of a list enclosed in <indexes> tags.\n3. You cannot use any field that is not available in the indexes.\n4. Generate a filter only if you are certain that user's intent matches the field name.\n5. Prices are always in USD.\n6. It's better not to generate a filter than to generate an incorrect one.\n\"\"\"\n\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": SYSTEM_PROMPT.strip(),\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Okay, I will follow all the rules.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>fruit salad with no more than 100 calories</query>\"\n                f\"<indexes>\\n{formatted_indexes}\\n</indexes>\"\n            )\n        }\n    ],\n)\n```\n\nCurrent output:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=None, \n    must_not=None\n)\n```",
    "metadata": {
      "chunk_id": "9b70691be664-0011",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Testing the LLM output"
      ],
      "heading_text": "Testing the LLM output",
      "token_count": 648,
      "char_count": 2801,
      "start_char": 26238,
      "end_char": 29039,
      "semantic_score": 0.7,
      "structural_score": 0.7,
      "retrieval_quality": 0.865,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.617039",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Testing the LLM output",
      "chunk_hash": "3e12a38b26bd00d2",
      "content_digest": "3e12a38b26bd00d2",
      "chunk_length": 2801,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "none",
          "query",
          "filter",
          "indexes",
          "are",
          "model",
          "not",
          "you",
          "how",
          "that",
          "python",
          "messages",
          "role",
          "user",
          "content",
          "should",
          "must",
          "llm",
          "output"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 22,
            "weight": 0.068323
          },
          {
            "term": "none",
            "tf": 15,
            "weight": 0.046584
          },
          {
            "term": "query",
            "tf": 12,
            "weight": 0.037267
          },
          {
            "term": "filter",
            "tf": 9,
            "weight": 0.02795
          },
          {
            "term": "indexes",
            "tf": 9,
            "weight": 0.02795
          },
          {
            "term": "are",
            "tf": 6,
            "weight": 0.018634
          },
          {
            "term": "model",
            "tf": 6,
            "weight": 0.018634
          },
          {
            "term": "not",
            "tf": 5,
            "weight": 0.015528
          },
          {
            "term": "you",
            "tf": 5,
            "weight": 0.015528
          },
          {
            "term": "how",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "that",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "python",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "messages",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "role",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "user",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "content",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "should",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "must",
            "tf": 4,
            "weight": 0.012422
          },
          {
            "term": "llm",
            "tf": 3,
            "weight": 0.009317
          },
          {
            "term": "output",
            "tf": 3,
            "weight": 0.009317
          }
        ],
        "unique_terms": 148,
        "total_terms": 322
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Testing the LLM output",
        "are",
        "filter",
        "how",
        "indexes",
        "model",
        "none",
        "not",
        "query",
        "the",
        "you"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7,
      "retrieval_quality": 0.865,
      "overall": 0.7549999999999999
    }
  },
  {
    "text": "### Handling complex queries\n\nWe have a bunch of indexes created on the collection, and it is quite interesting to see how the LLM will handle more complex queries. For example, let’s see how it will handle the following query:\n\n```python\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": SYSTEM_PROMPT.strip(),\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Okay, I will follow all the rules.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>\"\n                \"white T-shirt available no more than 30 miles from London, \"\n                \"but not in the city itself, below $15.70, not made from polyester\"\n                \"</query>\\n\"\n                \"<indexes>\\n\"\n                f\"{formatted_indexes}\\n\"\n                \"</indexes>\"\n            )\n        },\n    ],\n)\n```\n\nIt might be surprising, but Anthropic Claude is able to generate even such complex filters. Here is the output:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=[\n        FieldCondition(\n            key=\"color\", \n            match=MatchValue(value=\"white\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"city.location\", \n            match=None, \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=GeoRadius(\n                center=GeoPoint(lon=-0.1276, lat=51.5074), \n                radius=48280.0\n            ), \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"price\", \n            match=None, \n            range=Range(lt=15.7, gt=None, gte=None, lte=None), \n            geo_bounding_box=None,\n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        )\n    ], must_not=[\n        FieldCondition(\n            key=\"city.name\", \n            match=MatchValue(value=\"London\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"fabric\", \n            match=MatchValue(value=\"polyester\"),\n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None,\n            geo_polygon=None, \n            values_count=None\n        )\n    ]\n)\n```\n\nThe model even knows the coordinates of London and uses them to generate the geo filter. It isn’t the best idea to rely on the model to generate such complex filters, but it’s quite impressive that it can do it.",
    "metadata": {
      "chunk_id": "9b70691be664-0012",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Handling complex queries"
      ],
      "heading_text": "Handling complex queries",
      "token_count": 624,
      "char_count": 2868,
      "start_char": 29041,
      "end_char": 31909,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.8893333333333333,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.618534",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Handling complex queries",
      "chunk_hash": "f2e826c6be00ed8c",
      "content_digest": "f2e826c6be00ed8c",
      "chunk_length": 2868,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "none",
          "geo",
          "the",
          "range",
          "radius",
          "fieldcondition",
          "key",
          "match",
          "bounding",
          "box",
          "polygon",
          "values",
          "count",
          "complex",
          "indexes",
          "filter",
          "model",
          "will",
          "query",
          "role"
        ],
        "term_weights": [
          {
            "term": "none",
            "tf": 30,
            "weight": 0.11236
          },
          {
            "term": "geo",
            "tf": 16,
            "weight": 0.059925
          },
          {
            "term": "the",
            "tf": 11,
            "weight": 0.041199
          },
          {
            "term": "range",
            "tf": 6,
            "weight": 0.022472
          },
          {
            "term": "radius",
            "tf": 6,
            "weight": 0.022472
          },
          {
            "term": "fieldcondition",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "key",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "match",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "bounding",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "box",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "polygon",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "values",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "count",
            "tf": 5,
            "weight": 0.018727
          },
          {
            "term": "complex",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "indexes",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "filter",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "model",
            "tf": 4,
            "weight": 0.014981
          },
          {
            "term": "will",
            "tf": 3,
            "weight": 0.011236
          },
          {
            "term": "query",
            "tf": 3,
            "weight": 0.011236
          },
          {
            "term": "role",
            "tf": 3,
            "weight": 0.011236
          }
        ],
        "unique_terms": 117,
        "total_terms": 267
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Handling complex queries",
        "bounding",
        "box",
        "fieldcondition",
        "geo",
        "key",
        "match",
        "none",
        "radius",
        "range",
        "the"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.8893333333333333,
      "overall": 0.863111111111111
    }
  },
  {
    "text": "## Further steps\n\nReal production systems would rather require more testing and validation of the LLM output. Building a ground truth dataset with the queries and the expected filters would be a good idea. You can use this dataset to evaluate the model performance and to see how it behaves in different scenarios.",
    "metadata": {
      "chunk_id": "9b70691be664-0013",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Further steps"
      ],
      "heading_text": "Further steps",
      "token_count": 58,
      "char_count": 314,
      "start_char": 31911,
      "end_char": 32225,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.739056603773585,
      "chunking_strategy": "hierarchical_context_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.618716",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Further steps",
      "chunk_hash": "c7b9161ed35be8ce",
      "content_digest": "c7b9161ed35be8ce",
      "chunk_length": 314,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "the",
          "and",
          "would",
          "dataset",
          "further",
          "steps",
          "real",
          "production",
          "systems",
          "rather",
          "require",
          "more",
          "testing",
          "validation",
          "llm",
          "output",
          "building",
          "ground",
          "truth",
          "with"
        ],
        "term_weights": [
          {
            "term": "the",
            "tf": 4,
            "weight": 0.090909
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.068182
          },
          {
            "term": "would",
            "tf": 2,
            "weight": 0.045455
          },
          {
            "term": "dataset",
            "tf": 2,
            "weight": 0.045455
          },
          {
            "term": "further",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "steps",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "real",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "production",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "systems",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "rather",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "require",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "more",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "testing",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "validation",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "llm",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "output",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "building",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "ground",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "truth",
            "tf": 1,
            "weight": 0.022727
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.022727
          }
        ],
        "unique_terms": 37,
        "total_terms": 44
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Further steps",
        "and",
        "dataset",
        "further",
        "production",
        "rather",
        "real",
        "steps",
        "systems",
        "the",
        "would"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.739056603773585,
      "overall": 0.7796855345911949
    }
  },
  {
    "text": "##### Was this page useful? Yes No  Thank you for your feedback! 🙏  We are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/search-precision/automate-filtering-with-llms.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue. On this page:  - [Automate filtering with LLMs](#automate-filtering-with-llms.md)    - [Few notes on Qdrant filters](#few-notes-on-qdrant-filters.md)    - [Structured output from LLMs](#structured-output-from-llms.md)    - [Using Instructor to generate Qdrant filters](#using-instructor-to-generate-qdrant-filters.md)      - [Restricting the available fields](#restricting-the-available-fields.md)     - [Testing the LLM output](#testing-the-llm-output.md)     - [Handling complex queries](#handling-complex-queries.md)    - [Further steps](#further-steps.md)  * [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/search-precision/automate-filtering-with-llms.md) * [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)",
    "metadata": {
      "chunk_id": "9b70691be664-0014",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Was this page useful?"
      ],
      "heading_text": "Was this page useful?",
      "token_count": 312,
      "char_count": 1151,
      "start_char": 32227,
      "end_char": 33378,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5285397590361446,
      "chunking_strategy": "hierarchical_context_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T07:48:26.619240",
      "document_id": "9b70691be664",
      "document_name": "_documentation_search-precision_automate-filtering-with-llms_",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_filename": "_documentation_search-precision_automate-filtering-with-llms_.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms",
      "relative_path": "Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "hierarchy_path": "Was this page useful?",
      "chunk_hash": "c12619589469b824",
      "content_digest": "c12619589469b824",
      "chunk_length": 1151,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "qdrant",
          "page",
          "github",
          "landing",
          "llms",
          "https",
          "com",
          "automate",
          "filtering",
          "with",
          "filters",
          "output",
          "the",
          "this",
          "you",
          "edit",
          "tree",
          "master",
          "content",
          "documentation"
        ],
        "term_weights": [
          {
            "term": "qdrant",
            "tf": 10,
            "weight": 0.06993
          },
          {
            "term": "page",
            "tf": 7,
            "weight": 0.048951
          },
          {
            "term": "github",
            "tf": 7,
            "weight": 0.048951
          },
          {
            "term": "landing",
            "tf": 6,
            "weight": 0.041958
          },
          {
            "term": "llms",
            "tf": 6,
            "weight": 0.041958
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "com",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "automate",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "filtering",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "filters",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "output",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "the",
            "tf": 4,
            "weight": 0.027972
          },
          {
            "term": "this",
            "tf": 3,
            "weight": 0.020979
          },
          {
            "term": "you",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "edit",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "tree",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "master",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "content",
            "tf": 2,
            "weight": 0.013986
          },
          {
            "term": "documentation",
            "tf": 2,
            "weight": 0.013986
          }
        ],
        "unique_terms": 56,
        "total_terms": 143
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Was this page useful?",
        "automate",
        "com",
        "filtering",
        "github",
        "https",
        "landing",
        "llms",
        "page",
        "qdrant",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5285397590361446,
      "overall": 0.7095132530120482
    }
  }
]