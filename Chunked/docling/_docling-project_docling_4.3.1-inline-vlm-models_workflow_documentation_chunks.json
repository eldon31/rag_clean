[
  {
    "text": "## Architecture Overview\n\nThe inline VLM model system provides three specialized implementations sharing a common interface:\n\n```\n```\n\n**Sources:** [docling/models/base\\_model.py46-127](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L127) [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L376) [docling/models/vlm\\_models\\_inline/mlx\\_model.py33-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L33-L318) [docling/models/vlm\\_models\\_inline/vllm\\_model.py25-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L25-L301)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0001",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Architecture Overview"
      ],
      "heading_text": "Architecture Overview",
      "token_count": 225,
      "char_count": 818,
      "start_char": 6440,
      "end_char": 7258,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5769565217391305,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.345115",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Architecture Overview",
      "chunk_hash": "4361725be3d44c3b",
      "content_digest": "4361725be3d44c3b",
      "chunk_length": 818,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docling",
          "models",
          "model",
          "inline",
          "vlm",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "base",
          "transformers",
          "mlx",
          "vllm",
          "architecture",
          "overview",
          "the",
          "system",
          "provides"
        ],
        "term_weights": [
          {
            "term": "docling",
            "tf": 16,
            "weight": 0.141593
          },
          {
            "term": "models",
            "tf": 14,
            "weight": 0.123894
          },
          {
            "term": "model",
            "tf": 9,
            "weight": 0.079646
          },
          {
            "term": "inline",
            "tf": 7,
            "weight": 0.061947
          },
          {
            "term": "vlm",
            "tf": 7,
            "weight": 0.061947
          },
          {
            "term": "https",
            "tf": 4,
            "weight": 0.035398
          },
          {
            "term": "github",
            "tf": 4,
            "weight": 0.035398
          },
          {
            "term": "com",
            "tf": 4,
            "weight": 0.035398
          },
          {
            "term": "project",
            "tf": 4,
            "weight": 0.035398
          },
          {
            "term": "blob",
            "tf": 4,
            "weight": 0.035398
          },
          {
            "term": "f7244a43",
            "tf": 4,
            "weight": 0.035398
          },
          {
            "term": "base",
            "tf": 2,
            "weight": 0.017699
          },
          {
            "term": "transformers",
            "tf": 2,
            "weight": 0.017699
          },
          {
            "term": "mlx",
            "tf": 2,
            "weight": 0.017699
          },
          {
            "term": "vllm",
            "tf": 2,
            "weight": 0.017699
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.00885
          },
          {
            "term": "overview",
            "tf": 1,
            "weight": 0.00885
          },
          {
            "term": "the",
            "tf": 1,
            "weight": 0.00885
          },
          {
            "term": "system",
            "tf": 1,
            "weight": 0.00885
          },
          {
            "term": "provides",
            "tf": 1,
            "weight": 0.00885
          }
        ],
        "unique_terms": 43,
        "total_terms": 113
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Architecture Overview",
        "blob",
        "com",
        "docling",
        "github",
        "https",
        "inline",
        "model",
        "models",
        "project",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5769565217391305,
      "overall": 0.69231884057971
    }
  },
  {
    "text": "## Configuration via InlineVlmOptions  All inline VLM models are configured through `InlineVlmOptions`, which specifies the model repository, inference framework, and generation parameters:  | Parameter                   | Type                                               | Description                                                                                                | | --------------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | | `repo_id`                   | `str`                                              | Hugging Face repository identifier (e.g., `\"ibm-granite/granite-docling-258M\"`)                            | | `inference_framework`       | `InferenceFramework`                               | One of `TRANSFORMERS`, `MLX`, or `VLLM`                                                                    | | `transformers_model_type`   | `TransformersModelType`                            | Auto-loading class: `AUTOMODEL`, `AUTOMODEL_VISION2SEQ`, `AUTOMODEL_CAUSALLM`, `AUTOMODEL_IMAGETEXTTOTEXT` | | `transformers_prompt_style` | `TransformersPromptStyle`                          | Prompt formatting: `CHAT`, `RAW`, or `NONE`                                                                | | `response_format`           | `ResponseFormat`                                   | Expected output format: `DOCTAGS`, `MARKDOWN`, `HTML`, `OTSL`, or `PLAINTEXT`                              | | `torch_dtype`               | `Optional[str]`                                    | PyTorch dtype (e.g., `\"bfloat16\"`)                                                                         | | `max_new_tokens`            | `int`                                              | Maximum tokens to generate (default: `4096`)                                                               | | `temperature`               | `float`                                            | Sampling temperature (default: `0.0` for greedy)                                                           | | `scale`                     | `float`                                            | Image scaling factor (default: `2.0`)                                                                      | | `max_size`                  | `Optional[int]`                                    | Maximum image dimension                                                                                    | | `use_kv_cache`              | `bool`                                             | Enable key-value caching (default: `True`)                                                                 | | `stop_strings`              | `List[str]`                                        | Strings that trigger generation stop                                                                       | | `custom_stopping_criteria`  | `List[Union[StoppingCriteria, GenerationStopper]]` | Custom stopping logic                                                                                      | | `extra_generation_config`   | `Dict[str, Any]`                                   | Additional framework-specific generation parameters                                                        | | `extra_processor_kwargs`    | `Dict[str, Any]`                                   | Additional processor parameters                                                                            | | `quantized`                 | `bool`                                             | Enable quantization (default: `False`)                                                                     | | `load_in_8bit`              | `bool`                                             | Use 8-bit quantization (default: `True`)                                                                   | | `trust_remote_code`         | `bool`                                             | Allow remote code execution (default: `False`)                                                             | | `revision`                  | `str`                                              | Model revision/branch (default: `\"main\"`)                                                                  |  **Sources:** [docling/datamodel/pipeline\\_options\\_vlm\\_model.py54-89](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L54-L89)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0002",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Configuration via InlineVlmOptions"
      ],
      "heading_text": "Configuration via InlineVlmOptions",
      "token_count": 663,
      "char_count": 4450,
      "start_char": 7260,
      "end_char": 11710,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.6673306772908367,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "table_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.347559",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Configuration via InlineVlmOptions",
      "chunk_hash": "15eedd740b8f7638",
      "content_digest": "15eedd740b8f7638",
      "chunk_length": 4450,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "default",
          "str",
          "model",
          "docling",
          "generation",
          "automodel",
          "bool",
          "vlm",
          "framework",
          "parameters",
          "transformers",
          "inlinevlmoptions",
          "repository",
          "inference",
          "type",
          "granite",
          "prompt",
          "format",
          "dtype",
          "optional"
        ],
        "term_weights": [
          {
            "term": "default",
            "tf": 8,
            "weight": 0.037209
          },
          {
            "term": "str",
            "tf": 6,
            "weight": 0.027907
          },
          {
            "term": "model",
            "tf": 5,
            "weight": 0.023256
          },
          {
            "term": "docling",
            "tf": 5,
            "weight": 0.023256
          },
          {
            "term": "generation",
            "tf": 4,
            "weight": 0.018605
          },
          {
            "term": "automodel",
            "tf": 4,
            "weight": 0.018605
          },
          {
            "term": "bool",
            "tf": 4,
            "weight": 0.018605
          },
          {
            "term": "vlm",
            "tf": 3,
            "weight": 0.013953
          },
          {
            "term": "framework",
            "tf": 3,
            "weight": 0.013953
          },
          {
            "term": "parameters",
            "tf": 3,
            "weight": 0.013953
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.013953
          },
          {
            "term": "inlinevlmoptions",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "repository",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "inference",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "type",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "granite",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "prompt",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "format",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "dtype",
            "tf": 2,
            "weight": 0.009302
          },
          {
            "term": "optional",
            "tf": 2,
            "weight": 0.009302
          }
        ],
        "unique_terms": 141,
        "total_terms": 215
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Configuration via InlineVlmOptions",
        "automodel",
        "bool",
        "default",
        "docling",
        "framework",
        "generation",
        "model",
        "parameters",
        "str",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.6673306772908367,
      "overall": 0.7891102257636122
    }
  },
  {
    "text": "## Hugging Face Transformers Implementation",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0003",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Hugging Face Transformers Implementation"
      ],
      "heading_text": "Hugging Face Transformers Implementation",
      "token_count": 6,
      "char_count": 43,
      "start_char": 11712,
      "end_char": 11755,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.348770",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Hugging Face Transformers Implementation",
      "chunk_hash": "2ebca3c48bcabc77",
      "content_digest": "2ebca3c48bcabc77",
      "chunk_length": 43,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "hugging",
          "face",
          "transformers",
          "implementation"
        ],
        "term_weights": [
          {
            "term": "hugging",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "face",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "transformers",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "implementation",
            "tf": 1,
            "weight": 0.25
          }
        ],
        "unique_terms": 4,
        "total_terms": 4
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Hugging Face Transformers Implementation",
        "face",
        "hugging",
        "implementation",
        "transformers"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### Model Loading and Initialization\n\n`HuggingFaceTransformersVlmModel` loads models using Transformers' auto-loading classes:\n\n```\n```\n\nThe model class is selected based on `transformers_model_type`:\n\n- `AUTOMODEL` → `AutoModel`\n- `AUTOMODEL_CAUSALLM` → `AutoModelForCausalLM`\n- `AUTOMODEL_VISION2SEQ` → `AutoModelForVision2Seq`\n- `AUTOMODEL_IMAGETEXTTOTEXT` → `AutoModelForImageTextToText`\n\nThe processor's tokenizer padding is configured with `padding_side = \"left\"` for batch processing.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-138](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L138)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0004",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Model Loading and Initialization"
      ],
      "heading_text": "Model Loading and Initialization",
      "token_count": 196,
      "char_count": 700,
      "start_char": 11757,
      "end_char": 12457,
      "semantic_score": 0.7999999999999999,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5277358490566038,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.349395",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Model Loading and Initialization",
      "chunk_hash": "5744e10675f2eee0",
      "content_digest": "5744e10675f2eee0",
      "chunk_length": 700,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "model",
          "models",
          "automodel",
          "transformers",
          "docling",
          "loading",
          "the",
          "padding",
          "vlm",
          "inline",
          "and",
          "initialization",
          "huggingfacetransformersvlmmodel",
          "loads",
          "using",
          "auto",
          "classes",
          "class",
          "selected",
          "based"
        ],
        "term_weights": [
          {
            "term": "model",
            "tf": 5,
            "weight": 0.071429
          },
          {
            "term": "models",
            "tf": 5,
            "weight": 0.071429
          },
          {
            "term": "automodel",
            "tf": 5,
            "weight": 0.071429
          },
          {
            "term": "transformers",
            "tf": 4,
            "weight": 0.057143
          },
          {
            "term": "docling",
            "tf": 4,
            "weight": 0.057143
          },
          {
            "term": "loading",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "padding",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "vlm",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "inline",
            "tf": 2,
            "weight": 0.028571
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "initialization",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "huggingfacetransformersvlmmodel",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "loads",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "using",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "auto",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "classes",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "class",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "selected",
            "tf": 1,
            "weight": 0.014286
          },
          {
            "term": "based",
            "tf": 1,
            "weight": 0.014286
          }
        ],
        "unique_terms": 47,
        "total_terms": 70
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": true,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Model Loading and Initialization",
        "automodel",
        "docling",
        "inline",
        "loading",
        "model",
        "models",
        "padding",
        "the",
        "transformers",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7999999999999999,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5277358490566038,
      "overall": 0.7092452830188677
    }
  },
  {
    "text": "### Batch Inference Pipeline\n\nThe Transformers implementation processes images in batches:\n\n```\n```\n\n**Key Implementation Details:**\n\n1. **Image Normalization** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py209-224](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L209-L224): Converts numpy arrays to PIL RGB images\n2. **Prompt Handling** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py229-236](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L229-L236): Accepts single prompt string or list of prompts (one per image)\n3. **Processor Integration** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py240-256](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L240-L256): Handles both text and image preprocessing with automatic padding\n4. **Stopping Criteria** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L296): Supports `StopStringCriteria` and custom `GenerationStopper` instances\n5. **Token Trimming** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py343-344](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L343-L344): Removes input tokens from output sequences using attention mask\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py139-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L139-L376)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0005",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Batch Inference Pipeline"
      ],
      "heading_text": "Batch Inference Pipeline",
      "token_count": 452,
      "char_count": 1764,
      "start_char": 12459,
      "end_char": 14223,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.6205999999999999,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.350758",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Batch Inference Pipeline",
      "chunk_hash": "db9f1931e0229b5a",
      "content_digest": "db9f1931e0229b5a",
      "chunk_length": 1764,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docling",
          "models",
          "transformers",
          "vlm",
          "inline",
          "model",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "image",
          "implementation",
          "images",
          "prompt",
          "and",
          "batch",
          "inference",
          "pipeline"
        ],
        "term_weights": [
          {
            "term": "docling",
            "tf": 24,
            "weight": 0.110092
          },
          {
            "term": "models",
            "tf": 24,
            "weight": 0.110092
          },
          {
            "term": "transformers",
            "tf": 13,
            "weight": 0.059633
          },
          {
            "term": "vlm",
            "tf": 12,
            "weight": 0.055046
          },
          {
            "term": "inline",
            "tf": 12,
            "weight": 0.055046
          },
          {
            "term": "model",
            "tf": 12,
            "weight": 0.055046
          },
          {
            "term": "https",
            "tf": 6,
            "weight": 0.027523
          },
          {
            "term": "github",
            "tf": 6,
            "weight": 0.027523
          },
          {
            "term": "com",
            "tf": 6,
            "weight": 0.027523
          },
          {
            "term": "project",
            "tf": 6,
            "weight": 0.027523
          },
          {
            "term": "blob",
            "tf": 6,
            "weight": 0.027523
          },
          {
            "term": "f7244a43",
            "tf": 6,
            "weight": 0.027523
          },
          {
            "term": "image",
            "tf": 3,
            "weight": 0.013761
          },
          {
            "term": "implementation",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "images",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "prompt",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.009174
          },
          {
            "term": "batch",
            "tf": 1,
            "weight": 0.004587
          },
          {
            "term": "inference",
            "tf": 1,
            "weight": 0.004587
          },
          {
            "term": "pipeline",
            "tf": 1,
            "weight": 0.004587
          }
        ],
        "unique_terms": 91,
        "total_terms": 218
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Batch Inference Pipeline",
        "com",
        "docling",
        "github",
        "https",
        "inline",
        "model",
        "models",
        "project",
        "transformers",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.6205999999999999,
      "overall": 0.7068666666666665
    }
  },
  {
    "text": "### Stopping Criteria Handling\n\nThe Transformers implementation supports two types of stopping criteria:\n\n```\n```\n\nThe implementation distinguishes between:\n\n- **String-based stopping** via `StopStringCriteria` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py264-269](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L264-L269)\n- **GenerationStopper classes/instances** wrapped in `HFStoppingCriteriaWrapper` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py276-283](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L276-L283)\n- **Native StoppingCriteria classes** instantiated with tokenizer [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py284-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L284-L287)\n- **StoppingCriteria instances** used directly [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py294-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L294-L296)\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L302)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0006",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Stopping Criteria Handling"
      ],
      "heading_text": "Stopping Criteria Handling",
      "token_count": 357,
      "char_count": 1403,
      "start_char": 14225,
      "end_char": 15628,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.56335,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.351845",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Stopping Criteria Handling",
      "chunk_hash": "5c732957fb852e3f",
      "content_digest": "5c732957fb852e3f",
      "chunk_length": 1403,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docling",
          "models",
          "transformers",
          "vlm",
          "inline",
          "model",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "stopping",
          "criteria",
          "the",
          "implementation",
          "classes",
          "instances",
          "stoppingcriteria",
          "handling"
        ],
        "term_weights": [
          {
            "term": "docling",
            "tf": 20,
            "weight": 0.120482
          },
          {
            "term": "models",
            "tf": 20,
            "weight": 0.120482
          },
          {
            "term": "transformers",
            "tf": 11,
            "weight": 0.066265
          },
          {
            "term": "vlm",
            "tf": 10,
            "weight": 0.060241
          },
          {
            "term": "inline",
            "tf": 10,
            "weight": 0.060241
          },
          {
            "term": "model",
            "tf": 10,
            "weight": 0.060241
          },
          {
            "term": "https",
            "tf": 5,
            "weight": 0.03012
          },
          {
            "term": "github",
            "tf": 5,
            "weight": 0.03012
          },
          {
            "term": "com",
            "tf": 5,
            "weight": 0.03012
          },
          {
            "term": "project",
            "tf": 5,
            "weight": 0.03012
          },
          {
            "term": "blob",
            "tf": 5,
            "weight": 0.03012
          },
          {
            "term": "f7244a43",
            "tf": 5,
            "weight": 0.03012
          },
          {
            "term": "stopping",
            "tf": 3,
            "weight": 0.018072
          },
          {
            "term": "criteria",
            "tf": 2,
            "weight": 0.012048
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.012048
          },
          {
            "term": "implementation",
            "tf": 2,
            "weight": 0.012048
          },
          {
            "term": "classes",
            "tf": 2,
            "weight": 0.012048
          },
          {
            "term": "instances",
            "tf": 2,
            "weight": 0.012048
          },
          {
            "term": "stoppingcriteria",
            "tf": 2,
            "weight": 0.012048
          },
          {
            "term": "handling",
            "tf": 1,
            "weight": 0.006024
          }
        ],
        "unique_terms": 59,
        "total_terms": 166
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": true,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Stopping Criteria Handling",
        "com",
        "docling",
        "github",
        "https",
        "inline",
        "model",
        "models",
        "project",
        "transformers",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.56335,
      "overall": 0.6877833333333333
    }
  },
  {
    "text": "## MLX Implementation (Apple Silicon)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0007",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "MLX Implementation (Apple Silicon)"
      ],
      "heading_text": "MLX Implementation (Apple Silicon)",
      "token_count": 8,
      "char_count": 37,
      "start_char": 15630,
      "end_char": 15667,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.352013",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "MLX Implementation (Apple Silicon)",
      "chunk_hash": "0d0082adfeb4fedd",
      "content_digest": "0d0082adfeb4fedd",
      "chunk_length": 37,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "mlx",
          "implementation",
          "apple",
          "silicon"
        ],
        "term_weights": [
          {
            "term": "mlx",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "implementation",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "apple",
            "tf": 1,
            "weight": 0.25
          },
          {
            "term": "silicon",
            "tf": 1,
            "weight": 0.25
          }
        ],
        "unique_terms": 4,
        "total_terms": 4
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "MLX Implementation (Apple Silicon)",
        "apple",
        "implementation",
        "mlx",
        "silicon"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### Architecture and Thread Safety\n\n`HuggingFaceMlxModel` uses the MLX framework for Apple Silicon acceleration with important thread safety considerations:\n\n```\n```\n\n**Critical Constraint:** MLX models are **not thread-safe**. All MLX inference operations are serialized using a global lock [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-30](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L30):\n\n```\n```\n\nThis means only one MLX model instance can perform inference at a time across the entire process.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-90](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L90)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0008",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Architecture and Thread Safety"
      ],
      "heading_text": "Architecture and Thread Safety",
      "token_count": 186,
      "char_count": 750,
      "start_char": 15669,
      "end_char": 16419,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.53,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.352616",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Architecture and Thread Safety",
      "chunk_hash": "1c9537b2b08864a4",
      "content_digest": "1c9537b2b08864a4",
      "chunk_length": 750,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "models",
          "mlx",
          "docling",
          "model",
          "vlm",
          "inline",
          "thread",
          "safety",
          "the",
          "are",
          "inference",
          "py28",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "l28",
          "architecture"
        ],
        "term_weights": [
          {
            "term": "models",
            "tf": 9,
            "weight": 0.089109
          },
          {
            "term": "mlx",
            "tf": 8,
            "weight": 0.079208
          },
          {
            "term": "docling",
            "tf": 8,
            "weight": 0.079208
          },
          {
            "term": "model",
            "tf": 5,
            "weight": 0.049505
          },
          {
            "term": "vlm",
            "tf": 4,
            "weight": 0.039604
          },
          {
            "term": "inline",
            "tf": 4,
            "weight": 0.039604
          },
          {
            "term": "thread",
            "tf": 3,
            "weight": 0.029703
          },
          {
            "term": "safety",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "are",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "inference",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "py28",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "https",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "github",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "com",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "project",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "blob",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "f7244a43",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "l28",
            "tf": 2,
            "weight": 0.019802
          },
          {
            "term": "architecture",
            "tf": 1,
            "weight": 0.009901
          }
        ],
        "unique_terms": 55,
        "total_terms": 101
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Architecture and Thread Safety",
        "are",
        "docling",
        "inline",
        "mlx",
        "model",
        "models",
        "safety",
        "the",
        "thread",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.53,
      "overall": 0.6766666666666666
    }
  },
  {
    "text": "### Streaming Generation and Token Collection\n\nUnlike the Transformers implementation, MLX uses streaming generation:\n\n```\n```\n\n**Key Characteristics:**\n\n1. **No Batching** [docling/models/vlm\\_models\\_inline/mlx\\_model.py186-188](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L186-L188): Images are processed sequentially within the global lock\n2. **Token-Level Collection** [doclog/models/vlm\\_models\\_inline/mlx\\_model.py232-254](https://github.com/docling-project/docling/blob/f7244a43/doclog/models/vlm_models_inline/mlx_model.py#L232-L254): Each token includes text, token ID, and log probability\n3. **Early Stopping** [docling/models/vlm\\_models\\_inline/mlx\\_model.py258-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L258-L302): Stop strings and `GenerationStopper` instances are checked during streaming\n4. **Lookback Window** [docling/models/vlm\\_models\\_inline/mlx\\_model.py279-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L279-L287): Custom stoppers can specify how many recent characters to examine\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py149-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L149-L318)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0009",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Streaming Generation and Token Collection"
      ],
      "heading_text": "Streaming Generation and Token Collection",
      "token_count": 360,
      "char_count": 1378,
      "start_char": 16421,
      "end_char": 17799,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5705,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.353759",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Streaming Generation and Token Collection",
      "chunk_hash": "3b4a297a174b9381",
      "content_digest": "3b4a297a174b9381",
      "chunk_length": 1378,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "models",
          "docling",
          "mlx",
          "vlm",
          "inline",
          "model",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "token",
          "streaming",
          "and",
          "generation",
          "collection",
          "the",
          "are",
          "doclog"
        ],
        "term_weights": [
          {
            "term": "models",
            "tf": 20,
            "weight": 0.106383
          },
          {
            "term": "docling",
            "tf": 18,
            "weight": 0.095745
          },
          {
            "term": "mlx",
            "tf": 11,
            "weight": 0.058511
          },
          {
            "term": "vlm",
            "tf": 10,
            "weight": 0.053191
          },
          {
            "term": "inline",
            "tf": 10,
            "weight": 0.053191
          },
          {
            "term": "model",
            "tf": 10,
            "weight": 0.053191
          },
          {
            "term": "https",
            "tf": 5,
            "weight": 0.026596
          },
          {
            "term": "github",
            "tf": 5,
            "weight": 0.026596
          },
          {
            "term": "com",
            "tf": 5,
            "weight": 0.026596
          },
          {
            "term": "project",
            "tf": 5,
            "weight": 0.026596
          },
          {
            "term": "blob",
            "tf": 5,
            "weight": 0.026596
          },
          {
            "term": "f7244a43",
            "tf": 5,
            "weight": 0.026596
          },
          {
            "term": "token",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "streaming",
            "tf": 3,
            "weight": 0.015957
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.015957
          },
          {
            "term": "generation",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "collection",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "the",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "are",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "doclog",
            "tf": 2,
            "weight": 0.010638
          }
        ],
        "unique_terms": 79,
        "total_terms": 188
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Streaming Generation and Token Collection",
        "com",
        "docling",
        "github",
        "https",
        "inline",
        "mlx",
        "model",
        "models",
        "project",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5705,
      "overall": 0.6901666666666667
    }
  },
  {
    "text": "## vLLM Implementation",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0011",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "vLLM Implementation"
      ],
      "heading_text": "vLLM Implementation",
      "token_count": 5,
      "char_count": 22,
      "start_char": 18715,
      "end_char": 18737,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.356041",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "vLLM Implementation",
      "chunk_hash": "5cbb8693af7ca8fe",
      "content_digest": "5cbb8693af7ca8fe",
      "chunk_length": 22,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "vllm",
          "implementation"
        ],
        "term_weights": [
          {
            "term": "vllm",
            "tf": 1,
            "weight": 0.5
          },
          {
            "term": "implementation",
            "tf": 1,
            "weight": 0.5
          }
        ],
        "unique_terms": 2,
        "total_terms": 2
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "implementation",
        "vLLM Implementation",
        "vllm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### Configuration and Initialization\n\n`VllmVlmModel` provides GPU-optimized inference with strict separation of load-time and runtime parameters:\n\n```\n```\n\n**Parameter Allowlists:**\n\nThe implementation maintains two explicit allowlists [docling/models/vlm\\_models\\_inline/vllm\\_model.py32-80](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L32-L80):\n\n1. **`_VLLM_ENGINE_KEYS`** - Parameters passed to `LLM.__init__()` (load time)\n2. **`_VLLM_SAMPLING_KEYS`** - Parameters passed to `SamplingParams` (runtime)\n\nAny keys in `extra_generation_config` not in either allowlist trigger a warning and are ignored.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py82-174](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L82-L174)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0012",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Configuration and Initialization"
      ],
      "heading_text": "Configuration and Initialization",
      "token_count": 225,
      "char_count": 849,
      "start_char": 18739,
      "end_char": 19588,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.55,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.356706",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Configuration and Initialization",
      "chunk_hash": "e3533234e2f1eb2c",
      "content_digest": "e3533234e2f1eb2c",
      "chunk_length": 849,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docling",
          "models",
          "vllm",
          "vlm",
          "inline",
          "model",
          "and",
          "parameters",
          "keys",
          "load",
          "time",
          "runtime",
          "allowlists",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "passed"
        ],
        "term_weights": [
          {
            "term": "docling",
            "tf": 8,
            "weight": 0.07619
          },
          {
            "term": "models",
            "tf": 8,
            "weight": 0.07619
          },
          {
            "term": "vllm",
            "tf": 6,
            "weight": 0.057143
          },
          {
            "term": "vlm",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "inline",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "model",
            "tf": 4,
            "weight": 0.038095
          },
          {
            "term": "and",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "parameters",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "keys",
            "tf": 3,
            "weight": 0.028571
          },
          {
            "term": "load",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "time",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "runtime",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "allowlists",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "https",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "github",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "com",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "project",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "blob",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "f7244a43",
            "tf": 2,
            "weight": 0.019048
          },
          {
            "term": "passed",
            "tf": 2,
            "weight": 0.019048
          }
        ],
        "unique_terms": 60,
        "total_terms": 105
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Configuration and Initialization",
        "and",
        "docling",
        "inline",
        "keys",
        "load",
        "model",
        "models",
        "parameters",
        "vllm",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.55,
      "overall": 0.6833333333333332
    }
  },
  {
    "text": "### Batch Inference with Multi-Modal Data\n\nvLLM processes images as multi-modal data in batch mode:\n\n```\n```\n\n**Key Features:**\n\n1. **True Batching** [docling/models/vlm\\_models\\_inline/vllm\\_model.py233-300](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L233-L300): vLLM processes all images in a single `generate()` call\n2. **Multi-Modal Data Format** [docling/models/vlm\\_models\\_inline/vllm\\_model.py277-280](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L277-L280): Images are passed via `multi_modal_data` dictionary with `\"image\"` key\n3. **Memory Limit** [docling/models/vlm\\_models\\_inline/vllm\\_model.py140](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L140-L140): `limit_mm_per_prompt={\"image\": 1}` restricts one image per prompt\n4. **GPU Memory Management** [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-151](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L151): Defaults to 30% GPU memory utilization to share with other models\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py175-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L175-L301)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0013",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Batch Inference with Multi-Modal Data"
      ],
      "heading_text": "Batch Inference with Multi-Modal Data",
      "token_count": 389,
      "char_count": 1372,
      "start_char": 19590,
      "end_char": 20962,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.5607,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.357854",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Batch Inference with Multi-Modal Data",
      "chunk_hash": "cdcebffe0e894271",
      "content_digest": "cdcebffe0e894271",
      "chunk_length": 1372,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "models",
          "docling",
          "vllm",
          "vlm",
          "inline",
          "model",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "multi",
          "modal",
          "data",
          "with",
          "images",
          "image",
          "memory",
          "batch"
        ],
        "term_weights": [
          {
            "term": "models",
            "tf": 21,
            "weight": 0.109375
          },
          {
            "term": "docling",
            "tf": 20,
            "weight": 0.104167
          },
          {
            "term": "vllm",
            "tf": 12,
            "weight": 0.0625
          },
          {
            "term": "vlm",
            "tf": 10,
            "weight": 0.052083
          },
          {
            "term": "inline",
            "tf": 10,
            "weight": 0.052083
          },
          {
            "term": "model",
            "tf": 10,
            "weight": 0.052083
          },
          {
            "term": "https",
            "tf": 5,
            "weight": 0.026042
          },
          {
            "term": "github",
            "tf": 5,
            "weight": 0.026042
          },
          {
            "term": "com",
            "tf": 5,
            "weight": 0.026042
          },
          {
            "term": "project",
            "tf": 5,
            "weight": 0.026042
          },
          {
            "term": "blob",
            "tf": 5,
            "weight": 0.026042
          },
          {
            "term": "f7244a43",
            "tf": 5,
            "weight": 0.026042
          },
          {
            "term": "multi",
            "tf": 4,
            "weight": 0.020833
          },
          {
            "term": "modal",
            "tf": 4,
            "weight": 0.020833
          },
          {
            "term": "data",
            "tf": 4,
            "weight": 0.020833
          },
          {
            "term": "with",
            "tf": 3,
            "weight": 0.015625
          },
          {
            "term": "images",
            "tf": 3,
            "weight": 0.015625
          },
          {
            "term": "image",
            "tf": 3,
            "weight": 0.015625
          },
          {
            "term": "memory",
            "tf": 3,
            "weight": 0.015625
          },
          {
            "term": "batch",
            "tf": 2,
            "weight": 0.010417
          }
        ],
        "unique_terms": 66,
        "total_terms": 192
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Batch Inference with Multi-Modal Data",
        "com",
        "docling",
        "github",
        "https",
        "inline",
        "model",
        "models",
        "project",
        "vllm",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.5607,
      "overall": 0.6869
    }
  },
  {
    "text": "## Model Download and Caching\n\nAll inline VLM implementations inherit from `HuggingFaceModelDownloadMixin`:\n\n```\n```\n\nThe `repo_cache_folder` property converts slashes in `repo_id` to dashes (e.g., `\"ibm-granite/granite-docling-258M\"` → `\"ibm-granite--granite-docling-258M\"`).\n\n**Sources:** [docling/models/utils/hf\\_model\\_download.py8-45](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py#L8-L45) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py86-88](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L86-L88)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0015",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Model Download and Caching"
      ],
      "heading_text": "Model Download and Caching",
      "token_count": 179,
      "char_count": 619,
      "start_char": 21941,
      "end_char": 22560,
      "semantic_score": 0.7,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.58,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.359267",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Model Download and Caching",
      "chunk_hash": "027604ca7ffc7415",
      "content_digest": "027604ca7ffc7415",
      "chunk_length": 619,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docling",
          "model",
          "granite",
          "download",
          "vlm",
          "repo",
          "ibm",
          "258m",
          "models",
          "utils",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "datamodel",
          "pipeline",
          "options",
          "and"
        ],
        "term_weights": [
          {
            "term": "docling",
            "tf": 10,
            "weight": 0.135135
          },
          {
            "term": "model",
            "tf": 5,
            "weight": 0.067568
          },
          {
            "term": "granite",
            "tf": 4,
            "weight": 0.054054
          },
          {
            "term": "download",
            "tf": 3,
            "weight": 0.040541
          },
          {
            "term": "vlm",
            "tf": 3,
            "weight": 0.040541
          },
          {
            "term": "repo",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "ibm",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "258m",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "models",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "utils",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "https",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "github",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "com",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "project",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "blob",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "f7244a43",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "datamodel",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "pipeline",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "options",
            "tf": 2,
            "weight": 0.027027
          },
          {
            "term": "and",
            "tf": 1,
            "weight": 0.013514
          }
        ],
        "unique_terms": 40,
        "total_terms": 74
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "258m",
        "Model Download and Caching",
        "docling",
        "download",
        "granite",
        "ibm",
        "model",
        "models",
        "repo",
        "utils",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.58,
      "overall": 0.6933333333333332
    }
  },
  {
    "text": "## Available Model Specifications\n\nDocling provides pre-configured model specifications in `vlm_model_specs`:",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0016",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Available Model Specifications"
      ],
      "heading_text": "Available Model Specifications",
      "token_count": 20,
      "char_count": 109,
      "start_char": 22562,
      "end_char": 22671,
      "semantic_score": 0.8,
      "structural_score": 0.7,
      "retrieval_quality": 0.5354545454545454,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.359444",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Available Model Specifications",
      "chunk_hash": "4ff43217c9c0906c",
      "content_digest": "4ff43217c9c0906c",
      "chunk_length": 109,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "model",
          "specifications",
          "available",
          "docling",
          "provides",
          "pre",
          "configured",
          "vlm",
          "specs"
        ],
        "term_weights": [
          {
            "term": "model",
            "tf": 3,
            "weight": 0.25
          },
          {
            "term": "specifications",
            "tf": 2,
            "weight": 0.166667
          },
          {
            "term": "available",
            "tf": 1,
            "weight": 0.083333
          },
          {
            "term": "docling",
            "tf": 1,
            "weight": 0.083333
          },
          {
            "term": "provides",
            "tf": 1,
            "weight": 0.083333
          },
          {
            "term": "pre",
            "tf": 1,
            "weight": 0.083333
          },
          {
            "term": "configured",
            "tf": 1,
            "weight": 0.083333
          },
          {
            "term": "vlm",
            "tf": 1,
            "weight": 0.083333
          },
          {
            "term": "specs",
            "tf": 1,
            "weight": 0.083333
          }
        ],
        "unique_terms": 9,
        "total_terms": 12
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Available Model Specifications",
        "available",
        "configured",
        "docling",
        "model",
        "pre",
        "provides",
        "specifications",
        "specs",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.7,
      "retrieval_quality": 0.5354545454545454,
      "overall": 0.6784848484848484
    }
  },
  {
    "text": "### Extraction Models  | Model Spec                   | Repository                | Framework    | Devices        | Response Format | | ---------------------------- | ------------------------- | ------------ | -------------- | --------------- | | `NU_EXTRACT_2B_TRANSFORMERS` | `numind/NuExtract-2.0-2B` | Transformers | CPU, CUDA, MPS | PLAINTEXT       |  **Special Configuration Notes:**  1. **GOT-OCR-2.0** uses `TransformersPromptStyle.NONE` and includes `extra_processor_kwargs={\"format\": True}` 2. **Phi-4** requires `transformers<4.52.0` and uses `extra_generation_config={\"num_logits_to_keep\": 0}` 3. **Dolphin** uses `TransformersPromptStyle.RAW` with a custom prompt format 4. **GraniteDocling VLLM** uses `revision=\"untied\"` for compatibility with vLLM ≤0.10.2  **Sources:** [docling/datamodel/vlm\\_model\\_specs.py1-303](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L1-L303)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0020",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Extraction Models"
      ],
      "heading_text": "Extraction Models",
      "token_count": 252,
      "char_count": 934,
      "start_char": 25879,
      "end_char": 26813,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.50375,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "table_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.363786",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Extraction Models",
      "chunk_hash": "f0c6b068db7b9a8c",
      "content_digest": "f0c6b068db7b9a8c",
      "chunk_length": 934,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "uses",
          "docling",
          "model",
          "format",
          "transformers",
          "transformerspromptstyle",
          "and",
          "extra",
          "with",
          "vllm",
          "datamodel",
          "vlm",
          "specs",
          "extraction",
          "models",
          "spec",
          "repository",
          "framework",
          "devices",
          "response"
        ],
        "term_weights": [
          {
            "term": "uses",
            "tf": 4,
            "weight": 0.048193
          },
          {
            "term": "docling",
            "tf": 4,
            "weight": 0.048193
          },
          {
            "term": "model",
            "tf": 3,
            "weight": 0.036145
          },
          {
            "term": "format",
            "tf": 3,
            "weight": 0.036145
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.036145
          },
          {
            "term": "transformerspromptstyle",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "and",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "extra",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "vllm",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "datamodel",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "vlm",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "specs",
            "tf": 2,
            "weight": 0.024096
          },
          {
            "term": "extraction",
            "tf": 1,
            "weight": 0.012048
          },
          {
            "term": "models",
            "tf": 1,
            "weight": 0.012048
          },
          {
            "term": "spec",
            "tf": 1,
            "weight": 0.012048
          },
          {
            "term": "repository",
            "tf": 1,
            "weight": 0.012048
          },
          {
            "term": "framework",
            "tf": 1,
            "weight": 0.012048
          },
          {
            "term": "devices",
            "tf": 1,
            "weight": 0.012048
          },
          {
            "term": "response",
            "tf": 1,
            "weight": 0.012048
          }
        ],
        "unique_terms": 63,
        "total_terms": 83
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Extraction Models",
        "and",
        "docling",
        "extra",
        "format",
        "model",
        "transformers",
        "transformerspromptstyle",
        "uses",
        "vllm",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.50375,
      "overall": 0.7012499999999999
    }
  },
  {
    "text": "## Usage Examples",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0021",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Usage Examples"
      ],
      "heading_text": "Usage Examples",
      "token_count": 3,
      "char_count": 17,
      "start_char": 26815,
      "end_char": 26832,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.364197",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Usage Examples",
      "chunk_hash": "40ba86df7413777b",
      "content_digest": "40ba86df7413777b",
      "chunk_length": 17,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "usage",
          "examples"
        ],
        "term_weights": [
          {
            "term": "usage",
            "tf": 1,
            "weight": 0.5
          },
          {
            "term": "examples",
            "tf": 1,
            "weight": 0.5
          }
        ],
        "unique_terms": 2,
        "total_terms": 2
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Usage Examples",
        "examples",
        "usage"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### Basic Usage with Default Model\n\n```\n```",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0022",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Basic Usage with Default Model"
      ],
      "heading_text": "Basic Usage with Default Model",
      "token_count": 10,
      "char_count": 43,
      "start_char": 26834,
      "end_char": 26877,
      "semantic_score": 0.8,
      "structural_score": 0.7,
      "retrieval_quality": 0.5525,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.364331",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Basic Usage with Default Model",
      "chunk_hash": "88b830909318f5dc",
      "content_digest": "88b830909318f5dc",
      "chunk_length": 43,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "basic",
          "usage",
          "with",
          "default",
          "model"
        ],
        "term_weights": [
          {
            "term": "basic",
            "tf": 1,
            "weight": 0.2
          },
          {
            "term": "usage",
            "tf": 1,
            "weight": 0.2
          },
          {
            "term": "with",
            "tf": 1,
            "weight": 0.2
          },
          {
            "term": "default",
            "tf": 1,
            "weight": 0.2
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.2
          }
        ],
        "unique_terms": 5,
        "total_terms": 5
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Basic Usage with Default Model",
        "basic",
        "default",
        "model",
        "usage",
        "with"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.7,
      "retrieval_quality": 0.5525,
      "overall": 0.6841666666666667
    }
  },
  {
    "text": "### Selecting a Specific Model\n\n```\n```",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0023",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Selecting a Specific Model"
      ],
      "heading_text": "Selecting a Specific Model",
      "token_count": 10,
      "char_count": 39,
      "start_char": 26879,
      "end_char": 26918,
      "semantic_score": 0.8,
      "structural_score": 0.7,
      "retrieval_quality": 0.5471428571428572,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.364463",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Selecting a Specific Model",
      "chunk_hash": "a82174cf8404a47d",
      "content_digest": "a82174cf8404a47d",
      "chunk_length": 39,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "selecting",
          "specific",
          "model"
        ],
        "term_weights": [
          {
            "term": "selecting",
            "tf": 1,
            "weight": 0.333333
          },
          {
            "term": "specific",
            "tf": 1,
            "weight": 0.333333
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.333333
          }
        ],
        "unique_terms": 3,
        "total_terms": 3
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Selecting a Specific Model",
        "model",
        "selecting",
        "specific"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.7,
      "retrieval_quality": 0.5471428571428572,
      "overall": 0.6823809523809524
    }
  },
  {
    "text": "### Custom Model Configuration\n\n```\n```",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0024",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Custom Model Configuration"
      ],
      "heading_text": "Custom Model Configuration",
      "token_count": 8,
      "char_count": 39,
      "start_char": 26920,
      "end_char": 26959,
      "semantic_score": 0.8,
      "structural_score": 0.7,
      "retrieval_quality": 0.54,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.364576",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Custom Model Configuration",
      "chunk_hash": "01e80ef9cd54c975",
      "content_digest": "01e80ef9cd54c975",
      "chunk_length": 39,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "custom",
          "model",
          "configuration"
        ],
        "term_weights": [
          {
            "term": "custom",
            "tf": 1,
            "weight": 0.333333
          },
          {
            "term": "model",
            "tf": 1,
            "weight": 0.333333
          },
          {
            "term": "configuration",
            "tf": 1,
            "weight": 0.333333
          }
        ],
        "unique_terms": 3,
        "total_terms": 3
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Custom Model Configuration",
        "configuration",
        "custom",
        "model"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.7,
      "retrieval_quality": 0.54,
      "overall": 0.68
    }
  },
  {
    "text": "### Direct Image Processing\n\nAll inline VLM models support direct image processing via the `process_images()` method:\n\n```\n```\n\n**Sources:** [docs/examples/minimal\\_vlm\\_pipeline.py1-71](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py#L1-L71) [docs/usage/vision\\_models.md1-124](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L1-L124)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0025",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Direct Image Processing"
      ],
      "heading_text": "Direct Image Processing",
      "token_count": 114,
      "char_count": 419,
      "start_char": 26961,
      "end_char": 27380,
      "semantic_score": 0.6,
      "structural_score": 0.7999999999999998,
      "retrieval_quality": 0.7328571428571428,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.364907",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Direct Image Processing",
      "chunk_hash": "3b9df8da08b35bde",
      "content_digest": "3b9df8da08b35bde",
      "chunk_length": 419,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docs",
          "docling",
          "vlm",
          "models",
          "direct",
          "image",
          "processing",
          "examples",
          "minimal",
          "pipeline",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "usage",
          "vision",
          "all",
          "inline"
        ],
        "term_weights": [
          {
            "term": "docs",
            "tf": 4,
            "weight": 0.071429
          },
          {
            "term": "docling",
            "tf": 4,
            "weight": 0.071429
          },
          {
            "term": "vlm",
            "tf": 3,
            "weight": 0.053571
          },
          {
            "term": "models",
            "tf": 3,
            "weight": 0.053571
          },
          {
            "term": "direct",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "image",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "processing",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "examples",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "minimal",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "pipeline",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "https",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "github",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "com",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "project",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "blob",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "f7244a43",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "usage",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "vision",
            "tf": 2,
            "weight": 0.035714
          },
          {
            "term": "all",
            "tf": 1,
            "weight": 0.017857
          },
          {
            "term": "inline",
            "tf": 1,
            "weight": 0.017857
          }
        ],
        "unique_terms": 32,
        "total_terms": 56
      },
      "modal_hint": "code",
      "content_flags": {
        "has_code_block": true,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Direct Image Processing",
        "direct",
        "docling",
        "docs",
        "examples",
        "image",
        "minimal",
        "models",
        "pipeline",
        "processing",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7999999999999998,
      "retrieval_quality": 0.7328571428571428,
      "overall": 0.7109523809523809
    }
  },
  {
    "text": "## Performance Considerations",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0026",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Performance Considerations"
      ],
      "heading_text": "Performance Considerations",
      "token_count": 4,
      "char_count": 29,
      "start_char": 27382,
      "end_char": 27411,
      "semantic_score": 0.8,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.365017",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Performance Considerations",
      "chunk_hash": "f0c6f691b1d6938e",
      "content_digest": "f0c6f691b1d6938e",
      "chunk_length": 29,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "performance",
          "considerations"
        ],
        "term_weights": [
          {
            "term": "performance",
            "tf": 1,
            "weight": 0.5
          },
          {
            "term": "considerations",
            "tf": 1,
            "weight": 0.5
          }
        ],
        "unique_terms": 2,
        "total_terms": 2
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Performance Considerations",
        "considerations",
        "performance"
      ]
    },
    "advanced_scores": {
      "semantic": 0.8,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.59,
      "overall": 0.7633333333333333
    }
  },
  {
    "text": "### Memory Management\n\n1. **Transformers**: Uses PyTorch's default memory management; consider `torch_dtype=\"bfloat16\"` for memory savings\n2. **MLX**: Automatically manages unified memory on Apple Silicon\n3. **vLLM**: Set `gpu_memory_utilization` (default 0.3) to reserve GPU memory for other models",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0028",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Memory Management"
      ],
      "heading_text": "Memory Management",
      "token_count": 74,
      "char_count": 299,
      "start_char": 27975,
      "end_char": 28274,
      "semantic_score": 0.6,
      "structural_score": 0.7,
      "retrieval_quality": 0.5494594594594595,
      "chunking_strategy": "hierarchical_balanced_semchunk",
      "content_type": "prose_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.366006",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Memory Management",
      "chunk_hash": "3a6a4b24d18a0126",
      "content_digest": "3a6a4b24d18a0126",
      "chunk_length": 299,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "memory",
          "management",
          "default",
          "for",
          "gpu",
          "transformers",
          "uses",
          "pytorch",
          "consider",
          "torch",
          "dtype",
          "bfloat16",
          "savings",
          "mlx",
          "automatically",
          "manages",
          "unified",
          "apple",
          "silicon",
          "vllm"
        ],
        "term_weights": [
          {
            "term": "memory",
            "tf": 6,
            "weight": 0.176471
          },
          {
            "term": "management",
            "tf": 2,
            "weight": 0.058824
          },
          {
            "term": "default",
            "tf": 2,
            "weight": 0.058824
          },
          {
            "term": "for",
            "tf": 2,
            "weight": 0.058824
          },
          {
            "term": "gpu",
            "tf": 2,
            "weight": 0.058824
          },
          {
            "term": "transformers",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "uses",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "pytorch",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "consider",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "torch",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "dtype",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "bfloat16",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "savings",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "mlx",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "automatically",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "manages",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "unified",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "apple",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "silicon",
            "tf": 1,
            "weight": 0.029412
          },
          {
            "term": "vllm",
            "tf": 1,
            "weight": 0.029412
          }
        ],
        "unique_terms": 25,
        "total_terms": 34
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Memory Management",
        "consider",
        "default",
        "for",
        "gpu",
        "management",
        "memory",
        "pytorch",
        "torch",
        "transformers",
        "uses"
      ]
    },
    "advanced_scores": {
      "semantic": 0.6,
      "structural": 0.7,
      "retrieval_quality": 0.5494594594594595,
      "overall": 0.6164864864864864
    }
  },
  {
    "text": "### Acceleration Options  - **Flash Attention 2**: Automatically enabled on CUDA devices when `accelerator_options.cuda_use_flash_attention2=True` - **Quantization**: Enable with `quantized=True` and `load_in_8bit=True` (Transformers and vLLM only) - **KV Cache**: Enabled by default with `use_kv_cache=True`; disable only if memory is constrained  **Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py123-128](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L123-L128) [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-155](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L155) [docs/usage/vision\\_models.md46-58](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L46-L58)  Dismiss  Refresh this wiki  This wiki was recently refreshed. Please wait 4 days to refresh again.",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0029",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "Acceleration Options"
      ],
      "heading_text": "Acceleration Options",
      "token_count": 257,
      "char_count": 962,
      "start_char": 28276,
      "end_char": 29238,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.5493220338983051,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.366682",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "Acceleration Options",
      "chunk_hash": "ec08c3b5de999ecf",
      "content_digest": "ec08c3b5de999ecf",
      "chunk_length": 962,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "docling",
          "models",
          "true",
          "vlm",
          "inline",
          "model",
          "transformers",
          "vllm",
          "https",
          "github",
          "com",
          "project",
          "blob",
          "f7244a43",
          "options",
          "flash",
          "enabled",
          "cuda",
          "use",
          "with"
        ],
        "term_weights": [
          {
            "term": "docling",
            "tf": 10,
            "weight": 0.079365
          },
          {
            "term": "models",
            "tf": 10,
            "weight": 0.079365
          },
          {
            "term": "true",
            "tf": 4,
            "weight": 0.031746
          },
          {
            "term": "vlm",
            "tf": 4,
            "weight": 0.031746
          },
          {
            "term": "inline",
            "tf": 4,
            "weight": 0.031746
          },
          {
            "term": "model",
            "tf": 4,
            "weight": 0.031746
          },
          {
            "term": "transformers",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "vllm",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "https",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "github",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "com",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "project",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "blob",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "f7244a43",
            "tf": 3,
            "weight": 0.02381
          },
          {
            "term": "options",
            "tf": 2,
            "weight": 0.015873
          },
          {
            "term": "flash",
            "tf": 2,
            "weight": 0.015873
          },
          {
            "term": "enabled",
            "tf": 2,
            "weight": 0.015873
          },
          {
            "term": "cuda",
            "tf": 2,
            "weight": 0.015873
          },
          {
            "term": "use",
            "tf": 2,
            "weight": 0.015873
          },
          {
            "term": "with",
            "tf": 2,
            "weight": 0.015873
          }
        ],
        "unique_terms": 65,
        "total_terms": 126
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "Acceleration Options",
        "docling",
        "github",
        "https",
        "inline",
        "model",
        "models",
        "transformers",
        "true",
        "vllm",
        "vlm"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.5493220338983051,
      "overall": 0.7164406779661018
    }
  },
  {
    "text": "### On this page  - [Inline VLM Models](#inline-vlm-models.md) - [Architecture Overview](#architecture-overview.md) - [Configuration via InlineVlmOptions](#configuration-via-inlinevlmoptions.md) - [Hugging Face Transformers Implementation](#hugging-face-transformers-implementation.md) - [Model Loading and Initialization](#model-loading-and-initialization.md) - [Batch Inference Pipeline](#batch-inference-pipeline.md) - [Stopping Criteria Handling](#stopping-criteria-handling.md) - [MLX Implementation (Apple Silicon)](#mlx-implementation-apple-silicon.md) - [Architecture and Thread Safety](#architecture-and-thread-safety.md) - [Streaming Generation and Token Collection](#streaming-generation-and-token-collection.md) - [Stopping Criteria Validation](#stopping-criteria-validation.md) - [vLLM Implementation](#vllm-implementation.md) - [Configuration and Initialization](#configuration-and-initialization.md) - [Batch Inference with Multi-Modal Data](#batch-inference-with-multi-modal-data.md) - [Prompt Formatting](#prompt-formatting.md) - [Model Download and Caching](#model-download-and-caching.md) - [Available Model Specifications](#available-model-specifications.md) - [DocTags Output Models](#doctags-output-models.md) - [Markdown Output Models](#markdown-output-models.md) - [Plaintext Output Models](#plaintext-output-models.md) - [Extraction Models](#extraction-models.md) - [Usage Examples](#usage-examples.md) - [Basic Usage with Default Model](#basic-usage-with-default-model.md) - [Selecting a Specific Model](#selecting-a-specific-model.md) - [Custom Model Configuration](#custom-model-configuration.md) - [Direct Image Processing](#direct-image-processing.md) - [Performance Considerations](#performance-considerations.md) - [Framework Comparison](#framework-comparison.md) - [Memory Management](#memory-management.md) - [Acceleration Options](#acceleration-options.md)",
    "metadata": {
      "chunk_id": "1a8018efc3dd-0030",
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 30,
      "document_level": 1,
      "parent_chunk_id": null,
      "child_chunk_ids": [],
      "section_path": [
        "On this page"
      ],
      "heading_text": "On this page",
      "token_count": 439,
      "char_count": 1891,
      "start_char": 29240,
      "end_char": 31131,
      "semantic_score": 0.7,
      "structural_score": 0.8999999999999999,
      "retrieval_quality": 0.7277629921259843,
      "chunking_strategy": "hierarchical_balanced_structural",
      "content_type": "list_section",
      "embedding_model": "jina-code-embeddings-1.5b",
      "embedding_dimension": 1536,
      "processing_timestamp": "2025-10-20T09:13:45.368958",
      "document_id": "1a8018efc3dd",
      "document_name": "_docling-project_docling_4.3.1-inline-vlm-models",
      "source_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_directory": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\docling",
      "relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "hierarchy_path": "On this page",
      "chunk_hash": "b4b5ee3bca7f3271",
      "content_digest": "b4b5ee3bca7f3271",
      "chunk_length": 1891,
      "payload_version": "1.3",
      "collection_hints": [
        "qdrant_ecosystem"
      ],
      "sparse_features": {
        "version": "1.0",
        "weighting": "tf-normalized",
        "top_terms": [
          "model",
          "models",
          "and",
          "configuration",
          "implementation",
          "output",
          "architecture",
          "initialization",
          "batch",
          "inference",
          "stopping",
          "criteria",
          "with",
          "usage",
          "inline",
          "vlm",
          "overview",
          "via",
          "inlinevlmoptions",
          "hugging"
        ],
        "term_weights": [
          {
            "term": "model",
            "tf": 12,
            "weight": 0.06383
          },
          {
            "term": "models",
            "tf": 10,
            "weight": 0.053191
          },
          {
            "term": "and",
            "tf": 10,
            "weight": 0.053191
          },
          {
            "term": "configuration",
            "tf": 6,
            "weight": 0.031915
          },
          {
            "term": "implementation",
            "tf": 6,
            "weight": 0.031915
          },
          {
            "term": "output",
            "tf": 6,
            "weight": 0.031915
          },
          {
            "term": "architecture",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "initialization",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "batch",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "inference",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "stopping",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "criteria",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "with",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "usage",
            "tf": 4,
            "weight": 0.021277
          },
          {
            "term": "inline",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "vlm",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "overview",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "via",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "inlinevlmoptions",
            "tf": 2,
            "weight": 0.010638
          },
          {
            "term": "hugging",
            "tf": 2,
            "weight": 0.010638
          }
        ],
        "unique_terms": 68,
        "total_terms": 188
      },
      "modal_hint": "prose",
      "content_flags": {
        "has_code_block": false,
        "has_table": false,
        "has_list": false,
        "has_json": false,
        "has_formula": false
      },
      "search_keywords": [
        "On this page",
        "and",
        "architecture",
        "batch",
        "configuration",
        "implementation",
        "inference",
        "initialization",
        "model",
        "models",
        "output"
      ]
    },
    "advanced_scores": {
      "semantic": 0.7,
      "structural": 0.8999999999999999,
      "retrieval_quality": 0.7277629921259843,
      "overall": 0.7759209973753279
    }
  }
]