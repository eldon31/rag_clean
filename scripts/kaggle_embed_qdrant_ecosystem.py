"""
Kaggle-optimized Qdrant Ecosystem Embedding Pipeline
For GPU T4 x2 with nomic-ai/nomic-embed-code (3584-dim) + MEMORY-OPTIMIZED DATA PARALLELISM

This script embeds pre-processed chunks from output/qdrant_ecosystem/
Collection name: qdrant_ecosystem

Subdirectories (processed by process_qdrant_ecosystem.py):
1. qdrant_documentation/   - Qdrant official documentation
2. qdrant_examples/        - Qdrant code examples
3. qdrant_fastembed/       - Qdrant FastEmbed library docs
4. qdrant_mcp-server-qdrant/ - MCP server for Qdrant
5. qdrant_qdrant/          - Qdrant core repository
6. qdrant_qdrant-client/   - Qdrant client library

MEMORY-OPTIMIZED PARALLELISM STRATEGY:
- Sequential worker execution to prevent OOM
- Each worker uses one GPU exclusively
- Aggressive memory cleanup between workers
- Reduced batch sizes for memory efficiency
- PyTorch memory fragmentation prevention

UNIQUE ID STRATEGY:
- Format: qdrant_ecosystem:{subdir}:{filename}:chunk:{index}
- Example: qdrant_ecosystem:qdrant_documentation:getting-started.md:chunk:0
- IDs already generated by process_qdrant_ecosystem.py
"""

import os
import gc

# Memory optimization environment variables
os.environ.setdefault("TRANSFORMERS_NO_TF", "1")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True,max_split_size_mb:128")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
import time

import numpy as np
import torch
from sentence_transformers import SentenceTransformer

# Guard against NumPy 2.x
if tuple(map(int, np.__version__.split(".")[:2])) >= (2, 0):
    raise RuntimeError(
        "NumPy 2.x detected. Please run `pip install -q --force-reinstall \"numpy==1.26.4\" \"scikit-learn==1.4.2\"`"
    )

# Check GPU availability
print(f"\n{'='*60}")
print("GPU SETUP")
print(f"{'='*60}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    num_gpus = torch.cuda.device_count()
    print(f"Number of GPUs: {num_gpus}")
    for i in range(num_gpus):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
else:
    num_gpus = 1
    print("No CUDA GPUs available, using CPU")
print(f"{'='*60}\n")

# Configuration
COLLECTION_NAME = "qdrant_ecosystem"

# Auto-detect path (GitHub clones into rad_clean, not RAG_CLEAN)
if Path("/kaggle/working/rad_clean/output/qdrant_ecosystem").exists():
    INPUT_DIR = Path("/kaggle/working/rad_clean/output/qdrant_ecosystem")
else:
    # Fallback for local testing
    INPUT_DIR = Path("output/qdrant_ecosystem")

OUTPUT_DIR = Path("/kaggle/working/embeddings")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Model configuration
MODEL_NAME = "nomic-ai/nomic-embed-code"
EMBEDDING_DIM = 3584
BATCH_SIZE = 16  # Reduced for memory efficiency
MAX_SEQ_LENGTH = 8192

# Memory-optimized configuration
MEMORY_CLEANUP_INTERVAL = 2  # Clean memory every N subdirectories


def load_chunks_from_subdirectory(subdir_path: Path) -> List[Dict]:
    """
    Load chunks from a subdirectory's chunks.json file.
    
    Args:
        subdir_path: Path to subdirectory (e.g., output/qdrant_ecosystem/qdrant_documentation/)
    
    Returns:
        List of chunk dictionaries with chunk_id, content, metadata
    """
    chunks_file = subdir_path / "chunks.json"
    
    if not chunks_file.exists():
        print(f"âš ï¸  No chunks.json found in {subdir_path.name}")
        return []
    
    with open(chunks_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    chunks = data.get('chunks', [])
    print(f"âœ… Loaded {len(chunks)} chunks from {subdir_path.name}")
    
    return chunks


def clear_gpu_memory():
    """Clear GPU memory cache and run garbage collection"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def embed_subdirectory_gpu(
    subdir_name: str,
    input_dir: Path,
    output_dir: Path,
    gpu_id: int = 0
) -> Tuple[int, bool]:
    """
    Embed chunks from a single subdirectory using specified GPU.
    Memory-optimized for T4 GPUs.
    
    Args:
        subdir_name: Name of subdirectory to process
        input_dir: Input directory path
        output_dir: Output directory path
        gpu_id: GPU device ID to use
    
    Returns:
        Tuple of (chunks_processed, success)
    """
    device = f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu"
    subdir_path = input_dir / subdir_name
    
    if not subdir_path.is_dir():
        print(f"âš ï¸  Skipping {subdir_name} (not a directory)")
        return 0, False
    
    print(f"\nðŸš€ Processing {subdir_name} on {device}...")
    
    # Clear memory before starting
    clear_gpu_memory()
    
    # Load chunks
    chunks = load_chunks_from_subdirectory(subdir_path)
    
    if not chunks:
        print(f"âš ï¸  No chunks found in {subdir_name}")
        return 0, False
    
    try:
        # Load model on specified GPU with memory optimization
        print(f"ðŸ“¥ Loading {MODEL_NAME} on {device}...")
        model = SentenceTransformer(
            MODEL_NAME,
            device=device,
            trust_remote_code=True
        )
        model.max_seq_length = MAX_SEQ_LENGTH
        print(f"âœ… Model loaded (dim={model.get_sentence_embedding_dimension()})")
        
        # Prepare texts for embedding (use search_document task)
        texts = [chunk['content'] for chunk in chunks]
        
        # Embed with smaller batches for memory efficiency
        print(f"ðŸ”„ Embedding {len(texts)} chunks with batch_size={BATCH_SIZE}...")
        embeddings = model.encode(
            texts,
            batch_size=BATCH_SIZE,
            show_progress_bar=True,
            normalize_embeddings=True,
            convert_to_numpy=True,
            prompt_name="search_document"  # Nomic-specific for documents
        )
        
        # Add embeddings to chunks
        for chunk, embedding in zip(chunks, embeddings):
            chunk['embedding'] = embedding.tolist()
        
        # Save embedded chunks
        output_file = output_dir / f"{subdir_name}_embedded.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'collection': COLLECTION_NAME,
                'subdirectory': subdir_name,
                'total_chunks': len(chunks),
                'embedding_model': MODEL_NAME,
                'embedding_dim': EMBEDDING_DIM,
                'processed_at': datetime.now().isoformat(),
                'chunks': chunks
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… {subdir_name}: {len(chunks)} chunks â†’ {output_file.name}")
        
        # Clear model and memory immediately after processing
        del model
        del embeddings
        clear_gpu_memory()
        
        return len(chunks), True
        
    except Exception as e:
        print(f"âŒ Error processing {subdir_name}: {e}")
        # Clear memory on error
        clear_gpu_memory()
        return 0, False


def process_with_sequential_gpus(
    subdirs: List[str],
    input_dir: Path,
    output_dir: Path,
    available_gpus: int
) -> Tuple[int, int]:
    """
    Process subdirectories sequentially across available GPUs.
    Memory-optimized to prevent OOM errors.
    
    Args:
        subdirs: List of subdirectory names
        input_dir: Input directory path  
        output_dir: Output directory path
        available_gpus: Number of available GPUs
    
    Returns:
        Tuple of (total_chunks, total_subdirs_processed)
    """
    total_chunks = 0
    total_subdirs = 0
    current_gpu = 0
    
    print(f"\nðŸš€ Starting sequential processing with {available_gpus} GPU(s)...")
    print(f"ðŸ“ Processing {len(subdirs)} subdirectories...")
    
    for i, subdir_name in enumerate(subdirs):
        # Use GPU round-robin if multiple GPUs available
        gpu_id = current_gpu % available_gpus if available_gpus > 0 else 0
        
        print(f"\n[{i+1}/{len(subdirs)}] GPU {gpu_id}: {subdir_name}")
        
        chunks_processed, success = embed_subdirectory_gpu(
            subdir_name=subdir_name,
            input_dir=input_dir,
            output_dir=output_dir,
            gpu_id=gpu_id
        )
        
        if success:
            total_chunks += chunks_processed
            total_subdirs += 1
        
        # Memory cleanup every few subdirectories
        if (i + 1) % MEMORY_CLEANUP_INTERVAL == 0:
            print(f"ðŸ§¹ Memory cleanup after {i+1} subdirectories...")
            clear_gpu_memory()
            time.sleep(1)  # Brief pause for cleanup
        
        # Move to next GPU
        current_gpu += 1
    
    return total_chunks, total_subdirs


def main():
    """Main execution with memory-optimized sequential processing."""
    start_time = datetime.now()
    
    print(f"\n{'='*60}")
    print(f"QDRANT ECOSYSTEM EMBEDDING PIPELINE (MEMORY-OPTIMIZED)")
    print(f"{'='*60}")
    print(f"Collection: {COLLECTION_NAME}")
    print(f"Input: {INPUT_DIR}")
    print(f"Output: {OUTPUT_DIR}")
    print(f"Model: {MODEL_NAME}")
    print(f"Batch size: {BATCH_SIZE} (memory-optimized)")
    print(f"GPUs available: {num_gpus}")
    print(f"Processing: Sequential (memory-safe)")
    print(f"{'='*60}\n")
    
    # Discover all subdirectories
    subdirs = [d for d in INPUT_DIR.iterdir() if d.is_dir()]
    subdir_names = [d.name for d in subdirs]
    
    print(f"Found {len(subdir_names)} subdirectories:")
    for name in subdir_names:
        print(f"  - {name}")
    
    if not subdir_names:
        print("âŒ No subdirectories found. Exiting.")
        return
    
    # Process sequentially across available GPUs
    total_chunks, total_subdirs = process_with_sequential_gpus(
        subdirs=subdir_names,
        input_dir=INPUT_DIR,
        output_dir=OUTPUT_DIR,
        available_gpus=num_gpus
    )
    
    # Final memory cleanup
    clear_gpu_memory()
    
    # Summary
    elapsed_time = (datetime.now() - start_time).total_seconds()
    
    print(f"\n{'='*60}")
    print("EMBEDDING COMPLETE")
    print(f"{'='*60}")
    print(f"Total subdirectories: {total_subdirs}")
    print(f"Total chunks embedded: {total_chunks:,}")
    print(f"Elapsed time: {elapsed_time:.2f}s")
    if total_chunks > 0 and elapsed_time > 0:
        print(f"Throughput: {total_chunks/elapsed_time:.2f} chunks/sec")
    print(f"Output directory: {OUTPUT_DIR}")
    print(f"{'='*60}\n")
    
    # List output files
    output_files = sorted(OUTPUT_DIR.glob("*_embedded.json"))
    print(f"Generated {len(output_files)} output files:")
    for file in output_files:
        size_mb = file.stat().st_size / 1e6
        print(f"  - {file.name} ({size_mb:.2f} MB)")
    
    print(f"\nâœ… All embeddings saved to {OUTPUT_DIR}")
    print("ðŸ’¡ Memory usage optimized for T4 GPUs")


if __name__ == "__main__":
    main()
