[
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:0",
    "content": "Inference - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Getting Started\n\n[Cloud Quickstart](https://qdrant.tech/documentation/cloud-quickstart/)\n\n### Managed Services\n\n[Getting Started](https://qdrant.tech/documentation/cloud-getting-started/)\n\n[Account Setup](https://qdrant.tech/documentation/cloud-account-setup/)\n\n[Cloud RBAC](https://qdrant.tech/documentation/cloud-rbac/)\n\n- [Role Management](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 272,
      "char_count": 1022,
      "start_char": 0,
      "end_char": 1022
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:1",
    "content": "/)\n\n[Cloud RBAC](https://qdrant.tech/documentation/cloud-rbac/)\n\n- [Role Management](https://qdrant.tech/documentation/cloud-rbac/role-management/)\n- [User Management](https://qdrant.tech/documentation/cloud-rbac/user-management/)\n- [Permission Reference](https://qdrant.tech/documentation/cloud-rbac/permission-reference/)\n\n[Managed Cloud](https://qdrant.tech/documentation/cloud/)\n\n- [Create a Cluster](https://qdrant.tech/documentation/cloud/create-cluster/)\n- [Authentication](https://qdrant.tech/documentation/cloud/authentication/)\n- [Cluster Access](https://qdrant.tech/documentation/cloud/cluster-access/)\n- [Scale Clusters](https://qdrant.tech/documentation/cloud/cluster-scaling/)\n- [Configure Clusters](https://qdrant.tech/documentation/cloud/configure-cluster/)\n- [Monitor Clusters](https://qdrant.tech/documentation/cloud/cluster-monitoring/)\n- [Update Clusters](https://qdrant.tech/documentation/cloud/cluster-upgrades/)\n- [Backup Clusters](https://qdrant.tech/documentation/cloud/backups/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 230,
      "char_count": 1004,
      "start_char": 922,
      "end_char": 1927
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:2",
    "content": "tion/cloud/cluster-upgrades/)\n- [Backup Clusters](https://qdrant.tech/documentation/cloud/backups/)\n- [Inference](https://qdrant.tech/documentation/cloud/inference/)\n\n[Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/)\n\n- [Setup Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/hybrid-cloud-setup/)\n- [Create a Cluster](https://qdrant.tech/documentation/hybrid-cloud/hybrid-cloud-cluster-creation/)\n- [Configure, Scale & Update Clusters](https://qdrant.tech/documentation/hybrid-cloud/configure-scale-upgrade/)\n- [Configure the Qdrant Operator](https://qdrant.tech/documentation/hybrid-cloud/operator-configuration/)\n- [Networking, Logging & Monitoring](https://qdrant.tech/documentation/hybrid-cloud/networking-logging-monitoring/)\n- [Deployment Platforms](https://qdrant.tech/documentation/hybrid-cloud/platform-deployment-options/)\n\n[Private Cloud](https://qdrant.tech/documentation/private-cloud/)\n\n- [Setup Private Cloud](https://qdrant.tech/documentation/private-cloud/private-cloud-setup/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 231,
      "char_count": 1020,
      "start_char": 1827,
      "end_char": 2848
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:3",
    "content": "ud/)\n\n- [Setup Private Cloud](https://qdrant.tech/documentation/private-cloud/private-cloud-setup/)\n- [Configuration](https://qdrant.tech/documentation/private-cloud/configuration/)\n- [Managing a Cluster](https://qdrant.tech/documentation/private-cloud/qdrant-cluster-management/)\n- [Backups](https://qdrant.tech/documentation/private-cloud/backups/)\n- [Logging & Monitoring](https://qdrant.tech/documentation/private-cloud/logging-monitoring/)\n- [API Reference](https://qdrant.tech/documentation/private-cloud/api-reference/)\n- [Changelog](https://qdrant.tech/documentation/private-cloud/changelog/)\n\n[Billing & Payments](https://qdrant.tech/documentation/cloud-pricing-payments/)\n\n[Premium Tier](https://qdrant.tech/documentation/cloud-premium/)\n\n### Interfaces & Tools\n\n[Qdrant Cloud API](https://qdrant.tech/documentation/cloud-api/)\n\n[Infrastructure Tools](https://qdrant.tech/documentation/cloud-tools/)\n\n- [Pulumi](https://qdrant.tech/documentation/cloud-tools/pulumi/)\n- [Terraform](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 231,
      "char_count": 1006,
      "start_char": 2748,
      "end_char": 3754
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:4",
    "content": "s/)\n\n- [Pulumi](https://qdrant.tech/documentation/cloud-tools/pulumi/)\n- [Terraform](https://qdrant.tech/documentation/cloud-tools/terraform/)\n\n### Support\n\n[Support](https://qdrant.tech/documentation/support/)\n\n[Security](https://qdrant.tech/documentation/cloud-security/)\n\n[Tutorials & Examples](https://qdrant.tech/documentation/tutorials-and-examples/)\n\n- [Using Cloud Inference to Build Hybrid Search](https://qdrant.tech/documentation/tutorials-and-examples/cloud-inference-hybrid-search/)\n- [Monitoring Hybrid/Private Cloud with Prometheus and Grafana](https://qdrant.tech/documentation/tutorials-and-examples/hybrid-cloud-prometheus/)\n\n### Getting Started\n\n[Cloud Quickstart](https://qdrant.tech/documentation/cloud-quickstart/)\n\n### Managed Services\n\n[Getting Started](https://qdrant.tech/documentation/cloud-getting-started/)\n\n[Account Setup](https://qdrant.tech/documentation/cloud-account-setup/)\n\n[Cloud RBAC](https://qdrant.tech/documentation/cloud-rbac/)\n\n- [Role Management](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 237,
      "char_count": 1006,
      "start_char": 3654,
      "end_char": 4660
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:5",
    "content": "/)\n\n[Cloud RBAC](https://qdrant.tech/documentation/cloud-rbac/)\n\n- [Role Management](https://qdrant.tech/documentation/cloud-rbac/role-management/)\n- [User Management](https://qdrant.tech/documentation/cloud-rbac/user-management/)\n- [Permission Reference](https://qdrant.tech/documentation/cloud-rbac/permission-reference/)\n\n[Managed Cloud](https://qdrant.tech/documentation/cloud/)\n\n- [Create a Cluster](https://qdrant.tech/documentation/cloud/create-cluster/)\n- [Authentication](https://qdrant.tech/documentation/cloud/authentication/)\n- [Cluster Access](https://qdrant.tech/documentation/cloud/cluster-access/)\n- [Scale Clusters](https://qdrant.tech/documentation/cloud/cluster-scaling/)\n- [Configure Clusters](https://qdrant.tech/documentation/cloud/configure-cluster/)\n- [Monitor Clusters](https://qdrant.tech/documentation/cloud/cluster-monitoring/)\n- [Update Clusters](https://qdrant.tech/documentation/cloud/cluster-upgrades/)\n- [Backup Clusters](https://qdrant.tech/documentation/cloud/backups/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 230,
      "char_count": 1004,
      "start_char": 4560,
      "end_char": 5565
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:6",
    "content": "tion/cloud/cluster-upgrades/)\n- [Backup Clusters](https://qdrant.tech/documentation/cloud/backups/)\n- [Inference](https://qdrant.tech/documentation/cloud/inference/)\n\n[Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/)\n\n- [Setup Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/hybrid-cloud-setup/)\n- [Create a Cluster](https://qdrant.tech/documentation/hybrid-cloud/hybrid-cloud-cluster-creation/)\n- [Configure, Scale & Update Clusters](https://qdrant.tech/documentation/hybrid-cloud/configure-scale-upgrade/)\n- [Configure the Qdrant Operator](https://qdrant.tech/documentation/hybrid-cloud/operator-configuration/)\n- [Networking, Logging & Monitoring](https://qdrant.tech/documentation/hybrid-cloud/networking-logging-monitoring/)\n- [Deployment Platforms](https://qdrant.tech/documentation/hybrid-cloud/platform-deployment-options/)\n\n[Private Cloud](https://qdrant.tech/documentation/private-cloud/)\n\n- [Setup Private Cloud](https://qdrant.tech/documentation/private-cloud/private-cloud-setup/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 231,
      "char_count": 1020,
      "start_char": 5465,
      "end_char": 6486
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:7",
    "content": "ud/)\n\n- [Setup Private Cloud](https://qdrant.tech/documentation/private-cloud/private-cloud-setup/)\n- [Configuration](https://qdrant.tech/documentation/private-cloud/configuration/)\n- [Managing a Cluster](https://qdrant.tech/documentation/private-cloud/qdrant-cluster-management/)\n- [Backups](https://qdrant.tech/documentation/private-cloud/backups/)\n- [Logging & Monitoring](https://qdrant.tech/documentation/private-cloud/logging-monitoring/)\n- [API Reference](https://qdrant.tech/documentation/private-cloud/api-reference/)\n- [Changelog](https://qdrant.tech/documentation/private-cloud/changelog/)\n\n[Billing & Payments](https://qdrant.tech/documentation/cloud-pricing-payments/)\n\n[Premium Tier](https://qdrant.tech/documentation/cloud-premium/)\n\n### Interfaces & Tools\n\n[Qdrant Cloud API](https://qdrant.tech/documentation/cloud-api/)\n\n[Infrastructure Tools](https://qdrant.tech/documentation/cloud-tools/)\n\n- [Pulumi](https://qdrant.tech/documentation/cloud-tools/pulumi/)\n- [Terraform](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 231,
      "char_count": 1006,
      "start_char": 6386,
      "end_char": 7392
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:8",
    "content": "s/)\n\n- [Pulumi](https://qdrant.tech/documentation/cloud-tools/pulumi/)\n- [Terraform](https://qdrant.tech/documentation/cloud-tools/terraform/)\n\n### Support\n\n[Support](https://qdrant.tech/documentation/support/)\n\n[Security](https://qdrant.tech/documentation/cloud-security/)\n\n[Tutorials & Examples](https://qdrant.tech/documentation/tutorials-and-examples/)\n\n- [Using Cloud Inference to Build Hybrid Search](https://qdrant.tech/documentation/tutorials-and-examples/cloud-inference-hybrid-search/)\n- [Monitoring Hybrid/Private Cloud with Prometheus and Grafana](https://qdrant.tech/documentation/tutorials-and-examples/hybrid-cloud-prometheus/)\n\n* [Documentation](https://qdrant.tech/documentation/)\n*\n* [Cloud](https://qdrant.tech/documentation/cloud/)\n*\n* Inference\n\n# Inference in Qdrant Managed Cloud\n\nInference is the process of creating vector embeddings from text, images, or other data types using a machine learning model.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 214,
      "char_count": 929,
      "start_char": 7292,
      "end_char": 8223
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:9",
    "content": "creating vector embeddings from text, images, or other data types using a machine learning model.\n\nQdrant Managed Cloud allows you to use inference directly in the cloud, without the need to set up and maintain your own inference infrastructure.\n\nInference is currently only available in US regions for paid clusters. Support for inference in other regions is coming soon.\n\n## Supported Models\n\nYou can see the list of supported models in the Inference tab of the Cluster Detail page in the Qdrant Cloud Console. The list includes models for text, both to produce dense and sparse vectors, as well as multi-modal models for images.\n\n## Enabling/Disabling Inference\n\nInference is enabled by default for all new clusters, created after July, 7th 2025. You can enable it for existing clusters directly from the Inference tab of the Cluster Detail page in the Qdrant Cloud Console. Activating inference will trigger a restart of your cluster to apply the new configuration.\n\n## Billing",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 201,
      "char_count": 981,
      "start_char": 8123,
      "end_char": 9107
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:10",
    "content": "ating inference will trigger a restart of your cluster to apply the new configuration.\n\n## Billing\n\nInference is billed based on the number of tokens processed by the model. The cost is calculated per 1,000,000 tokens. The price depends on the model and is displayed ont the Inference tab of the Cluster Detail page. You also can see the current usage of each model there.\n\n## Using Inference\n\nInference can be easily used through the Qdrant SDKs and the REST or GRPC APIs when upserting points and when querying the database.\n\nInstead of a vector, you can use special *Interface Objects*:\n\n- **`Document`** object, used for text inference\n\n```js\n// Document\n{\n    // Text input\n    text: \"Your text\",\n    // Name of the model, to do inference with\n    model: \"<the-model-to-use>\",\n    // Extra parameters for the model, Optional\n    options: {}\n}\n```\n\n- **`Image`** object, used for image inference\n\n```js\n// Image\n{\n    // Image input\n    image: \"<url>\", // Or base64 encoded image",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 241,
      "char_count": 983,
      "start_char": 9007,
      "end_char": 9991
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:11",
    "content": "image inference\n\n```js\n// Image\n{\n    // Image input\n    image: \"<url>\", // Or base64 encoded image\n    // Name of the model, to do inference with\n    model: \"<the-model-to-use>\",\n    // Extra parameters for the model, Optional\n    options: {}\n}\n```\n\n- **`Object`** object, reserved for other types of input, which might be implemented in the future.\n\nThe Qdrant API supports usage of these Inference Objects in all places, where regular vectors can be used.\n\nFor example:\n\n```http\nPOST /collections/<your-collection>/points/query\n{\n  \"query\": {\n    \"nearest\": [0.12, 0.34, 0.56, 0.78, ...]\n  }\n}\n```\n\nCan be replaced with\n\n```http\nPOST /collections/<your-collection>/points/query\n{\n  \"query\": {\n    \"nearest\": {\n      \"text\": \"My Query Text\",\n      \"model\": \"<the-model-to-use>\"\n    }\n  }\n}\n```\n\nIn this case, the Qdrant Cloud will use the configured embedding model to automatically create a vector from the Inference Object and then perform the search query with it. All of this happens within a low-latency network.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 270,
      "char_count": 1019,
      "start_char": 9891,
      "end_char": 10912
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:12",
    "content": "bject and then perform the search query with it. All of this happens within a low-latency network.\n\nThe input used for inference will not be saved anywhere. If you want to persist it in Qdrant, make sure to explicitly include it in the payload.\n\n### Text Inference\n\nLet’s consider an example of using Cloud Inference with a text model producing dense vectors.\n\nHere, we create one point and use a simple search query with a `Document` Inference Object.\n\n```http\n# Insert new points with cloud-side inference\nPUT /collections/<your-collection>/points?wait=true\n{\n  \"points\": [\n    {\n      \"id\": 1,\n      \"payload\": { \"topic\": \"cooking\", \"type\": \"dessert\" },\n      \"vector\": {\n        \"text\": \"Recipe for baking chocolate chip cookies\",\n        \"model\": \"<the-model-to-use>\"\n      }\n    }\n  ]\n}\n\n# Search in the collection using cloud-side inference\nPOST /collections/<your-collection>/points/query\n{\n  \"query\": {\n    \"text\": \"How to bake cookies?\",\n    \"model\": \"<the-model-to-use>\"\n  }\n}\n```\n\n```bash\n# Create a new vector",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 250,
      "char_count": 1022,
      "start_char": 10812,
      "end_char": 11835
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:13",
    "content": "\"How to bake cookies?\",\n    \"model\": \"<the-model-to-use>\"\n  }\n}\n```\n\n```bash\n# Create a new vector\ncurl -X PUT \"https://xyz-example.qdrant.io:6333/collections/<your-collection>/points?wait=true\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"api-key: <paste-your-api-key-here>\" \\\n  -d '{\n    \"points\": [\n      {\n        \"id\": 1,\n        \"payload\": { \"topic\": \"cooking\", \"type\": \"dessert\" },\n        \"vector\": {\n          \"text\": \"Recipe for baking chocolate chip cookies\",\n          \"model\": \"<the-model-to-use>\"\n        }\n      }\n    ]\n  }'\n\n# Perform a search query\ncurl -X POST \"https://xyz-example.qdrant.io:6333/collections/<your-collection>/points/query\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"api-key: <paste-your-api-key-here>\" \\\n  -d '{\n    \"query\": {\n      \"text\": \"How to bake cookies?\",\n      \"model\": \"<the-model-to-use>\"\n    }\n  }'\n```\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct, Document\n\nclient = QdrantClient(\n    url=\"https://xyz-example.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 301,
      "char_count": 1018,
      "start_char": 11735,
      "end_char": 12754
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:14",
    "content": "ant_client.models import PointStruct, Document\n\nclient = QdrantClient(\n    url=\"https://xyz-example.qdrant.io:6333\",\n    api_key=\"<paste-your-api-key-here>\",\n    # IMPORTANT\n    # If not enabled, inference will be performed locally\n    cloud_inference=True,\n)\n\npoints = [\n    PointStruct(\n        id=1,\n        payload={\"topic\": \"cooking\", \"type\": \"dessert\"},\n        vector=Document(\n            text=\"Recipe for baking chocolate chip cookies\",\n            model=\"<the-model-to-use>\"\n        )\n    )\n]\n\nclient.upsert(collection_name=\"<your-collection>\", points=points)\n\nresult = client.query_points(\n    collection_name=\"<your-collection>\",\n    query=Document(\n        text=\"How to bake cookies?\",\n        model=\"<the-model-to-use>\"\n    )\n)\n\nprint(result)\n```\n\n```typescript\nimport {QdrantClient} from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({\n    url: 'https://xyz-example.qdrant.io:6333',\n    apiKey: '<paste-your-api-key-here>',\n});\n\nconst points = [\n  {\n    id: 1,",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 248,
      "char_count": 988,
      "start_char": 12654,
      "end_char": 13643
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:15",
    "content": "mple.qdrant.io:6333',\n    apiKey: '<paste-your-api-key-here>',\n});\n\nconst points = [\n  {\n    id: 1,\n    payload: { topic: \"cooking\", type: \"dessert\" },\n    vector: {\n        text: \"Recipe for baking chocolate chip cookies\",\n        model: \"<the-model-to-use>\"\n      }\n  }\n];\n\nawait client.upsert(\"<your-collection>\", { wait: true, points });\n\nconst result = await client.query(\n    \"<your-collection>\",\n    {\n      query: {\n          text: \"How to bake cookies?\",\n          model: \"<the-model-to-use>\"\n      },\n    }\n)\n\nconsole.log(result);\n```\n\n```rust\nuse qdrant_client::qdrant::Query;\nuse qdrant_client::qdrant::QueryPointsBuilder;\nuse qdrant_client::Payload;\nuse qdrant_client::Qdrant;\nuse qdrant_client::qdrant::{Document};\nuse qdrant_client::qdrant::{PointStruct, UpsertPointsBuilder};\n\n#[tokio::main]\nasync fn main() {\n    let client = Qdrant::from_url(\"https://xyz-example.qdrant.io:6334\")\n        .api_key(\"<paste-your-api-key-here>\")\n        .build()\n        .unwrap();\n\n    let points = vec![",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "token_count": 283,
      "char_count": 1003,
      "start_char": 13543,
      "end_char": 14547
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:16",
    "content": ".api_key(\"<paste-your-api-key-here>\")\n        .build()\n        .unwrap();\n\n    let points = vec![\n        PointStruct::new(\n            1,\n            Document::new(\n                \"Recipe for baking chocolate chip cookies\",\n                \"<the-model-to-use>\"\n            ),\n            Payload::try_from(serde_json::json!(\n                {\"topic\": \"cooking\", \"type\": \"dessert\"}\n            )).unwrap(),\n        )\n    ];\n\n    let upsert_request = UpsertPointsBuilder::new(\n        \"<your-collection>\",\n        points\n    ).wait(true);\n\n    let _ = client.upsert_points(upsert_request).await;\n\n    let query_document = Document::new(\n        \"How to bake cookies?\",\n        \"<the-model-to-use>\"\n    );\n\n    let query_request = QueryPointsBuilder::new(\"<your-collection>\")\n        .query(Query::new_nearest(query_document));\n\n    let result = client.query(query_request).await.unwrap();\n    println!(\"Result: {:?}\", result);\n}\n```\n\n```java\npackage org.example;\n\nimport static io.qdrant.client.PointIdFactory.id;",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "token_count": 226,
      "char_count": 1013,
      "start_char": 14447,
      "end_char": 15463
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:17",
    "content": "}\", result);\n}\n```\n\n```java\npackage org.example;\n\nimport static io.qdrant.client.PointIdFactory.id;\nimport static io.qdrant.client.QueryFactory.nearest;\nimport static io.qdrant.client.ValueFactory.value;\nimport static io.qdrant.client.VectorsFactory.vectors;\n\nimport io.qdrant.client.grpc.Points;\nimport io.qdrant.client.grpc.Points.Document;\nimport io.qdrant.client.grpc.Points.PointStruct;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ExecutionException;\n\npublic class Main {\n  public static void main(String[] args)\n      throws ExecutionException, InterruptedException {\n    QdrantClient client =\n      new QdrantClient(\n        QdrantGrpcClient.newBuilder(\"xyz-example.qdrant.io\", 6334, true)\n        .withApiKey(\"<paste-your-api-key-here>\")\n        .build());\n\n    client\n      .upsertAsync(\n        \"<your-collection>\",\n        List.of(\n          PointStruct.newBuilder()\n          .setId(id(1))\n          .setVectors(\n            vectors(\n              Document.newBuilder()",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "token_count": 224,
      "char_count": 1010,
      "start_char": 15363,
      "end_char": 16374
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:18",
    "content": ".setId(id(1))\n          .setVectors(\n            vectors(\n              Document.newBuilder()\n              .setText(\"Recipe for baking chocolate chip cookies\")\n              .setModel(\"<the-model-to-use>\")\n              .build()))\n          .putAllPayload(Map.of(\"topic\", value(\"cooking\"), \"type\", value(\"dessert\")))\n          .build()))\n      .get();\n\n    List <Points.ScoredPoint> points =\n      client\n      .queryAsync(\n        Points.QueryPoints.newBuilder()\n        .setCollectionName(\"<your-collection>\")\n        .setQuery(\n          nearest(\n            Document.newBuilder()\n            .setText(\"How to bake cookies?\")\n            .setModel(\"<the-model-to-use>\")\n            .build()))\n        .build())\n      .get();\n\n    System.out.printf(points.toString());\n  }\n}\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\nusing Value = Qdrant.Client.Grpc.Value;\n\nvar client = new QdrantClient(\n  host: \"xyz-example.qdrant.io\",\n  port: 6334,\n  https: true,\n  apiKey: \"<paste-your-api-key-here>\"\n);",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 18,
      "token_count": 237,
      "char_count": 1014,
      "start_char": 16274,
      "end_char": 17296
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:19",
    "content": "st: \"xyz-example.qdrant.io\",\n  port: 6334,\n  https: true,\n  apiKey: \"<paste-your-api-key-here>\"\n);\n\nawait client.UpsertAsync(\n  collectionName: \"<your-collection>\",\n  points: new List <PointStruct> {\n    new() {\n      Id = 1,\n        Vectors = new Document() {\n          Text = \"Recipe for baking chocolate chip cookies\",\n          Model = \"<the-model-to-use>\",\n        },\n        Payload = {\n          [\"topic\"] = \"cooking\",\n          [\"type\"] = \"dessert\"\n        },\n    },\n  }\n);\n\nvar points = await client.QueryAsync(\n  collectionName: \"<your-collection>\",\n  query: new Document() {\n    Text = \"How to bake cookies?\",\n    Model = \"<the-model-to-use>\"\n  }\n);\n\nforeach(var point in points) {\n  Console.WriteLine(point);\n}\n```\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"log\"\n    \"time\"\n\n    \"github.com/qdrant/go-client/qdrant\"\n)\n\nfunc main() {\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second)\n    defer cancel()\n\n    client, err := qdrant.NewClient(&qdrant.Config{",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "token_count": 261,
      "char_count": 996,
      "start_char": 17196,
      "end_char": 18193
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:20",
    "content": ".Background(), time.Second)\n    defer cancel()\n\n    client, err := qdrant.NewClient(&qdrant.Config{\n        Host:   \"xyz-example.qdrant.io\",\n        Port:   6334,\n        APIKey: \"<paste-your-api-key-here>\",\n        UseTLS: true,\n    })\n    if err != nil {\n        log.Fatalf(\"did not connect: %v\", err)\n    }\n    defer client.Close()\n\n    _, err = client.GetPointsClient().Upsert(ctx, &qdrant.UpsertPoints{\n        CollectionName: \"<your-collection>\",\n        Points: []*qdrant.PointStruct{\n            {\n                Id: qdrant.NewIDNum(uint64(1)),\n                Vectors: qdrant.NewVectorsDocument(&qdrant.Document{\n                    Text:  \"Recipe for baking chocolate chip cookies\",\n                    Model: \"<the-model-to-use>\",\n                }),\n                Payload: qdrant.NewValueMap(map[string]any{\n                    \"topic\": \"cooking\",\n                    \"type\":  \"dessert\",\n                }),\n            },\n        },\n    })\n    if err != nil {\n        log.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "token_count": 237,
      "char_count": 988,
      "start_char": 18093,
      "end_char": 19081
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:21",
    "content": "\":  \"dessert\",\n                }),\n            },\n        },\n    })\n    if err != nil {\n        log.Fatalf(\"error creating point: %v\", err)\n    }\n\n    points, err := client.Query(ctx, &qdrant.QueryPoints{\n        CollectionName: \"<your-collection>\",\n        Query: qdrant.NewQueryNearest(\n            qdrant.NewVectorInputDocument(&qdrant.Document{\n                Text:  \"How to bake cookies?\",\n                Model: \"<the-model-to-use>\",\n            }),\n        ),\n    })\n    log.Printf(\"List of points: %s\", points)\n}\n```\n\nUsage examples, specific to each cluster and model, can also be found in the Inference tab of the Cluster Detail page in the Qdrant Cloud Console.\n\nNote that each model has a context window, which is the maximum number of tokens that can be processed by the model in a single request. If the input text exceeds the context window, it will be truncated to fit within the limit. The context window size is displayed in the Inference tab of the Cluster Detail page.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "token_count": 221,
      "char_count": 989,
      "start_char": 18981,
      "end_char": 19972
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:22",
    "content": "n the limit. The context window size is displayed in the Inference tab of the Cluster Detail page.\n\nFor dense vector models, you also have to ensure that the vector size configured in the collection matches the output size of the model. If the vector size does not match, the upsert will fail with an error.\n\n### Image Inference\n\nHere is another example of using Cloud Inference with an image model. This time, we will use the `CLIP` model to encode an image and then use a text query to search for it.\n\nSince the `CLIP` model is multimodal, we can use both image and text inputs on the same vector field.\n\n```http\n# Insert new points with cloud-side inference\nPUT /collections/<your-collection>/points?wait=true\n{\n  \"points\": [\n    {\n      \"id\": 1,\n      \"vector\": {\n        \"image\": \"https://qdrant.tech/example.png\",\n        \"model\": \"qdrant/clip-vit-b-32-vision\"\n      },\n      \"payload\": {\n        \"title\": \"Example Image\"\n      }\n    }\n  ]\n}\n\n# Search in the collection using cloud-side inference",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "token_count": 244,
      "char_count": 1002,
      "start_char": 19872,
      "end_char": 20875
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:23",
    "content": "\"title\": \"Example Image\"\n      }\n    }\n  ]\n}\n\n# Search in the collection using cloud-side inference\nPOST /collections/<your-collection>/points/query\n{\n  \"query\": {\n    \"text\": \"Mission to Mars\",\n    \"model\": \"qdrant/clip-vit-b-32-text\"\n  }\n}\n```\n\n```bash\n# Create a new vector\ncurl -X PUT \"https://xyz-example.qdrant.io:6333/collections/<your-collection>/points?wait=true\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"api-key: <paste-your-api-key-here>\" \\\n  -d '{\n    \"points\": [\n      {\n        \"id\": 1,\n        \"vector\": {\n          \"image\": \"https://qdrant.tech/example.png\",\n          \"model\": \"qdrant/clip-vit-b-32-vision\"\n        },\n        \"payload\": {\n          \"title\": \"Example Image\"\n        }\n      }\n    ]\n  }'\n\n# Perform a search query\ncurl -X POST \"https://xyz-example.qdrant.io:6333/collections/<your-collection>/points/query\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"api-key: <paste-your-api-key-here>\" \\\n  -d '{\n    \"query\": {\n      \"text\": \"Mission to Mars\",",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "token_count": 298,
      "char_count": 987,
      "start_char": 20775,
      "end_char": 21763
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:24",
    "content": "-H \"api-key: <paste-your-api-key-here>\" \\\n  -d '{\n    \"query\": {\n      \"text\": \"Mission to Mars\",\n      \"model\": \"qdrant/clip-vit-b-32-text\"\n    }\n  }'\n```\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct, Image, Document\n\nclient = QdrantClient(\n    url=\"https://xyz-example.qdrant.io:6333\",\n    api_key=\"<paste-your-api-key-here>\",\n    # IMPORTANT\n    # If not enabled, inference will be performed locally\n    cloud_inference=True,\n)\n\npoints = [\n    PointStruct(\n        id=1,\n        vector=Image(\n            image=\"https://qdrant.tech/example.png\",\n            model=\"qdrant/clip-vit-b-32-vision\"\n        ),\n        payload={\n            \"title\": \"Example Image\"\n        }\n    )\n]\n\nclient.upsert(collection_name=\"<your-collection>\", points=points)\n\nresult = client.query_points(\n    collection_name=\"<your-collection>\",\n    query=Document(\n        text=\"Mission to Mars\",\n        model=\"qdrant/clip-vit-b-32-text\"\n    )\n)\n\nprint(result)\n```\n\n```typescript",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "token_count": 275,
      "char_count": 1005,
      "start_char": 21663,
      "end_char": 22671
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:25",
    "content": "ssion to Mars\",\n        model=\"qdrant/clip-vit-b-32-text\"\n    )\n)\n\nprint(result)\n```\n\n```typescript\nimport {QdrantClient} from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({\n    url: 'https://xyz-example.qdrant.io:6333',\n    apiKey: '<paste-your-api-key-here>',\n});\n\nconst points = [\n  {\n    id: 1,\n    vector: {\n      image: \"https://qdrant.tech/example.png\",\n      model: \"qdrant/clip-vit-b-32-vision\"\n    },\n    payload: {\n      title: \"Example Image\"\n    }\n  }\n];\n\nawait client.upsert(\"<your-collection>\", { wait: true, points });\n\nconst result = await client.query(\n    \"<your-collection>\",\n    {\n      query: {\n          text: \"Mission to Mars\",\n          model: \"qdrant/clip-vit-b-32-text\"\n      },\n    }\n)\n\nconsole.log(result);\n```\n\n```rust\nuse qdrant_client::qdrant::Query;\nuse qdrant_client::qdrant::QueryPointsBuilder;\nuse qdrant_client::Payload;\nuse qdrant_client::Qdrant;\nuse qdrant_client::qdrant::{Document, Image};\nuse qdrant_client::qdrant::{PointStruct, UpsertPointsBuilder};\n\n#[tokio::main]",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "token_count": 306,
      "char_count": 1023,
      "start_char": 22571,
      "end_char": 23595
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:26",
    "content": "::{Document, Image};\nuse qdrant_client::qdrant::{PointStruct, UpsertPointsBuilder};\n\n#[tokio::main]\nasync fn main() {\n    let client = Qdrant::from_url(\"https://xyz-example.qdrant.io:6334\")\n        .api_key(\"<paste-your-api-key-here>\")\n        .build()\n        .unwrap();\n\n    let points = vec![\n        PointStruct::new(\n            1,\n            Image::new_from_url(\n                \"https://qdrant.tech/example.png\",\n                \"qdrant/clip-vit-b-32-vision\"\n            ),\n            Payload::try_from(serde_json::json!({\n                \"title\": \"Example Image\"\n            })).unwrap(),\n        )\n    ];\n\n    let upsert_request = UpsertPointsBuilder::new(\n        \"<your-collection>\",\n        points\n    ).wait(true);\n\n    let _ = client.upsert_points(upsert_request).await;\n\n    let query_document = Document::new(\n        \"Mission to Mars\",\n        \"qdrant/clip-vit-b-32-text\"\n    );\n\n    let query_request = QueryPointsBuilder::new(\"<your-collection>\")\n        .query(Query::new_nearest(query_document));",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "token_count": 256,
      "char_count": 1019,
      "start_char": 23495,
      "end_char": 24516
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:27",
    "content": "= QueryPointsBuilder::new(\"<your-collection>\")\n        .query(Query::new_nearest(query_document));\n\n    let result = client.query(query_request).await.unwrap();\n    println!(\"Result: {:?}\", result);\n}\n```\n\n```java\npackage org.example;\n\nimport static io.qdrant.client.PointIdFactory.id;\nimport static io.qdrant.client.QueryFactory.nearest;\nimport static io.qdrant.client.ValueFactory.value;\nimport static io.qdrant.client.VectorsFactory.vectors;\n\nimport io.qdrant.client.grpc.Points;\nimport io.qdrant.client.grpc.Points.Document;\nimport io.qdrant.client.grpc.Points.Image;\nimport io.qdrant.client.grpc.Points.PointStruct;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ExecutionException;\n\npublic class Main {\n  public static void main(String[] args)\n      throws ExecutionException, InterruptedException {\n    QdrantClient client =\n      new QdrantClient(\n        QdrantGrpcClient.newBuilder(\"xyz-example.qdrant.io\", 6334, true)\n        .withApiKey(\"<paste-your-api-key-here>\")\n        .build());",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "token_count": 230,
      "char_count": 1022,
      "start_char": 24416,
      "end_char": 25440
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:28",
    "content": "xample.qdrant.io\", 6334, true)\n        .withApiKey(\"<paste-your-api-key-here>\")\n        .build());\n\n    client\n      .upsertAsync(\n        \"<your-collection>\",\n        List.of(\n          PointStruct.newBuilder()\n          .setId(id(1))\n          .setVectors(\n            vectors(\n              Image.newBuilder()\n              .setImage(\"https://qdrant.tech/example.png\")\n              .setModel(\"qdrant/clip-vit-b-32-vision\")\n              .build()))\n          .putAllPayload(Map.of(\"title\", value(\"Example Image\")))\n          .build()))\n      .get();\n\n    List <Points.ScoredPoint> points =\n      client\n      .queryAsync(\n        Points.QueryPoints.newBuilder()\n        .setCollectionName(\"<your-collection>\")\n        .setQuery(\n          nearest(\n            Document.newBuilder()\n            .setText(\"Mission to Mars\")\n            .setModel(\"qdrant/clip-vit-b-32-text\")\n            .build()))\n        .build())\n      .get();\n\n    System.out.printf(points.toString());\n  }\n}\n```\n\n```csharp\nusing Qdrant.Client;",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "token_count": 236,
      "char_count": 1015,
      "start_char": 25340,
      "end_char": 26356
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:29",
    "content": ".get();\n\n    System.out.printf(points.toString());\n  }\n}\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\nusing Value = Qdrant.Client.Grpc.Value;\n\nvar client = new QdrantClient(\n  host: \"xyz-example.qdrant.io\",\n  port: 6334,\n  https: true,\n  apiKey: \"<paste-your-api-key-here>\"\n);\n\nawait client.UpsertAsync(\n  collectionName: \"<your-collection>\",\n  points: new List <PointStruct> {\n    new() {\n      Id = 1,\n        Vectors = new Image() {\n          Image = \"https://qdrant.tech/example.png\",\n          Model = \"qdrant/clip-vit-b-32-vision\",\n        },\n        Payload = {\n          [\"title\"] = \"Example Image\"\n        },\n    },\n  }\n);\n\nvar points = await client.QueryAsync(\n  collectionName: \"<your-collection>\",\n  query: new Document() {\n    Text = \"Mission to Mars\",\n    Model = \"qdrant/clip-vit-b-32-text\"\n  }\n);\n\nforeach(var point in points) {\n  Console.WriteLine(point);\n}\n```\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"log\"\n    \"time\"\n\n    \"github.com/qdrant/go-client/qdrant\"\n)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "token_count": 288,
      "char_count": 1004,
      "start_char": 26256,
      "end_char": 27269
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:30",
    "content": "kage main\n\nimport (\n    \"context\"\n    \"log\"\n    \"time\"\n\n    \"github.com/qdrant/go-client/qdrant\"\n)\n\nfunc main() {\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second)\n    defer cancel()\n\n    client, err := qdrant.NewClient(&qdrant.Config{\n        Host:   \"xyz-example.qdrant.io\",\n        Port:   6334,\n        APIKey: \"<paste-your-api-key-here>\",\n        UseTLS: true,\n    })\n    if err != nil {\n        log.Fatalf(\"did not connect: %v\", err)\n    }\n    defer client.Close()\n\n    _, err = client.GetPointsClient().Upsert(ctx, &qdrant.UpsertPoints{\n        CollectionName: \"<your-collection>\",\n        Points: []*qdrant.PointStruct{\n            {\n                Id: qdrant.NewIDNum(uint64(1)),\n                Vectors: qdrant.NewVectorsImage(&qdrant.Image{\n                    Image: \"https://qdrant.tech/example.png\",\n                    Model: \"qdrant/clip-vit-b-32-vision\",\n                }),\n                Payload: qdrant.NewValueMap(map[string]any{\n                    \"title\": \"Example image\",",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 30,
      "token_count": 266,
      "char_count": 1022,
      "start_char": 27169,
      "end_char": 28192
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:31",
    "content": "Payload: qdrant.NewValueMap(map[string]any{\n                    \"title\": \"Example image\",\n                }),\n            },\n        },\n    })\n    if err != nil {\n        log.Fatalf(\"error creating point: %v\", err)\n    }\n\n    points, err := client.Query(ctx, &qdrant.QueryPoints{\n        CollectionName: \"<your-collection>\",\n        Query: qdrant.NewQueryNearest(\n            qdrant.NewVectorInputDocument(&qdrant.Document{\n                Text:  \"Mission to Mars\",\n                Model: \"qdrant/clip-vit-b-32-text\",\n            }),\n        ),\n    })\n    log.Printf(\"List of points: %s\", points)\n}\n```\n\nQdrant Cloud Inference server will download the images using the provided link. Alternatively, you can upload the image as a base64 encoded string.\n\nNote that each model has limitations on the file size and extensions it can work with.\n\nPlease refer to the model card for details.\n\n### Local Inference Compatibility\n\nThe Python SDK offers a unique capability: it supports both [local](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 31,
      "token_count": 231,
      "char_count": 1004,
      "start_char": 28092,
      "end_char": 29106
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:32",
    "content": "e Compatibility\n\nThe Python SDK offers a unique capability: it supports both [local](https://qdrant.tech/documentation/fastembed/fastembed-semantic-search/) and cloud inference through an identical interface.\n\nYou can easily switch between local and cloud inference by setting the cloud\\_inference flag when initializing the QdrantClient. For example:\n\n```python\nclient = QdrantClient(\n    url=\"https://your-cluster.qdrant.io\",\n    api_key=\"<your-api-key>\",\n    cloud_inference=True,  # Set to False to use local inference\n)\n```\n\nThis flexibility allows you to develop and test your applications locally or in continuous integration (CI) environments without requiring access to cloud inference resources.\n\n- When `cloud_inference` is set to `False`, inference is performed locally usign `fastembed`.\n- When set to `True`, inference requests are handled by Qdrant Cloud.\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 32,
      "token_count": 221,
      "char_count": 999,
      "start_char": 29006,
      "end_char": 30005
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:33",
    "content": "Yes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/cloud/inference.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Inference in Qdrant Managed Cloud](#inference-in-qdrant-managed-cloud.md)\n\n  - [Supported Models](#supported-models.md)\n\n  - [Enabling/Disabling Inference](#enablingdisabling-inference.md)\n\n  - [Billing](#billing.md)\n\n  - [Using Inference](#using-inference.md)\n\n    - [Text Inference](#text-inference.md)\n    - [Image Inference](#image-inference.md)\n    - [Local Inference Compatibility](#local-inference-compatibility.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/cloud/inference.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 33,
      "token_count": 274,
      "char_count": 1015,
      "start_char": 29905,
      "end_char": 30922
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md:chunk:34",
    "content": "anding_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_cloud_inference\\_documentation_cloud_inference_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_cloud_inference_.md",
      "file_extension": ".md",
      "chunk_index": 34,
      "token_count": 86,
      "char_count": 294,
      "start_char": 30822,
      "end_char": 31846
    }
  }
]