[
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:0",
    "content": "Automating Processes with Qdrant and n8n - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 268,
      "char_count": 975,
      "start_char": 0,
      "end_char": 977
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:1",
    "content": "wai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 269,
      "char_count": 985,
      "start_char": 877,
      "end_char": 1862
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:2",
    "content": "gnee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 264,
      "char_count": 1014,
      "start_char": 1762,
      "end_char": 2777
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:3",
    "content": "ation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 271,
      "char_count": 1009,
      "start_char": 2677,
      "end_char": 3686
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:4",
    "content": "[AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 274,
      "char_count": 1005,
      "start_char": 3586,
      "end_char": 4591
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:5",
    "content": "angchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 272,
      "char_count": 995,
      "start_char": 4491,
      "end_char": 5487
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:6",
    "content": "entation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 271,
      "char_count": 1019,
      "start_char": 5387,
      "end_char": 6407
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:7",
    "content": "cumentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 262,
      "char_count": 997,
      "start_char": 6307,
      "end_char": 7304
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:8",
    "content": "tation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 260,
      "char_count": 985,
      "start_char": 7204,
      "end_char": 8190
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:9",
    "content": "Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 233,
      "char_count": 1009,
      "start_char": 8090,
      "end_char": 9100
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:10",
    "content": "ation Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 275,
      "char_count": 1020,
      "start_char": 9000,
      "end_char": 10020
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:11",
    "content": "qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 259,
      "char_count": 1012,
      "start_char": 9920,
      "end_char": 10932
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:12",
    "content": "ured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 270,
      "char_count": 1022,
      "start_char": 10832,
      "end_char": 11855
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:13",
    "content": "documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 265,
      "char_count": 1000,
      "start_char": 11755,
      "end_char": 12756
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:14",
    "content": "umentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 277,
      "char_count": 1017,
      "start_char": 12656,
      "end_char": 13674
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:15",
    "content": "frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "token_count": 274,
      "char_count": 1008,
      "start_char": 13574,
      "end_char": 14582
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:16",
    "content": "VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "token_count": 273,
      "char_count": 1014,
      "start_char": 14482,
      "end_char": 15497
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:17",
    "content": "ation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "token_count": 259,
      "char_count": 1013,
      "start_char": 15397,
      "end_char": 16410
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:18",
    "content": "-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 18,
      "token_count": 254,
      "char_count": 1005,
      "start_char": 16310,
      "end_char": 17315
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:19",
    "content": "r-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- Automating Processes with Qdrant and n8n\n\n# Automating Processes with Qdrant and n8n beyond simple RAG",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "token_count": 243,
      "char_count": 1010,
      "start_char": 17215,
      "end_char": 18227
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:20",
    "content": "mating Processes with Qdrant and n8n\n\n# Automating Processes with Qdrant and n8n beyond simple RAG\n\n| Time: 45 min | Level: Intermediate |\n| ------------ | ------------------- |\n\nThis tutorial shows how to combine Qdrant with [n8n](https://n8n.io/) low-code automation platform to cover **use cases beyond basic Retrieval-Augmented Generation (RAG)**. You’ll learn how to use vector search for **recommendations** and **unstructured big data analysis**.\n\nSince this tutorial was created, [an official Qdrant node for n8n](https://qdrant.tech/documentation/platforms/n8n/) has been released. It simplifies workflows and replaces the HTTP request nodes used in the examples below. Watch [a quick video introduction](https://youtu.be/sYP_kHWptHY) to it.\n\n## Setting Up Qdrant in n8n\n\nTo start using Qdrant with n8n, you need to provide your Qdrant instance credentials in the [credentials](https://docs.n8n.io/integrations/builtin/credentials/qdrant/#using-api-key) tab. Select `QdrantApi` from the list.\n\n### Qdrant Cloud",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "token_count": 262,
      "char_count": 1019,
      "start_char": 18127,
      "end_char": 19148
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:21",
    "content": "uiltin/credentials/qdrant/#using-api-key) tab. Select `QdrantApi` from the list.\n\n### Qdrant Cloud\n\nTo connect [Qdrant Cloud](https://qdrant.tech/documentation/cloud/) to n8n:\n\n1. Open the [Cloud Dashboard](https://qdrant.to/cloud) and select a cluster.\n2. From the **Cluster Details**, copy the `Endpoint` address—this will be used as the `Qdrant URL` in n8n.\n3. Navigate to the **API Keys** tab and copy your API key—this will be the `API Key` in n8n.\n\nFor a walkthrough, see this [step-by-step video guide](https://youtu.be/fYMGpXyAsfQ?feature=shared\\&t=177).\n\n### Local Mode\n\nFor a fully local experimnets-driven setup, a valuable option is n8n’s [Self-hosted AI Starter Kit](https://github.com/n8n-io/self-hosted-ai-starter-kit). This is an open-source Docker Compose template for local AI & low-code development environment.\n\nThis kit includes a [local instance of Qdrant](https://qdrant.tech/documentation/quickstart/). To get started:\n\n1. Follow the instructions in the repository to install the AI Starter Kit.\n2.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "token_count": 282,
      "char_count": 1022,
      "start_char": 19048,
      "end_char": 20070
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:22",
    "content": "t/). To get started:\n\n1. Follow the instructions in the repository to install the AI Starter Kit.\n2. Use the values from the `docker-compose.yml` file to fill in the connection details.\n\nRemember to update to the latest Qdrant Docker image using `docker-compose pull`.\n\nThe default Qdrant configuration in AI Starter Kit’s `docker-compose.yml` looks like this:\n\n```yaml\nqdrant:\n  image: qdrant/qdrant\n  hostname: qdrant\n  container_name: qdrant\n  networks: ['demo']\n  restart: unless-stopped\n  ports:\n    - 6333:6333\n  volumes:\n    - qdrant_storage:/qdrant/storage\n```\n\nFrom this configuration, the `Qdrant URL` in n8n Qdrant credentials is `http://qdrant:6333/`. To set up a local Qdrant API key, add the following lines to the YAML file:\n\n```yaml\nqdrant:\n  ...\n  volumes:\n    - qdrant_storage:/qdrant/storage\n  environment:\n    - QDRANT_API_KEY=test\n```\n\nAfter saving the configuration and running the Starter Kit, use `QDRANT_API_KEY` value (e.g., `test`) as the `API Key` and `http://qdrant:6333/` as the `Qdrant URL`.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "token_count": 302,
      "char_count": 1022,
      "start_char": 19970,
      "end_char": 20994
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:23",
    "content": "RANT_API_KEY` value (e.g., `test`) as the `API Key` and `http://qdrant:6333/` as the `Qdrant URL`.\n\n## Qdrant + n8n Beyond Simple Similarity Search\n\nVector search’s ability to determine semantic similarity between objects is often used to address models’ hallucinations, powering the memory of Retrieval-Augmented Generation-based applications. Yet there’s more to vector search than just a “knowledge base” role.\n\nThe combination of similarity and dissimilarity metrics in vector space expands vector search to recommendations, discovery search, and large-scale unstructured data analysis.\n\n### Recommendations\n\nWhen searching for new music, films, books, or food, it can be difficult to articulate exactly what we want. Instead, we often rely on discovering new content through comparison to examples of what we like or dislike.\n\nThe [Qdrant Recommendation API](https://qdrant.tech/articles/new-recommendation-api/) is built to make these discovery searches possible by using positive and negative examples as anchors.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "token_count": 217,
      "char_count": 1020,
      "start_char": 20894,
      "end_char": 21914
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:24",
    "content": "built to make these discovery searches possible by using positive and negative examples as anchors. It helps find new relevant results based on your preferences.\n\n#### Movie Recommendations\n\nImagine a home cinema night—you’ve already watched Harry Potter 666 times and crave a new series featuring young wizards. Your favorite streaming service repetitively recommends all seven parts of the millennial saga. Frustrated, you turn to n8n to create an **Agentic Movie Recommendation tool**.\n\n**Setup:**\n\n1. **Dataset**: We use movie descriptions from the [IMDB Top 1000 Kaggle dataset](https://www.kaggle.com/datasets/omarhanyy/imdb-top-1000).\n2. **Embedding Model**: We’ll use OpenAI `text-embedding-3-small`, but you can opt for any other suitable embedding model.\n\n**Workflow:**\n\nA [Template Agentic Movie Recommendation Workflow](https://n8n.io/workflows/2440-building-rag-chatbot-for-movie-recommendations-with-qdrant-and-open-ai/) consists of three parts:\n\n1.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "token_count": 231,
      "char_count": 963,
      "start_char": 21814,
      "end_char": 22778
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:25",
    "content": "uilding-rag-chatbot-for-movie-recommendations-with-qdrant-and-open-ai/) consists of three parts:\n\n1. **Movie Data Uploader**: Embeds movie descriptions and uploads them to Qdrant using the [Qdrant Vector Store Node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant) (now this can also be done using the [official Qdrant Node for n8n](https://github.com/qdrant/n8n-nodes-qdrant)). In the template workflow, the dataset is fetched from GitHub, but you can use any supported storage, for example [Google Cloud Storage node](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.googlecloudstorage).\n2. **AI Agent**: Uses the [AI Agent Node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent) to formulate Recommendation API calls based on your natural language requests. Choose an LLM as a “brain” and define a [JSON schema](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "token_count": 272,
      "char_count": 1016,
      "start_char": 22678,
      "end_char": 23694
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:26",
    "content": "[JSON schema](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolworkflow/#specify-input-schema) for the recommendations tool powered by Qdrant. This schema lets the LLM map your requests to the tool input format.\n3. **Recommendations Tool**: A [subworkflow](https://docs.n8n.io/flow-logic/subworkflows/) that calls the Qdrant Recommendation API using the [HTTP Request Node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest) (now this can also be done using the [official Qdrant Node for n8n](https://github.com/qdrant/n8n-nodes-qdrant)). The agent extracts relevant and irrelevant movie descriptions from your chat message and passes them to the tool. The tool embeds them with `text-embedding-3-small` and uses the Qdrant Recommendation API to get movie recommendations, which are passed back to the agent.\n\nSet it up, run a chat and ask for “*something about wizards but not Harry Potter*.” What results do you get?\n\n---",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "token_count": 247,
      "char_count": 992,
      "start_char": 23594,
      "end_char": 24589
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:27",
    "content": "a chat and ask for “*something about wizards but not Harry Potter*.” What results do you get?\n\n---\n\nIf you’d like a detailed walkthrough of building this workflow step-by-step, watch the video below:\n\nThis recommendation scenario is easily adaptable to any language or data type (images, audio, video).\n\n### Big Data Analysis\n\nThe ability to map data to a vector space that reflects items’ similarity and dissimilarity relationships provides a range of mathematical tools for data analysis.\n\nVector search dedicated solutions are built to handle billions of data points and quickly compute distances between them, simplifying **clustering, classification, dissimilarity sampling, deduplication, interpolation**, and **anomaly detection at scale**.\n\nThe combination of this vector search feature with automation tools like n8n creates production-level solutions capable of monitoring data temporal shifts, managing data drift, and discovering patterns in seemingly unstructured data.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "token_count": 181,
      "char_count": 982,
      "start_char": 24489,
      "end_char": 25473
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:28",
    "content": "ata temporal shifts, managing data drift, and discovering patterns in seemingly unstructured data.\n\nA practical example is worth a thousand words. Let’s look at **Qdrant-based anomaly detection and classification tools**, which are designed to be used by the [n8n AI Agent node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent) for data analysis automation.\n\nTo make it more interesting, this time we’ll focus on image data.\n\n#### Anomaly Detection Tool\n\nOne definition of “anomaly” comes intuitively after projecting vector representations of data points into a 2D space—[Qdrant webUI](https://qdrant.tech/documentation/web-ui/) provides this functionality.\n\nPoints that don’t belong to any clusters are more likely to be anomalous.\n\nWith that intuition comes the recipe for building an anomaly detection tool. We will demonstrate it on anomaly detection in agricultural crops. Qdrant will be used to:\n\n1. Store vectorized images.\n2.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "token_count": 213,
      "char_count": 978,
      "start_char": 25373,
      "end_char": 26351
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:29",
    "content": "on anomaly detection in agricultural crops. Qdrant will be used to:\n\n1. Store vectorized images.\n2. Identify a “center” (representative) for each crop cluster.\n3. Define the borders of each cluster.\n4. Check if new images fall within these boundaries. If an image does not fit within any cluster, it is flagged as anomalous. Alternatively, you can check if an image is anomalous to a specific cluster.\n\n**Setup:**\n\n1. **Dataset**: We use the [Agricultural Crops Image Classification dataset](https://www.kaggle.com/datasets/mdwaquarazam/agricultural-crops-image-classification).\n2. **Embedding Model**: The [Voyage AI multimodal embedding model](https://docs.voyageai.com/docs/multimodal-embeddings). It can project images and text data into a shared vector space.\n\n**1. Uploading Images to Qdrant**\n\nSince the [Qdrant Vector Store node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "token_count": 233,
      "char_count": 924,
      "start_char": 26251,
      "end_char": 27176
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:30",
    "content": "r Store node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/) does not support embedding models outside the predefined list (which doesn’t include Voyage AI), we embed and upload data to Qdrant via direct API calls in [HTTP Request nodes](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/).\n\nWith the release of the [official Qdrant node](https://github.com/qdrant/n8n-nodes-qdrant), which supports arbitrary vectorized input, the HTTP Request node can now be replaced with this native integration.\n\n**Workflow:**\n\n*There are three workflows: (1) Uploading images to Qdrant (2) Setting up cluster centers and thresholds (3) Anomaly detection tool itself.*\n\nAn [1/3 Uploading Images to Qdrant Template Workflow](https://n8n.io/workflows/2654-vector-database-as-a-big-data-analysis-tool-for-ai-agents-13-anomaly12-knn/) consists of the following blocks:\n\n1.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 30,
      "token_count": 243,
      "char_count": 943,
      "start_char": 27076,
      "end_char": 28019
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:31",
    "content": "e-as-a-big-data-analysis-tool-for-ai-agents-13-anomaly12-knn/) consists of the following blocks:\n\n1. **Check Collection**: Verifies if a collection with the specified name exists in Qdrant. If not, it creates one.\n2. **Payload Index**: Adds a [payload index](https://qdrant.tech/documentation/concepts/indexing/#payload-index) on the `crop_name` payload (metadata) field. This field stores crop class labels, and indexing it improves the speed of filterable searches in Qdrant. It changes the way a vector index is constructed, adapting it for fast vector search under filtering constraints. For more details, refer to this [guide on filtering in Qdrant](https://qdrant.tech/articles/vector-search-filtering/).\n3. **Fetch Images**: Fetches images from Google Cloud Storage using the [Google Cloud Storage node](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.googlecloudstorage).\n4. **Generate IDs**: Assigns UUIDs to each data point.\n5. **Embed Images**: Embeds the images using the Voyage API.\n6.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 31,
      "token_count": 244,
      "char_count": 1016,
      "start_char": 27919,
      "end_char": 28935
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:32",
    "content": "*: Assigns UUIDs to each data point.\n5. **Embed Images**: Embeds the images using the Voyage API.\n6. **Batch Upload**: Uploads the embeddings to Qdrant in batches.\n\n**2. Defining a Cluster Representative**\n\nWe used two approaches (it’s not an exhaustive list) to defining a cluster representative, depending on the availability of labeled data:\n\n| Method                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 32,
      "token_count": 95,
      "char_count": 1024,
      "start_char": 28835,
      "end_char": 29859
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:33",
    "content": "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Medoids**                | A point within the cluster that has the smallest total distance to all other cluster points. This approach needs labeled data for each cluster.                                                                                                                                                                                                                                                                                                  |\n| **Perfect Representative** | A representative defined by a textual description of the ideal cluster member—the multimodality of Voyage AI embeddings allows for this trick.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 33,
      "token_count": 76,
      "char_count": 993,
      "start_char": 29759,
      "end_char": 30752
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:34",
    "content": "ription of the ideal cluster member—the multimodality of Voyage AI embeddings allows for this trick. For example, for cherries: *“Small, glossy red fruits on a medium-sized tree with slender branches and serrated leaves.”* The closest image to this description in the vector space is selected as the representative. This method requires experimentation to align descriptions with real data. |\n\n**Workflow:**\n\nBoth methods are demonstrated in the [2/3 Template Workflow for Anomaly Detection](https://n8n.io/workflows/2655-vector-database-as-a-big-data-analysis-tool-for-ai-agents-23-anomaly/).\n\n| **Method**                 | **Steps**",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 34,
      "token_count": 144,
      "char_count": 635,
      "start_char": 30652,
      "end_char": 31676
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:35",
    "content": "|\n| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 35,
      "token_count": 20,
      "char_count": 667,
      "start_char": 31576,
      "end_char": 32579
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:36",
    "content": "------------------------------------------------------------------------------------------------- |\n| **Medoids**                | 1. Sample labeled cluster points from Qdrant. 2. Compute a **pairwise distance matrix** for the cluster using Qdrant’s [Distance Matrix API](https://qdrant.tech/documentation/concepts/explore/?q=distance+#distance-matrix). This API helps with scalable cluster analysis and data points relationship exploration. Learn more in [this article](https://qdrant.tech/articles/distance-based-exploration/). 3. For each point, calculate the sum of its distances to all other points. The point with the smallest total distance (or highest similarity for COSINE distance metric) is the medoid. 4. Mark this point as the cluster representative. |\n| **Perfect Representative** | 1. Define textual descriptions for each cluster (e.g., AI-generated). 2. Embed these descriptions using Voyage. 3. Find the image embedding closest to the description one. 4. Mark this image as the cluster representative.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 36,
      "token_count": 208,
      "char_count": 1018,
      "start_char": 32479,
      "end_char": 33497
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:37",
    "content": "he image embedding closest to the description one. 4. Mark this image as the cluster representative.                                                                                                                                                                                                                                                                                                                                                                                                                            |\n\n**3. Defining the Cluster Border**\n\n**Workflow:**\n\nThe approach demonstrated in [2/3 Template Workflow for Anomaly Detection](https://n8n.io/workflows/2655-vector-database-as-a-big-data-analysis-tool-for-ai-agents-23-anomaly/) works similarly for both types of cluster representatives.\n\n1. Within a cluster, identify the furthest data point from the cluster representative (it can also be the 2nd or Xth furthest point; the best way to define it is through experimentation—for us, the 5th furthest point worke",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 37,
      "token_count": 148,
      "char_count": 1024,
      "start_char": 33397,
      "end_char": 34421
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:38",
    "content": "est point; the best way to define it is through experimentation—for us, the 5th furthest point worked well). Since we use COSINE similarity, this is equivalent to the most similar point to the [opposite](https://mathinsight.org/image/vector_opposite) of the cluster representative (its vector multiplied by -1).\n2. Save the distance between the representative and respective furthest point as the cluster border (threshold).\n\n**4. Anomaly Detection Tool**\n\n**Workflow:**\n\nWith the preparatory steps complete, you can set up the anomaly detection tool, demonstrated in the [3/3 Template Workflow for Anomaly Detection](https://n8n.io/workflows/2656-vector-database-as-a-big-data-analysis-tool-for-ai-agents-33-anomaly/).\n\nSteps:\n\n1. Choose the method of the cluster representative definition.\n2. Fetch all the clusters to compare the candidate image against.\n3. Using Voyage AI, embed the candidate image in the same vector space.\n4. Calculate the candidate’s similarity to each cluster representative.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 38,
      "token_count": 219,
      "char_count": 1001,
      "start_char": 34321,
      "end_char": 35322
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:39",
    "content": "ge in the same vector space.\n4. Calculate the candidate’s similarity to each cluster representative. The image is flagged as anomalous if the similarity is below the threshold for all clusters (outside the cluster borders). Alternatively, you can check if it’s anomalous to a particular cluster, for example, the cherries one.\n\n---\n\nAnomaly detection in image data has diverse applications, including:\n\n- Moderation of advertisements.\n- Anomaly detection in vertical farming.\n- Quality control in the food industry, such as [detecting anomalies in coffee beans](https://qdrant.tech/articles/detecting-coffee-anomalies/).\n- Identifying anomalies in map tiles for tasks like automated map updates or ecological monitoring.\n\nThis tool is easily adaptable to these use cases.\n\n#### Classification Tool\n\nThe anomaly detection tool can also be used for classification, but there’s a simpler approach: K-Nearest Neighbors (KNN) classification.\n\n> “Show me your friends, and I will tell you who you are.”",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 39,
      "token_count": 204,
      "char_count": 996,
      "start_char": 35222,
      "end_char": 36220
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:40",
    "content": "earest Neighbors (KNN) classification.\n\n> “Show me your friends, and I will tell you who you are.”\n\nThe KNN method labels a data point by analyzing its classified neighbors and assigning this point the majority class in the neighborhood. This approach doesn’t require all data points to be labeled—a subset of labeled examples can serve as anchors to propagate labels across the dataset.\n\nLet’s build a KNN-based image classification tool.\n\n**Setup**\n\n1. **Dataset**: We’ll use the [Land-Use Scene Classification dataset](https://www.kaggle.com/datasets/apollo2506/landuse-scene-classification). Satellite imagery analysis has applications in ecology, rescue operations, and map updates.\n2. **Embedding Model**: As for anomaly detection, we’ll use the [Voyage AI multimodal embedding model](https://docs.voyageai.com/docs/multimodal-embeddings).\n\nAdditionally, it’s good to have test and validation data to determine the optimal value of K for your dataset.\n\n**Workflow:**",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 40,
      "token_count": 215,
      "char_count": 972,
      "start_char": 36120,
      "end_char": 37094
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:41",
    "content": "have test and validation data to determine the optimal value of K for your dataset.\n\n**Workflow:**\n\nUploading images to Qdrant can be done using the same workflow—[1/3 Uploading Images to Qdrant Template Workflow](https://n8n.io/workflows/2654-vector-database-as-a-big-data-analysis-tool-for-ai-agents-13-anomaly12-knn/), just by swapping the dataset.\n\nThe [KNN-Classification Tool Template](https://n8n.io/workflows/2657-vector-database-as-a-big-data-analysis-tool-for-ai-agents-22-knn/) has the following steps:\n\n1. **Embed Image**: Embeds the candidate for classification using Voyage.\n2. **Fetch neighbors**: Retrieves the K closest labeled neighbors from Qdrant.\n3. **Majority Voting**: Determines the prevailing class in the neighborhood by simple majority voting.\n4. **Optional: Ties Resolving**: In case of ties, expands the neighborhood radius.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 41,
      "token_count": 211,
      "char_count": 853,
      "start_char": 36994,
      "end_char": 37849
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:42",
    "content": "ajority voting.\n4. **Optional: Ties Resolving**: In case of ties, expands the neighborhood radius.\n\nOf course, this is a simple solution, and there exist more advanced approaches with higher precision & no need for labeled data—for example, you could try [metric learning with Qdrant](https://qdrant.tech/articles/metric-learning-tips/).\n\nThough classification seems like a task that was solved in machine learning decades ago, it’s not so trivial to deal with in production. Issues like data drift, shifting class definitions, mislabeled data, and fuzzy differences between classes create unexpected problems, which require continuous adjustments of classifiers, and vector search can be an unusual but effective solution, due to its scalability.\n\n#### Live Walkthrough\n\nTo see how n8n agents use these tools in practice, and to revisit the main ideas of the “*Big Data Analysis*” section, watch our integration webinar:\n\n## Conclusion\n\nVector search is not limited to similarity search or basic RAG.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 42,
      "token_count": 204,
      "char_count": 1001,
      "start_char": 37749,
      "end_char": 38750
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:43",
    "content": "integration webinar:\n\n## Conclusion\n\nVector search is not limited to similarity search or basic RAG. When combined with automation platforms like n8n, it becomes a powerful tool for building smarter systems. Think dynamic routing in customer support, content moderation based on user behavior, or AI-driven alerts in data monitoring dashboards.\n\nThis tutorial showed how to use Qdrant and n8n for AI-backed recommendations, classification, and anomaly detection. But that’s just the start—try vector search for:\n\n- **Deduplication**\n- **Dissimilarity search**\n- **Diverse sampling**\n\nWith Qdrant and n8n, there’s plenty of room to create something unique!\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/qdrant-n8n.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 43,
      "token_count": 231,
      "char_count": 993,
      "start_char": 38650,
      "end_char": 39645
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:44",
    "content": "[create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Automating Processes with Qdrant and n8n beyond simple RAG](#automating-processes-with-qdrant-and-n8n-beyond-simple-rag.md)\n\n  - [Setting Up Qdrant in n8n](#setting-up-qdrant-in-n8n.md)\n\n    - [Qdrant Cloud](#qdrant-cloud.md)\n    - [Local Mode](#local-mode.md)\n\n  - [Qdrant + n8n Beyond Simple Similarity Search](#qdrant--n8n-beyond-simple-similarity-search.md)\n\n    - [Recommendations](#recommendations.md)\n    - [Big Data Analysis](#big-data-analysis.md)\n\n  - [Conclusion](#conclusion.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/qdrant-n8n.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 44,
      "token_count": 294,
      "char_count": 980,
      "start_char": 39545,
      "end_char": 40526
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md:chunk:45",
    "content": "5 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_qdrant-n8n\\_documentation_qdrant-n8n_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_qdrant-n8n_.md",
      "file_extension": ".md",
      "chunk_index": 45,
      "token_count": 51,
      "char_count": 177,
      "start_char": 40426,
      "end_char": 41450
    }
  }
]