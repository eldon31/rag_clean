[
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:0",
    "content": "Chat With Product PDF Manuals Using Hybrid Search - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 265,
      "char_count": 984,
      "start_char": 0,
      "end_char": 986
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:1",
    "content": "wai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 269,
      "char_count": 985,
      "start_char": 886,
      "end_char": 1871
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:2",
    "content": "gnee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 264,
      "char_count": 1014,
      "start_char": 1771,
      "end_char": 2786
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:3",
    "content": "ation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 271,
      "char_count": 1009,
      "start_char": 2686,
      "end_char": 3695
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:4",
    "content": "[AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 274,
      "char_count": 1005,
      "start_char": 3595,
      "end_char": 4600
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:5",
    "content": "angchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 272,
      "char_count": 995,
      "start_char": 4500,
      "end_char": 5496
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:6",
    "content": "entation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 271,
      "char_count": 1019,
      "start_char": 5396,
      "end_char": 6416
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:7",
    "content": "cumentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 262,
      "char_count": 997,
      "start_char": 6316,
      "end_char": 7313
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:8",
    "content": "tation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 260,
      "char_count": 985,
      "start_char": 7213,
      "end_char": 8199
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:9",
    "content": "Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 233,
      "char_count": 1009,
      "start_char": 8099,
      "end_char": 9109
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:10",
    "content": "ation Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 275,
      "char_count": 1020,
      "start_char": 9009,
      "end_char": 10029
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:11",
    "content": "qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 259,
      "char_count": 1012,
      "start_char": 9929,
      "end_char": 10941
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:12",
    "content": "ured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 270,
      "char_count": 1022,
      "start_char": 10841,
      "end_char": 11864
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:13",
    "content": "documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 265,
      "char_count": 1000,
      "start_char": 11764,
      "end_char": 12765
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:14",
    "content": "umentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 277,
      "char_count": 1017,
      "start_char": 12665,
      "end_char": 13683
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:15",
    "content": "frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "token_count": 274,
      "char_count": 1008,
      "start_char": 13583,
      "end_char": 14591
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:16",
    "content": "VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "token_count": 273,
      "char_count": 1014,
      "start_char": 14491,
      "end_char": 15506
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:17",
    "content": "ation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "token_count": 259,
      "char_count": 1013,
      "start_char": 15406,
      "end_char": 16419
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:18",
    "content": "-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 18,
      "token_count": 254,
      "char_count": 1005,
      "start_char": 16319,
      "end_char": 17324
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:19",
    "content": "r-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- [Examples](https://qdrant.tech/documentation/examples/)\n-\n- Chat With Product PDF Manuals Using Hybrid Search",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "token_count": 237,
      "char_count": 1017,
      "start_char": 17224,
      "end_char": 18243
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:20",
    "content": "https://qdrant.tech/documentation/examples/)\n-\n- Chat With Product PDF Manuals Using Hybrid Search\n\n# Chat With Product PDF Manuals Using Hybrid Search\n\n| Time: 120 min | Level: Advanced | Output: [GitHub](https://github.com/infoslack/qdrant-example/blob/main/HC-demo/HC-DO-LlamaIndex-Jina-v2.ipynb) | [](https://githubtocolab.com/infoslack/qdrant-example/blob/main/HC-demo/HC-DO-LlamaIndex-Jina-v2.ipynb) |\n| ------------- | --------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n\nWith the proliferation of digital manuals and the increasing demand for quick and accurate customer support, having a chatbot capable of efficiently parsing through complex PDF documents and delivering precise information can be a game-changer for any business.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "token_count": 173,
      "char_count": 925,
      "start_char": 18143,
      "end_char": 19070
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:21",
    "content": "h complex PDF documents and delivering precise information can be a game-changer for any business.\n\nIn this tutorial, we’ll walk you through the process of building a RAG-based chatbot, designed specifically to assist users with understanding the operation of various household appliances. We’ll cover the essential steps required to build your system, including data ingestion, natural language understanding, and response generation for customer support use cases.\n\n## Components\n\n- **Embeddings:** Jina Embeddings, served via the [Jina Embeddings API](https://jina.ai/embeddings/#apiform)\n- **Database:** [Qdrant Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/), deployed in a managed Kubernetes cluster on [DigitalOcean (DOKS)](https://www.digitalocean.com/products/kubernetes)\n- **LLM:** [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) language model on HuggingFace\n- **Framework:** [LlamaIndex](https://www.llamaindex.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "token_count": 235,
      "char_count": 980,
      "start_char": 18970,
      "end_char": 19950
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:22",
    "content": "B-Instruct-v0.1) language model on HuggingFace\n- **Framework:** [LlamaIndex](https://www.llamaindex.ai/) for extended RAG functionality and [Hybrid Search support](https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid/).\n- **Parser:** [LlamaParse](https://github.com/run-llama/llama_parse) as a way to parse complex documents with embedded objects such as tables and figures.\n\n### Procedure\n\nRetrieval Augmented Generation (RAG) combines search with language generation. An external information retrieval system is used to identify documents likely to provide information relevant to the user’s query. These documents, along with the user’s request, are then passed on to a text-generating language model, producing a natural response.\n\nThis method enables a language model to respond to questions and access information from a much larger set of documents than it could see otherwise.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "token_count": 198,
      "char_count": 905,
      "start_char": 19850,
      "end_char": 20755
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:23",
    "content": "to questions and access information from a much larger set of documents than it could see otherwise. The language model only looks at a few relevant sections of the documents when generating responses, which also helps to reduce inexplicable errors.\n\n##\n\n[Service Managed Kubernetes](https://www.ovhcloud.com/en-in/public-cloud/kubernetes/), powered by OVH Public Cloud Instances, a leading European cloud provider. With OVHcloud Load Balancers and disks built in. OVHcloud Managed Kubernetes provides high availability, compliance, and CNCF conformance, allowing you to focus on your containerized software layers with total reversibility.\n\n## Prerequisites\n\n### Deploying Qdrant Hybrid Cloud on DigitalOcean\n\n[DigitalOcean Kubernetes (DOKS)](https://www.digitalocean.com/products/kubernetes) is a managed Kubernetes service that lets you deploy Kubernetes clusters without the complexities of handling the control plane and containerized infrastructure.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "token_count": 182,
      "char_count": 955,
      "start_char": 20655,
      "end_char": 21610
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:24",
    "content": "es clusters without the complexities of handling the control plane and containerized infrastructure. Clusters are compatible with standard Kubernetes toolchains and integrate natively with DigitalOcean Load Balancers and volumes.\n\n1. To start using managed Kubernetes on DigitalOcean, follow the [platform-specific documentation](https://qdrant.tech/documentation/hybrid-cloud/platform-deployment-options/#digital-ocean).\n2. Once your Kubernetes clusters are up, [you can begin deploying Qdrant Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/).\n3. Once it’s deployed, you should have a running Qdrant cluster with an API key.\n\n### Development environment\n\nThen, install all dependencies:\n\n```python\n!pip install -U  \\\n    llama-index  \\\n    llama-parse \\\n    python-dotenv \\\n    llama-index-embeddings-jinaai  \\\n    llama-index-llms-huggingface  \\\n    llama-index-vector-stores-qdrant  \\\n    \"huggingface_hub[inference]\"  \\\n    datasets\n```\n\nSet up secret key values on `.env` file:\n\n```bash\nJINAAI_API_KEY",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "token_count": 231,
      "char_count": 1020,
      "start_char": 21510,
      "end_char": 22531
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:25",
    "content": "b[inference]\"  \\\n    datasets\n```\n\nSet up secret key values on `.env` file:\n\n```bash\nJINAAI_API_KEY\nHF_INFERENCE_API_KEY\nLLAMA_CLOUD_API_KEY\nQDRANT_HOST\nQDRANT_API_KEY\n```\n\nLoad all environment variables:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv('./.env')\n```\n\n## Implementation\n\n### Connect Jina Embeddings and Mixtral LLM\n\nLlamaIndex provides built-in support for the [Jina Embeddings API](https://jina.ai/embeddings/#apiform). To use it, you need to initialize the `JinaEmbedding` object with your API Key and model name.\n\nFor the LLM, you need wrap it in a subclass of `llama_index.llms.CustomLLM` to make it compatible with LlamaIndex.\n\n```python\n# connect embeddings\nfrom llama_index.embeddings.jinaai import JinaEmbedding\n\njina_embedding_model = JinaEmbedding(\n    model=\"jina-embeddings-v2-base-en\",\n    api_key=os.getenv(\"JINAAI_API_KEY\"),\n)\n\n# connect LLM\nfrom llama_index.llms.huggingface import HuggingFaceInferenceAPI\n\nmixtral_llm = HuggingFaceInferenceAPI(",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "token_count": 276,
      "char_count": 993,
      "start_char": 22431,
      "end_char": 23425
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:26",
    "content": "llama_index.llms.huggingface import HuggingFaceInferenceAPI\n\nmixtral_llm = HuggingFaceInferenceAPI(\n    model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    token=os.getenv(\"HF_INFERENCE_API_KEY\"),\n)\n```\n\n### Prepare data for RAG\n\nThis example will use household appliance manuals, which are generally available as PDF documents. LlamaPar In the `data` folder, we have three documents, and we will use it to extract the textual content from the PDF and use it as a knowledge base in a simple RAG.\n\nThe free LlamaIndex Cloud plan is sufficient for our example:\n\n```python\nimport nest_asyncio\nnest_asyncio.apply()\nfrom llama_parse import LlamaParse\n\nllamaparse_api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n\nllama_parse_documents = LlamaParse(api_key=llamaparse_api_key, result_type=\"markdown\").load_data([\n    \"data/DJ68-00682F_0.0.pdf\", \n    \"data/F500E_WF80F5E_03445F_EN.pdf\", \n    \"data/O_ME4000R_ME19R7041FS_AA_EN.pdf\"\n])\n```\n\n### Store data into Qdrant\n\nThe code below does the following:",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "token_count": 296,
      "char_count": 993,
      "start_char": 23325,
      "end_char": 24320
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:27",
    "content": "000R_ME19R7041FS_AA_EN.pdf\"\n])\n```\n\n### Store data into Qdrant\n\nThe code below does the following:\n\n- create a vector store with Qdrant client;\n- get an embedding for each chunk using Jina Embeddings API;\n- combines `sparse` and `dense` vectors for hybrid search;\n- stores all data into Qdrant;\n\nHybrid search with Qdrant must be enabled from the beginning - we can simply set `enable_hybrid=True`.\n\n```python\n# By default llamaindex uses OpenAI models\n# setting embed_model to Jina and llm model to Mixtral\nfrom llama_index.core import Settings\nSettings.embed_model = jina_embedding_model\nSettings.llm = mixtral_llm\n\nfrom llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nimport qdrant_client\n\nclient = qdrant_client.QdrantClient(\n    url=os.getenv(\"QDRANT_HOST\"),\n    api_key=os.getenv(\"QDRANT_API_KEY\")\n)\n\nvector_store = QdrantVectorStore(\n    client=client, collection_name=\"demo\", enable_hybrid=True, batch_size=20\n)\nSettings.chunk_size = 512",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "token_count": 271,
      "char_count": 1017,
      "start_char": 24220,
      "end_char": 25239
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:28",
    "content": "ient=client, collection_name=\"demo\", enable_hybrid=True, batch_size=20\n)\nSettings.chunk_size = 512\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(\n    documents=llama_parse_documents, \n    storage_context=storage_context\n)\n```\n\n### Prepare a prompt\n\nHere we will create a custom prompt template. This prompt asks the LLM to use only the context information retrieved from Qdrant. When querying with hybrid mode, we can set `similarity_top_k` and `sparse_top_k` separately:\n\n- `sparse_top_k` represents how many nodes will be retrieved from each dense and sparse query.\n- `similarity_top_k` controls the final number of returned nodes. In the above setting, we end up with 10 nodes.\n\nThen, we assemble the query engine using the prompt.\n\n```python\nfrom llama_index.core import PromptTemplate\n\nqa_prompt_tmpl = (\n    \"Context information is below.\\n\"\n    \"-------------------------------\"\n    \"{context_str}\\n\"\n    \"-------------------------------\"",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "token_count": 228,
      "char_count": 1014,
      "start_char": 25139,
      "end_char": 26154
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:29",
    "content": "\"\n    \"-------------------------------\"\n    \"{context_str}\\n\"\n    \"-------------------------------\"\n    \"Given the context information and not prior knowledge,\"\n    \"answer the query. Please be concise, and complete.\\n\"\n    \"If the context does not contain an answer to the query,\"\n    \"respond with \\\"I don't know!\\\".\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_prompt = PromptTemplate(qa_prompt_tmpl)\n\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core import get_response_synthesizer\nfrom llama_index.core import Settings\nSettings.embed_model = jina_embedding_model\nSettings.llm = mixtral_llm\n\n# retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=2,\n    sparse_top_k=12,\n    vector_store_query_mode=\"hybrid\"\n)\n\n# response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n    llm=mixtral_llm,\n    text_qa_template=qa_prompt,\n    response_mode=\"compact\",\n)\n\n# query engine",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "token_count": 250,
      "char_count": 1015,
      "start_char": 26054,
      "end_char": 27070
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:30",
    "content": "llm=mixtral_llm,\n    text_qa_template=qa_prompt,\n    response_mode=\"compact\",\n)\n\n# query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n```\n\n## Run a test query\n\nNow you can ask questions and receive answers based on the data:\n\n**Question**\n\n```python\nresult = query_engine.query(\"What temperature should I use for my laundry?\")\nprint(result.response)\n```\n\n**Answer**\n\n```text\nThe water temperature is set to 70 ˚C during the Eco Drum Clean cycle. You cannot change the water temperature. However, the temperature for other cycles is not specified in the context.\n```\n\nAnd that’s it! Feel free to scale this up to as many documents and complex PDFs as you like.\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/examples/hybrid-search-llamaindex-jinaai.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 30,
      "token_count": 241,
      "char_count": 975,
      "start_char": 26970,
      "end_char": 27949
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:31",
    "content": "ding_page/tree/master/qdrant-landing/content/documentation/examples/hybrid-search-llamaindex-jinaai.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Chat With Product PDF Manuals Using Hybrid Search](#chat-with-product-pdf-manuals-using-hybrid-search.md)\n\n  - [Components](#components.md)\n    - [Procedure](#procedure.md)\n\n  - [](#heading.md)\n\n  - [Prerequisites](#prerequisites.md)\n\n    - [Deploying Qdrant Hybrid Cloud on DigitalOcean](#deploying-qdrant-hybrid-cloud-on-digitalocean.md)\n    - [Development environment](#development-environment.md)\n\n  - [Implementation](#implementation.md)\n\n    - [Connect Jina Embeddings and Mixtral LLM](#connect-jina-embeddings-and-mixtral-llm.md)\n    - [Prepare data for RAG](#prepare-data-for-rag.md)\n    - [Store data into Qdrant](#store-data-into-qdrant.md)\n    - [Prepare a prompt](#prepare-a-prompt.md)\n\n  - [Run a test query](#run-a-test-query.md)\n\n* [Edit on Github](https://github.",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 31,
      "token_count": 277,
      "char_count": 1009,
      "start_char": 27849,
      "end_char": 28858
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md:chunk:32",
    "content": "epare-a-prompt.md)\n\n  - [Run a test query](#run-a-test-query.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/examples/hybrid-search-llamaindex-jinaai.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
    "metadata": {
      "source_file": "qdrant_documentation\\documentation_examples_hybrid-search-llamaindex-jinaai\\_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_documentation",
      "filename": "_documentation_examples_hybrid-search-llamaindex-jinaai_.md",
      "file_extension": ".md",
      "chunk_index": 32,
      "token_count": 161,
      "char_count": 559,
      "start_char": 28758,
      "end_char": 29782
    }
  }
]