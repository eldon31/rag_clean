[
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:0",
    "content": "Supported Models | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 311,
      "char_count": 1013,
      "start_char": 0,
      "end_char": 1014
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:1",
    "content": "teinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 298,
      "char_count": 994,
      "start_char": 914,
      "end_char": 1909
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:2",
    "content": "c-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Supported Models\n\nRelevant source files\n\n- [NOTICE](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE)\n- [docs/examples/Supported\\_Models.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb)\n- [fastembed/late\\_interaction\\_multimodal/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/__init__.py)\n\nThis page provides a comprehensive list of all embedding models supported by FastEmbed, organized by model type.",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 284,
      "char_count": 987,
      "start_char": 1809,
      "end_char": 2796
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:3",
    "content": "ovides a comprehensive list of all embedding models supported by FastEmbed, organized by model type. FastEmbed offers a diverse range of pre-trained models for various embedding tasks, including dense text embedding, sparse text embedding, late interaction models, image embedding, and cross-encoder reranking.\n\n## Model Types Overview\n\nFastEmbed supports five main types of embedding models, each with specific use cases and implementations.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb35-45](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L35-L45)\n\n## Dense Text Embedding Models\n\nDense text embedding models generate fixed-length vector representations of text, capturing semantic meaning. These models are typically used for semantic search and measuring text similarity.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb64-72](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L64-L72)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 231,
      "char_count": 988,
      "start_char": 2696,
      "end_char": 3686
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:4",
    "content": "2](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L64-L72)\n\nFastEmbed supports a wide range of dense text embedding models, from small, efficient models to large, high-performance ones:\n\n| Model Size | Example Models                                                                       | Dimension | Size (GB)  | Primary Use                                          |\n| ---------- | ------------------------------------------------------------------------------------ | --------- | ---------- | ---------------------------------------------------- |\n| Small      | BAAI/bge-small-en-v1.5, jinaai/jina-embeddings-v2-small-en                           | 384-512   | 0.067-0.13 | Efficient semantic search with low resource usage    |\n| Medium     | BAAI/bge-base-en, nomic-ai/nomic-embed-text-v1.5, snowflake/snowflake-arctic-embed-m | 768       | 0.21-0.54  | Balanced performance and resource usage              |\n| Large      | BAAI/bge-large-en-v1.",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 231,
      "char_count": 991,
      "start_char": 3586,
      "end_char": 4577
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:5",
    "content": "-0.54  | Balanced performance and resource usage              |\n| Large      | BAAI/bge-large-en-v1.5, thenlper/gte-large, intfloat/multilingual-e5-large           | 1024      | 1.0-2.24   | Highest quality embeddings for critical applications |\n\nThe full list includes 25 dense text embedding models with diverse characteristics:\n\n- BGE models (small/base/large variants)\n- Snowflake Arctic models (xs/s/m/l variants)\n- Sentence Transformers models\n- Jina AI embedding models\n- Nomic AI embedding models\n- CLIP text models\n- Multilingual models (Chinese, German, etc.)\n\nThese models can be accessed through the `TextEmbedding` class using the `model_name` parameter:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb64-129](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L64-L129)\n\n## Sparse Text Embedding Models\n\nSparse embedding models generate high-dimensional, sparse vector representations where most values are zero.",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 249,
      "char_count": 972,
      "start_char": 4477,
      "end_char": 5449
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:6",
    "content": "mbedding models generate high-dimensional, sparse vector representations where most values are zero. These models excel at lexical matching and are often used in hybrid search systems alongside dense embeddings.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb386-392](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L386-L392)\n\nFastEmbed supports the following sparse text embedding models:\n\n| Model                                   | Vocab Size | Description                                          | Size (GB) | Requires IDF |\n| --------------------------------------- | ---------- | ---------------------------------------------------- | --------- | ------------ |\n| Qdrant/bm25                             | -          | BM25 as sparse embeddings                            | 0.01      | Yes          |\n| Qdrant/bm42-all-minilm-l6-v2-attentions | 30,522     | Light sparse embedding model using attention weights | 0.09      | Yes          |",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 215,
      "char_count": 996,
      "start_char": 5349,
      "end_char": 6346
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:7",
    "content": "ns | 30,522     | Light sparse embedding model using attention weights | 0.09      | Yes          |\n| prithivida/Splade\\_PP\\_en\\_v1           | 30,522     | SPLADE++ Model for sparse retrieval                  | 0.532     | No           |\n\nThese models can be accessed through the `SparseTextEmbedding` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb386-478](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L386-L478)\n\n## Late Interaction Text Embedding Models\n\nLate interaction models defer the computation of relevance scores until query time, preserving token-level representations instead of pooling them into a single vector. This approach provides higher precision at the cost of more complex retrieval.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb510-516](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L510-L516)\n\nFastEmbed supports the following late interaction text embedding models:",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 270,
      "char_count": 1003,
      "start_char": 6246,
      "end_char": 7251
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:8",
    "content": "_Models.ipynb#L510-L516)\n\nFastEmbed supports the following late interaction text embedding models:\n\n| Model                                 | Dimension | Description                         | License      | Size (GB) |\n| ------------------------------------- | --------- | ----------------------------------- | ------------ | --------- |\n| answerdotai/answerai-colbert-small-v1 | 96        | Multilingual late interaction model | apache-2.0   | 0.13      |\n| colbert-ir/colbertv2.0                | 128       | Late interaction model              | mit          | 0.44      |\n| jinaai/jina-colbert-v2                | 128       | Enhanced ColBERT capabilities       | cc-by-nc-4.0 | 2.24      |\n\nThese models can be accessed through the `LateInteractionTextEmbedding` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb510-590](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L510-L590)\n\n## Image Embedding Models",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 252,
      "char_count": 965,
      "start_char": 7151,
      "end_char": 8118
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:9",
    "content": "fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L510-L590)\n\n## Image Embedding Models\n\nImage embedding models generate vector representations from images, enabling visual similarity search and multimodal applications.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb622-628](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L622-L628)\n\nFastEmbed supports the following image embedding models:\n\n| Model                       | Dimension | Description                     | License    | Size (GB) |\n| --------------------------- | --------- | ------------------------------- | ---------- | --------- |\n| Qdrant/resnet50-onnx        | 2048      | Image embeddings (2016)         | apache-2.0 | 0.10      |\n| Qdrant/clip-ViT-B-32-vision | 512       | CLIP vision embeddings          | mit        | 0.34      |\n| Qdrant/Unicom-ViT-B-32      | 512       | Unicom image embeddings         | apache-2.0 | 0.48      |",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 273,
      "char_count": 968,
      "start_char": 8018,
      "end_char": 8987
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:10",
    "content": "drant/Unicom-ViT-B-32      | 512       | Unicom image embeddings         | apache-2.0 | 0.48      |\n| Qdrant/Unicom-ViT-B-16      | 768       | Higher detail Unicom embeddings | apache-2.0 | 0.82      |\n\nThese models can be accessed through the `ImageEmbedding` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb622-704](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L622-L704)\n\n## Multimodal Late Interaction Models\n\nFastEmbed also supports multimodal late interaction models, such as ColPali, which enable matching between text and images using token-level representations.\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/\\_\\_init\\_\\_.py1-6](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/__init__.py#L1-L6) [NOTICE16-18](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L16-L18)\n\nCurrently, FastEmbed supports the following multimodal late interaction model:",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 305,
      "char_count": 977,
      "start_char": 8887,
      "end_char": 9866
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:11",
    "content": "0b/NOTICE#L16-L18)\n\nCurrently, FastEmbed supports the following multimodal late interaction model:\n\n| Model               | Description                                               | License |\n| ------------------- | --------------------------------------------------------- | ------- |\n| vidore/colpali-v1.3 | Multimodal late interaction model for text-image matching | gemma   |\n\nThis model can be accessed through the `LateInteractionMultimodalEmbedding` class.\n\nSources: [NOTICE16-19](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L16-L19)\n\n## Cross-Encoder Models for Reranking\n\nCross-encoder models evaluate text pairs together rather than encoding them separately, providing more accurate relevance scores for reranking search results.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb732-738](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L732-L738)\n\nFastEmbed supports the following cross-encoder models for reranking:",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 238,
      "char_count": 988,
      "start_char": 9766,
      "end_char": 10756
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:12",
    "content": "rted_Models.ipynb#L732-L738)\n\nFastEmbed supports the following cross-encoder models for reranking:\n\n| Model                                     | Description                   | License      | Size (GB) |\n| ----------------------------------------- | ----------------------------- | ------------ | --------- |\n| Xenova/ms-marco-MiniLM-L-6-v2             | MiniLM-L-6-v2 for reranking   | apache-2.0   | 0.08      |\n| Xenova/ms-marco-MiniLM-L-12-v2            | MiniLM-L-12-v2 for reranking  | apache-2.0   | 0.12      |\n| jinaai/jina-reranker-v1-tiny-en           | Fast reranker with 8K context | apache-2.0   | 0.13      |\n| jinaai/jina-reranker-v1-turbo-en          | Fast reranker with 8K context | apache-2.0   | 0.15      |\n| BAAI/bge-reranker-base                    | BGE reranker base model       | mit          | 1.04      |\n| jinaai/jina-reranker-v2-base-multilingual | Multilingual reranker         | cc-by-nc-4.0 | 1.11      |\n\nThese models can be accessed through the `TextCrossEncoder` class:\n\n```\n```",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 306,
      "char_count": 1016,
      "start_char": 10656,
      "end_char": 11674
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:13",
    "content": "-nc-4.0 | 1.11      |\n\nThese models can be accessed through the `TextCrossEncoder` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb732-826](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L732-L826)\n\n## Model Selection Guide\n\nWhen choosing a model for your application, consider these factors:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb)\n\n## License Considerations\n\nWhen using FastEmbed models, be aware of licensing restrictions:\n\n- Most models are available under permissive licenses (MIT, Apache-2.0)\n\n- Some Jina AI models have non-commercial licenses (cc-by-nc-4.0):\n\n  - jinaai/jina-colbert-v2\n  - jinaai/jina-reranker-v2-base-multilingual\n  - jinaai/jina-embeddings-v3\n\n- The ColPali model (vidore/colpali-v1.3) is subject to the Gemma Terms of Use",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 275,
      "char_count": 913,
      "start_char": 11574,
      "end_char": 12489
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:14",
    "content": "jina-embeddings-v3\n\n- The ColPali model (vidore/colpali-v1.3) is subject to the Gemma Terms of Use\n\nAlways check the license information before using a model in production applications, especially for commercial use.\n\nSources: [NOTICE1-23](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L1-L23)\n\nFor more detailed information about using these models with FastEmbed, see the [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md) and [Usage Examples](qdrant/fastembed/7-usage-examples.md) sections.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Supported Models](#supported-models.md)\n- [Model Types Overview](#model-types-overview.md)\n- [Dense Text Embedding Models](#dense-text-embedding-models.md)\n- [Sparse Text Embedding Models](#sparse-text-embedding-models.md)\n- [Late Interaction Text Embedding Models](#late-interaction-text-embedding-models.md)\n- [Image Embedding Models](#image-embedding-models.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 256,
      "char_count": 963,
      "start_char": 12389,
      "end_char": 13353
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md:chunk:15",
    "content": "(#late-interaction-text-embedding-models.md)\n- [Image Embedding Models](#image-embedding-models.md)\n- [Multimodal Late Interaction Models](#multimodal-late-interaction-models.md)\n- [Cross-Encoder Models for Reranking](#cross-encoder-models-for-reranking.md)\n- [Model Selection Guide](#model-selection-guide.md)\n- [License Considerations](#license-considerations.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed_6-supported-models.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "token_count": 95,
      "char_count": 365,
      "start_char": 13253,
      "end_char": 14277
    }
  }
]