[
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:0",
    "content": "qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 308,
      "char_count": 994,
      "start_char": 0,
      "end_char": 995
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:1",
    "content": "teinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 298,
      "char_count": 994,
      "start_char": 895,
      "end_char": 1890
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:2",
    "content": "c-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Overview\n\nRelevant source files\n\n- [.pre-commit-config.yaml](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml)\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [fastembed/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py)\n- [pyproject.toml](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 311,
      "char_count": 995,
      "start_char": 1790,
      "end_char": 2786
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:3",
    "content": "d/__init__.py)\n- [pyproject.toml](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml)\n- [tests/test\\_late\\_interaction\\_multimodal.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py)\n\nFastEmbed is a lightweight, fast Python library designed for generating high-quality embeddings from text and images. It focuses on performance optimization through ONNX Runtime integration, providing a more efficient alternative to traditional embedding libraries like PyTorch-based Sentence Transformers.\n\nThis overview introduces the core concepts, architecture, and components of FastEmbed. For installation instructions, see [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md).\n\nSources: [README.md1-14](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L1-L14) [pyproject.toml1-12](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L12)\n\n## Core Features\n\nFastEmbed offers several key advantages over other embedding libraries:",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 256,
      "char_count": 1023,
      "start_char": 2686,
      "end_char": 3711
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:4",
    "content": "L1-L12)\n\n## Core Features\n\nFastEmbed offers several key advantages over other embedding libraries:\n\n1. **Lightweight**: Minimal external dependencies, making it suitable for resource-constrained environments like serverless functions\n\n2. **Fast**: ONNX Runtime integration and data parallelism for efficient embedding generation, providing significant performance gains over PyTorch-based alternatives\n\n3. **Accurate**: Support for state-of-the-art embedding models that deliver performance comparable to or better than commercial options like OpenAI's Ada-002\n\n4. **Versatile**: Support for multiple embedding strategies including dense, sparse, late interaction, and multimodal approaches\n\n5. **GPU Acceleration**: Optional GPU support through the `fastembed-gpu` package\n\nSources: [README.md7-14](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L7-L14) [pyproject.toml13-34](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L13-L34) [docs/examples/FastEmbed\\_GPU.ipynb9-21](https://github.",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 250,
      "char_count": 1020,
      "start_char": 3611,
      "end_char": 4631
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:5",
    "content": "embed/blob/b785640b/pyproject.toml#L13-L34) [docs/examples/FastEmbed\\_GPU.ipynb9-21](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L9-L21)\n\n## System Architecture\n\nFastEmbed is organized around a modular architecture with specialized classes for different embedding approaches and modalities.\n\n### Core Components Diagram\n\n```\n```\n\nSources: [fastembed/\\_\\_init\\_\\_.py1-22](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py#L1-L22)\n\n### Embedding Process Flow\n\n```\n```\n\nSources: [README.md28-190](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L190)\n\n## Core Components\n\nFastEmbed provides specialized classes for different embedding approaches, each optimized for specific use cases:\n\n| Component                            | Description                                                | Primary Use Cases                             |",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 238,
      "char_count": 911,
      "start_char": 4531,
      "end_char": 5443
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:6",
    "content": "on                                                | Primary Use Cases                             |\n| ------------------------------------ | ---------------------------------------------------------- | --------------------------------------------- |\n| `TextEmbedding`                      | Dense text embeddings, supports various pooling strategies | Semantic search, document similarity          |\n| `SparseTextEmbedding`                | Sparse text embeddings (SPLADE, BM25, BM42)                | Hybrid search, traditional search integration |\n| `LateInteractionTextEmbedding`       | Token-level embeddings (ColBERT)                           | Advanced retrieval with token matching        |\n| `ImageEmbedding`                     | CLIP and similar image embeddings                          | Image search, visual similarity               |\n| `LateInteractionMultimodalEmbedding` | Multimodal token-level embeddings (ColPali)                | Document image search, multimodal retrieval   |",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 158,
      "char_count": 999,
      "start_char": 5343,
      "end_char": 6343
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:7",
    "content": "l token-level embeddings (ColPali)                | Document image search, multimodal retrieval   |\n| `TextCrossEncoder`                   | Text pair scoring for reranking                            | Search result refinement, question-answering  |\n\nSources: [README.md49-190](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L49-L190) [fastembed/\\_\\_init\\_\\_.py3-22](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py#L3-L22)\n\n### Text Embeddings\n\nThe `TextEmbedding` class is the most commonly used component, providing dense vector representations for text:\n\n```\n```\n\nKey features:\n\n- Default model is \"BAAI/bge-small-en-v1.5\", a performant English embedding model\n- Automatic model downloading and caching\n- Parallel processing for large batches of documents\n- Optional GPU acceleration with the `fastembed-gpu` package\n\nSources: [README.md28-47](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L47)\n\n### Sparse Text Embeddings",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 269,
      "char_count": 980,
      "start_char": 6243,
      "end_char": 7225
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:8",
    "content": "](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L47)\n\n### Sparse Text Embeddings\n\nThe `SparseTextEmbedding` class provides sparse vector representations:\n\n```\n```\n\nThese sparse embeddings are particularly useful for hybrid search approaches that combine traditional term-based retrieval with semantic search.\n\nSources: [README.md87-99](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L87-L99)\n\n### Late Interaction Models\n\nThe `LateInteractionTextEmbedding` class implements ColBERT-style embeddings with token-level representations:\n\n```\n```\n\nThese models produce a matrix of embeddings per document (one vector per token), enabling more sophisticated matching during retrieval.\n\nSources: [README.md119-136](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L119-L136)\n\n### Image Embeddings\n\nThe `ImageEmbedding` class provides embedding generation for images:\n\n```\n```\n\nThis class supports both file paths and PIL Image objects as input.\n\nSources: [README.md140-154](https://github.",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 259,
      "char_count": 1024,
      "start_char": 7125,
      "end_char": 8149
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:9",
    "content": "upports both file paths and PIL Image objects as input.\n\nSources: [README.md140-154](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L140-L154)\n\n### Multimodal Embeddings\n\nThe `LateInteractionMultimodalEmbedding` class enables token-level embeddings for both text and images:\n\n```\n```\n\nThis allows for sophisticated cross-modal retrieval between text queries and document images.\n\nSources: [README.md158-176](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L158-L176) [tests/test\\_late\\_interaction\\_multimodal.py1-83](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py#L1-L83)\n\n### Text Cross Encoder for Reranking\n\nThe `TextCrossEncoder` class provides text pair scoring for reranking search results:\n\n```\n```\n\nThis is useful for refining search results after initial retrieval.\n\nSources: [README.md180-207](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L180-L207)\n\n## Performance Optimization",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 286,
      "char_count": 974,
      "start_char": 8049,
      "end_char": 9025
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:10",
    "content": "ttps://github.com/qdrant/fastembed/blob/b785640b/README.md#L180-L207)\n\n## Performance Optimization\n\nFastEmbed focuses on performance through several key optimizations:\n\n1. **ONNX Runtime**: Uses ONNX models for efficient inference without requiring PyTorch/TensorFlow\n2. **Parallel Processing**: Automatically distributes embedding generation across CPU cores\n3. **GPU Acceleration**: Optional GPU support through `fastembed-gpu` package\n4. **Model Caching**: Automatic downloading and caching of models\n5. **Batching**: Efficient batching of inputs for optimized throughput\n\nA simple benchmark comparing CPU vs GPU performance shows orders of magnitude improvement:\n\n```\nCPU execution time: 4.33s (500 documents)\nGPU execution time: 43.4ms (500 documents)\n```\n\nSources: [docs/examples/FastEmbed\\_GPU.ipynb390-511](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L390-L511)\n\n## Integration with Qdrant",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 245,
      "char_count": 937,
      "start_char": 8925,
      "end_char": 9864
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:11",
    "content": "t/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L390-L511)\n\n## Integration with Qdrant\n\nFastEmbed is maintained by Qdrant and has native integration with the Qdrant vector database:\n\n```\n```\n\nFor more details on using FastEmbed with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nSources: [README.md232-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L232-L281)\n\n## Supported Models\n\nFastEmbed supports a wide range of embedding models:\n\n1. **Dense Text Models**: BGE embeddings, Sentence Transformers, CLIP text models\n2. **Sparse Text Models**: SPLADE, BM25, BM42\n3. **Late Interaction Models**: ColBERT, Jina ColBERT\n4. **Image Models**: CLIP vision models\n5. **Multimodal Models**: ColPali\n\nFor a complete list of supported models and their configuration details, see [Supported Models](qdrant/fastembed/6-supported-models.md).\n\nThe library also supports extending with custom models through API methods like `TextEmbedding.add_custom_model()`.",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 281,
      "char_count": 1017,
      "start_char": 9764,
      "end_char": 10783
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:12",
    "content": "supports extending with custom models through API methods like `TextEmbedding.add_custom_model()`.\n\nSources: [README.md66-82](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L66-L82) [README.md196-207](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L196-L207)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Overview](#overview.md)\n- [Core Features](#core-features.md)\n- [System Architecture](#system-architecture.md)\n- [Core Components Diagram](#core-components-diagram.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Core Components](#core-components.md)\n- [Text Embeddings](#text-embeddings.md)\n- [Sparse Text Embeddings](#sparse-text-embeddings.md)\n- [Late Interaction Models](#late-interaction-models.md)\n- [Image Embeddings](#image-embeddings.md)\n- [Multimodal Embeddings](#multimodal-embeddings.md)\n- [Text Cross Encoder for Reranking](#text-cross-encoder-for-reranking.md)\n- [Performance Optimization](#performance-optimization.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 287,
      "char_count": 1004,
      "start_char": 10683,
      "end_char": 11688
    }
  },
  {
    "chunk_id": "qdrant_ecosystem:qdrant_fastembed\\_qdrant_fastembed.md:chunk:13",
    "content": "g](#text-cross-encoder-for-reranking.md)\n- [Performance Optimization](#performance-optimization.md)\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Supported Models](#supported-models.md)",
    "metadata": {
      "source_file": "qdrant_fastembed\\_qdrant_fastembed.md",
      "source_collection": "qdrant_ecosystem",
      "subdirectory": "qdrant_fastembed",
      "filename": "_qdrant_fastembed.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 52,
      "char_count": 199,
      "start_char": 11588,
      "end_char": 12612
    }
  }
]