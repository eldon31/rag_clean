[
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:0",
    "content": "# Vision Language Models\n\n\nRelevant source files\n\n- [README.md](https://github.com/docling-project/docling/blob/f7244a43/README.md)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docs/examples/minimal\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py)\n- [docs/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/index.md)\n- [docs/usage/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/index.md)\n- [docs/usage/mcp.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/mcp.md)\n- [docs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Vision Language Models",
      "heading_level": 1,
      "chunk_index": 0,
      "collection": "docling",
      "char_count": 1958,
      "estimated_tokens": 489,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:1",
    "content": "ocs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)\n\nVision Language Models (VLMs) in Docling enable end-to-end document understanding by processing document page images directly through multimodal AI models. Unlike the traditional pipeline approach that uses specialized models for layout, tables, and OCR, VLMs can perform document analysis in a single inference pass, generating structured output formats like DOCTAGS, Markdown, or HTML.\n\nThis page provides an overview of VLM integration in Docling, covering available model variants, response formats, and configuration options. For detailed implementation of inline VLM models (Transformers, MLX, vLLM), see [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md). For API-based VLM integration, see [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md). For pipeline-level VLM usage, see [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md).\n\nSources: [docling/models/base\\_model.py46-66](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L66) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py13-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L13-L32) [docs/usage/vision\\_models.md1-10](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L1-L10)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Vision Language Models",
      "heading_level": 1,
      "chunk_index": 1,
      "collection": "docling",
      "char_count": 1534,
      "estimated_tokens": 383,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:2",
    "content": "## VLM Integration Architecture\n\n\nDocling provides a unified interface for VLM integration supporting both local model execution and external API services. The architecture separates model deployment strategy from the VLM capabilities exposed to pipelines.\n\n**Diagram: VLM Integration Architecture**\n\n```\n```\n\nThe architecture provides two key abstractions:\n\n- **`BaseVlmPageModel`**: Defines the interface for page-level VLM processing, requiring implementations to provide `__call__(conv_res, page_batch)` and `process_images(image_batch, prompt)` methods\n- **`BaseVlmOptions`**: Provides configuration for VLM behavior including prompts, scaling, temperature, and response format handling\n\nSources: [docling/models/base\\_model.py46-127](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L127) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py13-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L13-L32) [docling/pipeline/vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "VLM Integration Architecture",
      "heading_level": 2,
      "chunk_index": 2,
      "collection": "docling",
      "char_count": 1134,
      "estimated_tokens": 283,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:3",
    "content": "## Available VLM Model Variants\n\n\nDocling provides pre-configured specifications for popular VLM models, optimized for document understanding tasks. These are defined in `vlm_model_specs` and can be used directly or customized.\n\n**Diagram: VLM Model Variants and Frameworks**\n\n```\n```",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Available VLM Model Variants",
      "heading_level": 2,
      "chunk_index": 3,
      "collection": "docling",
      "char_count": 284,
      "estimated_tokens": 71,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:4",
    "content": "### GraniteDocling Models\n\n\nGraniteDocling is a specialized 258M parameter model trained for document understanding that outputs structured DOCTAGS format. It represents the recommended choice for document conversion in Docling.\n\n| Variant                          | Repo ID                                | Framework    | Devices   | Notes                          |\n| -------------------------------- | -------------------------------------- | ------------ | --------- | ------------------------------ |\n| **GRANITEDOCLING\\_TRANSFORMERS** | `ibm-granite/granite-docling-258M`     | Transformers | CPU, CUDA | Default for non-Apple hardware |\n| **GRANITEDOCLING\\_MLX**          | `ibm-granite/granite-docling-258M-mlx` | MLX          | MPS       | Optimized for Apple Silicon    |\n| **GRANITEDOCLING\\_VLLM**         | `ibm-granite/granite-docling-258M`     | vLLM         | CUDA      | High-throughput inference      |\n\nConfiguration example:\n\n```\n```\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py21-56](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L21-L56) [docs/usage/vision\\_models.md40-87](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L40-L87)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "GraniteDocling Models",
      "heading_level": 3,
      "chunk_index": 4,
      "collection": "docling",
      "char_count": 1241,
      "estimated_tokens": 310,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:5",
    "content": "### SmolDocling Models\n\n\nSmolDocling is another 256M parameter model designed for document understanding with DOCTAGS output. It provides an alternative to GraniteDocling with similar capabilities.\n\n| Variant                       | Repo ID                                   | Framework    | Devices   |\n| ----------------------------- | ----------------------------------------- | ------------ | --------- |\n| **SMOLDOCLING\\_TRANSFORMERS** | `ds4sd/SmolDocling-256M-preview`          | Transformers | CPU, CUDA |\n| **SMOLDOCLING\\_MLX**          | `ds4sd/SmolDocling-256M-preview-mlx-bf16` | MLX          | MPS       |\n| **SMOLDOCLING\\_VLLM**         | `ds4sd/SmolDocling-256M-preview`          | vLLM         | CUDA      |\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py58-97](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L58-L97)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "SmolDocling Models",
      "heading_level": 3,
      "chunk_index": 5,
      "collection": "docling",
      "char_count": 882,
      "estimated_tokens": 220,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:6",
    "content": "### General-Purpose VLM Models\n\n\nDocling supports general-purpose VLMs that output Markdown or HTML, suitable for document conversion when DOCTAGS-trained models are not required.\n\n| Model                  | Primary Output | Notable Features                                |\n| ---------------------- | -------------- | ----------------------------------------------- |\n| **Granite Vision 3.2** | Markdown       | IBM's 2B vision model, multi-framework support  |\n| **Pixtral 12B**        | Markdown       | Mistral's 12B multimodal model                  |\n| **Qwen2.5-VL**         | Markdown       | 3B parameter model with strong OCR capabilities |\n| **Phi-4**              | Markdown       | Microsoft's 14B multimodal model                |\n| **GOT-OCR 2.0**        | Markdown       | Specialized OCR model with format preservation  |\n\nExample configuration for Granite Vision:\n\n```\n```\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py143-245](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L143-L245) [docs/usage/vision\\_models.md46-58](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L46-L58)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "General-Purpose VLM Models",
      "heading_level": 3,
      "chunk_index": 6,
      "collection": "docling",
      "char_count": 1183,
      "estimated_tokens": 295,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:7",
    "content": "### Custom Model Configuration\n\n\nBeyond pre-configured models, custom VLMs can be integrated by specifying `InlineVlmOptions` directly:\n\n```\n```\n\nSources: [docs/usage/vision\\_models.md88-113](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L88-L113) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py54-89](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L54-L89)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Custom Model Configuration",
      "heading_level": 3,
      "chunk_index": 7,
      "collection": "docling",
      "char_count": 458,
      "estimated_tokens": 114,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:8",
    "content": "## Response Formats\n\n\nVLM models support multiple output formats optimized for different document understanding tasks. The response format determines how the VLM structures its output and how Docling processes it into a `DoclingDocument`.\n\n**Diagram: Response Format Processing**\n\n```\n```",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Response Formats",
      "heading_level": 2,
      "chunk_index": 8,
      "collection": "docling",
      "char_count": 288,
      "estimated_tokens": 72,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:9",
    "content": "### DOCTAGS Format\n\n\nDOCTAGS is an XML-based structured format designed specifically for document understanding. It provides the most accurate representation of document structure and is the recommended format for document conversion.\n\nExample DOCTAGS output:\n\n```\n```\n\nModels trained for DOCTAGS output include:\n\n- GraniteDocling (all variants)\n- SmolDocling (all variants)\n\nConfiguration:\n\n```\n```\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py27-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L27-L32) [docling/datamodel/vlm\\_model\\_specs.py22-37](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L22-L37)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "DOCTAGS Format",
      "heading_level": 3,
      "chunk_index": 9,
      "collection": "docling",
      "char_count": 730,
      "estimated_tokens": 182,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:10",
    "content": "### Markdown Format\n\n\nMarkdown format outputs standard Markdown syntax, suitable for general-purpose document representation. This format is widely compatible with downstream tools and libraries.\n\nExample Markdown output:\n\n```\n```\n\nModels outputting Markdown include:\n\n- Granite Vision\n- Pixtral\n- Qwen2.5-VL\n- GOT-OCR 2.0\n\nConfiguration:\n\n```\n```\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py144-157](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L144-L157) [docs/usage/vision\\_models.md4-9](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L4-L9)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Markdown Format",
      "heading_level": 3,
      "chunk_index": 10,
      "collection": "docling",
      "char_count": 636,
      "estimated_tokens": 159,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:11",
    "content": "### HTML Format\n\n\nHTML format outputs HTML markup, preserving semantic document structure through HTML tags. This format is useful for web-based applications and rich document viewers.\n\nConfiguration:\n\n```\n```\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py27-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L27-L32)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "HTML Format",
      "heading_level": 3,
      "chunk_index": 11,
      "collection": "docling",
      "char_count": 391,
      "estimated_tokens": 97,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:12",
    "content": "### Custom Response Processing\n\n\nThe `decode_response()` method in `BaseVlmOptions` allows custom post-processing of VLM outputs. This enables integration with models that return structured responses requiring transformation.\n\nExample implementation:\n\n```\n```\n\nThis pattern is used internally for specialized models like OlmOcr that return JSON-structured responses.\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py20-24](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L20-L24)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Custom Response Processing",
      "heading_level": 3,
      "chunk_index": 12,
      "collection": "docling",
      "char_count": 548,
      "estimated_tokens": 137,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:13",
    "content": "## VLM Configuration Options\n\n\nVLM behavior is controlled through configuration classes that specify model selection, inference parameters, and processing options.\n\n**Diagram: VLM Configuration Hierarchy**\n\n```\n```",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "VLM Configuration Options",
      "heading_level": 2,
      "chunk_index": 13,
      "collection": "docling",
      "char_count": 214,
      "estimated_tokens": 53,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:14",
    "content": "### Core Configuration Parameters\n\n\n| Parameter            | Type             | Default        | Description                                |\n| -------------------- | ---------------- | -------------- | ------------------------------------------ |\n| **prompt**           | `str`            | Model-specific | Prompt text sent to VLM                    |\n| **scale**            | `float`          | `2.0`          | Image scaling factor for higher resolution |\n| **max\\_size**        | `Optional[int]`  | `None`         | Maximum image dimension (pixels)           |\n| **temperature**      | `float`          | `0.0`          | Sampling temperature (0.0 = deterministic) |\n| **response\\_format** | `ResponseFormat` | Required       | Expected output format                     |",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Core Configuration Parameters",
      "heading_level": 3,
      "chunk_index": 14,
      "collection": "docling",
      "char_count": 777,
      "estimated_tokens": 194,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:15",
    "content": "### Inline Model Parameters\n\n\n| Parameter                | Type                      | Purpose                                      |\n| ------------------------ | ------------------------- | -------------------------------------------- |\n| **repo\\_id**             | `str`                     | HuggingFace model repository identifier      |\n| **revision**             | `str`                     | Model version/branch (default: \"main\")       |\n| **inference\\_framework** | `InferenceFramework`      | Framework selection: MLX, TRANSFORMERS, VLLM |\n| **max\\_new\\_tokens**     | `int`                     | Maximum tokens to generate (default: 4096)   |\n| **stop\\_strings**        | `List[str]`               | Strings that trigger generation stop         |\n| **supported\\_devices**   | `List[AcceleratorDevice]` | Compatible hardware devices                  |",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Inline Model Parameters",
      "heading_level": 3,
      "chunk_index": 15,
      "collection": "docling",
      "char_count": 861,
      "estimated_tokens": 215,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:16",
    "content": "### API Model Parameters\n\n\n| Parameter       | Type             | Purpose                             |\n| --------------- | ---------------- | ----------------------------------- |\n| **url**         | `AnyUrl`         | API endpoint URL                    |\n| **headers**     | `Dict[str, str]` | HTTP headers (e.g., authentication) |\n| **timeout**     | `float`          | Request timeout in seconds          |\n| **concurrency** | `int`            | Number of parallel API requests     |\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py13-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L13-L112)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "API Model Parameters",
      "heading_level": 3,
      "chunk_index": 16,
      "collection": "docling",
      "char_count": 672,
      "estimated_tokens": 168,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:17",
    "content": "### Generation Control\n\n\nVLM generation behavior can be fine-tuned through stopping criteria and generation configuration:\n\n**Stop Strings**: Simple string-based stopping\n\n```\n```\n\n**Custom Stopping Criteria**: Programmatic stopping logic\n\n```\n```\n\n**Extra Generation Config**: Framework-specific parameters\n\n```\n```\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py78-82](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L78-L82) [docling/models/utils/generation\\_utils.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/generation_utils.py)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Generation Control",
      "heading_level": 3,
      "chunk_index": 17,
      "collection": "docling",
      "char_count": 641,
      "estimated_tokens": 160,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:18",
    "content": "## Prompt Construction and Formatting\n\n\nVLM prompts are constructed through the `build_prompt()` method, which can be customized to include page-specific context or structured instructions.\n\n**Diagram: Prompt Processing Flow**\n\n```\n```",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Prompt Construction and Formatting",
      "heading_level": 2,
      "chunk_index": 18,
      "collection": "docling",
      "char_count": 235,
      "estimated_tokens": 58,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:19",
    "content": "### Prompt Styles\n\n\n| Style    | Usage                            | Example                         |\n| -------- | -------------------------------- | ------------------------------- |\n| **CHAT** | Uses model's chat template       | \\`<                             |\n| **RAW**  | Direct prompt without formatting | `Convert this page to docling.` |\n| **NONE** | No text prompt (image-only)      | `\"\"`                            |",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Prompt Styles",
      "heading_level": 3,
      "chunk_index": 19,
      "collection": "docling",
      "char_count": 429,
      "estimated_tokens": 107,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:20",
    "content": "### Dynamic Prompt Construction\n\n\nThe `build_prompt()` method can access page metadata for context-aware prompts:\n\n```\n```\n\nSources: [docling/models/base\\_model.py85-126](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L85-L126) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py20-24](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L20-L24)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Dynamic Prompt Construction",
      "heading_level": 3,
      "chunk_index": 20,
      "collection": "docling",
      "char_count": 438,
      "estimated_tokens": 109,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:21",
    "content": "## Response Formats and Processing\n\n\nVLM models support multiple output formats optimized for different document understanding tasks and downstream processing requirements.",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Response Formats and Processing",
      "heading_level": 2,
      "chunk_index": 21,
      "collection": "docling",
      "char_count": 172,
      "estimated_tokens": 43,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:22",
    "content": "### Custom Response Processing\n\n\nVLM options support custom response processing through the `decode_response()` method, enabling specialized handling for specific model outputs:\n\n```\n```\n\nThis pattern allows integration with models that return structured responses requiring post-processing before integration into the document representation.\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py18-22](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L18-L22) [docs/examples/vlm\\_pipeline\\_api\\_model.py78-85](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/vlm_pipeline_api_model.py#L78-L85)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Custom Response Processing",
      "heading_level": 3,
      "chunk_index": 22,
      "collection": "docling",
      "char_count": 681,
      "estimated_tokens": 170,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:23",
    "content": "## VLM Integration Examples\n\n\nThe codebase includes comprehensive examples demonstrating VLM integration patterns for different deployment scenarios and model types.",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "VLM Integration Examples",
      "heading_level": 2,
      "chunk_index": 23,
      "collection": "docling",
      "char_count": 165,
      "estimated_tokens": 41,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:24",
    "content": "### Multi-Model Comparison Framework\n\n\nThe `compare_vlm_models.py` example provides a systematic approach for evaluating different VLM models and frameworks:\n\n```\n```\n\nThis framework enables systematic evaluation of model performance, output quality, and resource utilization across different VLM implementations.\n\nSources: [docs/examples/compare\\_vlm\\_models.py33-101](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/compare_vlm_models.py#L33-L101) [docs/examples/compare\\_vlm\\_models.py146-198](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/compare_vlm_models.py#L146-L198)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "Multi-Model Comparison Framework",
      "heading_level": 3,
      "chunk_index": 24,
      "collection": "docling",
      "char_count": 723,
      "estimated_tokens": 180,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3-vision-language-models.md:chunk:25",
    "content": "### On this page\n\n\n- [Vision Language Models](#vision-language-models.md)\n- [VLM Integration Architecture](#vlm-integration-architecture.md)\n- [Available VLM Model Variants](#available-vlm-model-variants.md)\n- [GraniteDocling Models](#granitedocling-models.md)\n- [SmolDocling Models](#smoldocling-models.md)\n- [General-Purpose VLM Models](#general-purpose-vlm-models.md)\n- [Custom Model Configuration](#custom-model-configuration.md)\n- [Response Formats](#response-formats.md)\n- [DOCTAGS Format](#doctags-format.md)\n- [Markdown Format](#markdown-format.md)\n- [HTML Format](#html-format.md)\n- [Custom Response Processing](#custom-response-processing.md)\n- [VLM Configuration Options](#vlm-configuration-options.md)\n- [Core Configuration Parameters](#core-configuration-parameters.md)\n- [Inline Model Parameters](#inline-model-parameters.md)\n- [API Model Parameters](#api-model-parameters.md)\n- [Generation Control](#generation-control.md)\n- [Prompt Construction and Formatting](#prompt-construction-and-formatting.md)\n- [Prompt Styles](#prompt-styles.md)\n- [Dynamic Prompt Construction](#dynamic-prompt-construction.md)\n- [Response Formats and Processing](#response-formats-and-processing.md)\n- [Response Format Types](#response-format-types.md)\n- [Custom Response Processing](#custom-response-processing-1.md)\n- [VLM Integration Examples](#vlm-integration-examples.md)\n- [Multi-Model Comparison Framework](#multi-model-comparison-framework.md)",
    "metadata": {
      "source": "_docling-project_docling_4.3-vision-language-models.md",
      "heading": "On this page",
      "heading_level": 3,
      "chunk_index": 25,
      "collection": "docling",
      "char_count": 1443,
      "estimated_tokens": 360,
      "total_chunks": 26
    }
  }
]