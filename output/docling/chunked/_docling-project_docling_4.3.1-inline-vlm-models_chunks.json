[
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:0",
    "content": "# Inline VLM Models\n\n\nRelevant source files\n\n- [README.md](https://github.com/docling-project/docling/blob/f7244a43/README.md)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docs/examples/minimal\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py)\n- [docs/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/index.md)\n- [docs/usage/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/index.md)\n- [docs/usage/mcp.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/mcp.md)\n- [docs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Inline VLM Models",
      "heading_level": 1,
      "chunk_index": 0,
      "collection": "docling",
      "char_count": 1953,
      "estimated_tokens": 488,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:1",
    "content": "ocs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)\n\nThis page documents the inline Vision Language Model (VLM) implementations in Docling. Inline VLM models run locally on the host machine, in contrast to API-based VLM models that connect to remote services. Three inference frameworks are supported: Hugging Face Transformers, MLX (for Apple Silicon acceleration), and vLLM (for optimized GPU inference).\n\nFor information about API-based VLM models that connect to remote services like Ollama or vLLM servers, see [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md). For general VLM integration concepts and configuration options, see [Vision Language Models](docling-project/docling/4.3-vision-language-models.md).",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Inline VLM Models",
      "heading_level": 1,
      "chunk_index": 1,
      "collection": "docling",
      "char_count": 890,
      "estimated_tokens": 222,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:2",
    "content": "## Architecture Overview\n\n\nThe inline VLM model system provides three specialized implementations sharing a common interface:\n\n```\n```\n\n**Sources:** [docling/models/base\\_model.py46-127](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L127) [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L376) [docling/models/vlm\\_models\\_inline/mlx\\_model.py33-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L33-L318) [docling/models/vlm\\_models\\_inline/vllm\\_model.py25-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L25-L301)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Architecture Overview",
      "heading_level": 2,
      "chunk_index": 2,
      "collection": "docling",
      "char_count": 819,
      "estimated_tokens": 204,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:3",
    "content": "## Configuration via InlineVlmOptions\n\n\nAll inline VLM models are configured through `InlineVlmOptions`, which specifies the model repository, inference framework, and generation parameters:",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Configuration via InlineVlmOptions",
      "heading_level": 2,
      "chunk_index": 3,
      "collection": "docling",
      "char_count": 190,
      "estimated_tokens": 47,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:4",
    "content": "| Parameter                   | Type                                               | Description                                                                                                |\n| --------------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| `repo_id`                   | `str`                                              | Hugging Face repository identifier (e.g., `\"ibm-granite/granite-docling-258M\"`)                            |\n| `inference_framework`       | `InferenceFramework`                               | One of `TRANSFORMERS`, `MLX`, or `VLLM`                                                                    |\n| `transformers_model_type`   | `TransformersModelType`                            | Auto-loading class: `AUTOMODEL`, `AUTOMODEL_VISION2SEQ`, `AUTOMODEL_CAUSALLM`, `AUTOMODEL_IMAGETEXTTOTEXT` |\n| `transformers_prompt_style` | `TransformersPromptStyle`                          | Prompt formatting: `CHAT`, `RAW`, or `NONE`                                                                |\n| `response_format`           | `ResponseFormat`                                   | Expected output format: `DOCTAGS`, `MARKDOWN`, `HTML`, `OTSL`, or `PLAINTEXT`                              |\n| `torch_dtype`               | `Optional[str]`                                    | PyTorch dtype (e.g., `\"bfloat16\"`)                                                                         |\n| `max_new_tokens`            | `int`                                              | Maximum tokens to generate (default: `4096`)                                                               |\n| `temperature`               | `float`                                            | Sampling temperature (default: `0.0` for greedy)                                                           |\n| `scale`                     | `float`                                            | Image scaling factor (default: `2.0`)                                                                      |\n| `max_size`                  | `Optional[int]`                                    | Maximum image dimension                                                                                    |\n| `use_kv_cache`              | `bool`                                             | Enable key-value caching (default: `True`)                                                                 |\n| `stop_strings`              | `List[str]`                                        | Strings that trigger generation stop                                                                       |\n| `custom_stopping_criteria`  | `List[Union[StoppingCriteria, GenerationStopper]]` | Custom stopping logic                                                                                      |\n| `extra_generation_config`   | `Dict[str, Any]`                                   | Additional framework-specific generation parameters                                                        |\n| `extra_processor_kwargs`    | `Dict[str, Any]`                                   | Additional processor parameters                                                                            |\n| `quantized`                 | `bool`                                             | Enable quantization (default: `False`)                                                                     |\n| `load_in_8bit`              | `bool`                                             | Use 8-bit quantization (default: `True`)                                                                   |\n| `trust_remote_code`         | `bool`                                             | Allow remote code execution (default: `False`)                                                             |\n| `revision`                  | `str`                                              | Model revision/branch (default: `\"main\"`)                                                                  |",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Configuration via InlineVlmOptions",
      "heading_level": 2,
      "chunk_index": 4,
      "collection": "docling",
      "char_count": 4073,
      "estimated_tokens": 1018,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:5",
    "content": "|\n| `revision`                  | `str`                                              | Model revision/branch (default: `\"main\"`)                                                                  |\n\n**Sources:** [docling/datamodel/pipeline\\_options\\_vlm\\_model.py54-89](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L54-L89)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Configuration via InlineVlmOptions",
      "heading_level": 2,
      "chunk_index": 5,
      "collection": "docling",
      "char_count": 381,
      "estimated_tokens": 95,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:6",
    "content": "### Model Loading and Initialization\n\n\n`HuggingFaceTransformersVlmModel` loads models using Transformers' auto-loading classes:\n\n```\n```\n\nThe model class is selected based on `transformers_model_type`:\n\n- `AUTOMODEL` → `AutoModel`\n- `AUTOMODEL_CAUSALLM` → `AutoModelForCausalLM`\n- `AUTOMODEL_VISION2SEQ` → `AutoModelForVision2Seq`\n- `AUTOMODEL_IMAGETEXTTOTEXT` → `AutoModelForImageTextToText`\n\nThe processor's tokenizer padding is configured with `padding_side = \"left\"` for batch processing.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-138](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L138)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Model Loading and Initialization",
      "heading_level": 3,
      "chunk_index": 6,
      "collection": "docling",
      "char_count": 701,
      "estimated_tokens": 175,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:7",
    "content": "### Batch Inference Pipeline\n\n\nThe Transformers implementation processes images in batches:\n\n```\n```\n\n**Key Implementation Details:**\n\n1. **Image Normalization** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py209-224](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L209-L224): Converts numpy arrays to PIL RGB images\n2. **Prompt Handling** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py229-236](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L229-L236): Accepts single prompt string or list of prompts (one per image)\n3. **Processor Integration** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py240-256](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L240-L256): Handles both text and image preprocessing with automatic padding\n4. **Stopping Criteria** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L296): Supports `StopStringCriteria` and custom `GenerationStopper` instances\n5. **Token Trimming** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py343-344](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L343-L344): Removes input tokens from output sequences using attention mask\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py139-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L139-L376)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Batch Inference Pipeline",
      "heading_level": 3,
      "chunk_index": 7,
      "collection": "docling",
      "char_count": 1765,
      "estimated_tokens": 441,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:8",
    "content": "### Stopping Criteria Handling\n\n\nThe Transformers implementation supports two types of stopping criteria:\n\n```\n```\n\nThe implementation distinguishes between:\n\n- **String-based stopping** via `StopStringCriteria` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py264-269](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L264-L269)\n- **GenerationStopper classes/instances** wrapped in `HFStoppingCriteriaWrapper` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py276-283](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L276-L283)\n- **Native StoppingCriteria classes** instantiated with tokenizer [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py284-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L284-L287)\n- **StoppingCriteria instances** used directly [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py294-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L294-L296)\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L302)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Stopping Criteria Handling",
      "heading_level": 3,
      "chunk_index": 8,
      "collection": "docling",
      "char_count": 1404,
      "estimated_tokens": 351,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:9",
    "content": "### Architecture and Thread Safety\n\n\n`HuggingFaceMlxModel` uses the MLX framework for Apple Silicon acceleration with important thread safety considerations:\n\n```\n```\n\n**Critical Constraint:** MLX models are **not thread-safe**. All MLX inference operations are serialized using a global lock [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-30](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L30):\n\n```\n```\n\nThis means only one MLX model instance can perform inference at a time across the entire process.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-90](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L90)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Architecture and Thread Safety",
      "heading_level": 3,
      "chunk_index": 9,
      "collection": "docling",
      "char_count": 751,
      "estimated_tokens": 187,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:10",
    "content": "### Streaming Generation and Token Collection\n\n\nUnlike the Transformers implementation, MLX uses streaming generation:\n\n```\n```\n\n**Key Characteristics:**\n\n1. **No Batching** [docling/models/vlm\\_models\\_inline/mlx\\_model.py186-188](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L186-L188): Images are processed sequentially within the global lock\n2. **Token-Level Collection** [doclog/models/vlm\\_models\\_inline/mlx\\_model.py232-254](https://github.com/docling-project/docling/blob/f7244a43/doclog/models/vlm_models_inline/mlx_model.py#L232-L254): Each token includes text, token ID, and log probability\n3. **Early Stopping** [docling/models/vlm\\_models\\_inline/mlx\\_model.py258-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L258-L302): Stop strings and `GenerationStopper` instances are checked during streaming\n4. **Lookback Window** [docling/models/vlm\\_models\\_inline/mlx\\_model.py279-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L279-L287): Custom stoppers can specify how many recent characters to examine\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py149-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L149-L318)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Streaming Generation and Token Collection",
      "heading_level": 3,
      "chunk_index": 10,
      "collection": "docling",
      "char_count": 1379,
      "estimated_tokens": 344,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:11",
    "content": "### Stopping Criteria Validation\n\n\nMLX enforces strict stopping criteria types [docling/models/vlm\\_models\\_inline/mlx\\_model.py75-89](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L75-L89):\n\n| Allowed                       | Not Allowed                  |\n| ----------------------------- | ---------------------------- |\n| `GenerationStopper` instances | `StoppingCriteria` instances |\n| `GenerationStopper` classes   | `StoppingCriteria` classes   |\n| Stop strings                  | -                            |\n\nIf Hugging Face `StoppingCriteria` is detected, a `ValueError` is raised with a clear message explaining that only `GenerationStopper` is supported for MLX.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py75-89](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L75-L89)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Stopping Criteria Validation",
      "heading_level": 3,
      "chunk_index": 11,
      "collection": "docling",
      "char_count": 913,
      "estimated_tokens": 228,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:12",
    "content": "### Configuration and Initialization\n\n\n`VllmVlmModel` provides GPU-optimized inference with strict separation of load-time and runtime parameters:\n\n```\n```\n\n**Parameter Allowlists:**\n\nThe implementation maintains two explicit allowlists [docling/models/vlm\\_models\\_inline/vllm\\_model.py32-80](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L32-L80):\n\n1. **`_VLLM_ENGINE_KEYS`** - Parameters passed to `LLM.__init__()` (load time)\n2. **`_VLLM_SAMPLING_KEYS`** - Parameters passed to `SamplingParams` (runtime)\n\nAny keys in `extra_generation_config` not in either allowlist trigger a warning and are ignored.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py82-174](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L82-L174)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Configuration and Initialization",
      "heading_level": 3,
      "chunk_index": 12,
      "collection": "docling",
      "char_count": 850,
      "estimated_tokens": 212,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:13",
    "content": "### Batch Inference with Multi-Modal Data\n\n\nvLLM processes images as multi-modal data in batch mode:\n\n```\n```\n\n**Key Features:**\n\n1. **True Batching** [docling/models/vlm\\_models\\_inline/vllm\\_model.py233-300](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L233-L300): vLLM processes all images in a single `generate()` call\n2. **Multi-Modal Data Format** [docling/models/vlm\\_models\\_inline/vllm\\_model.py277-280](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L277-L280): Images are passed via `multi_modal_data` dictionary with `\"image\"` key\n3. **Memory Limit** [docling/models/vlm\\_models\\_inline/vllm\\_model.py140](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L140-L140): `limit_mm_per_prompt={\"image\": 1}` restricts one image per prompt\n4. **GPU Memory Management** [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-151](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L151): Defaults to 30% GPU memory utilization to share with other models\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py175-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L175-L301)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Batch Inference with Multi-Modal Data",
      "heading_level": 3,
      "chunk_index": 13,
      "collection": "docling",
      "char_count": 1373,
      "estimated_tokens": 343,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:14",
    "content": "## Prompt Formatting\n\n\nAll inline VLM models share the `formulate_prompt()` method from `BaseVlmPageModel`:\n\n```\n```\n\n**Prompt Style Options:**\n\n| Style          | Behavior                               | Use Case                                            |\n| -------------- | -------------------------------------- | --------------------------------------------------- |\n| `RAW`          | Returns user prompt unchanged          | Models that handle formatting internally            |\n| `NONE`         | Returns empty string                   | Models that don't need text prompts (e.g., GOT-OCR) |\n| `CHAT`         | Applies processor's chat template      | Standard instruction-following models               |\n| Custom (Phi-4) | Special formatting for specific models | Model-specific requirements                         |\n\n**Sources:** [docling/models/base\\_model.py85-126](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L85-L126)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Prompt Formatting",
      "heading_level": 2,
      "chunk_index": 14,
      "collection": "docling",
      "char_count": 976,
      "estimated_tokens": 244,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:15",
    "content": "## Model Download and Caching\n\n\nAll inline VLM implementations inherit from `HuggingFaceModelDownloadMixin`:\n\n```\n```\n\nThe `repo_cache_folder` property converts slashes in `repo_id` to dashes (e.g., `\"ibm-granite/granite-docling-258M\"` → `\"ibm-granite--granite-docling-258M\"`).\n\n**Sources:** [docling/models/utils/hf\\_model\\_download.py8-45](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py#L8-L45) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py86-88](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L86-L88)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Model Download and Caching",
      "heading_level": 2,
      "chunk_index": 15,
      "collection": "docling",
      "char_count": 620,
      "estimated_tokens": 155,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:16",
    "content": "## Available Model Specifications\n\n\nDocling provides pre-configured model specifications in `vlm_model_specs`:",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Available Model Specifications",
      "heading_level": 2,
      "chunk_index": 16,
      "collection": "docling",
      "char_count": 110,
      "estimated_tokens": 27,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:17",
    "content": "### DocTags Output Models\n\n\n| Model Spec                    | Repository                                | Framework    | Devices   | Response Format |\n| ----------------------------- | ----------------------------------------- | ------------ | --------- | --------------- |\n| `GRANITEDOCLING_TRANSFORMERS` | `ibm-granite/granite-docling-258M`        | Transformers | CPU, CUDA | DOCTAGS         |\n| `GRANITEDOCLING_VLLM`         | `ibm-granite/granite-docling-258M`        | vLLM         | CUDA      | DOCTAGS         |\n| `GRANITEDOCLING_MLX`          | `ibm-granite/granite-docling-258M-mlx`    | MLX          | MPS       | DOCTAGS         |\n| `SMOLDOCLING_TRANSFORMERS`    | `ds4sd/SmolDocling-256M-preview`          | Transformers | CPU, CUDA | DOCTAGS         |\n| `SMOLDOCLING_VLLM`            | `ds4sd/SmolDocling-256M-preview`          | vLLM         | CUDA      | DOCTAGS         |\n| `SMOLDOCLING_MLX`             | `ds4sd/SmolDocling-256M-preview-mlx-bf16` | MLX          | MPS       | DOCTAGS         |",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "DocTags Output Models",
      "heading_level": 3,
      "chunk_index": 17,
      "collection": "docling",
      "char_count": 1011,
      "estimated_tokens": 252,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:18",
    "content": "### Markdown Output Models\n\n\n| Model Spec                    | Repository                                  | Framework    | Devices        | Response Format |\n| ----------------------------- | ------------------------------------------- | ------------ | -------------- | --------------- |\n| `GRANITE_VISION_TRANSFORMERS` | `ibm-granite/granite-vision-3.2-2b`         | Transformers | CPU, CUDA, MPS | MARKDOWN        |\n| `GRANITE_VISION_VLLM`         | `ibm-granite/granite-vision-3.2-2b`         | vLLM         | CUDA           | MARKDOWN        |\n| `PIXTRAL_12B_TRANSFORMERS`    | `mistral-community/pixtral-12b`             | Transformers | CPU, CUDA      | MARKDOWN        |\n| `PIXTRAL_12B_MLX`             | `mlx-community/pixtral-12b-bf16`            | MLX          | MPS            | MARKDOWN        |\n| `PHI4_TRANSFORMERS`           | `microsoft/Phi-4-multimodal-instruct`       | Transformers | CPU, CUDA      | MARKDOWN        |\n| `QWEN25_VL_3B_MLX`            | `mlx-community/Qwen2.5-VL-3B-Instruct-bf16` | MLX          | MPS            | MARKDOWN        |\n| `GOT2_TRANSFORMERS`           | `stepfun-ai/GOT-OCR-2.0-hf`                 | Transformers | CPU, CUDA      | MARKDOWN        |\n| `GEMMA3_12B_MLX`              | `mlx-community/gemma-3-12b-it-bf16`         | MLX          | MPS            | MARKDOWN        |\n| `GEMMA3_27B_MLX`              | `mlx-community/gemma-3-27b-it-bf16`         | MLX          | MPS            | MARKDOWN        |\n| `DOLPHIN_TRANSFORMERS`        | `ByteDance/Dolphin`                         | Transformers | CPU, CUDA, MPS | MARKDOWN        |",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Markdown Output Models",
      "heading_level": 3,
      "chunk_index": 18,
      "collection": "docling",
      "char_count": 1588,
      "estimated_tokens": 397,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:19",
    "content": "### Plaintext Output Models\n\n\n| Model Spec                | Repository                            | Framework    | Devices   | Response Format |\n| ------------------------- | ------------------------------------- | ------------ | --------- | --------------- |\n| `SMOLVLM256_TRANSFORMERS` | `HuggingFaceTB/SmolVLM-256M-Instruct` | Transformers | CPU, CUDA | PLAINTEXT       |\n| `SMOLVLM256_MLX`          | `moot20/SmolVLM-256M-Instruct-MLX`    | MLX          | MPS       | PLAINTEXT       |\n| `SMOLVLM256_VLLM`         | `HuggingFaceTB/SmolVLM-256M-Instruct` | vLLM         | CUDA      | PLAINTEXT       |",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Plaintext Output Models",
      "heading_level": 3,
      "chunk_index": 19,
      "collection": "docling",
      "char_count": 604,
      "estimated_tokens": 151,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:20",
    "content": "### Extraction Models\n\n\n| Model Spec                   | Repository                | Framework    | Devices        | Response Format |\n| ---------------------------- | ------------------------- | ------------ | -------------- | --------------- |\n| `NU_EXTRACT_2B_TRANSFORMERS` | `numind/NuExtract-2.0-2B` | Transformers | CPU, CUDA, MPS | PLAINTEXT       |\n\n**Special Configuration Notes:**\n\n1. **GOT-OCR-2.0** uses `TransformersPromptStyle.NONE` and includes `extra_processor_kwargs={\"format\": True}`\n2. **Phi-4** requires `transformers<4.52.0` and uses `extra_generation_config={\"num_logits_to_keep\": 0}`\n3. **Dolphin** uses `TransformersPromptStyle.RAW` with a custom prompt format\n4. **GraniteDocling VLLM** uses `revision=\"untied\"` for compatibility with vLLM ≤0.10.2\n\n**Sources:** [docling/datamodel/vlm\\_model\\_specs.py1-303](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L1-L303)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Extraction Models",
      "heading_level": 3,
      "chunk_index": 20,
      "collection": "docling",
      "char_count": 935,
      "estimated_tokens": 233,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:21",
    "content": "### Direct Image Processing\n\n\nAll inline VLM models support direct image processing via the `process_images()` method:\n\n```\n```\n\n**Sources:** [docs/examples/minimal\\_vlm\\_pipeline.py1-71](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py#L1-L71) [docs/usage/vision\\_models.md1-124](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L1-L124)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Direct Image Processing",
      "heading_level": 3,
      "chunk_index": 21,
      "collection": "docling",
      "char_count": 420,
      "estimated_tokens": 105,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:22",
    "content": "### Framework Comparison\n\n\n| Framework        | Batching             | Thread Safety          | Best For                            |\n| ---------------- | -------------------- | ---------------------- | ----------------------------------- |\n| **Transformers** | ✅ Full batch support | ✅ Thread-safe          | General purpose, CPU/CUDA/MPS       |\n| **MLX**          | ❌ Sequential only    | ❌ Global lock required | Apple Silicon (fastest on M-series) |\n| **vLLM**         | ✅ Optimized batching | ✅ Thread-safe          | High-throughput GPU inference       |",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Framework Comparison",
      "heading_level": 3,
      "chunk_index": 22,
      "collection": "docling",
      "char_count": 561,
      "estimated_tokens": 140,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:23",
    "content": "### Memory Management\n\n\n1. **Transformers**: Uses PyTorch's default memory management; consider `torch_dtype=\"bfloat16\"` for memory savings\n2. **MLX**: Automatically manages unified memory on Apple Silicon\n3. **vLLM**: Set `gpu_memory_utilization` (default 0.3) to reserve GPU memory for other models",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Memory Management",
      "heading_level": 3,
      "chunk_index": 23,
      "collection": "docling",
      "char_count": 300,
      "estimated_tokens": 75,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:24",
    "content": "### Acceleration Options\n\n\n- **Flash Attention 2**: Automatically enabled on CUDA devices when `accelerator_options.cuda_use_flash_attention2=True`\n- **Quantization**: Enable with `quantized=True` and `load_in_8bit=True` (Transformers and vLLM only)\n- **KV Cache**: Enabled by default with `use_kv_cache=True`; disable only if memory is constrained\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py123-128](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L123-L128) [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-155](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L155) [docs/usage/vision\\_models.md46-58](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L46-L58)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "Acceleration Options",
      "heading_level": 3,
      "chunk_index": 24,
      "collection": "docling",
      "char_count": 963,
      "estimated_tokens": 240,
      "total_chunks": 26
    }
  },
  {
    "chunk_id": "docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:25",
    "content": "### On this page\n\n\n- [Inline VLM Models](#inline-vlm-models.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Configuration via InlineVlmOptions](#configuration-via-inlinevlmoptions.md)\n- [Hugging Face Transformers Implementation](#hugging-face-transformers-implementation.md)\n- [Model Loading and Initialization](#model-loading-and-initialization.md)\n- [Batch Inference Pipeline](#batch-inference-pipeline.md)\n- [Stopping Criteria Handling](#stopping-criteria-handling.md)\n- [MLX Implementation (Apple Silicon)](#mlx-implementation-apple-silicon.md)\n- [Architecture and Thread Safety](#architecture-and-thread-safety.md)\n- [Streaming Generation and Token Collection](#streaming-generation-and-token-collection.md)\n- [Stopping Criteria Validation](#stopping-criteria-validation.md)\n- [vLLM Implementation](#vllm-implementation.md)\n- [Configuration and Initialization](#configuration-and-initialization.md)\n- [Batch Inference with Multi-Modal Data](#batch-inference-with-multi-modal-data.md)\n- [Prompt Formatting](#prompt-formatting.md)\n- [Model Download and Caching](#model-download-and-caching.md)\n- [Available Model Specifications](#available-model-specifications.md)\n- [DocTags Output Models](#doctags-output-models.md)\n- [Markdown Output Models](#markdown-output-models.md)\n- [Plaintext Output Models](#plaintext-output-models.md)\n- [Extraction Models](#extraction-models.md)\n- [Usage Examples](#usage-examples.md)\n- [Basic Usage with Default Model](#basic-usage-with-default-model.md)\n- [Selecting a Specific Model](#selecting-a-specific-model.md)\n- [Custom Model Configuration](#custom-model-configuration.md)\n- [Direct Image Processing](#direct-image-processing.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Framework Comparison](#framework-comparison.md)\n- [Memory Management](#memory-management.md)\n- [Acceleration Options](#acceleration-options.md)",
    "metadata": {
      "source": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "heading": "On this page",
      "heading_level": 3,
      "chunk_index": 25,
      "collection": "docling",
      "char_count": 1892,
      "estimated_tokens": 473,
      "total_chunks": 26
    }
  }
]