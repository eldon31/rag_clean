[
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:0",
    "content": "This page covers how to integrate sparse encoder models with search engines and vector databases for semantic search applications. Sparse encoders generate embeddings where most values are zero, enabling efficient storage and search while maintaining semantic understanding capabilities. \n\nFor information about dense semantic search with `SentenceTransformer` models, see [Semantic Search](#6.1). For general sparse encoder training and usage, see [SparseEncoder Training](#3.2).\n\n## Overview\n\nSparse search integration allows `SparseEncoder` models to work with external search systems that can efficiently handle sparse vector data. The integration supports both manual in-memory search and production-ready vector database solutions.\n\n```mermaid\ngraph TD\n    SparseEncoder[\"SparseEncoder\"]\n    EncodeDoc[\"encode_document()\"]\n    EncodeQuery[\"encode_query()\"]\n    SparseEmbeddings[\"Sparse Embeddings<br/>(COO Tensors)\"]\n    \n    SparseEncoder --> EncodeDoc\n    SparseEncoder --> EncodeQuery",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 188,
      "char_count": 993,
      "start_char": 0,
      "end_char": 994
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:1",
    "content": "beddings<br/>(COO Tensors)\"]\n    \n    SparseEncoder --> EncodeDoc\n    SparseEncoder --> EncodeQuery\n    EncodeDoc --> SparseEmbeddings\n    EncodeQuery --> SparseEmbeddings\n    \n    SparseEmbeddings --> ManualSearch[\"Manual Search<br/>util.semantic_search()\"]\n    SparseEmbeddings --> VectorDBs[\"Vector Databases\"]\n    \n    VectorDBs --> Qdrant[\"semantic_search_qdrant()\"]\n    VectorDBs --> Elasticsearch[\"semantic_search_elasticsearch()\"] \n    VectorDBs --> OpenSearch[\"semantic_search_opensearch()\"]\n    VectorDBs --> Seismic[\"semantic_search_seismic()\"]\n    VectorDBs --> SpladeIndex[\"SPLADE-index\"]\n```\n\n**Architecture Components for Sparse Search Integration**\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:1-556](), [examples/sparse_encoder/applications/semantic_search/README.md:1-529]()\n\n## Manual Search vs Vector Database Search\n\n### Manual Search Approach\n\nManual search performs similarity computation directly in memory using PyTorch operations.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 225,
      "char_count": 977,
      "start_char": 894,
      "end_char": 1871
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:2",
    "content": "Approach\n\nManual search performs similarity computation directly in memory using PyTorch operations. This approach is suitable for small to medium-sized corpora and provides full control over the search process.\n\n| Component | Function | Purpose |\n|-----------|----------|---------|\n| Document Encoding | `model.encode_document()` | Convert documents to sparse embeddings |\n| Query Encoding | `model.encode_query()` | Convert queries to sparse embeddings |\n| Similarity Computation | `util.semantic_search()` | Compute similarity scores between query and corpus |\n| Results Analysis | `model.intersection()` | Analyze token-level contributions to similarity |\n\n### Vector Database Search Approach\n\nVector database search leverages specialized systems optimized for sparse vector operations, providing better scalability and performance for large corpora.\n\n| Search Engine | Function | Index Type | Key Features |\n|---------------|----------|------------|--------------|",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 170,
      "char_count": 969,
      "start_char": 1771,
      "end_char": 2741
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:3",
    "content": "e | Function | Index Type | Key Features |\n|---------------|----------|------------|--------------|\n| Qdrant | `semantic_search_qdrant()` | Sparse vectors | Native sparse vector support |\n| Elasticsearch | `semantic_search_elasticsearch()` | rank_features | Elastic stack integration |\n| OpenSearch | `semantic_search_opensearch()` | neural_sparse | Amazon OpenSearch compatibility |\n| Seismic | `semantic_search_seismic()` | SeismicIndex | High-performance in-memory search |\n| SPLADE-index | External library | SciPy sparse matrices | BM25s-based implementation |\n\nSources: [examples/sparse_encoder/applications/semantic_search/README.md:11-132]()\n\n## Search Engine Integration Functions\n\n### Qdrant Integration\n\nThe `semantic_search_qdrant()` function provides native integration with Qdrant's sparse vector capabilities.\n\n```mermaid\ngraph LR\n    QueryEmb[\"query_embeddings<br/>(COO Tensor)\"]\n    CorpusEmb[\"corpus_embeddings<br/>(COO Tensor)\"]\n    \n    QueryEmb --> QdrantFunc[\"semantic_search_qdrant()\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 228,
      "char_count": 1008,
      "start_char": 2641,
      "end_char": 3650
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:4",
    "content": "[\"corpus_embeddings<br/>(COO Tensor)\"]\n    \n    QueryEmb --> QdrantFunc[\"semantic_search_qdrant()\"]\n    CorpusEmb --> QdrantFunc\n    \n    QdrantFunc --> QdrantClient[\"QdrantClient\"]\n    QdrantFunc --> Collection[\"Sparse Collection\"]\n    QdrantFunc --> SparseVector[\"SparseVector Models\"]\n    \n    QdrantClient --> Results[\"Search Results<br/>[{'corpus_id': int, 'score': float}]\"]\n```\n\n**Qdrant Integration Data Flow**\n\nThe integration handles COO sparse tensors directly and creates collections with `SparseVectorParams` configuration.\n\n**Key Parameters:**\n- Input: PyTorch COO sparse tensors\n- Collection: Auto-generated with timestamp\n- Indexing: Batch processing with configurable `batch_size`\n- Search: Native sparse vector queries using `models.SparseVector`\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:32-158](), [examples/sparse_encoder/applications/semantic_search/semantic_search_qdrant.py:1-64]()\n\n### Elasticsearch Integration",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 234,
      "char_count": 960,
      "start_char": 3550,
      "end_char": 4514
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:5",
    "content": "er/applications/semantic_search/semantic_search_qdrant.py:1-64]()\n\n### Elasticsearch Integration  \n\nThe `semantic_search_elasticsearch()` function uses Elasticsearch's `rank_features` field type for sparse vector storage and search.\n\n```mermaid\ngraph LR\n    DecodedEmb[\"query_embeddings_decoded<br/>[[('token', value)]]\"]\n    CorpusDecoded[\"corpus_embeddings_decoded<br/>[[('token', value)]]\"]\n    \n    DecodedEmb --> ESFunc[\"semantic_search_elasticsearch()\"]\n    CorpusDecoded --> ESFunc\n    \n    ESFunc --> ESClient[\"Elasticsearch Client\"]\n    ESFunc --> RankFeatures[\"rank_features Mapping\"]\n    ESFunc --> RankFeatureQuery[\"rank_feature Queries\"]\n    \n    ESClient --> ESResults[\"Search Results<br/>[{'corpus_id': int, '_score': float}]\"]\n```\n\n**Elasticsearch Integration Data Flow**\n\n**Key Features:**\n- Input: Decoded embeddings in `[('token', value)]` format\n- Mapping: Uses `rank_features` field type for sparse vectors\n- Indexing: Bulk operations with configurable batch size",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 234,
      "char_count": 984,
      "start_char": 4414,
      "end_char": 5399
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:6",
    "content": "nk_features` field type for sparse vectors\n- Indexing: Bulk operations with configurable batch size\n- Search: `rank_feature` queries with `saturation` and `boost` parameters\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:160-297](), [examples/sparse_encoder/applications/semantic_search/semantic_search_elasticsearch.py:1-68]()\n\n### OpenSearch Integration\n\nThe `semantic_search_opensearch()` function leverages OpenSearch's `neural_sparse` query capabilities.\n\n**Key Differences from Elasticsearch:**\n- Uses `neural_sparse` query type instead of `rank_feature`\n- Compatible with Amazon OpenSearch Service\n- Supports asymmetric sparse encoder architectures\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:428-556](), [examples/sparse_encoder/applications/semantic_search/semantic_search_opensearch.py:1-87]()\n\n### Seismic Integration\n\nThe `semantic_search_seismic()` function provides integration with the high-performance Seismic library for in-memory sparse vector search.\n\n```mermaid",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 229,
      "char_count": 1022,
      "start_char": 5299,
      "end_char": 6322
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:7",
    "content": "tegration with the high-performance Seismic library for in-memory sparse vector search.\n\n```mermaid\ngraph LR\n    DecodedQuery[\"query_embeddings_decoded\"]\n    DecodedCorpus[\"corpus_embeddings_decoded\"]\n    \n    DecodedQuery --> SeismicFunc[\"semantic_search_seismic()\"]\n    DecodedCorpus --> SeismicFunc\n    \n    SeismicFunc --> SeismicDataset[\"SeismicDataset.add_document()\"]\n    SeismicFunc --> SeismicIndex[\"SeismicIndex.build_from_dataset()\"]\n    SeismicFunc --> BatchSearch[\"SeismicIndex.batch_search()\"]\n    \n    BatchSearch --> SeismicResults[\"Sorted Results<br/>[{'corpus_id': int, 'score': float}]\"]\n```\n\n**Seismic Integration Architecture**\n\n**Performance Features:**\n- `SeismicDataset` for document management\n- `SeismicIndex.build_from_dataset()` with configurable index parameters\n- `batch_search()` with `query_cut` and `heap_factor` optimizations\n- Order-of-magnitude performance improvements over IVF approaches\n\nSources: [sentence_transformers/sparse_encoder/search_engines.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 226,
      "char_count": 989,
      "start_char": 6222,
      "end_char": 7211
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:8",
    "content": "nce improvements over IVF approaches\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:299-426](), [examples/sparse_encoder/applications/semantic_search/semantic_search_seismic.py:1-66]()\n\n## Data Format Requirements\n\n### Input Formats by Search Engine\n\n| Search Engine | Input Format | Conversion Method |\n|---------------|--------------|-------------------|\n| Qdrant | PyTorch COO sparse tensor | `convert_to_sparse_tensor=True` |\n| Elasticsearch | Decoded embeddings list | `model.decode(embeddings)` |\n| OpenSearch | Decoded embeddings list | `model.decode(embeddings)` |\n| Seismic | Decoded embeddings list | `model.decode(embeddings)` |\n\n### Encoding Workflow\n\n```mermaid\ngraph TD\n    TextInput[\"Text Input<br/>['query text', 'document text']\"]\n    \n    TextInput --> EncodeQuery[\"model.encode_query()\"]\n    TextInput --> EncodeDoc[\"model.encode_document()\"]\n    \n    EncodeQuery --> SparseQuery[\"Sparse Query Embeddings\"]\n    EncodeDoc --> SparseDoc[\"Sparse Document Embeddings\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 227,
      "char_count": 1002,
      "start_char": 7111,
      "end_char": 8119
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:9",
    "content": "arseQuery[\"Sparse Query Embeddings\"]\n    EncodeDoc --> SparseDoc[\"Sparse Document Embeddings\"]\n    \n    SparseQuery --> QdrantFormat[\"COO Tensor<br/>(for Qdrant)\"]\n    SparseQuery --> DecodedFormat[\"Decoded Format<br/>(for ES/OpenSearch/Seismic)\"]\n    \n    SparseDoc --> QdrantFormat\n    SparseDoc --> DecodedFormat\n    \n    QdrantFormat --> QdrantSearch[\"semantic_search_qdrant()\"]\n    DecodedFormat --> OtherSearches[\"Other search_* functions\"]\n```\n\n**Data Format Conversion Pipeline**\n\n### Sparse Tensor Formats\n\n**COO Sparse Tensor (Qdrant):**\n- Format: PyTorch coordinate format sparse tensor\n- Indices: `[row_indices, col_indices]`\n- Values: Sparse embedding values\n- Advantages: Direct tensor operations, GPU compatibility\n\n**Decoded Format (Others):**\n- Format: `List[List[Tuple[str, float]]]`\n- Structure: `[[('token1', 0.5), ('token2', 0.3)], ...]`\n- Advantages: Human-readable, search engine compatible\n\nSources: [sentence_transformers/sparse_encoder/search_engines.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 251,
      "char_count": 977,
      "start_char": 8019,
      "end_char": 8996
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:10",
    "content": "n-readable, search engine compatible\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:67-76](), [examples/sparse_encoder/applications/semantic_search/README.md:53-83]()\n\n## Integration Patterns\n\n### Reusable Index Pattern\n\nAll search engine integrations support index reuse through the `output_index` parameter:\n\n```mermaid\ngraph TD\n    FirstCall[\"First Function Call\"]\n    CreateIndex[\"Create Index<br/>(corpus_embeddings required)\"]\n    SearchResults1[\"Search Results + Index\"]\n    \n    SecondCall[\"Subsequent Calls\"]\n    ReuseIndex[\"Reuse Existing Index<br/>(corpus_index provided)\"]\n    SearchResults2[\"Search Results Only\"]\n    \n    FirstCall --> CreateIndex\n    CreateIndex --> SearchResults1\n    \n    SecondCall --> ReuseIndex\n    ReuseIndex --> SearchResults2\n    \n    SearchResults1 --> IndexStorage[\"Store Index Reference\"]\n    IndexStorage --> SecondCall\n```\n\n**Index Reuse Pattern for Production Workflows**\n\n### Error Handling and Validation",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 220,
      "char_count": 971,
      "start_char": 8896,
      "end_char": 9869
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:11",
    "content": "econdCall\n```\n\n**Index Reuse Pattern for Production Workflows**\n\n### Error Handling and Validation\n\nAll integration functions include comprehensive input validation:\n\n- Sparse tensor format validation for Qdrant\n- Decoded embedding format validation for other engines  \n- Client availability checks with helpful error messages\n- Required dependency import validation\n\nSources: [sentence_transformers/sparse_encoder/search_engines.py:67-76](), [sentence_transformers/sparse_encoder/search_engines.py:204-218]()\n\n## Performance Considerations\n\n### Sparsity Advantages\n\nSparse embeddings provide several performance benefits for search:\n\n| Advantage | Description | Impact |\n|-----------|-------------|---------|\n| Storage Efficiency | Most dimensions are zero | Reduced memory footprint |\n| Search Speed | Skip zero-value computations | Faster similarity calculations |\n| Interpretability | Non-zero dimensions map to tokens | Explainable search results |",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 183,
      "char_count": 953,
      "start_char": 9769,
      "end_char": 10723
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Sparse_Search_Integration.md:chunk:12",
    "content": "alculations |\n| Interpretability | Non-zero dimensions map to tokens | Explainable search results |\n| Exact Matching | Preserve lexical signals | Hybrid semantic-lexical search |\n\n### Scalability Recommendations\n\n- **Small Corpora (< 10K docs):** Manual search with `util.semantic_search()`\n- **Medium Corpora (10K-1M docs):** Qdrant or Seismic for performance\n- **Large Corpora (> 1M docs):** Elasticsearch/OpenSearch with distributed setup\n- **Real-time Applications:** Seismic for lowest latency in-memory search\n\nSources: [examples/sparse_encoder/applications/semantic_search/README.md:127-132](), [examples/sparse_encoder/applications/semantic_search/README.md:388-396]()",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Sparse_Search_Integration.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Sparse_Search_Integration.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 166,
      "char_count": 676,
      "start_char": 10623,
      "end_char": 11647
    }
  }
]