[
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md:chunk:0",
    "content": "model.prompts = {\"query\": \"query: \", \"document\": \"passage: \"}\nquery_emb = model.encode_query(\"What is AI?\")\ndoc_emb = model.encode_document(\"AI is artificial intelligence\")\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:61-407](), [sentence_transformers/SentenceTransformer.py:416-675](), [tests/test_sentence_transformer.py:309-346]()\n\n## SparseEncoder\n\nThe `SparseEncoder` class extends `SentenceTransformer` to produce sparse vector representations where most dimensions are zero. This architecture is particularly effective for lexical matching and hybrid retrieval scenarios.\n\n### Sparse Architecture Components\n\n```mermaid\ngraph TB\n    subgraph \"SparseEncoder Components\"\n        MLMTransformer[\"MLMTransformer<br/>Token-level predictions\"]\n        SpladePooling[\"SpladePooling<br/>Sparsification\"]\n        SparseAutoEncoder[\"SparseAutoEncoder<br/>k-sparse activation\"]\n        Router[\"Router<br/>Query/Document paths\"]\n    end\n    \n    subgraph \"Output Processing\"",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Task-specific_encoding_with_prompts.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 221,
      "char_count": 988,
      "start_char": 0,
      "end_char": 989
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md:chunk:1",
    "content": "n\"]\n        Router[\"Router<br/>Query/Document paths\"]\n    end\n    \n    subgraph \"Output Processing\"\n        ActiveDims[\"max_active_dims<br/>Sparsity control\"]\n        SparseOutput[\"Sparse COO Tensor<br/>[batch_size, vocab_size]\"]\n    end\n    \n    Input[\"Text\"] --> Router\n    Router --> MLMTransformer\n    MLMTransformer --> SpladePooling\n    SpladePooling --> ActiveDims\n    ActiveDims --> SparseOutput\n```\n\n### Key Differences from SentenceTransformer\n\n- **Vocabulary-Sized Output**: Embeddings have dimensions equal to tokenizer vocabulary size\n- **Sparsity Control**: `max_active_dims` parameter limits non-zero dimensions\n- **Sparse Tensor Format**: Outputs can be sparse COO tensors for memory efficiency\n- **Term Importance**: Non-zero values represent importance of vocabulary terms\n\n### Decoding and Interpretation\n\nThe `SparseEncoder` provides a `decode()` method to interpret sparse embeddings as weighted vocabulary terms:\n\n```python\nmodel = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Task-specific_encoding_with_prompts.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 219,
      "char_count": 1010,
      "start_char": 889,
      "end_char": 1900
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md:chunk:2",
    "content": "ghted vocabulary terms:\n\n```python\nmodel = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")\nembeddings = model.encode(\"machine learning\")\ntokens_weights = model.decode(embeddings, top_k=10)",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Task-specific_encoding_with_prompts.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 51,
      "char_count": 197,
      "start_char": 1800,
      "end_char": 2824
    }
  }
]