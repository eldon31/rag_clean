[
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:0",
    "content": "trainer = CrossEncoderTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n### Configuration Parameters\n\nCommon parameters across learning-to-rank losses:\n\n| Parameter | Type | Purpose |\n|-----------|------|---------|\n| `model` | `CrossEncoder` | Model to train |\n| `activation_fn` | `nn.Module` | Applied to logits before loss computation |\n| `mini_batch_size` | `int` | Controls memory usage and processing speed |\n\nSources: [examples/cross_encoder/training/ms_marco/training_ms_marco_listmle.py:93-94](), [examples/cross_encoder/training/ms_marco/training_ms_marco_plistmle.py:96]()\n\n## Performance Recommendations\n\nBased on the documentation and implementation comments:\n\n1. **LambdaLoss with NDCGLoss2PPScheme**: Generally performs best for ranking tasks\n2. **PListMLELoss**: Outperforms standard ListMLELoss due to position weighting\n3. **Mini-batch size**: Critical for memory management when processing many documents per query\n4.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 233,
      "char_count": 982,
      "start_char": 0,
      "end_char": 982
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:1",
    "content": "g\n3. **Mini-batch size**: Critical for memory management when processing many documents per query\n4. **Hard negative mining**: Use `mine_hard_negatives` with `output_format=\"labeled-list\"` for better training data\n\nThe learning-to-rank losses are optimized for handling variable numbers of documents per query and support both binary and continuous relevance labels.\n\nSources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:175-176](), [sentence_transformers/cross_encoder/losses/PListMLELoss.py:105-111](), [examples/cross_encoder/training/ms_marco/training_ms_marco_cmnrl.py:62-71]()\n\n# Memory-Efficient Training\n\n\n\n\nThis document covers memory-efficient training techniques in sentence-transformers that allow training with large batch sizes and complex loss functions while maintaining reasonable memory usage. These techniques are essential for achieving optimal performance on modern embedding models without requiring excessive GPU memory.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 205,
      "char_count": 957,
      "start_char": 882,
      "end_char": 1841
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:2",
    "content": "r achieving optimal performance on modern embedding models without requiring excessive GPU memory.\n\nFor general training information, see [SentenceTransformer Training](#3.1). For specific loss functions, see [Loss Functions for SentenceTransformer](#3.4).\n\n## Cached Loss Functions\n\nThe primary memory-efficient training technique uses **GradCache**, which enables training with much larger effective batch sizes while maintaining constant memory usage. This is implemented through cached versions of standard loss functions.\n\n### GradCache Architecture\n\n```mermaid\nflowchart TD\n    Input[\"Input Batch (e.g., 1024 samples)\"] --> CMNRL[\"CachedMultipleNegativesRankingLoss.forward()\"]\n    CMNRL --> Split[\"Split into mini_batch_size chunks\"]\n    Split --> Step1[\"Step 1: Forward Pass (torch.no_grad)\"]\n    Step1 --> EmbedIter[\"embed_minibatch_iter()\"]\n    EmbedIter --> EmbedMB[\"embed_minibatch()\"]\n    EmbedMB --> RandCtx[\"RandContext.copy_random_state\"]\n    RandCtx --> CacheEmb[\"reps.detach().requires_grad_()\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 223,
      "char_count": 1013,
      "start_char": 1741,
      "end_char": 2755
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:3",
    "content": "RandCtx[\"RandContext.copy_random_state\"]\n    RandCtx --> CacheEmb[\"reps.detach().requires_grad_()\"]\n    CacheEmb --> Step2[\"Step 2: Loss Calculation\"]\n    Step2 --> CalcLoss[\"calculate_loss_and_cache_gradients()\"]\n    CalcLoss --> GradCache[\"self.cache = [[r.grad for r in rs]]\"]\n    GradCache --> Step3[\"Step 3: Second Forward Pass (torch.enable_grad)\"]\n    Step3 --> Hook[\"loss.register_hook(_backward_hook)\"]\n    Hook --> Surrogate[\"torch.dot(reps_mb.flatten(), grad_mb.flatten())\"]\n    Surrogate --> Backprop[\"surrogate.backward()\"]\n```\n\n**Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:278-305](), [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:42-62](), [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:18-39]()\n\n### Available Cached Loss Functions\n\n| Standard Loss | Cached Version | Memory Benefit |\n|---------------|----------------|----------------|",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 238,
      "char_count": 931,
      "start_char": 2655,
      "end_char": 3587
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:4",
    "content": "andard Loss | Cached Version | Memory Benefit |\n|---------------|----------------|----------------|\n| `MultipleNegativesRankingLoss` | `CachedMultipleNegativesRankingLoss` | Constant memory for any batch size |\n| `GISTEmbedLoss` | `CachedGISTEmbedLoss` | Large batch sizes with guide model |\n| `MultipleNegativesSymmetricRankingLoss` | `CachedMultipleNegativesSymmetricRankingLoss` | Symmetric loss with caching |\n\n**Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:64](), [sentence_transformers/losses/CachedGISTEmbedLoss.py:65](), [sentence_transformers/losses/CachedMultipleNegativesSymmetricRankingLoss.py:41]()\n\n### Mini-batch Processing Implementation\n\n```mermaid\ngraph LR\n    Batch[\"Full Batch\"] --> Iterator[\"embed_minibatch_iter()\"]\n    Iterator --> MB1[\"Mini-batch 1<br/>embed_minibatch()\"]\n    Iterator --> MB2[\"Mini-batch 2<br/>embed_minibatch()\"]\n    Iterator --> MB3[\"Mini-batch N<br/>embed_minibatch()\"]\n    MB1 --> RS1[\"RandContext<br/>(Random State)\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 264,
      "char_count": 998,
      "start_char": 3487,
      "end_char": 4486
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:5",
    "content": "tor --> MB3[\"Mini-batch N<br/>embed_minibatch()\"]\n    MB1 --> RS1[\"RandContext<br/>(Random State)\"]\n    MB2 --> RS2[\"RandContext<br/>(Random State)\"]\n    MB3 --> RS3[\"RandContext<br/>(Random State)\"]\n    RS1 --> Cache1[\"Cached Embeddings\"]\n    RS2 --> Cache2[\"Cached Embeddings\"]\n    RS3 --> Cache3[\"Cached Embeddings\"]\n```\n\nThe `mini_batch_size` parameter controls memory usage during training. Each mini-batch is processed through `embed_minibatch()` with `RandContext` ensuring reproducible embeddings across forward passes.\n\n**Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:175-223](), [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:18-39]()\n\n## Matryoshka Training\n\nMatryoshka training allows models to work efficiently at multiple embedding dimensions, reducing storage and computation costs for downstream applications.\n\n### MatryoshkaLoss Architecture\n\n```mermaid\nflowchart TD\n    Model[\"SentenceTransformer\"] --> Forward[\"ForwardDecorator\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 251,
      "char_count": 1002,
      "start_char": 4386,
      "end_char": 5389
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:6",
    "content": "hitecture\n\n```mermaid\nflowchart TD\n    Model[\"SentenceTransformer\"] --> Forward[\"ForwardDecorator\"]\n    Loss[\"Base Loss Function\"] --> Matryoshka[\"MatryoshkaLoss\"]\n    Dims[\"matryoshka_dims<br/>[768, 512, 256, 128, 64]\"] --> Matryoshka\n    Weights[\"matryoshka_weights<br/>[1, 1, 1, 1, 1]\"] --> Matryoshka\n    \n    Forward --> Cache[\"Cache Full Embeddings\"]\n    Cache --> Shrink1[\"shrink(embeddings, 768)\"]\n    Cache --> Shrink2[\"shrink(embeddings, 512)\"]\n    Cache --> Shrink3[\"shrink(embeddings, 256)\"]\n    Cache --> Shrink4[\"shrink(embeddings, 128)\"]\n    Cache --> Shrink5[\"shrink(embeddings, 64)\"]\n    \n    Shrink1 --> Loss1[\"Loss at 768d\"]\n    Shrink2 --> Loss2[\"Loss at 512d\"]\n    Shrink3 --> Loss3[\"Loss at 256d\"]\n    Shrink4 --> Loss4[\"Loss at 128d\"]\n    Shrink5 --> Loss5[\"Loss at 64d\"]\n    \n    Loss1 --> Combine[\"Weighted Sum\"]\n    Loss2 --> Combine\n    Loss3 --> Combine\n    Loss4 --> Combine\n    Loss5 --> Combine\n```\n\n**Sources:** [sentence_transformers/losses/MatryoshkaLoss.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 350,
      "char_count": 989,
      "start_char": 5289,
      "end_char": 6278
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:7",
    "content": "s4 --> Combine\n    Loss5 --> Combine\n```\n\n**Sources:** [sentence_transformers/losses/MatryoshkaLoss.py:113-253](), [sentence_transformers/losses/MatryoshkaLoss.py:30-111]()\n\n### Cached Matryoshka Integration\n\nFor cached losses, Matryoshka uses `CachedLossDecorator` instead of `ForwardDecorator`:\n\n```mermaid\ngraph TD\n    CachedLoss[\"CachedMultipleNegativesRankingLoss\"] --> Decorator[\"CachedLossDecorator\"]\n    Embeddings[\"Pre-computed Embeddings\"] --> Decorator\n    Decorator --> Shrink[\"shrink() for each dimension\"]\n    Shrink --> Calculate[\"calculate_loss()\"]\n    Calculate --> Weighted[\"Apply matryoshka_weights\"]\n```\n\n**Sources:** [sentence_transformers/losses/MatryoshkaLoss.py:67-111](), [sentence_transformers/losses/MatryoshkaLoss.py:195-204]()\n\n## Router-based Asymmetric Models\n\nThe `Router` module enables memory-efficient asymmetric architectures where different encoders are used for queries and documents.\n\n### Router Architecture\n\n```mermaid\ngraph TD",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 261,
      "char_count": 968,
      "start_char": 6178,
      "end_char": 7147
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:8",
    "content": "ifferent encoders are used for queries and documents.\n\n### Router Architecture\n\n```mermaid\ngraph TD\n    Input[\"features: dict[str, Tensor]\"] --> RouterFwd[\"Router.forward()\"]\n    RouterFwd --> TaskCheck{\"task = features.get('task', self.default_route)\"}\n    TaskCheck -->|\"task='query'\"| QuerySeq[\"self.sub_modules['query']\"]\n    TaskCheck -->|\"task='document'\"| DocSeq[\"self.sub_modules['document']\"]\n    TaskCheck -->|\"None\"| DefaultRoute[\"self.default_route\"]\n    \n    QuerySeq --> QMod1[\"SparseStaticEmbedding\"]\n    QuerySeq --> QMod2[\"Additional Query Modules\"]\n    \n    DocSeq --> DMod1[\"MLMTransformer\"]\n    DocSeq --> DMod2[\"SpladePooling\"]\n    \n    Training[\"Training Phase\"] --> RouterMap[\"router_mapping in TrainingArguments\"]\n    RouterMap --> DataCollator[\"SentenceTransformerDataCollator\"]\n    DataCollator --> TaskAssign[\"task = router_mapping.get(column_name)\"]\n    TaskAssign --> TokenizeFn[\"self.tokenize_fn(inputs, task=task)\"]\n```",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 234,
      "char_count": 950,
      "start_char": 7047,
      "end_char": 7999
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:9",
    "content": "apping.get(column_name)\"]\n    TaskAssign --> TokenizeFn[\"self.tokenize_fn(inputs, task=task)\"]\n```\n\nThis enables memory-efficient asymmetric training where lightweight query encoders (e.g., `SparseStaticEmbedding`) can be combined with powerful document encoders, reducing both training and inference costs.\n\n**Sources:** [sentence_transformers/models/Router.py:217-245](), [sentence_transformers/models/Router.py:287-324](), [sentence_transformers/data_collator.py:90-118]()\n\n## Batch Sampling and Multi-Dataset Training\n\nThe `SentenceTransformerTrainer` provides memory-efficient batch sampling strategies for large-scale training:\n\n### Batch Sampler Architecture\n\n```mermaid\ngraph TD\n    TrainingArgs[\"SentenceTransformerTrainingArguments\"] --> BatchSampler[\"args.batch_sampler\"]\n    BatchSampler --> DefaultBS[\"DefaultBatchSampler\"]\n    BatchSampler --> NoDupBS[\"NoDuplicatesBatchSampler\"]\n    BatchSampler --> GroupBS[\"GroupByLabelBatchSampler\"]\n    \n    MultiDataset[\"Multi-Dataset Training\"] --> MultiBS[\"args.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 228,
      "char_count": 1017,
      "start_char": 7899,
      "end_char": 8916
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:10",
    "content": "oupBS[\"GroupByLabelBatchSampler\"]\n    \n    MultiDataset[\"Multi-Dataset Training\"] --> MultiBS[\"args.multi_dataset_batch_sampler\"]\n    MultiBS --> PropBS[\"ProportionalBatchSampler\"]\n    MultiBS --> RoundRobinBS[\"RoundRobinBatchSampler\"]\n    \n    Trainer[\"SentenceTransformerTrainer\"] --> GetBatchSampler[\"get_batch_sampler()\"]\n    GetBatchSampler --> ConcatDS[\"ConcatDataset\"]\n    ConcatDS --> GetMultiBS[\"get_multi_dataset_batch_sampler()\"]\n```\n\n**Sources:** [sentence_transformers/trainer.py:623-684](), [sentence_transformers/trainer.py:685-737](), [sentence_transformers/sampler.py:28-35]()\n\n### Memory Usage Patterns\n\n```mermaid\ngraph LR\n    subgraph Traditional[\"Traditional Training\"]\n        TB[\"Batch Size: 32\"] --> TM[\"Memory: Base\"]\n        TB2[\"Batch Size: 1024\"] --> TM2[\"Memory: 32x Base<br/>(OOM)\"]\n    end\n    \n    subgraph Cached[\"Cached Training\"]\n        CB[\"mini_batch_size: 32\"] --> CM[\"Memory: Base + Cache\"]\n        CB2[\"per_device_train_batch_size: 1024\"] --> CM2[\"Memory: Base + Cache<br/>(Constant!",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 282,
      "char_count": 1023,
      "start_char": 8816,
      "end_char": 9839
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:11",
    "content": "ache\"]\n        CB2[\"per_device_train_batch_size: 1024\"] --> CM2[\"Memory: Base + Cache<br/>(Constant!)\"]\n    end\n    \n    subgraph MultiDataset[\"Multi-Dataset Training\"]\n        MDS[\"DatasetDict\"] --> CDS[\"ConcatDataset\"]\n        CDS --> TrackDS[\"track dataset_name\"]\n        TrackDS --> LossSelect[\"loss[dataset_name]\"]\n    end\n```\n\n**Sources:** [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:100-107](), [sentence_transformers/trainer.py:416-422]()\n\n### Data Collator Memory Optimizations\n\nThe `SentenceTransformerDataCollator` includes several memory-efficient features:\n\n```mermaid\ngraph TD\n    DataCollator[\"SentenceTransformerDataCollator.__call__()\"] --> RouterMap[\"self.router_mapping\"]\n    DataCollator --> Prompts[\"self.prompts\"]\n    DataCollator --> PromptCache[\"self._prompt_length_mapping\"]\n    \n    RouterMap --> TaskAssign[\"task = router_mapping.get(column_name)\"]\n    Prompts --> PromptCheck[\"if isinstance(prompts, str)\"]\n    PromptCheck --> PromptPrefix[\"prompt + row[column_name]\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 259,
      "char_count": 1020,
      "start_char": 9739,
      "end_char": 10760
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:12",
    "content": "tCheck[\"if isinstance(prompts, str)\"]\n    PromptCheck --> PromptPrefix[\"prompt + row[column_name]\"]\n    \n    PromptCache --> GetPromptLen[\"_get_prompt_length()\"]\n    GetPromptLen --> TokenizeOnce[\"tokenize_fn([prompt], task=task)\"]\n    TokenizeOnce --> CacheLen[\"_prompt_length_mapping[(prompt, task)]\"]\n    \n    TaskAssign --> TokenizeFn[\"tokenize_fn(inputs, task=task)\"]\n    TokenizeFn --> BatchKeys[\"batch[f'{column_name}_{key}'] = value\"]\n```\n\nThe prompt length caching in `_get_prompt_length()` prevents repeated tokenization of the same prompts, significantly reducing memory overhead during data loading.\n\n**Sources:** [sentence_transformers/data_collator.py:35-119](), [sentence_transformers/data_collator.py:121-138]()\n\n## Implementation Examples\n\n### Using Cached Losses\n\n```python\n# Standard approach - memory scales with batch size\nloss = MultipleNegativesRankingLoss(model)\n\n# Cached approach - constant memory usage\nloss = CachedMultipleNegativesRankingLoss(\n    model,",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 237,
      "char_count": 983,
      "start_char": 10660,
      "end_char": 11645
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:13",
    "content": ")\n\n# Cached approach - constant memory usage\nloss = CachedMultipleNegativesRankingLoss(\n    model, \n    mini_batch_size=32,  # Controls actual memory usage\n    show_progress_bar=True\n)\n```\n\n## Trainer Memory Optimizations\n\nThe `SentenceTransformerTrainer` includes several memory-efficient features beyond cached losses:\n\n### Loss Component Tracking\n\n```mermaid\ngraph TD\n    ComputeLoss[\"SentenceTransformerTrainer.compute_loss()\"] --> TrackLoss[\"track_loss_components()\"]\n    TrackLoss --> AccumLoss[\"self.accum_loss_components[training_type]\"]\n    AccumLoss --> LogLoss[\"self.log()\"]\n    LogLoss --> NestedGather[\"self._nested_gather()\"]\n    NestedGather --> AvgLoss[\"value.sum() / steps\"]\n    \n    LossDict[\"loss: dict[str, torch.Tensor]\"] --> Stack[\"torch.stack(list(loss.values())).sum()\"]\n    Stack --> SingleLoss[\"Final Loss Tensor\"]\n```\n\nThis prevents memory spikes when losses return dictionaries with multiple components by accumulating and averaging them efficiently.\n\n**Sources:** [sentence_transformers/trainer.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 225,
      "char_count": 1024,
      "start_char": 11545,
      "end_char": 12569
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md:chunk:14",
    "content": "onents by accumulating and averaging them efficiently.\n\n**Sources:** [sentence_transformers/trainer.py:443-462](), [sentence_transformers/trainer.py:464-494](), [sentence_transformers/trainer.py:431-441]()\n\n### Training Arguments for Memory Efficiency\n\n```python\nargs = SentenceTransformerTrainingArguments(\n    per_device_train_batch_size=1024,  # Large effective batch size\n    gradient_accumulation_steps=1,     # No additional accumulation needed\n    dataloader_drop_last=True,         # Avoid uneven batches\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Memory-efficient sampling\n    multi_dataset_batch_sampler=MultiDatasetBatchSamplers.PROPORTIONAL,\n)\n```\n\n**Sources:** [sentence_transformers/trainer.py:623-684](), [sentence_transformers/training_args.py:37-39]()\n\n### Router Training Configuration\n\n```python",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "loss__losses.PListMLELossmodel___Position-aware_MLE.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 207,
      "char_count": 821,
      "start_char": 12469,
      "end_char": 13493
    }
  }
]