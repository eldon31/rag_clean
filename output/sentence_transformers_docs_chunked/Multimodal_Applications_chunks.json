[
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Multimodal_Applications.md:chunk:0",
    "content": "Multimodal applications in sentence-transformers enable processing and encoding of multiple input modalities (text, images) within unified embedding spaces. The primary implementation uses CLIP (Contrastive Language-Image Pre-training) models through the `CLIPModel` class, which supports image-text similarity, cross-modal retrieval, and multimodal semantic search.\n\nThe multimodal functionality integrates with the broader sentence-transformers ecosystem including multi-processing, evaluation frameworks, and deployment optimizations.\n\nFor text-only dense embeddings, see [SentenceTransformer Models](#5.1). For sparse embeddings, see [SparseEncoder Models](#5.2). For retrieval applications, see [Semantic Search](#6.1).\n\n## CLIPModel Architecture\n\nThe `CLIPModel` class provides the core functionality for multimodal applications by implementing the CLIP architecture within the sentence-transformers framework.\n\n**CLIPModel Component Architecture**\n```mermaid\ngraph TB\n    subgraph Input[\"Input Processing\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 194,
      "char_count": 1013,
      "start_char": 0,
      "end_char": 1014
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Multimodal_Applications.md:chunk:1",
    "content": "k.\n\n**CLIPModel Component Architecture**\n```mermaid\ngraph TB\n    subgraph Input[\"Input Processing\"]\n        IMG[\"PIL.Image objects\"]\n        TXT[\"Text strings\"]\n    end\n    \n    subgraph CLIPModel[\"CLIPModel Class\"]\n        PROC[\"CLIPProcessor\"]\n        VMODEL[\"model.vision_model\"]\n        TMODEL[\"model.text_model\"]\n        VPROJ[\"model.visual_projection\"]\n        TPROJ[\"model.text_projection\"]\n    end\n    \n    subgraph Processing[\"tokenize() Method\"]\n        IMGPROC[\"image_processor\"]\n        TXTPROC[\"tokenizer\"]\n        INFO[\"image_text_info tracking\"]\n    end\n    \n    subgraph Output[\"forward() Output\"]\n        VEMB[\"Image embeddings\"]\n        TEMB[\"Text embeddings\"]\n        UNIFIED[\"sentence_embedding tensor\"]\n    end\n    \n    IMG --> Processing\n    TXT --> Processing\n    Processing --> CLIPModel\n    CLIPModel --> Output\n```\n\nThe `CLIPModel` class inherits from `InputModule` and wraps `transformers.CLIPModel` and `transformers.CLIPProcessor` components.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 226,
      "char_count": 971,
      "start_char": 914,
      "end_char": 1885
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Multimodal_Applications.md:chunk:2",
    "content": "s from `InputModule` and wraps `transformers.CLIPModel` and `transformers.CLIPProcessor` components. It implements the `tokenize()` and `forward()` methods required by the sentence-transformers module system.\n\nSources: [sentence_transformers/models/CLIPModel.py:15-26](), [sentence_transformers/models/CLIPModel.py:70-92]()\n\n## Input Processing and Tokenization\n\nThe CLIP model handles mixed input types through its `tokenize` method, which can process both PIL Images and text strings in the same batch.\n\n```mermaid\ngraph LR\n    subgraph InputBatch[\"Mixed Input Batch\"]\n        IMG1[\"PIL.Image\"]\n        TXT1[\"'A dog in snow'\"]\n        IMG2[\"PIL.Image\"] \n        TXT2[\"'A cat on table'\"]\n    end\n    \n    subgraph Tokenization[\"tokenize() Method\"]\n        CLASSIFY[\"Classify Input Types\"]\n        IMGPROC[\"Image Processing\"]\n        TXTPROC[\"Text Tokenization\"]\n        MERGE[\"Merge Features\"]\n    end\n    \n    subgraph Features[\"Feature Tensors\"]\n        PIXELS[\"pixel_values\"]\n        TOKENS[\"input_ids\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 248,
      "char_count": 1007,
      "start_char": 1785,
      "end_char": 2793
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Multimodal_Applications.md:chunk:3",
    "content": "subgraph Features[\"Feature Tensors\"]\n        PIXELS[\"pixel_values\"]\n        TOKENS[\"input_ids\"]\n        MASK[\"attention_mask\"]\n        INFO[\"image_text_info\"]\n    end\n    \n    InputBatch --> CLASSIFY\n    CLASSIFY --> IMGPROC\n    CLASSIFY --> TXTPROC\n    IMGPROC --> PIXELS\n    TXTPROC --> TOKENS\n    TXTPROC --> MASK\n    CLASSIFY --> INFO\n    MERGE --> Features\n```\n\nThe `image_text_info` list tracks which inputs are images (0) versus text (1), enabling proper routing during the forward pass.\n\nSources: [sentence_transformers/models/CLIPModel.py:70-92]()\n\n## Forward Pass and Embedding Generation\n\nThe forward method processes mixed inputs and generates embeddings in a unified vector space:\n\n| Processing Step | Image Inputs | Text Inputs |\n|----------------|--------------|-------------|\n| Feature Extraction | `vision_model(pixel_values)` | `text_model(input_ids, attention_mask)` |\n| Projection | `visual_projection()` | `text_projection()` |\n| Output Format | `image_embeds` | `text_embeds` |\n\n```mermaid\ngraph TB",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 242,
      "char_count": 1020,
      "start_char": 2693,
      "end_char": 3718
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Multimodal_Applications.md:chunk:4",
    "content": "()` | `text_projection()` |\n| Output Format | `image_embeds` | `text_embeds` |\n\n```mermaid\ngraph TB\n    subgraph Forward[\"forward() Method\"]\n        CHECK[\"Check Input Types\"]\n        VISION[\"Process Images\"]\n        TEXT[\"Process Text\"]\n        REORDER[\"Reorder by image_text_info\"]\n    end\n    \n    subgraph VisionPath[\"Vision Processing\"]\n        VFORWARD[\"vision_model()\"]\n        VPROJECTION[\"visual_projection()\"]\n    end\n    \n    subgraph TextPath[\"Text Processing\"] \n        TFORWARD[\"text_model()\"]\n        TPROJECTION[\"text_projection()\"]\n    end\n    \n    CHECK --> VISION\n    CHECK --> TEXT\n    VISION --> VisionPath\n    TEXT --> TextPath\n    VisionPath --> REORDER\n    TextPath --> REORDER\n    REORDER --> UNIFIED[\"sentence_embedding tensor\"]\n```\n\nThe method maintains input order by using the `image_text_info` list to correctly sequence embeddings in the output tensor.\n\nSources: [sentence_transformers/models/CLIPModel.py:38-68]()\n\n## Usage Patterns\n\n### Basic Image-Text Similarity\n\n```python",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 243,
      "char_count": 1008,
      "start_char": 3618,
      "end_char": 4627
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\Multimodal_Applications.md:chunk:5",
    "content": "formers/models/CLIPModel.py:38-68]()\n\n## Usage Patterns\n\n### Basic Image-Text Similarity\n\n```python\nfrom sentence_transformers import SentenceTransformer\nfrom PIL import Image\n\nmodel = SentenceTransformer('clip-ViT-B-32')\n\n# Encode image\nimage = Image.open('path/to/image.jpg')\nimg_embedding = model.encode(image)\n\n# Encode text descriptions  \ntexts = [\"A dog in the snow\", \"A cat on a table\"]\ntext_embeddings = model.encode(texts)\n\n# Compute similarities\nsimilarities = model.similarity(img_embedding, text_embeddings)\n```\n\n### Mixed Batch Processing\n\nThe CLIP model can process images and text in the same batch call:\n\n```python",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\Multimodal_Applications.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "Multimodal_Applications.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 147,
      "char_count": 630,
      "start_char": 4527,
      "end_char": 5551
    }
  }
]