[
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\CrossEncoder_Training.md:chunk:0",
    "content": "This document covers the training system for CrossEncoder models in sentence-transformers. CrossEncoders are designed for reranking and classification tasks where two texts are jointly encoded to produce similarity scores or class predictions.\n\nFor information about training SentenceTransformer models (bi-encoders), see [3.1](#3.1). For SparseEncoder training, see [3.2](#3.2). For loss function details specific to CrossEncoders, see [3.6](#3.6).\n\n## CrossEncoder Training Architecture\n\nCrossEncoder training follows a similar pattern to other model types in sentence-transformers but with specific adaptations for joint text encoding and ranking/classification tasks.\n\n**CrossEncoder Training System Overview**\n```mermaid\ngraph TB\n    subgraph \"Core Components\"\n        CE[CrossEncoder]\n        CETrainer[CrossEncoderTrainer]\n        CEArgs[CrossEncoderTrainingArguments]\n        CELosses[CrossEncoder Losses]\n        CEEvals[CrossEncoder Evaluators]\n    end\n    \n    subgraph \"Data Processing\"\n        Dataset[datasets.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "CrossEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 217,
      "char_count": 1024,
      "start_char": 0,
      "end_char": 1024
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\CrossEncoder_Training.md:chunk:1",
    "content": "Evals[CrossEncoder Evaluators]\n    end\n    \n    subgraph \"Data Processing\"\n        Dataset[datasets.Dataset]\n        DataCollator[Data Collator]\n        HardNegMining[Hard Negatives Mining]\n    end\n    \n    subgraph \"Loss Functions\"\n        BCE[BinaryCrossEntropyLoss]\n        MNR[MultipleNegativesRankingLoss]\n        Lambda[LambdaLoss]\n        ListNet[ListNetLoss]\n        CrossEntropy[CrossEntropyLoss]\n    end\n    \n    subgraph \"Training Infrastructure\"\n        HFTrainer[Transformers Trainer]\n        ModelCard[Model Card Generation]\n        HFHub[Hugging Face Hub]\n    end\n    \n    CE --> CETrainer\n    Dataset --> DataCollator\n    CEArgs --> CETrainer\n    CELosses --> CETrainer\n    CEEvals --> CETrainer\n    \n    BCE --> CELosses\n    MNR --> CELosses\n    Lambda --> CELosses\n    ListNet --> CELosses\n    CrossEntropy --> CELosses\n    \n    CETrainer --> HFTrainer\n    CETrainer --> ModelCard\n    ModelCard --> HFHub\n    \n    HardNegMining --> Dataset\n```\n\nSources: [docs/cross_encoder/training_overview.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "CrossEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 254,
      "char_count": 1010,
      "start_char": 924,
      "end_char": 1934
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\CrossEncoder_Training.md:chunk:2",
    "content": "rd --> HFHub\n    \n    HardNegMining --> Dataset\n```\n\nSources: [docs/cross_encoder/training_overview.md:1-500](), [docs/cross_encoder/loss_overview.md:1-100]()\n\n## Training Components\n\nCrossEncoder training involves six main components that work together to fine-tune models for ranking and classification tasks.\n\n**CrossEncoder Training Data Flow**\n```mermaid\ngraph LR\n    subgraph \"Input Data\"\n        TextPairs[\"(text_A, text_B) pairs\"]\n        Triplets[\"(query, positive, negative)\"]\n        Rankings[\"(query, [doc1, doc2, ...])\"]\n        Labels[Class Labels / Scores]\n    end\n    \n    subgraph \"Data Processing\"\n        DataCollator[\"Data Collator\"]\n        Tokenization[Tokenization]\n        BatchFormat[Batch Formatting]\n    end\n    \n    subgraph \"Model & Loss\"\n        CrossEncoder[CrossEncoder Model]\n        LossFunction[Loss Function]\n        ForwardPass[Forward Pass]\n    end\n    \n    subgraph \"Training Loop\"\n        Optimizer[Optimizer]\n        BackwardPass[Backward Pass]\n        WeightUpdate[Weight Update]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "CrossEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 245,
      "char_count": 1021,
      "start_char": 1834,
      "end_char": 2856
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\CrossEncoder_Training.md:chunk:3",
    "content": "Optimizer[Optimizer]\n        BackwardPass[Backward Pass]\n        WeightUpdate[Weight Update]\n    end\n    \n    subgraph \"Evaluation\"\n        Evaluator[CrossEncoder Evaluator]\n        Metrics[Metrics Calculation]\n    end\n    \n    TextPairs --> DataCollator\n    Triplets --> DataCollator\n    Rankings --> DataCollator\n    Labels --> DataCollator\n    \n    DataCollator --> Tokenization\n    Tokenization --> BatchFormat\n    \n    BatchFormat --> CrossEncoder\n    CrossEncoder --> ForwardPass\n    ForwardPass --> LossFunction\n    \n    LossFunction --> BackwardPass\n    BackwardPass --> Optimizer\n    Optimizer --> WeightUpdate\n    \n    CrossEncoder --> Evaluator\n    Evaluator --> Metrics\n```\n\nSources: [docs/cross_encoder/training_overview.md:170-190](), [sentence_transformers/data_collator.py:35-120]()\n\n### Model Initialization\n\nCrossEncoder models are initialized by loading a pretrained transformers model with a sequence classification head. If the model doesn't have such a head, it's added automatically.\n\n```python",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "CrossEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 231,
      "char_count": 1017,
      "start_char": 2756,
      "end_char": 3781
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\CrossEncoder_Training.md:chunk:4",
    "content": "ce classification head. If the model doesn't have such a head, it's added automatically.\n\n```python\nfrom sentence_transformers import CrossEncoder",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "CrossEncoder_Training.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 29,
      "char_count": 146,
      "start_char": 3681,
      "end_char": 4705
    }
  }
]