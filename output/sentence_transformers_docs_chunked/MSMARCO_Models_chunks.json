[
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:0",
    "content": "This document covers specialized pretrained models trained on the MS MARCO dataset for passage retrieval tasks. These models are optimized for semantic search scenarios where queries need to be matched against relevant passages. For general dense embedding models, see [SentenceTransformer Models](#5.1). For cross-encoder models trained on other datasets, see [CrossEncoder Models](#5.3).\n\n## Dataset Overview\n\nMS MARCO (Microsoft Machine Reading Comprehension) is a large-scale information retrieval corpus created from real user search queries using the Bing search engine. The dataset consists of:\n\n- **Training data**: Over 500,000 query-passage examples\n- **Complete corpus**: Over 8.8 million passages  \n- **Evaluation**: TREC Deep Learning 2019 and MS MARCO Passage Retrieval datasets\n- **Task type**: Asymmetric semantic search (short queries → longer passages)",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 183,
      "char_count": 870,
      "start_char": 0,
      "end_char": 872
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:1",
    "content": "e Retrieval datasets\n- **Task type**: Asymmetric semantic search (short queries → longer passages)\n\nThe dataset enables training models that can find semantically relevant passages given natural language queries, making it ideal for search and question-answering applications.\n\n**Sources:** [docs/pretrained-models/msmarco-v3.md:1-5](), [docs/pretrained-models/ce-msmarco.md:1-6]()\n\n## Model Architecture Types\n\nMS MARCO models are available across multiple architectures, each optimized for different use cases in the retrieval pipeline:\n\n### Code Entity Mapping\n\n```mermaid\ngraph TB\n    subgraph \"sentence_transformers Classes\"\n        ST[SentenceTransformer]\n        CE[CrossEncoder]\n        \n        ST_INIT[\"SentenceTransformer(__init__)\"]\n        ST_ENCODE[\"encode()\"]\n        CE_INIT[\"CrossEncoder(__init__)\"]\n        CE_PREDICT[\"predict()\"]\n        \n        ST --> ST_INIT\n        ST --> ST_ENCODE\n        CE --> CE_INIT\n        CE --> CE_PREDICT\n    end\n    \n    subgraph \"Utility Functions\"\n        UTIL_COS[\"util.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 228,
      "char_count": 1024,
      "start_char": 772,
      "end_char": 1796
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:2",
    "content": "INIT\n        CE --> CE_PREDICT\n    end\n    \n    subgraph \"Utility Functions\"\n        UTIL_COS[\"util.cos_sim()\"]\n        UTIL_DOT[\"util.dot_score()\"]\n        TORCH_SIG[\"torch.nn.Sigmoid()\"]\n    end\n    \n    subgraph \"Model Identifiers\"\n        DENSE_MODELS[\"msmarco-distilbert-base-v4<br/>msmarco-MiniLM-L6-v3<br/>msmarco-roberta-base-v3<br/>msmarco-distilbert-base-tas-b\"]\n        CE_MODELS[\"cross-encoder/ms-marco-MiniLM-L6-v2<br/>cross-encoder/ms-marco-electra-base<br/>cross-encoder/ms-marco-TinyBERT-L2-v2\"]\n    end\n    \n    ST_INIT --> DENSE_MODELS\n    CE_INIT --> CE_MODELS\n    CE_INIT --> TORCH_SIG\n    \n    ST_ENCODE --> UTIL_COS\n    ST_ENCODE --> UTIL_DOT\n    CE_PREDICT --> SCORES[\"Relevance Scores Array\"]\n```\n\n### Training Components\n\n```mermaid\ngraph LR\n    subgraph \"Loss Functions\"\n        MSE_LOSS[\"MultipleNegativesRankingLoss\"]\n        COSENT_LOSS[\"CoSENTLoss\"] \n        MARGIN_MSE[\"MarginMSELoss\"]\n    end\n    \n    subgraph \"Evaluators\"\n        IR_EVAL[\"InformationRetrievalEvaluator\"]",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 287,
      "char_count": 1004,
      "start_char": 1696,
      "end_char": 2701
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:3",
    "content": "inMSELoss\"]\n    end\n    \n    subgraph \"Evaluators\"\n        IR_EVAL[\"InformationRetrievalEvaluator\"]\n        EMB_EVAL[\"EmbeddingSimilarityEvaluator\"]\n        CE_EVAL[\"CrossEncoderReranking\"]\n    end\n    \n    subgraph \"Datasets\"\n        MSMARCO_TRAIN[\"sentence-transformers/msmarco-*\"]\n        TREC_DL[\"TREC Deep Learning 2019\"]\n        MSMARCO_DEV[\"MS MARCO Dev Set\"]\n    end\n    \n    MSE_LOSS --> MSMARCO_TRAIN\n    MARGIN_MSE --> MSMARCO_TRAIN\n    IR_EVAL --> TREC_DL\n    IR_EVAL --> MSMARCO_DEV\n    CE_EVAL --> TREC_DL\n```\n\n**Sources:** [docs/pretrained-models/msmarco-v3.md:6-16](), [docs/cross_encoder/pretrained_models.md:27-44]()\n\n## Dense Embedding Models\n\nDense embedding models use the `SentenceTransformer` class and encode queries and passages into dense vector representations for efficient similarity search:\n\n### Cosine Similarity Models\n\nThese models are optimized for cosine similarity computation and tend to prefer shorter, more focused passages:",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 244,
      "char_count": 963,
      "start_char": 2601,
      "end_char": 3566
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:4",
    "content": "are optimized for cosine similarity computation and tend to prefer shorter, more focused passages:\n\n| Model Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Queries/sec (GPU/CPU) | Docs/sec (GPU/CPU) |\n|------------|---------------------|----------------------|---------------------|-------------------|\n| `msmarco-MiniLM-L6-v3` | 67.46 | 32.27 | 18,000 / 750 | 2,800 / 180 |\n| `msmarco-MiniLM-L12-v3` | 65.14 | 32.75 | 11,000 / 400 | 1,500 / 90 |\n| `msmarco-distilbert-base-v3` | 69.02 | 33.13 | 7,000 / 350 | 1,100 / 70 |\n| `msmarco-distilbert-base-v4` | **70.24** | **33.79** | 7,000 / 350 | 1,100 / 70 |\n| `msmarco-roberta-base-v3` | 69.08 | 33.01 | 4,000 / 170 | 540 / 30 |\n\n### Dot Product Models\n\nThese models are optimized for dot product similarity and tend to prefer longer, more comprehensive passages:\n\n| Model Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Queries/sec (GPU/CPU) | Docs/sec (GPU/CPU) |",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 402,
      "char_count": 926,
      "start_char": 3466,
      "end_char": 4393
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:5",
    "content": "Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Queries/sec (GPU/CPU) | Docs/sec (GPU/CPU) |\n|------------|---------------------|----------------------|---------------------|-------------------|\n| `msmarco-distilbert-base-dot-prod-v3` | 68.42 | 33.04 | 7,000 / 350 | 1,100 / 70 |\n| `msmarco-roberta-base-ance-firstp` | 67.84 | 33.01 | 4,000 / 170 | 540 / 30 |\n| `msmarco-distilbert-base-tas-b` | **71.04** | **34.43** | 7,000 / 350 | 1,100 / 70 |\n\n**Sources:** [docs/pretrained-models/msmarco-v3.md:27-44]()\n\n## Cross-Encoder Models\n\nCross-encoder models use the `CrossEncoder` class for reranking retrieved passages. They process query-passage pairs jointly and output relevance scores:\n\n| Model Name | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Docs/sec |\n|------------|---------------------|----------------------|----------|\n| `cross-encoder/ms-marco-TinyBERT-L2-v2` | 69.84 | 32.56 | 9,000 |\n| `cross-encoder/ms-marco-MiniLM-L2-v2` | 71.01 | 34.85 | 4,100 |\n| `cross-encoder/ms-marco-MiniLM-L4-v2` | 73.",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 414,
      "char_count": 1018,
      "start_char": 4293,
      "end_char": 5312
    }
  },
  {
    "chunk_id": "sentence_transformers_docs:UKPLab\\sentence-transformers\\MSMARCO_Models.md:chunk:6",
    "content": "coder/ms-marco-MiniLM-L2-v2` | 71.01 | 34.85 | 4,100 |\n| `cross-encoder/ms-marco-MiniLM-L4-v2` | 73.04 | 37.70 | 2,500 |\n| `cross-encoder/ms-marco-MiniLM-L6-v2` | **74.30** | **39.01** | 1,800 |\n| `cross-encoder/ms-marco-MiniLM-L12-v2` | 74.31 | 39.02 | 960 |\n| `cross-encoder/ms-marco-electra-base` | 71.99 | 36.41 | 340 |\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:35-43](), [docs/pretrained-models/ce-msmarco.md:41-48]()\n\n## Usage Examples\n\n### Dense Embedding Model Usage\n\n```python\nfrom sentence_transformers import SentenceTransformer, util",
    "metadata": {
      "source_file": "UKPLab\\sentence-transformers\\MSMARCO_Models.md",
      "source_collection": "sentence_transformers_docs",
      "subdirectory": "UKPLab",
      "filename": "MSMARCO_Models.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 244,
      "char_count": 558,
      "start_char": 5212,
      "end_char": 6236
    }
  }
]