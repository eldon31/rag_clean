{
  "collection": "qdrant_ecosystem",
  "subdirectory": "qdrant_examples",
  "total_chunks": 142,
  "chunks": [
    {
      "content": "qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu",
      "index": 0,
      "token_count": 591,
      "metadata": {
        "title": "_qdrant_examples",
        "source": "qdrant_examples\\_qdrant_examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples.md",
        "file_name": "_qdrant_examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.214518",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2048
    },
    {
      "content": ".md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Overview\n\nRelevant source files\n\n- [.gitignore](https://github.com/qdrant/examples/blob/b3c4b28f/.gitignore)\n- [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb)\n- [README.md](https://github.com/qdrant/examples/blob/b3c4b28f/README.md)\n- [code-search/code-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb)\n- [multivector-representation/multivector\\_representation\\_qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb)\n- [sparse-vectors-movies-reco/recommend-movies.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb)\n\nThis wiki provides documentation for the [Qdrant Examples](<https://github.com/qdrant/examples/blob/b3c4b28f/Qdrant Examples>) repository, a collection of tutorials, demos, and how-to guides demonstrating the use of Qdrant vector database and adjacent technologies for various applications. From basic similarity search to advanced retrieval-augmented generation (RAG) systems, these examples showcase real-world implementations across different data modalities (text, images, audio) and use cases.\n\nSources: [README.md1-3](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L1-L3)\n\n## Purpose and Scope\n\nThe Qdrant Examples repository aims to demonstrate practical implementations of vector search technology using Qdrant. The examples progress from basic vector database operations to sophisticated AI applications, serving both educational and reference purposes for developers.\n\nKey aspects covered:\n\n- Basic vector operations and similarity search\n- Domain-specific applications for text, image, and audio data\n- Advanced AI integrations such as RAG systems and agentic frameworks",
      "index": 1,
      "token_count": 507,
      "metadata": {
        "title": "_qdrant_examples",
        "source": "qdrant_examples\\_qdrant_examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples.md",
        "file_name": "_qdrant_examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.214518",
        "total_chunks": 6
      },
      "start_char": 1948,
      "end_char": 3937
    },
    {
      "content": "r text, image, and audio data\n- Advanced AI integrations such as RAG systems and agentic frameworks\n- Integration patterns with other technologies (OpenAI, Cohere, CLIP, etc.)\n\nFor specifics on getting started with basic Qdrant functionality, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md).\n\nSources: [README.md3-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L3-L17)\n\n## Repository Structure\n\nThe repository is organized into categories based on complexity and data modality, allowing users to find relevant examples for their specific needs.\n\n```\n```\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Example Types and Progression\n\nThe examples in the repository follow a progression from basic to advanced, demonstrating increasingly sophisticated applications of vector search technology.\n\n| Level        | Example Type                  | Technologies                              | Examples                                                    |\n| ------------ | ----------------------------- | ----------------------------------------- | ----------------------------------------------------------- |\n| Basic        | Fundamental vector operations | Qdrant, NumPy, Faker                      | Qdrant 101 Getting Started                                  |\n| Intermediate | Domain-specific applications  | Transformers, CLIP, Sentence-Transformers | Code Search, E-commerce Image Search, Movie Recommendations |\n| Advanced     | AI integration systems        | OpenAI, LlamaIndex, Cohere, CrewAI        | Recency-Aware RAG, Graph-Enhanced RAG, Agentic Systems      |\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Core Processing Pipeline\n\nMost examples in the repository follow a similar data processing pattern despite addressing different domains and use cases.\n\n```\n```\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17) [code-search/code-search.",
      "index": 2,
      "token_count": 449,
      "metadata": {
        "title": "_qdrant_examples",
        "source": "qdrant_examples\\_qdrant_examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples.md",
        "file_name": "_qdrant_examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.214518",
        "total_chunks": 6
      },
      "start_char": 3837,
      "end_char": 5881
    },
    {
      "content": "md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17) [code-search/code-search.ipynb6-9](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L6-L9) [sparse-vectors-movies-reco/recommend-movies.ipynb7-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L7-L25)\n\n## Vector Search Implementations\n\nThe repository demonstrates various vector search implementations across different data modalities:\n\n### Text Data Applications\n\nText-based examples showcase how to build search applications for natural language, code, and structured text data using embeddings.\n\n```\n```\n\nKey text applications include:\n\n- Code search using dual embeddings (general-purpose and code-specific)\n- Extractive question answering\n- Movie recommendations using collaborative filtering with sparse vectors\n\nSources: [README.md9-10](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L9-L10) [README.md16](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L16-L16) [code-search/code-search.ipynb6-9](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L6-L9) [sparse-vectors-movies-reco/recommend-movies.ipynb7-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L7-L25)\n\n### Image Data Applications\n\nImage-based examples demonstrate how to build visual search systems across domains:\n\n```\n```\n\nKey image applications include:\n\n- E-commerce reverse image search using CLIP embeddings\n- Medical image similarity search with vision transformers\n\nSources: [README.md12](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L12-L12) [README.md4](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L4-L4)\n\n### Audio Data Applications\n\nAudio examples showcase music recommendation and audio similarity search:\n\n```\n```\n\nKey audio applications include:\n\n- Music recommendation systems using audio feature embeddings\n\nSources: [README.md11](https://github.",
      "index": 3,
      "token_count": 549,
      "metadata": {
        "title": "_qdrant_examples",
        "source": "qdrant_examples\\_qdrant_examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples.md",
        "file_name": "_qdrant_examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.214518",
        "total_chunks": 6
      },
      "start_char": 5781,
      "end_char": 7807
    },
    {
      "content": "Music recommendation systems using audio feature embeddings\n\nSources: [README.md11](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L11-L11) [README.md5](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L5)\n\n## Advanced AI Integrations\n\nThe repository includes advanced examples integrating Qdrant with modern AI frameworks:\n\n### Retrieval-Augmented Generation (RAG) Systems\n\n```\n```\n\nKey RAG examples include:\n\n- Recency-aware RAG with LlamaIndex\n- Graph-enhanced RAG with Neo4j integration\n- Basic RAG pipelines with various LLM providers\n\nSources: [README.md8-9](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L8-L9) [README.md6](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L6-L6)\n\n### Agentic Systems\n\n```\n```\n\nKey agentic examples include:\n\n- Multi-agent systems using CrewAI for orchestration\n- Meeting analysis with agentic RAG\n\nSources: [README.md7](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L7-L7)\n\n## Integration Technologies\n\nThe examples in the repository integrate with various AI and data processing technologies:\n\n| Category         | Technologies                                  | Purpose                                     |\n| ---------------- | --------------------------------------------- | ------------------------------------------- |\n| Embedding Models | SentenceTransformers, CLIP, OpenL3, FastEmbed | Converting data to vector representations   |\n| Language Models  | OpenAI, Cohere, Hugging Face                  | Text generation, reranking, embeddings      |\n| Frameworks       | LlamaIndex, LangChain, CrewAI                 | Building LLM applications and agent systems |\n| Infrastructure   | AWS Lambda, Hugging Face Spaces               | Deployment and hosting                      |\n| Databases        | Neo4j, Qdrant                                 | Graph data, vector storage                  |\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Starting with the Examples",
      "index": 4,
      "token_count": 480,
      "metadata": {
        "title": "_qdrant_examples",
        "source": "qdrant_examples\\_qdrant_examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples.md",
        "file_name": "_qdrant_examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.214518",
        "total_chunks": 6
      },
      "start_char": 7707,
      "end_char": 9735
    },
    {
      "content": "(https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Starting with the Examples\n\nEach example in the repository is self-contained and includes the necessary code and documentation to understand and run the implementation. To get started:\n\n1. Clone the repository: `git clone https://github.com/qdrant/examples.git`\n2. Navigate to the specific example directory\n3. Follow the README or notebook instructions within each example\n\nFor fundamental Qdrant concepts and operations, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md).\n\nSources: [README.md3-4](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L3-L4)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Overview](#overview.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [Repository Structure](#repository-structure.md)\n- [Example Types and Progression](#example-types-and-progression.md)\n- [Core Processing Pipeline](#core-processing-pipeline.md)\n- [Vector Search Implementations](#vector-search-implementations.md)\n- [Text Data Applications](#text-data-applications.md)\n- [Image Data Applications](#image-data-applications.md)\n- [Audio Data Applications](#audio-data-applications.md)\n- [Advanced AI Integrations](#advanced-ai-integrations.md)\n- [Retrieval-Augmented Generation (RAG) Systems](#retrieval-augmented-generation-rag-systems.md)\n- [Agentic Systems](#agentic-systems.md)\n- [Integration Technologies](#integration-technologies.md)\n- [Starting with the Examples](#starting-with-the-examples.md)",
      "index": 5,
      "token_count": 384,
      "metadata": {
        "title": "_qdrant_examples",
        "source": "qdrant_examples\\_qdrant_examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples.md",
        "file_name": "_qdrant_examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.214518",
        "total_chunks": 6
      },
      "start_char": 9635,
      "end_char": 11683
    },
    {
      "content": "qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu",
      "index": 0,
      "token_count": 591,
      "metadata": {
        "title": "_qdrant_examples_1-overview",
        "source": "qdrant_examples\\_qdrant_examples_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_1-overview.md",
        "file_name": "_qdrant_examples_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.227480",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2048
    },
    {
      "content": ".md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Overview\n\nRelevant source files\n\n- [.gitignore](https://github.com/qdrant/examples/blob/b3c4b28f/.gitignore)\n- [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb)\n- [README.md](https://github.com/qdrant/examples/blob/b3c4b28f/README.md)\n- [code-search/code-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb)\n- [multivector-representation/multivector\\_representation\\_qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb)\n- [sparse-vectors-movies-reco/recommend-movies.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb)\n\nThis wiki provides documentation for the [Qdrant Examples](<https://github.com/qdrant/examples/blob/b3c4b28f/Qdrant Examples>) repository, a collection of tutorials, demos, and how-to guides demonstrating the use of Qdrant vector database and adjacent technologies for various applications. From basic similarity search to advanced retrieval-augmented generation (RAG) systems, these examples showcase real-world implementations across different data modalities (text, images, audio) and use cases.\n\nSources: [README.md1-3](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L1-L3)\n\n## Purpose and Scope\n\nThe Qdrant Examples repository aims to demonstrate practical implementations of vector search technology using Qdrant. The examples progress from basic vector database operations to sophisticated AI applications, serving both educational and reference purposes for developers.\n\nKey aspects covered:\n\n- Basic vector operations and similarity search\n- Domain-specific applications for text, image, and audio data\n- Advanced AI integrations such as RAG systems and agentic frameworks",
      "index": 1,
      "token_count": 507,
      "metadata": {
        "title": "_qdrant_examples_1-overview",
        "source": "qdrant_examples\\_qdrant_examples_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_1-overview.md",
        "file_name": "_qdrant_examples_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.227480",
        "total_chunks": 6
      },
      "start_char": 1948,
      "end_char": 3937
    },
    {
      "content": "r text, image, and audio data\n- Advanced AI integrations such as RAG systems and agentic frameworks\n- Integration patterns with other technologies (OpenAI, Cohere, CLIP, etc.)\n\nFor specifics on getting started with basic Qdrant functionality, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md).\n\nSources: [README.md3-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L3-L17)\n\n## Repository Structure\n\nThe repository is organized into categories based on complexity and data modality, allowing users to find relevant examples for their specific needs.\n\n```\n```\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Example Types and Progression\n\nThe examples in the repository follow a progression from basic to advanced, demonstrating increasingly sophisticated applications of vector search technology.\n\n| Level        | Example Type                  | Technologies                              | Examples                                                    |\n| ------------ | ----------------------------- | ----------------------------------------- | ----------------------------------------------------------- |\n| Basic        | Fundamental vector operations | Qdrant, NumPy, Faker                      | Qdrant 101 Getting Started                                  |\n| Intermediate | Domain-specific applications  | Transformers, CLIP, Sentence-Transformers | Code Search, E-commerce Image Search, Movie Recommendations |\n| Advanced     | AI integration systems        | OpenAI, LlamaIndex, Cohere, CrewAI        | Recency-Aware RAG, Graph-Enhanced RAG, Agentic Systems      |\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Core Processing Pipeline\n\nMost examples in the repository follow a similar data processing pattern despite addressing different domains and use cases.\n\n```\n```\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17) [code-search/code-search.",
      "index": 2,
      "token_count": 449,
      "metadata": {
        "title": "_qdrant_examples_1-overview",
        "source": "qdrant_examples\\_qdrant_examples_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_1-overview.md",
        "file_name": "_qdrant_examples_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.227480",
        "total_chunks": 6
      },
      "start_char": 3837,
      "end_char": 5881
    },
    {
      "content": "md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17) [code-search/code-search.ipynb6-9](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L6-L9) [sparse-vectors-movies-reco/recommend-movies.ipynb7-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L7-L25)\n\n## Vector Search Implementations\n\nThe repository demonstrates various vector search implementations across different data modalities:\n\n### Text Data Applications\n\nText-based examples showcase how to build search applications for natural language, code, and structured text data using embeddings.\n\n```\n```\n\nKey text applications include:\n\n- Code search using dual embeddings (general-purpose and code-specific)\n- Extractive question answering\n- Movie recommendations using collaborative filtering with sparse vectors\n\nSources: [README.md9-10](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L9-L10) [README.md16](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L16-L16) [code-search/code-search.ipynb6-9](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L6-L9) [sparse-vectors-movies-reco/recommend-movies.ipynb7-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L7-L25)\n\n### Image Data Applications\n\nImage-based examples demonstrate how to build visual search systems across domains:\n\n```\n```\n\nKey image applications include:\n\n- E-commerce reverse image search using CLIP embeddings\n- Medical image similarity search with vision transformers\n\nSources: [README.md12](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L12-L12) [README.md4](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L4-L4)\n\n### Audio Data Applications\n\nAudio examples showcase music recommendation and audio similarity search:\n\n```\n```\n\nKey audio applications include:\n\n- Music recommendation systems using audio feature embeddings\n\nSources: [README.md11](https://github.",
      "index": 3,
      "token_count": 549,
      "metadata": {
        "title": "_qdrant_examples_1-overview",
        "source": "qdrant_examples\\_qdrant_examples_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_1-overview.md",
        "file_name": "_qdrant_examples_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.227480",
        "total_chunks": 6
      },
      "start_char": 5781,
      "end_char": 7807
    },
    {
      "content": "Music recommendation systems using audio feature embeddings\n\nSources: [README.md11](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L11-L11) [README.md5](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L5)\n\n## Advanced AI Integrations\n\nThe repository includes advanced examples integrating Qdrant with modern AI frameworks:\n\n### Retrieval-Augmented Generation (RAG) Systems\n\n```\n```\n\nKey RAG examples include:\n\n- Recency-aware RAG with LlamaIndex\n- Graph-enhanced RAG with Neo4j integration\n- Basic RAG pipelines with various LLM providers\n\nSources: [README.md8-9](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L8-L9) [README.md6](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L6-L6)\n\n### Agentic Systems\n\n```\n```\n\nKey agentic examples include:\n\n- Multi-agent systems using CrewAI for orchestration\n- Meeting analysis with agentic RAG\n\nSources: [README.md7](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L7-L7)\n\n## Integration Technologies\n\nThe examples in the repository integrate with various AI and data processing technologies:\n\n| Category         | Technologies                                  | Purpose                                     |\n| ---------------- | --------------------------------------------- | ------------------------------------------- |\n| Embedding Models | SentenceTransformers, CLIP, OpenL3, FastEmbed | Converting data to vector representations   |\n| Language Models  | OpenAI, Cohere, Hugging Face                  | Text generation, reranking, embeddings      |\n| Frameworks       | LlamaIndex, LangChain, CrewAI                 | Building LLM applications and agent systems |\n| Infrastructure   | AWS Lambda, Hugging Face Spaces               | Deployment and hosting                      |\n| Databases        | Neo4j, Qdrant                                 | Graph data, vector storage                  |\n\nSources: [README.md5-17](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Starting with the Examples",
      "index": 4,
      "token_count": 480,
      "metadata": {
        "title": "_qdrant_examples_1-overview",
        "source": "qdrant_examples\\_qdrant_examples_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_1-overview.md",
        "file_name": "_qdrant_examples_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.227480",
        "total_chunks": 6
      },
      "start_char": 7707,
      "end_char": 9735
    },
    {
      "content": "(https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L5-L17)\n\n## Starting with the Examples\n\nEach example in the repository is self-contained and includes the necessary code and documentation to understand and run the implementation. To get started:\n\n1. Clone the repository: `git clone https://github.com/qdrant/examples.git`\n2. Navigate to the specific example directory\n3. Follow the README or notebook instructions within each example\n\nFor fundamental Qdrant concepts and operations, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md).\n\nSources: [README.md3-4](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L3-L4)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Overview](#overview.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [Repository Structure](#repository-structure.md)\n- [Example Types and Progression](#example-types-and-progression.md)\n- [Core Processing Pipeline](#core-processing-pipeline.md)\n- [Vector Search Implementations](#vector-search-implementations.md)\n- [Text Data Applications](#text-data-applications.md)\n- [Image Data Applications](#image-data-applications.md)\n- [Audio Data Applications](#audio-data-applications.md)\n- [Advanced AI Integrations](#advanced-ai-integrations.md)\n- [Retrieval-Augmented Generation (RAG) Systems](#retrieval-augmented-generation-rag-systems.md)\n- [Agentic Systems](#agentic-systems.md)\n- [Integration Technologies](#integration-technologies.md)\n- [Starting with the Examples](#starting-with-the-examples.md)",
      "index": 5,
      "token_count": 384,
      "metadata": {
        "title": "_qdrant_examples_1-overview",
        "source": "qdrant_examples\\_qdrant_examples_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_1-overview.md",
        "file_name": "_qdrant_examples_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.227480",
        "total_chunks": 6
      },
      "start_char": 9635,
      "end_char": 11683
    },
    {
      "content": "Getting Started with Qdrant | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 589,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2035
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Getting Started with Qdrant\n\nRelevant source files\n\n- [qdrant\\_101\\_getting\\_started/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md)\n- [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb)\n- [qdrant\\_101\\_text\\_data/02\\_qdrant\\_101\\_text\\_files/02\\_qdrant\\_101\\_text\\_22\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/02_qdrant_101_text_files/02_qdrant_101_text_22_0.png)\n- [qdrant\\_101\\_text\\_data/02\\_qdrant\\_101\\_text\\_files/02\\_qdrant\\_101\\_text\\_25\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/02_qdrant_101_text_files/02_qdrant_101_text_25_0.png)\n\nThis document provides a foundational introduction to Qdrant vector database concepts, installation, and core operations. It covers basic vector database operations including collection management, point manipulation, similarity search, and recommendation systems. For more advanced text processing applications, see [Text Data Applications](qdrant/examples/3-text-data-applications.md). For specialized retrieval patterns, see [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md).\n\n## Overview\n\nQdrant is a vector database designed for similarity search and recommendation systems. This tutorial demonstrates core concepts through practical examples using a music recommendation scenario with dummy data. The key components covered include:\n\n- **Collections**: Container for vectors with specified dimensions and distance metrics\n- **Points**: Individual records containing vectors, IDs, and optional metadata payloads\n- **Search Operations**: Similarity search and recommendation queries\n- **Filtering**: Payload-based filtering for refined results\n\n## Architecture Overview\n\n```\n```",
      "index": 1,
      "token_count": 555,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 1935,
      "end_char": 3948
    },
    {
      "content": "es\n- **Filtering**: Payload-based filtering for refined results\n\n## Architecture Overview\n\n```\n```\n\n**Qdrant System Architecture**: Shows the relationship between client components, server infrastructure, and data model classes used in vector operations.\n\nSources: [qdrant\\_101\\_getting\\_started/README.md67-75](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L67-L75) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb127-135](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L127-L135)\n\n## Installation and Setup\n\n### Docker Installation\n\nQdrant runs as a Docker container with persistent storage:\n\n```\n```\n\n### Python Dependencies\n\nRequired packages for client operations:\n\n```\n```\n\n### Client Initialization\n\nThe `QdrantClient` supports both remote server connections and in-memory instances:\n\n```\n```\n\nSources: [qdrant\\_101\\_getting\\_started/README.md25-96](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L25-L96) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb76-174](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L76-L174)\n\n## Collection Management\n\n### Collection Creation\n\nCollections define vector dimensions and distance metrics. The `recreate_collection()` method creates or overwrites existing collections:\n\n```\n```\n\n### Supported Distance Metrics\n\n- **Cosine Similarity**: `models.Distance.COSINE`\n- **Dot Product**: `models.Distance.DOT`\n- **Euclidean Distance**: `models.Distance.EUCLIDEAN`\n\n### Collection Health Monitoring\n\nThe `get_collection()` method returns status information:\n\n```\n```\n\nSources: [qdrant\\_101\\_getting\\_started/README.md99-173](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L99-L173) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb181-276](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L181-L276)\n\n## Point Operations",
      "index": 2,
      "token_count": 607,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 3848,
      "end_char": 5891
    },
    {
      "content": "les/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L181-L276)\n\n## Point Operations\n\n### Point Structure and Data Model\n\n```\n```\n\n**Point Data Model**: Illustrates the structure of points and the two main patterns for adding data to collections.\n\nSources: [qdrant\\_101\\_getting\\_started/README.md177-185](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L177-L185) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb313-327](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L313-L327)\n\n### Adding Points in Batches\n\nThe `upsert()` method with `models.Batch` efficiently adds multiple points:\n\n```\n```\n\n### Adding Individual Points\n\nSingle point insertion using `models.PointStruct`:\n\n```\n```\n\n### Point Retrieval and Deletion\n\n```\n```\n\nSources: [qdrant\\_101\\_getting\\_started/README.md186-344](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L186-L344) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb334-588](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L334-L588)\n\n## Payload Management\n\n### Payload Structure\n\nPayloads store metadata as JSON objects alongside vectors. Example payload for music recommendation:\n\n```\n```\n\n### Adding Points with Payloads\n\n```\n```\n\n### Payload Manipulation\n\n```\n```\n\nSources: [qdrant\\_101\\_getting\\_started/README.md345-588](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L345-L588) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb595-991](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L595-L991)\n\n## Search Operations\n\n### Basic Similarity Search\n\nThe `search()` method (legacy) or `query_points()` method performs vector similarity search:\n\n```\n```\n\n### Filtered Search Operations\n\n```\n```\n\n**Search Filtering Workflow**: Shows how filter conditions are constructed and applied to search operations.",
      "index": 3,
      "token_count": 644,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 5791,
      "end_char": 7823
    },
    {
      "content": "iltering Workflow**: Shows how filter conditions are constructed and applied to search operations.\n\n### Implementing Filtered Search\n\n```\n```\n\nSources: [qdrant\\_101\\_getting\\_started/README.md489-577](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L489-L577) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb833-991](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L833-L991)\n\n## Recommendation Systems\n\n### Recommendation API\n\nQdrant's recommendation system uses positive and negative examples to find similar items:\n\n```\n```\n\n### Advanced Recommendation Features\n\n```\n```\n\n### Score Threshold Usage\n\nThe `score_threshold` parameter filters results below a minimum similarity score, preventing low-quality recommendations:\n\n```\n```\n\nSources: [qdrant\\_101\\_getting\\_started/README.md590-700](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L590-L700) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb998-1163](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L998-L1163)\n\n## Storage and Persistence\n\n### Storage Options\n\nQdrant supports two storage modes:\n\n| Storage Type  | Description                             | Use Case                            |\n| ------------- | --------------------------------------- | ----------------------------------- |\n| **In-memory** | Stores vectors in RAM                   | Highest speed, temporary data       |\n| **Memmap**    | Virtual address space with disk backing | Persistent storage, larger datasets |\n\n### Storage Directory Structure\n\n```\nqdrant_storage/\n├── aliases/\n│   └── data.json\n├── collections/\n│   └── first_collection/\n└── raft_state/\n```\n\nThe `qdrant_storage` directory is created when running the Docker container and persists all collection data and metadata.\n\nSources: [qdrant\\_101\\_getting\\_started/README.md154-171](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.",
      "index": 4,
      "token_count": 562,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 7723,
      "end_char": 9770
    },
    {
      "content": "EADME.md154-171](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L154-L171) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb284-305](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L284-L305)\n\n## Key Methods Reference\n\n### Collection Operations\n\n| Method                  | Purpose                    | Parameters                          |\n| ----------------------- | -------------------------- | ----------------------------------- |\n| `recreate_collection()` | Create/recreate collection | `collection_name`, `vectors_config` |\n| `create_collection()`   | Create new collection only | `collection_name`, `vectors_config` |\n| `get_collection()`      | Retrieve collection info   | `collection_name`                   |\n| `count()`               | Count points in collection | `collection_name`, `exact`          |\n\n### Point Operations\n\n| Method            | Purpose             | Parameters                               |\n| ----------------- | ------------------- | ---------------------------------------- |\n| `upsert()`        | Add/update points   | `collection_name`, `points`              |\n| `retrieve()`      | Get specific points | `collection_name`, `ids`, `with_vectors` |\n| `delete()`        | Remove points       | `collection_name`, `points_selector`     |\n| `clear_payload()` | Remove payload data | `collection_name`, `points_selector`     |\n\n### Search Operations\n\n| Method           | Purpose                  | Parameters                                                 |\n| ---------------- | ------------------------ | ---------------------------------------------------------- |\n| `query_points()` | Modern search/recommend  | `collection_name`, `query`, `query_filter`, `limit`        |\n| `search()`       | Legacy similarity search | `collection_name`, `query_vector`, `query_filter`, `limit` |\n| `recommend()`    | Legacy recommendation    | `collection_name`, `positive`, `negative`, `limit`         |",
      "index": 5,
      "token_count": 442,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 9670,
      "end_char": 11694
    },
    {
      "content": "nd()`    | Legacy recommendation    | `collection_name`, `positive`, `negative`, `limit`         |\n\nSources: [qdrant\\_101\\_getting\\_started/README.md1-720](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/README.md#L1-L720) [qdrant\\_101\\_getting\\_started/getting\\_started.ipynb1-1237](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_getting_started/getting_started.ipynb#L1-L1237)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Getting Started with Qdrant](#getting-started-with-qdrant.md)\n- [Overview](#overview.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Installation and Setup](#installation-and-setup.md)\n- [Docker Installation](#docker-installation.md)\n- [Python Dependencies](#python-dependencies.md)\n- [Client Initialization](#client-initialization.md)\n- [Collection Management](#collection-management.md)\n- [Collection Creation](#collection-creation.md)\n- [Supported Distance Metrics](#supported-distance-metrics.md)\n- [Collection Health Monitoring](#collection-health-monitoring.md)\n- [Point Operations](#point-operations.md)\n- [Point Structure and Data Model](#point-structure-and-data-model.md)\n- [Adding Points in Batches](#adding-points-in-batches.md)\n- [Adding Individual Points](#adding-individual-points.md)\n- [Point Retrieval and Deletion](#point-retrieval-and-deletion.md)\n- [Payload Management](#payload-management.md)\n- [Payload Structure](#payload-structure.md)\n- [Adding Points with Payloads](#adding-points-with-payloads.md)\n- [Payload Manipulation](#payload-manipulation.md)\n- [Search Operations](#search-operations.md)\n- [Basic Similarity Search](#basic-similarity-search.md)\n- [Filtered Search Operations](#filtered-search-operations.md)\n- [Implementing Filtered Search](#implementing-filtered-search.md)\n- [Recommendation Systems](#recommendation-systems.md)\n- [Recommendation API](#recommendation-api.md)\n- [Advanced Recommendation Features](#advanced-recommendation-features.md)\n- [Score Threshold Usage](#score-threshold-usage.md)",
      "index": 6,
      "token_count": 535,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 11594,
      "end_char": 13637
    },
    {
      "content": "eatures](#advanced-recommendation-features.md)\n- [Score Threshold Usage](#score-threshold-usage.md)\n- [Storage and Persistence](#storage-and-persistence.md)\n- [Storage Options](#storage-options.md)\n- [Storage Directory Structure](#storage-directory-structure.md)\n- [Key Methods Reference](#key-methods-reference.md)\n- [Collection Operations](#collection-operations.md)\n- [Point Operations](#point-operations-1.md)\n- [Search Operations](#search-operations-1.md)",
      "index": 7,
      "token_count": 111,
      "metadata": {
        "title": "_qdrant_examples_2-getting-started-with-qdrant",
        "source": "qdrant_examples\\_qdrant_examples_2-getting-started-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "file_name": "_qdrant_examples_2-getting-started-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.240836",
        "total_chunks": 8
      },
      "start_char": 13537,
      "end_char": 15585
    },
    {
      "content": "Text Data Applications | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 586,
      "metadata": {
        "title": "_qdrant_examples_3-text-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_3-text-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3-text-data-applications.md",
        "file_name": "_qdrant_examples_3-text-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.257663",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2030
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Text Data Applications\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/examples/blob/b3c4b28f/README.md)\n- [code-search/code-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb)\n- [qdrant\\_101\\_audio\\_data/03\\_qdrant\\_101\\_audio.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/03_qdrant_101_audio.ipynb)\n- [qdrant\\_101\\_audio\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md)\n- [qdrant\\_101\\_text\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_25\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_25_0.png)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_28\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_28_0.png)\n- [sparse-vectors-movies-reco/recommend-movies.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb)\n\nThis page covers text processing, embedding generation, and various text-based search and recommendation systems using Qdrant. It demonstrates how to transform textual data into vector representations and build semantic search capabilities for different domains including news articles, code repositories, and collaborative filtering systems.\n\nFor audio data applications, see [Audio Data Applications](qdrant/examples/5-audio-data-applications.md). For advanced RAG implementations that combine text processing with generation, see [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md).\n\n## Core Text Processing Pipeline",
      "index": 1,
      "token_count": 610,
      "metadata": {
        "title": "_qdrant_examples_3-text-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_3-text-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3-text-data-applications.md",
        "file_name": "_qdrant_examples_3-text-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.257663",
        "total_chunks": 6
      },
      "start_char": 1930,
      "end_char": 3940
    },
    {
      "content": "dvanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md).\n\n## Core Text Processing Pipeline\n\nThe foundation of text applications involves transforming raw text into numerical vector representations that capture semantic meaning. This process enables similarity search and recommendation systems.\n\n```\n```\n\n**Text Processing Pipeline Architecture**\n\nSources: [qdrant\\_101\\_text\\_data/README.md1-100](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md#L1-L100) [code-search/code-search.ipynb1-50](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L1-L50) [sparse-vectors-movies-reco/recommend-movies.ipynb1-50](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L1-L50)\n\n## Embedding Generation Approaches\n\n### Dense Text Embeddings\n\nThe primary approach uses transformer models to generate dense vector representations. The `embed_text` function demonstrates the core embedding pipeline:\n\n```\n```\n\n**Dense Embedding Generation Flow**\n\nThe `mean_pooling` function handles attention-weighted averaging to convert token-level embeddings to sentence-level representations:\n\nSources: [qdrant\\_101\\_text\\_data/README.md378-415](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md#L378-L415)\n\n### Code-Specific Text Processing\n\nFor code search applications, the `textify` function normalizes code structures into human-readable descriptions:\n\n```\n```\n\n**Code Search Dual Embedding Architecture**\n\nSources: [code-search/code-search.ipynb125-191](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L125-L191) [code-search/code-search.ipynb333-349](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L333-L349)\n\n### Sparse Vector Representations\n\nFor collaborative filtering, user preferences are encoded as sparse vectors where indices represent items and values represent ratings:\n\nSources: [sparse-vectors-movies-reco/recommend-movies.",
      "index": 2,
      "token_count": 543,
      "metadata": {
        "title": "_qdrant_examples_3-text-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_3-text-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3-text-data-applications.md",
        "file_name": "_qdrant_examples_3-text-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.257663",
        "total_chunks": 6
      },
      "start_char": 3840,
      "end_char": 5864
    },
    {
      "content": "epresent items and values represent ratings:\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb417-429](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L417-L429)\n\n## Qdrant Collection Management\n\n### Collection Configuration\n\nDifferent text applications require specific collection configurations based on the embedding approach:\n\n| Application           | Vector Config             | Distance Metric | Dimensions |\n| --------------------- | ------------------------- | --------------- | ---------- |\n| News Articles         | Dense only                | Cosine          | 768        |\n| Code Search           | Named vectors (text/code) | Cosine          | 384/768    |\n| Movie Recommendations | Sparse vectors only       | Dot Product     | Dynamic    |\n\n### Data Upload Patterns\n\nThe `upsert` operation loads embeddings with structured payloads:\n\n```\n```\n\n**Qdrant Data Upload Flow**\n\nSources: [qdrant\\_101\\_text\\_data/README.md574-583](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md#L574-L583) [code-search/code-search.ipynb387-414](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L387-L414)\n\n## Search and Recommendation Systems\n\n### Semantic Search Implementation\n\nThe search functionality uses vector similarity to find relevant documents:\n\n```\n```\n\n**Semantic Search Architecture**\n\nSources: [qdrant\\_101\\_text\\_data/README.md680-690](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md#L680-L690)\n\n### Recommendation API Usage\n\nThe recommendation system finds similar items using positive and negative examples:\n\n```\n```\n\n**Recommendation System Flow**\n\nSources: [qdrant\\_101\\_text\\_data/README.md726-750](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md#L726-L750)\n\n### Code Search Query Processing\n\nCode search supports both natural language and code-specific queries using named vectors:\n\nSources: [code-search/code-search.ipynb504-518](https://github.",
      "index": 3,
      "token_count": 556,
      "metadata": {
        "title": "_qdrant_examples_3-text-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_3-text-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3-text-data-applications.md",
        "file_name": "_qdrant_examples_3-text-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.257663",
        "total_chunks": 6
      },
      "start_char": 5764,
      "end_char": 7813
    },
    {
      "content": "ecific queries using named vectors:\n\nSources: [code-search/code-search.ipynb504-518](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L504-L518) [code-search/code-search.ipynb554-568](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L554-L568)\n\n## Advanced Text Applications\n\n### Collaborative Filtering with Sparse Vectors\n\nMovie recommendation uses sparse vectors where each dimension represents a movie and values represent normalized ratings. The `user_sparse_vectors` structure efficiently encodes user preferences:\n\n```\n```\n\n**Collaborative Filtering Architecture**\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb403-409](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L403-L409) [sparse-vectors-movies-reco/recommend-movies.ipynb420-429](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L420-L429) [sparse-vectors-movies-reco/recommend-movies.ipynb527-535](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L527-L535)\n\n### Multi-Model Code Search\n\nThe code search system demonstrates how to combine different embedding approaches for comprehensive search capabilities. The `points` structure uses named vectors to store both text and code representations simultaneously.\n\nSources: [code-search/code-search.ipynb387-398](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L387-L398)\n\n## Implementation Examples\n\n### Basic Text Collection Setup\n\n```\n```\n\n### Named Vector Configuration\n\n```\n```\n\n### Sparse Vector Setup\n\n```\n```\n\nSources: [qdrant\\_101\\_text\\_data/README.md539-544](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md#L539-L544) [code-search/code-search.ipynb336-349](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L336-L349) [sparse-vectors-movies-reco/recommend-movies.ipynb463-469](https://github.",
      "index": 4,
      "token_count": 626,
      "metadata": {
        "title": "_qdrant_examples_3-text-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_3-text-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3-text-data-applications.md",
        "file_name": "_qdrant_examples_3-text-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.257663",
        "total_chunks": 6
      },
      "start_char": 7713,
      "end_char": 9746
    },
    {
      "content": "e-search.ipynb#L336-L349) [sparse-vectors-movies-reco/recommend-movies.ipynb463-469](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L463-L469)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Text Data Applications](#text-data-applications.md)\n- [Core Text Processing Pipeline](#core-text-processing-pipeline.md)\n- [Embedding Generation Approaches](#embedding-generation-approaches.md)\n- [Dense Text Embeddings](#dense-text-embeddings.md)\n- [Code-Specific Text Processing](#code-specific-text-processing.md)\n- [Sparse Vector Representations](#sparse-vector-representations.md)\n- [Qdrant Collection Management](#qdrant-collection-management.md)\n- [Collection Configuration](#collection-configuration.md)\n- [Data Upload Patterns](#data-upload-patterns.md)\n- [Search and Recommendation Systems](#search-and-recommendation-systems.md)\n- [Semantic Search Implementation](#semantic-search-implementation.md)\n- [Recommendation API Usage](#recommendation-api-usage.md)\n- [Code Search Query Processing](#code-search-query-processing.md)\n- [Advanced Text Applications](#advanced-text-applications.md)\n- [Collaborative Filtering with Sparse Vectors](#collaborative-filtering-with-sparse-vectors.md)\n- [Multi-Model Code Search](#multi-model-code-search.md)\n- [Implementation Examples](#implementation-examples.md)\n- [Basic Text Collection Setup](#basic-text-collection-setup.md)\n- [Named Vector Configuration](#named-vector-configuration.md)\n- [Sparse Vector Setup](#sparse-vector-setup.md)",
      "index": 5,
      "token_count": 387,
      "metadata": {
        "title": "_qdrant_examples_3-text-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_3-text-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3-text-data-applications.md",
        "file_name": "_qdrant_examples_3-text-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.257663",
        "total_chunks": 6
      },
      "start_char": 9646,
      "end_char": 11694
    },
    {
      "content": "Code Search with Dual Embeddings | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 589,
      "metadata": {
        "title": "_qdrant_examples_3.1-code-search-with-dual-embeddings",
        "source": "qdrant_examples\\_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "file_name": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.292706",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2040
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Code Search with Dual Embeddings\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/examples/blob/b3c4b28f/README.md)\n- [code-search/code-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb)\n- [sparse-vectors-movies-reco/recommend-movies.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb)\n\nThis document describes the implementation of a semantic code search system that uses dual embedding approaches to enable both natural language and code-specific queries. The system leverages two different neural encoders to provide comprehensive search capabilities across a codebase.\n\nFor information about basic text processing and embedding generation, see [Text Data Applications](qdrant/examples/3-text-data-applications.md). For advanced retrieval patterns and multivector approaches, see [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md).\n\n## System Architecture\n\nThe code search system implements a dual embedding strategy using Qdrant's named vectors capability to store and query code structures using two different embedding models simultaneously.\n\n```\n```\n\nSources: [code-search/code-search.ipynb1-700](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L1-L700)\n\n## Data Processing Pipeline\n\nThe system processes code structures through two parallel transformation pipelines to create embeddings suitable for different types of queries.\n\n### Code Structure Format\n\nEach code structure contains comprehensive metadata about code entities:\n\n| Field               | Description          | Example                                 |\n| ------------------- | -------------------- | --------------------------------------- |\n| `name`              | Entity identifier    | `\"InvertedIndexRam\"`                    |",
      "index": 1,
      "token_count": 436,
      "metadata": {
        "title": "_qdrant_examples_3.1-code-search-with-dual-embeddings",
        "source": "qdrant_examples\\_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "file_name": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.292706",
        "total_chunks": 6
      },
      "start_char": 1940,
      "end_char": 3961
    },
    {
      "content": "-------- |\n| `name`              | Entity identifier    | `\"InvertedIndexRam\"`                    |\n| `signature`         | Full code signature  | `\"pub struct InvertedIndexRam { ... }\"` |\n| `code_type`         | Entity type          | `\"Struct\"`, `\"Function\"`                |\n| `docstring`         | Documentation string | `\"Inverted flatten index...\"`           |\n| `context.module`    | Module location      | `\"inverted_index\"`                      |\n| `context.file_path` | Full file path       | `\"lib/sparse/src/index/...\"`            |\n| `context.snippet`   | Raw code snippet     | Actual code with syntax                 |\n\nSources: [code-search/code-search.ipynb77-105](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L77-L105)\n\n### Text Normalization Process\n\nThe `textify()` function converts code structures into natural language representations for general purpose embedding models.\n\n```\n```\n\nSources: [code-search/code-search.ipynb130-191](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L130-L191)\n\n## Embedding Strategy Comparison\n\nThe dual embedding approach leverages the strengths of both general purpose and code-specific models:\n\n| Aspect     | General Purpose Model                    | Code-Specific Model                   |\n| ---------- | ---------------------------------------- | ------------------------------------- |\n| Model      | `sentence-transformers/all-MiniLM-L6-v2` | `jinaai/jina-embeddings-v2-base-code` |\n| Input      | Normalized text via `textify()`          | Raw code snippets                     |\n| Dimension  | 384                                      | 768                                   |\n| Query Type | Natural language descriptions            | Code syntax patterns                  |\n| Use Case   | \"How do I count points?\"                 | Function signatures, structures       |\n\nSources: [code-search/code-search.ipynb114-191](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.",
      "index": 2,
      "token_count": 462,
      "metadata": {
        "title": "_qdrant_examples_3.1-code-search-with-dual-embeddings",
        "source": "qdrant_examples\\_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "file_name": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.292706",
        "total_chunks": 6
      },
      "start_char": 3861,
      "end_char": 5885
    },
    {
      "content": "/code-search.ipynb114-191](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L114-L191) [code-search/code-search.ipynb282-287](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L282-L287)\n\n## Collection Configuration\n\nThe Qdrant collection uses named vectors to store both embedding types in a single collection:\n\n```\n```\n\nSources: [code-search/code-search.ipynb336-348](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L336-L348) [code-search/code-search.ipynb387-399](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L387-L399)\n\n## Query Processing Methods\n\nThe system supports multiple query processing approaches to balance relevance and diversity:\n\n### Single Model Query\n\n```\n```\n\n### Batch Query Processing\n\n```\n```\n\n### Grouped Query for Diversity\n\n```\n```\n\nSources: [code-search/code-search.ipynb504-518](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L504-L518) [code-search/code-search.ipynb609-637](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L609-L637) [code-search/code-search.ipynb672-690](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L672-L690)\n\n## Implementation Details\n\n### Data Upload Process\n\nThe system uses `models.PointStruct` to create indexed points with dual embeddings:\n\n```\n```\n\nSources: [code-search/code-search.ipynb387-399](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L387-L399) [code-search/code-search.ipynb409-414](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L409-L414)\n\n### Query Result Processing\n\nResults from both embedding models are processed to extract relevant code entities:\n\n| Result Field                          | Content          | Usage                      |\n| ------------------------------------- | ---------------- | -------------------------- |\n| `hit.",
      "index": 3,
      "token_count": 597,
      "metadata": {
        "title": "_qdrant_examples_3.1-code-search-with-dual-embeddings",
        "source": "qdrant_examples\\_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "file_name": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.292706",
        "total_chunks": 6
      },
      "start_char": 5785,
      "end_char": 7781
    },
    {
      "content": "|\n| ------------------------------------- | ---------------- | -------------------------- |\n| `hit.score`                           | Similarity score | Ranking and filtering      |\n| `hit.payload[\"context\"][\"module\"]`    | Module name      | Grouping and organization  |\n| `hit.payload[\"context\"][\"file_name\"]` | File identifier  | Source location            |\n| `hit.payload[\"signature\"]`            | Code signature   | Display and identification |\n\nSources: [code-search/code-search.ipynb511-518](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L511-L518) [code-search/code-search.ipynb562-568](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L562-L568)\n\n## Search Performance Characteristics\n\nThe dual embedding approach provides different search behaviors:\n\n### General Purpose Model Results\n\n- Higher semantic understanding of natural language queries\n- Better handling of conceptual searches like \"count points in collection\"\n- Returns broader, more conceptually related results\n\n### Code-Specific Model Results\n\n- More precise matching of code patterns and structures\n- Better for searching specific function signatures or implementations\n- Higher relevance for code syntax-based queries\n\nSources: [code-search/code-search.ipynb504-518](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L504-L518) [code-search/code-search.ipynb554-569](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb#L554-L569)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Code Search with Dual Embeddings](#code-search-with-dual-embeddings.md)\n- [System Architecture](#system-architecture.md)\n- [Data Processing Pipeline](#data-processing-pipeline.md)\n- [Code Structure Format](#code-structure-format.md)\n- [Text Normalization Process](#text-normalization-process.md)\n- [Embedding Strategy Comparison](#embedding-strategy-comparison.md)\n- [Collection Configuration](#collection-configuration.md)",
      "index": 4,
      "token_count": 491,
      "metadata": {
        "title": "_qdrant_examples_3.1-code-search-with-dual-embeddings",
        "source": "qdrant_examples\\_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "file_name": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.292706",
        "total_chunks": 6
      },
      "start_char": 7681,
      "end_char": 9707
    },
    {
      "content": "ison](#embedding-strategy-comparison.md)\n- [Collection Configuration](#collection-configuration.md)\n- [Query Processing Methods](#query-processing-methods.md)\n- [Single Model Query](#single-model-query.md)\n- [Batch Query Processing](#batch-query-processing.md)\n- [Grouped Query for Diversity](#grouped-query-for-diversity.md)\n- [Implementation Details](#implementation-details.md)\n- [Data Upload Process](#data-upload-process.md)\n- [Query Result Processing](#query-result-processing.md)\n- [Search Performance Characteristics](#search-performance-characteristics.md)\n- [General Purpose Model Results](#general-purpose-model-results.md)\n- [Code-Specific Model Results](#code-specific-model-results.md)",
      "index": 5,
      "token_count": 152,
      "metadata": {
        "title": "_qdrant_examples_3.1-code-search-with-dual-embeddings",
        "source": "qdrant_examples\\_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "file_name": "_qdrant_examples_3.1-code-search-with-dual-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.292706",
        "total_chunks": 6
      },
      "start_char": 9607,
      "end_char": 11655
    },
    {
      "content": "Extractive Question Answering | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 0,
      "end_char": 2037
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Extractive Question Answering\n\nRelevant source files\n\n- [extractive\\_qa/extractive-question-answering.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data.ipynb)\n\n## Overview\n\nThe Extractive Question Answering system implements a retriever-reader architecture that extracts precise answers from movie plot data. The system uses the DuoRC dataset containing movie plots from Wikipedia and IMDb to demonstrate semantic search and answer extraction capabilities.\n\n**Key Components:**\n\n- **Retriever**: `TextEmbedding` model (`BAAI/bge-small-en-v1.5`) for semantic search\n- **Vector Database**: Qdrant collection named `extractive-question-answering`\n- **Reader**: `bert-large-uncased-whole-word-masking-finetuned-squad` for answer extraction\n\nThe system extracts answers directly from source text rather than generating new content, ensuring factual accuracy and traceability.\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb17-27](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L17-L27) [extractive\\_qa/extractive-question-answering.ipynb622-623](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L622-L623) [extractive\\_qa/extractive-question-answering.ipynb733-737](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L733-L737)\n\n## Architecture\n\n**System Architecture with Code Entities**\n\n```\n```\n\n**Component Mapping:**\n\n| Component       | Code Entity                                                                                     | Purpose                                  |",
      "index": 1,
      "token_count": 540,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 1937,
      "end_char": 3931
    },
    {
      "content": "| Purpose                                  |\n| --------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------- |\n| Retriever       | `TextEmbedding(\"BAAI/bge-small-en-v1.5\")`                                                       | Converts text to 384-dimensional vectors |\n| Vector Database | `QdrantClient(\":memory:\")`                                                                      | Stores and searches embeddings           |\n| Collection      | `\"extractive-question-answering\"`                                                               | Named collection with cosine distance    |\n| Reader          | `pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")` | Extracts answer spans                    |\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb91-96](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L91-L96) [extractive\\_qa/extractive-question-answering.ipynb473](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L473-L473) [extractive\\_qa/extractive-question-answering.ipynb522](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L522-L522) [extractive\\_qa/extractive-question-answering.ipynb735-737](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L735-L737)\n\n## Implementation Details\n\n### Data Preparation\n\n**Dataset Processing Pipeline**\n\n```\n```\n\nThe implementation processes 9,919 unique movie plots from the DuoRC dataset:\n\n- Dataset loading: `load_dataset(\"duorc\", \"ParaphraseRC\", split=\"train\")`\n- Deduplication: `df.drop_duplicates(subset=\"plot\")`\n- Batch processing: 64 documents per batch for efficient embedding generation\n- Storage format: `models.Batch(ids=ids, vectors=emb, payloads=meta)`",
      "index": 2,
      "token_count": 499,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 3831,
      "end_char": 5861
    },
    {
      "content": "ficient embedding generation\n- Storage format: `models.Batch(ids=ids, vectors=emb, payloads=meta)`\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb442-449](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L442-L449) [extractive\\_qa/extractive-question-answering.ipynb688-702](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L688-L702)\n\n### Retriever Component\n\n**TextEmbedding Configuration**\n\n| Parameter  | Value                      | Purpose                       |\n| ---------- | -------------------------- | ----------------------------- |\n| Model      | `\"BAAI/bge-small-en-v1.5\"` | Optimized for semantic search |\n| Dimensions | 384                        | Vector size for embeddings    |\n| Library    | `fastembed.TextEmbedding`  | Fast embedding generation     |\n\n**Core Functions:**\n\n- **Indexing**: `retriever.embed(batch[\"plot\"].tolist())` - converts plot text to vectors\n- **Querying**: `retriever.query_embed(question)` - converts questions to search vectors\n\nThe retriever ensures semantic similarity between questions and relevant contexts by mapping both to the same 384-dimensional vector space.\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb622-623](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L622-L623) [extractive\\_qa/extractive-question-answering.ipynb693](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L693-L693) [extractive\\_qa/extractive-question-answering.ipynb786](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L786-L786)\n\n### Qdrant Vector Database\n\n**Collection Configuration**\n\n```\n```\n\n**Key Implementation Details:**\n\n- Client: `QdrantClient(\":memory:\")` for demonstration\n- Collection: `\"extractive-question-answering\"`\n- Vector configuration: `models.VectorParams(size=384, distance=models.Distance.COSINE)`",
      "index": 3,
      "token_count": 587,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 5761,
      "end_char": 7796
    },
    {
      "content": "nswering\"`\n- Vector configuration: `models.VectorParams(size=384, distance=models.Distance.COSINE)`\n- Search method: `client.query_points(collection_name, query, limit)`\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb473](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L473-L473) [extractive\\_qa/extractive-question-answering.ipynb529-535](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L529-L535) [extractive\\_qa/extractive-question-answering.ipynb788-792](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L788-L792)\n\n### Reader Model\n\nThe reader model extracts the precise answer from the retrieved context passages:\n\n- Model: `bert-large-uncased-whole-word-masking-finetuned-squad`\n- Type: Transformer-based question answering model\n- Training: Fine-tuned on the SQuAD dataset\n\nThe reader processes each retrieved context separately and returns:\n\n- The extracted answer text\n- A confidence score\n- The title of the source document\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb718-721](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L718-L721) [extractive\\_qa/extractive-question-answering.ipynb735-737](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L735-L737)\n\n## Workflow\n\n**Function Call Sequence**\n\n```\n```\n\n**Core Function Implementations:**\n\n| Function              | Input                               | Output                     | Line Reference                                                                                                                                                            |\n| --------------------- | ----------------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------",
      "index": 4,
      "token_count": 511,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 7696,
      "end_char": 9744
    },
    {
      "content": "---------------------------------------------------------------------------------------------------------------------- |\n| `get_relevant_plot()` | `question: str, top_k: int`         | `List[str]` context pairs  | [extractive\\_qa/extractive-question-answering.ipynb774-800](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L774-L800) |\n| `extract_answer()`    | `question: str, context: List[str]` | Ranked answers with scores | [extractive\\_qa/extractive-question-answering.ipynb835-863](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L835-L863) |\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb774-800](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L774-L800) [extractive\\_qa/extractive-question-answering.ipynb835-863](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L835-L863) [extractive\\_qa/extractive-question-answering.ipynb786](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L786-L786) [extractive\\_qa/extractive-question-answering.ipynb846](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L846-L846)\n\n## Data Flow Details\n\n**Code Entity Data Transformation**\n\n```\n```\n\n**Variable Flow:**\n\n- Input: `question: str`, `top_k: int`\n- Embedding: `encoded_query = next(retriever.query_embed(question)).tolist()`\n- Search: `result = client.query_points(...).points`\n- Context: `context = [[x.payload[\"title\"], x.payload[\"plot\"]] for x in result]`\n- Answers: `answer = reader(question=question, context=c[1])`\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb786](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L786-L786) [extractive\\_qa/extractive-question-answering.ipynb788-796](https://github.",
      "index": 5,
      "token_count": 654,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 9644,
      "end_char": 11630
    },
    {
      "content": "swering.ipynb#L786-L786) [extractive\\_qa/extractive-question-answering.ipynb788-796](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L788-L796) [extractive\\_qa/extractive-question-answering.ipynb844-853](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L844-L853)\n\n## Implementation Code Structure\n\n**Function Signatures and Dependencies**\n\n| Function              | Signature                                  | Dependencies          | Purpose                                   |\n| --------------------- | ------------------------------------------ | --------------------- | ----------------------------------------- |\n| `get_relevant_plot()` | `(question: str, top_k: int) -> List[str]` | `retriever`, `client` | Vector search for relevant contexts       |\n| `extract_answer()`    | `(question: str, context: List[str])`      | `reader` pipeline     | Answer extraction with confidence scoring |\n\n**Batch Processing Implementation**\n\n```\n```\n\n**Key Variables:**\n\n- `batch_size = 64` for memory-efficient processing\n- `collection_name = \"extractive-question-answering\"`\n- `retriever = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")`\n- `reader = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")`\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb688-702](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L688-L702) [extractive\\_qa/extractive-question-answering.ipynb774-800](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L774-L800) [extractive\\_qa/extractive-question-answering.ipynb835-863](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L835-L863)\n\n## Example Usage\n\nThe system can answer various types of questions about movie plots. Here are some examples from the implementation:\n\n### Example 1: College Name in \"3 Idiots\"\n\n```",
      "index": 6,
      "token_count": 600,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 11530,
      "end_char": 13578
    },
    {
      "content": "ts. Here are some examples from the implementation:\n\n### Example 1: College Name in \"3 Idiots\"\n\n```\nQuestion: \"In the movie 3 Idiots, what is the name of the college where the main characters Rancho, Farhan, and Raju study\"\nAnswer: \"Imperial College of Engineering\" (Score: 0.90)\nTitle: \"Three Idiots\"\n```\n\n### Example 2: Escape Tool in \"The Shawshank Redemption\"\n\n```\nQuestion: \"In the movie The Shawshank Redemption, what was the item that Andy Dufresne used to escape from Shawshank State Penitentiary?\"\nAnswer: \"rock hammer\" (Score: 0.87)\nTitle: \"The Shawshank Redemption\"\n```\n\n### Example 3: Multiple Sources\n\n```\nQuestion: \"who killed the spy\"\nAnswers: \n1. \"Soviet agents\" (Score: 0.79)\n   Title: \"Tinker, Tailor, Soldier, Spy\"\n2. \"Gila\" (Score: 0.12)\n   Title: \"Our Man Flint\"\n3. \"Gabriel's assassins\" (Score: 0.06)\n   Title: \"Live Free or Die Hard\"\n```\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb905-906](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L905-L906) [extractive\\_qa/extractive-question-answering.ipynb1006-1008](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L1006-L1008) [extractive\\_qa/extractive-question-answering.ipynb1048-1050](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L1048-L1050)\n\n## Limitations and Considerations\n\n1. **Answer Confidence**: The system might return low-confidence answers when the question is difficult or the answer isn't clearly stated in the context\n2. **Context Relevance**: The quality of answers depends on retrieving relevant contexts\n3. **Exact Answer Extraction**: The system is designed to extract spans of text, not generate new content\n\nFor implementing a more advanced question answering system that can generate responses, consider exploring the RAG systems documented in [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md).\n\nSources: [extractive\\_qa/extractive-question-answering.",
      "index": 7,
      "token_count": 614,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 13478,
      "end_char": 15513
    },
    {
      "content": "qdrant/examples/6-advanced-rag-systems.md).\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb915-916](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L915-L916) [extractive\\_qa/extractive-question-answering.ipynb949-954](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L949-L954)\n\n## Technical Requirements\n\nThe implementation requires the following libraries:\n\n- datasets (2.12.0)\n- qdrant-client (1.10.1)\n- fastembed (0.3.3)\n- sentence-transformers (2.2.2)\n- torch (2.0.1)\n- transformers (for the reader model)\n\nSources: [extractive\\_qa/extractive-question-answering.ipynb67](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L67-L67) [extractive\\_qa/extractive-question-answering.ipynb90-96](https://github.com/qdrant/examples/blob/b3c4b28f/extractive_qa/extractive-question-answering.ipynb#L90-L96)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Extractive Question Answering](#extractive-question-answering.md)\n- [Overview](#overview.md)\n- [Architecture](#architecture.md)\n- [Implementation Details](#implementation-details.md)\n- [Data Preparation](#data-preparation.md)\n- [Retriever Component](#retriever-component.md)\n- [Qdrant Vector Database](#qdrant-vector-database.md)\n- [Reader Model](#reader-model.md)\n- [Workflow](#workflow.md)\n- [Data Flow Details](#data-flow-details.md)\n- [Implementation Code Structure](#implementation-code-structure.md)\n- [Example Usage](#example-usage.md)\n- [Example 1: College Name in \"3 Idiots\"](#example-1-college-name-in-3-idiots.md)\n- [Example 2: Escape Tool in \"The Shawshank Redemption\"](#example-2-escape-tool-in-the-shawshank-redemption.md)\n- [Example 3: Multiple Sources](#example-3-multiple-sources.md)\n- [Limitations and Considerations](#limitations-and-considerations.md)\n- [Technical Requirements](#technical-requirements.md)",
      "index": 8,
      "token_count": 612,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 15413,
      "end_char": 17461
    },
    {
      "content": "l-requirements.md)",
      "index": 9,
      "token_count": 5,
      "metadata": {
        "title": "_qdrant_examples_3.2-extractive-question-answering",
        "source": "qdrant_examples\\_qdrant_examples_3.2-extractive-question-answering.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.2-extractive-question-answering.md",
        "file_name": "_qdrant_examples_3.2-extractive-question-answering.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.316193",
        "total_chunks": 10
      },
      "start_char": 17361,
      "end_char": 19409
    },
    {
      "content": "Movie Recommendations with Sparse Vectors | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 589,
      "metadata": {
        "title": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors",
        "source": "qdrant_examples\\_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "file_name": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.353905",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2049
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Movie Recommendations with Sparse Vectors\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/examples/blob/b3c4b28f/README.md)\n- [code-search/code-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/code-search/code-search.ipynb)\n- [sparse-vectors-movies-reco/recommend-movies.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb)\n\n## Purpose\n\nThis document explains how to implement a movie recommendation system using collaborative filtering with Qdrant's sparse vector capabilities. We'll demonstrate how to represent user preferences as sparse vectors, find similar users based on these vectors, and generate personalized movie recommendations. This approach is particularly efficient for recommendation systems with large, sparse feature spaces.\n\nSources: [README.md9-16](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L9-L16) [sparse-vectors-movies-reco/recommend-movies.ipynb6-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L6-L25)\n\n## Overview\n\nCollaborative filtering is a recommendation technique that finds patterns in user behavior to predict preferences. It works on the premise that if two users have similar tastes (rated similar movies similarly), they will likely have similar preferences for other movies. This example leverages Qdrant's sparse vector capabilities to implement a collaborative filtering system using the MovieLens dataset.\n\nThe key workflow is:\n\n1. Represent each user's movie ratings as a sparse vector\n2. Index these vectors in Qdrant\n3. Find users with similar taste patterns\n4. Recommend movies that similar users liked but the target user hasn't seen\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb6-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.",
      "index": 1,
      "token_count": 504,
      "metadata": {
        "title": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors",
        "source": "qdrant_examples\\_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "file_name": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.353905",
        "total_chunks": 6
      },
      "start_char": 1949,
      "end_char": 3998
    },
    {
      "content": "b6-25](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L6-L25)\n\n## Sparse Vectors for Collaborative Filtering\n\n### What are Sparse Vectors?\n\nSparse vectors contain mostly zero values, with only a few non-zero entries. In the context of movie recommendations:\n\n- Each dimension represents a movie ID\n- The value at that dimension represents the user's rating\n- Most users rate only a tiny fraction of all available movies, making the vector sparse\n\nQdrant efficiently handles sparse vectors by only storing non-zero values (values and their indices), significantly reducing storage requirements and improving search performance for recommendation systems.\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb416-428](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L416-L428)\n\n## Implementation Steps\n\n### 1. Data Preparation\n\nFirst, we load the MovieLens dataset, which contains:\n\n- User information (demographics)\n- Movie information (title, genres)\n- User ratings for movies\n\n```\n```\n\nThe ratings are normalized to have a mean of 0 and standard deviation of 1. This normalization is particularly important for sparse vectors as it allows us to capture both positive and negative preferences.\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb210-408](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L210-L408)\n\n### 2. Converting Ratings to Sparse Vectors\n\nEach user's ratings are converted into a sparse vector format with two components:\n\n- `values`: The normalized rating values\n- `indices`: The corresponding movie IDs\n\n```\n```\n\nThis structure efficiently represents each user's preferences across the entire movie space by only storing the movies they've actually rated.\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb416-428](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L416-L428)\n\n### 3.",
      "index": 2,
      "token_count": 519,
      "metadata": {
        "title": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors",
        "source": "qdrant_examples\\_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "file_name": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.353905",
        "total_chunks": 6
      },
      "start_char": 3898,
      "end_char": 5941
    },
    {
      "content": "m/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L416-L428)\n\n### 3. Setting Up Qdrant Collection\n\nUnlike dense vector collections that require a pre-defined dimension, sparse vector collections in Qdrant don't need dimension specification since the dimensionality is determined by the indices in the data.\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb460-469](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L460-L469)\n\n### 4. Indexing User Preference Vectors\n\nEach user's sparse rating vector is uploaded to Qdrant along with their demographic information:\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb480-494](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L480-L494)\n\n### 5. Querying Similar Users\n\nTo generate recommendations, we first create a sparse vector of our own movie preferences:\n\n```\n```\n\nThen we search for similar users using Qdrant's vector similarity search:\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb510-554](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L510-L554)\n\n### 6. Generating Recommendations\n\nFrom the similar users' ratings, we identify movies they liked that the target user hasn't rated yet:\n\n```\n```\n\nThe final recommendations are generated by sorting movies by their aggregated scores:\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb562-603](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L562-L603)\n\n## Advanced Features\n\n### Filtered Recommendations\n\nQdrant allows filtering recommendations based on user demographics or other metadata. For example, you can find similar users within a specific age group:\n\n```\n```\n\nThis feature enables more personalized recommendations by leveraging both similarity in taste and demographic information.",
      "index": 3,
      "token_count": 545,
      "metadata": {
        "title": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors",
        "source": "qdrant_examples\\_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "file_name": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.353905",
        "total_chunks": 6
      },
      "start_char": 5841,
      "end_char": 7854
    },
    {
      "content": "e personalized recommendations by leveraging both similarity in taste and demographic information.\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb622-649](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L622-L649)\n\n## System Architecture\n\nThe overall architecture of the movie recommendation system is as follows:\n\n```\n```\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb6-649](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L6-L649)\n\n## Performance Considerations\n\nSparse vectors are particularly efficient for collaborative filtering recommendation systems because:\n\n1. **Storage Efficiency**: Only non-zero values are stored, greatly reducing memory requirements for large catalogs with millions of items.\n\n2. **Computational Efficiency**: Similarity calculations only consider dimensions with non-zero values in at least one of the vectors.\n\n3. **Interpretability**: The sparse approach maintains clear relationships between dimensions (movie IDs) and values (ratings), making the system more interpretable.\n\n4. **Scalability**: The system can easily accommodate new movies without retraining, as they simply become new dimensions in the sparse vector space.\n\nFor production use cases, it's recommended to use a server-based Qdrant instance rather than the in-memory version shown in the example.\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb436-440](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L436-L440)\n\n## Conclusion\n\nThis implementation demonstrates how Qdrant's sparse vector capabilities can be leveraged to build an effective collaborative filtering recommendation system. The approach is particularly suitable for domains with large, sparse feature spaces like movie recommendations, e-commerce product recommendations, or content suggestions.",
      "index": 4,
      "token_count": 439,
      "metadata": {
        "title": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors",
        "source": "qdrant_examples\\_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "file_name": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.353905",
        "total_chunks": 6
      },
      "start_char": 7754,
      "end_char": 9710
    },
    {
      "content": "ure spaces like movie recommendations, e-commerce product recommendations, or content suggestions.\n\nBy representing user preferences as sparse vectors and utilizing Qdrant's vector search capabilities, we can efficiently find similar users and generate personalized recommendations with minimal computational overhead.\n\nSources: [sparse-vectors-movies-reco/recommend-movies.ipynb6-649](https://github.com/qdrant/examples/blob/b3c4b28f/sparse-vectors-movies-reco/recommend-movies.ipynb#L6-L649) [README.md16](https://github.com/qdrant/examples/blob/b3c4b28f/README.md#L16-L16)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Movie Recommendations with Sparse Vectors](#movie-recommendations-with-sparse-vectors.md)\n- [Purpose](#purpose.md)\n- [Overview](#overview.md)\n- [Sparse Vectors for Collaborative Filtering](#sparse-vectors-for-collaborative-filtering.md)\n- [What are Sparse Vectors?](#what-are-sparse-vectors.md)\n- [Implementation Steps](#implementation-steps.md)\n- [1. Data Preparation](#1-data-preparation.md)\n- [2. Converting Ratings to Sparse Vectors](#2-converting-ratings-to-sparse-vectors.md)\n- [3. Setting Up Qdrant Collection](#3-setting-up-qdrant-collection.md)\n- [4. Indexing User Preference Vectors](#4-indexing-user-preference-vectors.md)\n- [5. Querying Similar Users](#5-querying-similar-users.md)\n- [6. Generating Recommendations](#6-generating-recommendations.md)\n- [Advanced Features](#advanced-features.md)\n- [Filtered Recommendations](#filtered-recommendations.md)\n- [System Architecture](#system-architecture.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Conclusion](#conclusion.md)",
      "index": 5,
      "token_count": 422,
      "metadata": {
        "title": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors",
        "source": "qdrant_examples\\_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "file_name": "_qdrant_examples_3.3-movie-recommendations-with-sparse-vectors.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.353905",
        "total_chunks": 6
      },
      "start_char": 9610,
      "end_char": 11658
    },
    {
      "content": "Image Data Applications | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 586,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2031
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Image Data Applications\n\nRelevant source files\n\n- [ecommerce\\_reverse\\_image\\_search/ecommerce-reverse-image-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/ecommerce-reverse-image-search.ipynb)\n- [ecommerce\\_reverse\\_image\\_search/queries/cable.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/cable.jpg)\n- [ecommerce\\_reverse\\_image\\_search/queries/cleaning.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/cleaning.jpg)\n- [ecommerce\\_reverse\\_image\\_search/queries/skating.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/skating.jpg)\n- [ecommerce\\_reverse\\_image\\_search/queries/spoon.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/spoon.jpg)\n- [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb)\n- [qdrant\\_101\\_image\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md)\n\nThis document covers image-based applications using Qdrant vector database for visual similarity search and retrieval. The content focuses on two primary implementations: e-commerce reverse image search and medical image analysis systems. These applications demonstrate how to extract visual embeddings from images and perform semantic search operations.\n\nFor text-based search applications, see [Text Data Applications](qdrant/examples/3-text-data-applications.md). For audio processing systems, see [Audio Data Applications](qdrant/examples/5-audio-data-applications.md).\n\n## Core Architecture and Components\n\nImage data applications in Qdrant follow a common pattern of embedding extraction, storage, and similarity search.",
      "index": 1,
      "token_count": 531,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 1931,
      "end_char": 3945
    },
    {
      "content": "lications in Qdrant follow a common pattern of embedding extraction, storage, and similarity search. The architecture involves image preprocessing, feature extraction using pre-trained models, vector storage, and search operations.\n\n```\n```\n\n**Core Technical Components**\n\n| Component             | Implementation            | Purpose                                    |\n| --------------------- | ------------------------- | ------------------------------------------ |\n| `ViTImageProcessor`   | Hugging Face Transformers | Image preprocessing and tokenization       |\n| `ViTModel`            | Vision Transformer models | Feature extraction from images             |\n| `QdrantClient`        | Qdrant Python client      | Vector database operations                 |\n| `models.VectorParams` | Qdrant configuration      | Collection setup with embedding dimensions |\n| `models.Filter`       | Qdrant filtering          | Metadata-based search refinement           |\n\nSources: [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb137-142](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L137-L142) [ecommerce\\_reverse\\_image\\_search/ecommerce-reverse-image-search.ipynb1-100](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/ecommerce-reverse-image-search.ipynb#L1-L100)\n\n## E-commerce Reverse Image Search\n\nThe e-commerce implementation enables visual product search using the Amazon Product Dataset 2020. Users can submit product images to find visually similar items in the catalog.\n\n### Dataset and Setup\n\nThe system processes product data with image URLs and metadata including product names, categories, and pricing information. The implementation uses a configurable dataset fraction for testing and development.\n\n```\n```\n\n**Key Implementation Details**\n\nThe dataset fraction is controlled by the `DATASET_FRACTION` variable, typically set to `0.1` for development to process only 10% of the full dataset.",
      "index": 2,
      "token_count": 444,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 3845,
      "end_char": 5831
    },
    {
      "content": "_FRACTION` variable, typically set to `0.1` for development to process only 10% of the full dataset. The system downloads images from provided URLs and extracts embeddings using vision models.\n\nSources: [ecommerce\\_reverse\\_image\\_search/ecommerce-reverse-image-search.ipynb275-277](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/ecommerce-reverse-image-search.ipynb#L275-L277) [ecommerce\\_reverse\\_image\\_search/ecommerce-reverse-image-search.ipynb51-58](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/ecommerce-reverse-image-search.ipynb#L51-L58)\n\n### Visual Search Implementation\n\nThe search system processes query images through the same embedding pipeline and performs cosine similarity matching against stored product vectors.\n\n```\n```\n\nSources: [ecommerce\\_reverse\\_image\\_search/ecommerce-reverse-image-search.ipynb1-50](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/ecommerce-reverse-image-search.ipynb#L1-L50)\n\n## Medical Image Search with Vision Transformers\n\nThe medical application focuses on skin cancer image analysis using the `marmal88/skin_cancer` dataset from Hugging Face. This system assists medical professionals in comparing diagnostic images.\n\n### Dataset Structure and Medical Metadata\n\nThe skin cancer dataset contains 9,577 images with comprehensive medical metadata including diagnosis types, patient demographics, and lesion locations.\n\n**Dataset Schema**\n\n| Field          | Type      | Description                                               |\n| -------------- | --------- | --------------------------------------------------------- |\n| `image`        | PIL Image | 600x450 RGB medical images                                |\n| `image_id`     | String    | Unique image identifier                                   |\n| `lesion_id`    | String    | Lesion type identifier                                    |\n| `dx`           | String    | Diagnosis (melanoma, basal\\_cell\\_carcinoma, etc.)        |",
      "index": 3,
      "token_count": 460,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 5731,
      "end_char": 7763
    },
    {
      "content": "|\n| `dx`           | String    | Diagnosis (melanoma, basal\\_cell\\_carcinoma, etc.)        |\n| `dx_type`      | String    | Diagnosis method (histo, follow\\_up, consensus, confocal) |\n| `age`          | Float     | Patient age (5-86 years)                                  |\n| `sex`          | String    | Patient gender (female, male, unknown)                    |\n| `localization` | String    | Body location of lesion                                   |\n\nSources: [qdrant\\_101\\_image\\_data/README.md23-34](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L23-L34) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb257-258](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L257-L258)\n\n### Vision Transformer Integration\n\nThe system uses Facebook's DINO model (`facebook/dino-vits16`) for feature extraction. The implementation processes images through `ViTImageProcessor` and generates 384-dimensional embeddings.\n\n```\n```\n\n**Key Technical Implementation**\n\nThe embedding process uses mean pooling across patches to compress the 197-patch ViT output into a single 384-dimensional vector per image.\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb204-207](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L204-L207) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb296-302](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L296-L302) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb187-192](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L187-L192)\n\n### Medical Search and Filtering\n\nThe medical search system supports demographic filtering to find cases similar to specific patient profiles. This enables targeted diagnostic assistance based on patient characteristics.\n\n```\n```\n\n**Filter Implementation Example**\n\nThe system implements complex demographic filtering using Qdrant's filter syntax:",
      "index": 4,
      "token_count": 662,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 7663,
      "end_char": 9712
    },
    {
      "content": "ation Example**\n\nThe system implements complex demographic filtering using Qdrant's filter syntax:\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb484-492](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L484-L492) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb496-503](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L496-L503)\n\n## Data Processing and Storage Pipeline\n\nBoth applications follow similar patterns for data ingestion, embedding generation, and vector storage in Qdrant collections.\n\n### Batch Processing and Collection Management\n\nThe systems implement batch processing for efficient embedding generation and storage. The medical application processes images in batches of 16, while the e-commerce system uses configurable batch sizes of 1000 for vector uploads.\n\n```\n```\n\n**Collection Configuration**\n\nBoth systems use cosine distance for similarity measurement, with embedding dimensions matching the model output (384 for ViT models).\n\nSources: [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb306-307](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L306-L307) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb380-398](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L380-L398) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb187-192](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L187-L192)\n\n### Metadata and Payload Management\n\nThe applications store comprehensive metadata alongside vectors to enable filtering and result enrichment. The medical system handles missing values by filling NaN ages with 0, while preserving other patient information.\n\n**Payload Structure**\n\n```\n```",
      "index": 5,
      "token_count": 614,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 9612,
      "end_char": 11481
    },
    {
      "content": "lling NaN ages with 0, while preserving other patient information.\n\n**Payload Structure**\n\n```\n```\n\nThe payload conversion process transforms HuggingFace dataset columns into Qdrant-compatible dictionary records, ensuring proper handling of missing values and data type compatibility.\n\nSources: [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb332-337](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L332-L337) [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb363-365](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb#L363-L365)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Image Data Applications](#image-data-applications.md)\n- [Core Architecture and Components](#core-architecture-and-components.md)\n- [E-commerce Reverse Image Search](#e-commerce-reverse-image-search.md)\n- [Dataset and Setup](#dataset-and-setup.md)\n- [Visual Search Implementation](#visual-search-implementation.md)\n- [Medical Image Search with Vision Transformers](#medical-image-search-with-vision-transformers.md)\n- [Dataset Structure and Medical Metadata](#dataset-structure-and-medical-metadata.md)\n- [Vision Transformer Integration](#vision-transformer-integration.md)\n- [Medical Search and Filtering](#medical-search-and-filtering.md)\n- [Data Processing and Storage Pipeline](#data-processing-and-storage-pipeline.md)\n- [Batch Processing and Collection Management](#batch-processing-and-collection-management.md)\n- [Metadata and Payload Management](#metadata-and-payload-management.md)",
      "index": 6,
      "token_count": 426,
      "metadata": {
        "title": "_qdrant_examples_4-image-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_4-image-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4-image-data-applications.md",
        "file_name": "_qdrant_examples_4-image-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.389972",
        "total_chunks": 7
      },
      "start_char": 11381,
      "end_char": 13429
    },
    {
      "content": "E-commerce Reverse Image Search | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_examples_4.1-e-commerce-reverse-image-search",
        "source": "qdrant_examples\\_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "file_name": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.422324",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2039
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# E-commerce Reverse Image Search\n\nRelevant source files\n\n- [ecommerce\\_reverse\\_image\\_search/ecommerce-reverse-image-search.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/ecommerce-reverse-image-search.ipynb)\n- [ecommerce\\_reverse\\_image\\_search/queries/cable.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/cable.jpg)\n- [ecommerce\\_reverse\\_image\\_search/queries/cleaning.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/cleaning.jpg)\n- [ecommerce\\_reverse\\_image\\_search/queries/skating.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/skating.jpg)\n- [ecommerce\\_reverse\\_image\\_search/queries/spoon.jpg](https://github.com/qdrant/examples/blob/b3c4b28f/ecommerce_reverse_image_search/queries/spoon.jpg)\n\n## Purpose and Scope\n\nThis document explains the implementation of a reverse image search system for e-commerce applications using Qdrant vector database. The system enables users to find visually similar products by uploading an image instead of typing text queries. This implementation focuses on:\n\n- Processing product images into vector embeddings\n- Storing these embeddings with product metadata in Qdrant\n- Performing efficient similarity searches based on user-uploaded images\n- Filtering results based on product attributes such as price and category\n\nFor general information about image data applications in Qdrant, see [Image Data Applications](qdrant/examples/4-image-data-applications.md). For medical imaging applications, see [Medical Image Search with ViT](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md).\n\n## System Overview\n\nThe e-commerce reverse image search system implements a complete pipeline for both indexing product images and handling search queries.\n\n```\n```\n\nSources:",
      "index": 1,
      "token_count": 477,
      "metadata": {
        "title": "_qdrant_examples_4.1-e-commerce-reverse-image-search",
        "source": "qdrant_examples\\_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "file_name": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.422324",
        "total_chunks": 6
      },
      "start_char": 1939,
      "end_char": 3984
    },
    {
      "content": "complete pipeline for both indexing product images and handling search queries.\n\n```\n```\n\nSources:\n\n## Data Flow and Processing\n\n### Product Data Ingestion\n\nThe system begins by processing a catalog of product images and their metadata:\n\n```\n```\n\nThe ingestion process involves:\n\n1. **Data Loading**: Product images and metadata are loaded from a structured dataset\n2. **Image Processing**: Images are resized, cropped, and normalized to meet the input requirements of the vision model\n3. **Embedding Generation**: A pre-trained vision model converts images into high-dimensional vector embeddings\n4. **Metadata Preparation**: Product information is structured for efficient storage and retrieval\n5. **Data Storage**: Both embeddings and metadata are stored in a Qdrant collection\n\nSources:\n\n### Search Process\n\nWhen a user submits a query image, the system processes it as follows:\n\n```\n```\n\nThe search workflow consists of:\n\n1. **Query Processing**: The query image undergoes the same preprocessing and embedding generation as catalog images\n2. **Filter Construction**: User-specified filters (e.g., category, price range) are converted into Qdrant filter conditions\n3. **Similarity Search**: Qdrant finds vectors similar to the query embedding, optionally filtered by metadata\n4. **Result Processing**: Search results are formatted and returned to the user\n\nSources:\n\n## Technical Implementation\n\n### Vision Model\n\nThe implementation uses a pre-trained vision model to generate embeddings from product images. Common options include:\n\n- ResNet models (ResNet-50, ResNet-101)\n- Vision Transformer (ViT)\n- CLIP (for multi-modal capabilities)\n\nThese models produce fixed-size vector embeddings (typically 512-2048 dimensions) that capture visual features of products.\n\n### Qdrant Collection Structure\n\nThe Qdrant collection for e-commerce products is configured with:\n\n```\n```\n\n### Product Metadata Schema\n\nEach product record in Qdrant includes:\n\n| Field          | Type   | Description                       |",
      "index": 2,
      "token_count": 407,
      "metadata": {
        "title": "_qdrant_examples_4.1-e-commerce-reverse-image-search",
        "source": "qdrant_examples\\_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "file_name": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.422324",
        "total_chunks": 6
      },
      "start_char": 3884,
      "end_char": 5896
    },
    {
      "content": "product record in Qdrant includes:\n\n| Field          | Type   | Description                       |\n| -------------- | ------ | --------------------------------- |\n| `product_id`   | string | Unique identifier for the product |\n| `name`         | string | Product name                      |\n| `description`  | string | Product description               |\n| `price`        | float  | Product price                     |\n| `currency`     | string | Currency code (e.g., USD, EUR)    |\n| `category`     | string | Product category                  |\n| `subcategory`  | string | Product subcategory               |\n| `brand`        | string | Product brand or manufacturer     |\n| `image_url`    | string | URL to the product image          |\n| `availability` | string | Product availability status       |\n\n### Vector Search with Filtering\n\nA key feature of the e-commerce implementation is the ability to combine vector similarity search with metadata filters:\n\n```\n```\n\nThis allows for queries like \"find visually similar clothing items between $20 and $100.\"\n\nSources:\n\n## Example Implementation Workflow\n\n### Indexing Product Data\n\nThe general workflow for product data indexing involves:\n\n1. **Dataset Preparation**: Organizing product images and metadata\n2. **Collection Initialization**: Creating a Qdrant collection with appropriate vector configuration\n3. **Batch Processing**: Generating embeddings for all product images\n4. **Batch Upload**: Storing embeddings and metadata in Qdrant\n\nExample pseudo-implementation:\n\n```\n```\n\n### Processing Search Queries\n\nThe search workflow handles user queries as follows:\n\n1. **Image Upload**: User submits a query image\n2. **Preprocessing**: Image is preprocessed as done during indexing\n3. **Embedding Generation**: Vision model generates an embedding for the query image\n4. **Filter Application**: Optional user filters are applied\n5. **Vector Search**: Similar products are retrieved from Qdrant\n6. **Result Presentation**: Results are returned to the user\n\nExample pseudo-implementation:\n\n```\n```",
      "index": 3,
      "token_count": 414,
      "metadata": {
        "title": "_qdrant_examples_4.1-e-commerce-reverse-image-search",
        "source": "qdrant_examples\\_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "file_name": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.422324",
        "total_chunks": 6
      },
      "start_char": 5796,
      "end_char": 7845
    },
    {
      "content": "**Result Presentation**: Results are returned to the user\n\nExample pseudo-implementation:\n\n```\n```\n\nSources:\n\n## Use Cases and Applications\n\nThe e-commerce reverse image search system enables several valuable scenarios:\n\n1. **Visual Product Discovery**: Users can find products by uploading images instead of typing keywords\n2. **\"Shop the Look\"**: Users can upload lifestyle images to find specific items or similar styles\n3. **Product Recommendations**: Visually similar products can be suggested based on what a user is viewing\n4. **Inventory Management**: Visually similar products can be identified for catalog organization\n5. **Competitive Analysis**: Products similar to competitors' offerings can be found\n\n## System Performance Considerations\n\n### Vector Indexing\n\nFor large e-commerce catalogs, vector indexing is crucial for performance:\n\n```\n```\n\nQdrant's HNSW (Hierarchical Navigable Small World) index provides efficient approximate nearest neighbor search, which is essential for real-time performance with large product catalogs.\n\n### Filtering Optimization\n\nPre-filtering with Qdrant improves performance by reducing the candidate set before vector comparison:\n\n```\n```\n\nSources:\n\n## Conclusion\n\nThe E-commerce Reverse Image Search example demonstrates how Qdrant can be used to implement a powerful visual search system for retail applications. By combining efficient vector search with metadata filtering, the system enables intuitive product discovery based on visual similarity.\n\nThis implementation can serve as a starting point for building production-ready visual search features in e-commerce platforms, with potential for enhancements such as multi-modal embeddings, attribute extraction, and personalization.\n\nSources:\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [E-commerce Reverse Image Search](#e-commerce-reverse-image-search.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Overview](#system-overview.md)\n- [Data Flow and Processing](#data-flow-and-processing.md)",
      "index": 4,
      "token_count": 387,
      "metadata": {
        "title": "_qdrant_examples_4.1-e-commerce-reverse-image-search",
        "source": "qdrant_examples\\_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "file_name": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.422324",
        "total_chunks": 6
      },
      "start_char": 7745,
      "end_char": 9781
    },
    {
      "content": "- [System Overview](#system-overview.md)\n- [Data Flow and Processing](#data-flow-and-processing.md)\n- [Product Data Ingestion](#product-data-ingestion.md)\n- [Search Process](#search-process.md)\n- [Technical Implementation](#technical-implementation.md)\n- [Vision Model](#vision-model.md)\n- [Qdrant Collection Structure](#qdrant-collection-structure.md)\n- [Product Metadata Schema](#product-metadata-schema.md)\n- [Vector Search with Filtering](#vector-search-with-filtering.md)\n- [Example Implementation Workflow](#example-implementation-workflow.md)\n- [Indexing Product Data](#indexing-product-data.md)\n- [Processing Search Queries](#processing-search-queries.md)\n- [Use Cases and Applications](#use-cases-and-applications.md)\n- [System Performance Considerations](#system-performance-considerations.md)\n- [Vector Indexing](#vector-indexing.md)\n- [Filtering Optimization](#filtering-optimization.md)\n- [Conclusion](#conclusion.md)",
      "index": 5,
      "token_count": 222,
      "metadata": {
        "title": "_qdrant_examples_4.1-e-commerce-reverse-image-search",
        "source": "qdrant_examples\\_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "file_name": "_qdrant_examples_4.1-e-commerce-reverse-image-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.422324",
        "total_chunks": 6
      },
      "start_char": 9681,
      "end_char": 11729
    },
    {
      "content": "Medical Image Search with Vision Transformers | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)",
      "index": 0,
      "token_count": 576,
      "metadata": {
        "title": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers",
        "source": "qdrant_examples\\_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "file_name": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.457642",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2001
    },
    {
      "content": "md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Medical Image Search with Vision Transformers\n\nRelevant source files\n\n- [qdrant\\_101\\_image\\_data/04\\_qdrant\\_101\\_cv.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/04_qdrant_101_cv.ipynb)\n- [qdrant\\_101\\_image\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md)\n\n## Purpose and Scope\n\nThis document describes the medical image search system that enables healthcare professionals to find similar skin lesion images using semantic search powered by Vision Transformers and Qdrant vector database. The system processes medical images to extract visual embeddings and provides similarity-based retrieval with advanced filtering capabilities for demographic and diagnostic criteria.\n\nFor general image search applications in e-commerce contexts, see [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md). For foundational Qdrant concepts, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md).\n\n## System Architecture Overview\n\nThe medical image search system combines computer vision models with vector database technology to enable semantic similarity search across dermatological images.\n\n### Core System Components\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md1-984](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L1-L984)\n\n### Data Flow Pipeline\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md295-398](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L295-L398) [qdrant\\_101\\_image\\_data/README.md453-520](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L453-L520)\n\n## Vision Transformer Implementation",
      "index": 1,
      "token_count": 546,
      "metadata": {
        "title": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers",
        "source": "qdrant_examples\\_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "file_name": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.457642",
        "total_chunks": 6
      },
      "start_char": 1901,
      "end_char": 3862
    },
    {
      "content": "les/blob/b3c4b28f/qdrant_101_image_data/README.md#L453-L520)\n\n## Vision Transformer Implementation\n\nThe system utilizes Facebook's DINO Vision Transformer model for extracting meaningful visual features from medical images.\n\n### Model Configuration\n\n| Component         | Specification                                               |\n| ----------------- | ----------------------------------------------------------- |\n| Processor         | `ViTImageProcessor.from_pretrained('facebook/dino-vits16')` |\n| Model             | `ViTModel.from_pretrained('facebook/dino-vits16')`          |\n| Input Size        | 224x224 pixels (3 channels)                                 |\n| Output Dimensions | 384-dimensional embeddings                                  |\n| Patch Processing  | 197 patches per image                                       |\n| Pooling Method    | Mean pooling across patches                                 |\n\nThe embedding extraction process transforms raw medical images into dense vector representations that capture visual similarities relevant for diagnostic comparison.\n\nSources: [qdrant\\_101\\_image\\_data/README.md203-207](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L203-L207) [qdrant\\_101\\_image\\_data/README.md295-302](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L295-L302)\n\n### Dataset Structure\n\nThe system processes a comprehensive skin lesion dataset with the following metadata schema:\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md23-34](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L23-L34) [qdrant\\_101\\_image\\_data/README.md332-360](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L332-L360)\n\n## Qdrant Vector Database Integration\n\nThe system leverages Qdrant for efficient similarity search and metadata filtering across medical image embeddings.\n\n### Collection Configuration\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md95-100](https://github.",
      "index": 2,
      "token_count": 532,
      "metadata": {
        "title": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers",
        "source": "qdrant_examples\\_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "file_name": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.457642",
        "total_chunks": 6
      },
      "start_char": 3762,
      "end_char": 5800
    },
    {
      "content": "llection Configuration\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md95-100](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L95-L100) [qdrant\\_101\\_image\\_data/README.md379-398](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L379-L398)\n\n## Advanced Search Capabilities\n\nThe system provides sophisticated search functionality tailored for medical diagnostic applications.\n\n### Filter Types and Use Cases\n\n| Filter Type | Purpose                             | Example Implementation                                                     |\n| ----------- | ----------------------------------- | -------------------------------------------------------------------------- |\n| Demographic | Age/gender-based filtering          | `FieldCondition(key=\"sex\", match=MatchValue(value=\"female\"))`              |\n| Diagnostic  | Disease category filtering          | `FieldCondition(key=\"dx\", match=MatchExcept(except=[\"melanoma\"]))`         |\n| Anatomical  | Body location filtering             | `FieldCondition(key=\"localization\", match=MatchAny(any=[\"face\", \"neck\"]))` |\n| ID-based    | Specific sample inclusion/exclusion | `HasIdCondition(has_id=range_list)`                                        |\n| Score-based | Similarity threshold filtering      | `score_threshold=0.92`                                                     |\n\n### Search Query Patterns\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md484-520](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L484-L520) [qdrant\\_101\\_image\\_data/README.md632-704](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L632-L704) [qdrant\\_101\\_image\\_data/README.md872-900](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L872-L900)\n\n## Streamlit User Interface\n\nThe system includes a web-based interface that enables medical professionals to upload images and retrieve similar cases from the database.\n\n### Application Components",
      "index": 3,
      "token_count": 541,
      "metadata": {
        "title": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers",
        "source": "qdrant_examples\\_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "file_name": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.457642",
        "total_chunks": 6
      },
      "start_char": 5700,
      "end_char": 7749
    },
    {
      "content": "ssionals to upload images and retrieve similar cases from the database.\n\n### Application Components\n\n```\n```\n\nSources: [qdrant\\_101\\_image\\_data/README.md928-964](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L928-L964)\n\n## Implementation Details\n\n### Key Functions and Methods\n\n| Function                     | Purpose                                   | Location                                                                                                                                |\n| ---------------------------- | ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n| `get_embeddings(batch)`      | Extract ViT embeddings from image batches | [qdrant\\_101\\_image\\_data/README.md296-302](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L296-L302) |\n| `see_images(results, top_k)` | Visualize search results with metadata    | [qdrant\\_101\\_image\\_data/README.md532-548](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L532-L548) |\n| `client.search()`            | Execute similarity search with filters    | [qdrant\\_101\\_image\\_data/README.md454-458](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L454-L458) |\n| `client.search_batch()`      | Process multiple search requests          | [qdrant\\_101\\_image\\_data/README.md896-900](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L896-L900) |\n\n### Performance Considerations\n\nThe system processes 9,577 medical images with the following specifications:\n\n- Batch processing: 16 images per batch for embedding extraction\n- Storage efficiency: 384-dimensional vectors with cosine similarity\n- Upsert batching: 1,000 vectors per database transaction\n- Query performance: Filtered search across demographic and diagnostic criteria\n\nSources: [qdrant\\_101\\_image\\_data/README.",
      "index": 4,
      "token_count": 527,
      "metadata": {
        "title": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers",
        "source": "qdrant_examples\\_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "file_name": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.457642",
        "total_chunks": 6
      },
      "start_char": 7649,
      "end_char": 9679
    },
    {
      "content": "ltered search across demographic and diagnostic criteria\n\nSources: [qdrant\\_101\\_image\\_data/README.md306-307](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L306-L307) [qdrant\\_101\\_image\\_data/README.md380-398](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_image_data/README.md#L380-L398)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Medical Image Search with Vision Transformers](#medical-image-search-with-vision-transformers.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture Overview](#system-architecture-overview.md)\n- [Core System Components](#core-system-components.md)\n- [Data Flow Pipeline](#data-flow-pipeline.md)\n- [Vision Transformer Implementation](#vision-transformer-implementation.md)\n- [Model Configuration](#model-configuration.md)\n- [Dataset Structure](#dataset-structure.md)\n- [Qdrant Vector Database Integration](#qdrant-vector-database-integration.md)\n- [Collection Configuration](#collection-configuration.md)\n- [Advanced Search Capabilities](#advanced-search-capabilities.md)\n- [Filter Types and Use Cases](#filter-types-and-use-cases.md)\n- [Search Query Patterns](#search-query-patterns.md)\n- [Streamlit User Interface](#streamlit-user-interface.md)\n- [Application Components](#application-components.md)\n- [Implementation Details](#implementation-details.md)\n- [Key Functions and Methods](#key-functions-and-methods.md)\n- [Performance Considerations](#performance-considerations.md)",
      "index": 5,
      "token_count": 396,
      "metadata": {
        "title": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers",
        "source": "qdrant_examples\\_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "file_name": "_qdrant_examples_4.2-medical-image-search-with-vision-transformers.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.457642",
        "total_chunks": 6
      },
      "start_char": 9579,
      "end_char": 11627
    },
    {
      "content": "Audio Data Applications | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 586,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2031
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Audio Data Applications\n\nRelevant source files\n\n- [qdrant\\_101\\_audio\\_data/03\\_qdrant\\_101\\_audio.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/03_qdrant_101_audio.ipynb)\n- [qdrant\\_101\\_audio\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md)\n- [qdrant\\_101\\_text\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_25\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_25_0.png)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_28\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_28_0.png)\n\nThis document covers audio data processing and music recommendation systems using Qdrant vector database. The implementation demonstrates how to extract embeddings from audio files and build semantic search and recommendation engines for music discovery.\n\nFor text-based search and recommendations, see [Text Data Applications](qdrant/examples/3-text-data-applications.md). For image-based similarity search, see [Image Data Applications](qdrant/examples/4-image-data-applications.md).\n\n## Overview\n\nThe audio data applications showcase a complete pipeline for processing music files and building recommendation systems. The implementation uses the Ludwig Music Dataset containing over 10,000 songs across different genres and subgenres, demonstrating three different approaches to audio embedding generation and their integration with Qdrant's vector search capabilities.\n\n**Audio Processing Pipeline**\n\n```\n```\n\nSources: [qdrant\\_101\\_audio\\_data/README.md1-52](https://github.",
      "index": 1,
      "token_count": 570,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 1931,
      "end_char": 3944
    },
    {
      "content": "io Processing Pipeline**\n\n```\n```\n\nSources: [qdrant\\_101\\_audio\\_data/README.md1-52](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1-L52) [qdrant\\_101\\_audio\\_data/README.md620-642](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L620-L642)\n\n## Dataset and Data Preparation\n\nThe system processes the Ludwig Music Dataset, which provides a comprehensive collection for music information retrieval (MIR). The dataset structure includes multiple data modalities and metadata for each track.\n\n**Dataset Structure**\n\n| Component     | Description                         | Purpose                                   |\n| ------------- | ----------------------------------- | ----------------------------------------- |\n| `mp3/`        | Audio files by genre                | Raw audio data for embedding generation   |\n| `labels.json` | Track metadata                      | Artist, genre, subgenre, name information |\n| `spectogram/` | Visual frequency representations    | Alternative data representation           |\n| `mfccs/`      | Mel-frequency cepstral coefficients | Audio feature representations             |\n\n**Data Processing Components**\n\n```\n```\n\nThe data preparation pipeline extracts unique identifiers from audio file paths and processes the complex nested metadata structure from `labels.json`. The `get_metadata()` function normalizes artist, genre, name, and subgenre information, while `get_vals()` flattens the subgenre lists for easier processing.\n\nSources: [qdrant\\_101\\_audio\\_data/README.md131-617](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L131-L617) [qdrant\\_101\\_audio\\_data/README.md196-235](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L196-L235)\n\n## Audio Embedding Generation\n\nThe system implements three distinct approaches for generating audio embeddings, each with different characteristics and use cases.",
      "index": 2,
      "token_count": 488,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 3844,
      "end_char": 5822
    },
    {
      "content": "tinct approaches for generating audio embeddings, each with different characteristics and use cases. The embeddings capture important audio features such as pitch, timbre, and spatial characteristics of sound.\n\n**Embedding Architecture Comparison**\n\n```\n```\n\n### OpenL3 Implementation\n\nOpenL3 provides pre-trained models specifically designed for audio processing tasks. The implementation uses the music-specific model with mel128 input representation.\n\n**Key Functions:**\n\n- `get_open_embs()`: Batch processing function for embedding extraction\n- `openl3.models.load_audio_embedding_model()`: Model initialization with specific parameters\n- Mean pooling over timestamp dimension for fixed-size embeddings\n\n### PANNS Inference Implementation\n\nThe PANNS (PANNs: Large-Scale Pretrained Audio Neural Networks) approach offers the best performance for music classification tasks and is used as the primary method in the tutorial.\n\n**Core Components:**\n\n- `AudioTagging` class initialization with automatic checkpoint download\n- `at.inference()` method for batch processing\n- Direct 2048-dimensional embedding output without additional pooling\n\n### Transformers Wav2Vec2 Implementation\n\nThe Wav2Vec2 approach demonstrates transformer architecture adaptation for audio, though optimized for speech rather than music.\n\n**Processing Pipeline:**\n\n- Audio resampling to 16kHz for model compatibility\n- `AutoFeatureExtractor` for input preprocessing\n- `mean_pooling()` function for sequence-to-vector conversion\n\nSources: [qdrant\\_101\\_audio\\_data/README.md644-897](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L644-L897) [qdrant\\_101\\_audio\\_data/README.md746-800](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L746-L800)\n\n## Qdrant Vector Database Integration\n\nThe audio embeddings are stored and managed using Qdrant vector database, configured specifically for audio similarity search and recommendation tasks.\n\n**Database Configuration**\n\n```\n```\n\n### Collection Configuration",
      "index": 3,
      "token_count": 458,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 5722,
      "end_char": 7766
    },
    {
      "content": "earch and recommendation tasks.\n\n**Database Configuration**\n\n```\n```\n\n### Collection Configuration\n\nThe `music_collection` is configured with:\n\n- **Vector dimension**: 2048 (matching PANNS output)\n- **Distance metric**: COSINE similarity\n- **Payload structure**: Artist, genre, name, subgenres, file URLs\n\n### Payload Structure\n\nThe metadata payload contains structured information for each track:\n\n```\n```\n\nSources: [qdrant\\_101\\_audio\\_data/README.md898-919](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L898-L919) [qdrant\\_101\\_audio\\_data/README.md115-126](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L115-L126)\n\n## Music Recommendation System\n\nThe recommendation system leverages Qdrant's search and recommendation APIs to provide semantic music discovery based on audio content similarity.\n\n**Recommendation Architecture**\n\n```\n```\n\n### Search Functionality\n\nThe system implements multiple search patterns:\n\n1. **Vector Similarity Search**: Direct comparison using pre-computed embeddings\n2. **Recommendation API**: Using positive and negative examples for refined results\n3. **Filtered Search**: Combining semantic similarity with metadata constraints\n\n### Example Usage Patterns\n\n**Basic Similarity Search**:\n\n- Query with embedding vector from existing track\n- Returns top-k most similar tracks with similarity scores\n- Includes full metadata payload for each result\n\n**Recommendation with Preferences**:\n\n- Specify positive examples (liked tracks)\n- Optional negative examples (disliked tracks)\n- Qdrant computes optimized recommendation vector\n\n**Genre-Filtered Search**:\n\n- Combine vector similarity with metadata filtering\n- Use `models.Filter` and `models.FieldCondition` for constraints\n- Example: Find similar tracks within \"Business\" or \"Latin\" genres\n\nSources: [qdrant\\_101\\_audio\\_data/README.md920-1216](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L920-L1216) [qdrant\\_101\\_audio\\_data/README.",
      "index": 4,
      "token_count": 509,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 7666,
      "end_char": 9689
    },
    {
      "content": "examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L920-L1216) [qdrant\\_101\\_audio\\_data/README.md1053-1091](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1053-L1091)\n\n## System Integration and Architecture\n\nThe complete audio processing system integrates multiple components into a cohesive music recommendation platform.\n\n**End-to-End System Flow**\n\n```\n```\n\n### Key Performance Characteristics\n\n| Embedding Method | Dimensions | Quality | Speed  | Use Case            |\n| ---------------- | ---------- | ------- | ------ | ------------------- |\n| OpenL3           | 512        | High    | Slow   | Research/Accuracy   |\n| PANNS Inference  | 2048       | Highest | Fast   | Production          |\n| Wav2Vec2         | 768        | Lower   | Medium | Speech/Experimental |\n\n### Scalability Considerations\n\nThe system architecture supports scaling through:\n\n- **Batch Processing**: Efficient embedding generation for large datasets\n- **Vector Database**: Qdrant's optimized storage and retrieval\n- **Flexible Embedding**: Support for multiple embedding approaches\n- **Metadata Integration**: Rich payload structure for complex queries\n\nSources: [qdrant\\_101\\_audio\\_data/README.md1-52](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1-L52) [qdrant\\_101\\_audio\\_data/README.md898-1216](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L898-L1216)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Audio Data Applications](#audio-data-applications.md)\n- [Overview](#overview.md)\n- [Dataset and Data Preparation](#dataset-and-data-preparation.md)\n- [Audio Embedding Generation](#audio-embedding-generation.md)\n- [OpenL3 Implementation](#openl3-implementation.md)\n- [PANNS Inference Implementation](#panns-inference-implementation.md)\n- [Transformers Wav2Vec2 Implementation](#transformers-wav2vec2-implementation.md)\n- [Qdrant Vector Database Integration](#qdrant-vector-database-integration.md)",
      "index": 5,
      "token_count": 561,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 9589,
      "end_char": 11612
    },
    {
      "content": "2-implementation.md)\n- [Qdrant Vector Database Integration](#qdrant-vector-database-integration.md)\n- [Collection Configuration](#collection-configuration.md)\n- [Payload Structure](#payload-structure.md)\n- [Music Recommendation System](#music-recommendation-system.md)\n- [Search Functionality](#search-functionality.md)\n- [Example Usage Patterns](#example-usage-patterns.md)\n- [System Integration and Architecture](#system-integration-and-architecture.md)\n- [Key Performance Characteristics](#key-performance-characteristics.md)\n- [Scalability Considerations](#scalability-considerations.md)",
      "index": 6,
      "token_count": 132,
      "metadata": {
        "title": "_qdrant_examples_5-audio-data-applications",
        "source": "qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5-audio-data-applications.md",
        "file_name": "_qdrant_examples_5-audio-data-applications.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.619878",
        "total_chunks": 7
      },
      "start_char": 11512,
      "end_char": 13560
    },
    {
      "content": "Music Recommendation Engine | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 586,
      "metadata": {
        "title": "_qdrant_examples_5.1-music-recommendation-engine",
        "source": "qdrant_examples\\_qdrant_examples_5.1-music-recommendation-engine.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "file_name": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.659490",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2035
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Music Recommendation Engine\n\nRelevant source files\n\n- [qdrant\\_101\\_audio\\_data/03\\_qdrant\\_101\\_audio.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/03_qdrant_101_audio.ipynb)\n- [qdrant\\_101\\_audio\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md)\n- [qdrant\\_101\\_text\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_25\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_25_0.png)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_28\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_28_0.png)\n\nThis system demonstrates how to build a music recommendation engine using audio embeddings and Qdrant vector database. The implementation covers the complete pipeline from audio data preprocessing through embedding generation to building a web-based recommendation interface using the Ludwig Music Dataset.\n\nFor information about other recommendation systems in this repository, see [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md).\n\n## System Architecture\n\nThe music recommendation system follows a multi-stage pipeline that transforms raw audio files into searchable vector representations:\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md1-1300](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1-L1300)\n\n## Data Pipeline\n\n### Dataset Structure\n\nThe system uses the Ludwig Music Dataset containing over 10,000 songs across multiple genres:",
      "index": 1,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_examples_5.1-music-recommendation-engine",
        "source": "qdrant_examples\\_qdrant_examples_5.1-music-recommendation-engine.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "file_name": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.659490",
        "total_chunks": 6
      },
      "start_char": 1935,
      "end_char": 3921
    },
    {
      "content": "ure\n\nThe system uses the Ludwig Music Dataset containing over 10,000 songs across multiple genres:\n\n| Component     | Description                               | Location              |\n| ------------- | ----------------------------------------- | --------------------- |\n| `mp3/`        | Audio files organized by genre            | Genre subdirectories  |\n| `labels.json` | Track metadata (artist, genre, subgenres) | Root directory        |\n| `spectogram/` | Visual frequency representations          | Optional for analysis |\n| `mfccs/`      | Mel-frequency cepstral coefficients       | Alternative features  |\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md21-37](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L21-L37)\n\n### Data Preprocessing\n\nThe preprocessing pipeline transforms raw audio data into structured formats:\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md133-617](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L133-L617)\n\n## Embedding Generation\n\n### Multiple Model Approaches\n\nThe system supports three different audio embedding approaches, each with distinct characteristics:\n\n| Model    | Library           | Dimensions | Use Case             | Performance       |\n| -------- | ----------------- | ---------- | -------------------- | ----------------- |\n| PANNs    | `panns_inference` | 2048       | Music classification | Fast inference    |\n| OpenL3   | `openl3`          | 512        | General audio        | High quality      |\n| Wav2Vec2 | `transformers`    | 768        | Speech-focused       | Research baseline |\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md631-640](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L631-L640)\n\n### PANNs Implementation\n\nThe primary embedding approach uses PANNs (Pre-trained Audio Neural Networks):\n\n```\n```\n\nThe `get_panns_embs` function processes audio in batches:\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md715-810](https://github.",
      "index": 2,
      "token_count": 534,
      "metadata": {
        "title": "_qdrant_examples_5.1-music-recommendation-engine",
        "source": "qdrant_examples\\_qdrant_examples_5.1-music-recommendation-engine.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "file_name": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.659490",
        "total_chunks": 6
      },
      "start_char": 3821,
      "end_char": 5845
    },
    {
      "content": "rocesses audio in batches:\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md715-810](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L715-L810)\n\n## Vector Database Configuration\n\n### Qdrant Collection Setup\n\nThe system creates a dedicated collection with specific parameters:\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md115-125](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L115-L125) [qdrant\\_101\\_audio\\_data/README.md910-925](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L910-L925)\n\n### Payload Structure\n\nEach vector point contains rich metadata for filtering and display:\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md594-617](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L594-L617)\n\n## Query Operations\n\n### Search Functionality\n\nThe system provides semantic similarity search using vector queries:\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md964-1100](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L964-L1100)\n\n### Recommendation System\n\nThe recommendation API uses positive and negative examples:\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md1085-1220](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1085-L1220)\n\n## Performance Characteristics\n\n### Model Comparison\n\nBased on the implementation, the different embedding approaches offer distinct trade-offs:\n\n| Aspect           | PANNs     | OpenL3          | Wav2Vec2        |\n| ---------------- | --------- | --------------- | --------------- |\n| Inference Speed  | Fast      | Slowest         | Medium          |\n| Music Quality    | Excellent | Excellent       | Poor            |\n| Model Size       | Large     | Medium          | Large           |\n| GPU Support      | Yes       | Yes             | Yes             |\n| Batch Processing | Efficient | Manual batching | Manual batching |\n\n### Collection Statistics",
      "index": 3,
      "token_count": 611,
      "metadata": {
        "title": "_qdrant_examples_5.1-music-recommendation-engine",
        "source": "qdrant_examples\\_qdrant_examples_5.1-music-recommendation-engine.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "file_name": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.659490",
        "total_chunks": 6
      },
      "start_char": 5745,
      "end_char": 7787
    },
    {
      "content": "|\n| Batch Processing | Efficient | Manual batching | Manual batching |\n\n### Collection Statistics\n\nThe tutorial demonstrates performance with a Latin music subset:\n\n- **Dataset Size**: 979 songs (subset of full dataset)\n- **Vector Dimensions**: 2048 (PANNs embeddings)\n- **Distance Metric**: Cosine similarity\n- **Search Speed**: Real-time for interactive use\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md707-896](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L707-L896)\n\n## Integration Examples\n\n### Basic Search Example\n\n```\n```\n\n### Filtered Recommendation\n\n```\n```\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md1052-1220](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1052-L1220)\n\n## Usage Patterns\n\nThe system supports multiple interaction patterns:\n\n1. **Content-Based Similarity**: Find songs similar to a given track using `client.search()`\n2. **Collaborative Filtering**: Use recommendation API with positive/negative feedback\n3. **Filtered Discovery**: Combine vector search with metadata filters for genre-specific recommendations\n4. **Interactive Exploration**: Web interface for real-time music discovery\n\nThis implementation provides a foundation for building production music recommendation systems with Qdrant's vector database capabilities.\n\n**Sources:** [qdrant\\_101\\_audio\\_data/README.md898-1220](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L898-L1220)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Music Recommendation Engine](#music-recommendation-engine.md)\n- [System Architecture](#system-architecture.md)\n- [Data Pipeline](#data-pipeline.md)\n- [Dataset Structure](#dataset-structure.md)\n- [Data Preprocessing](#data-preprocessing.md)\n- [Embedding Generation](#embedding-generation.md)\n- [Multiple Model Approaches](#multiple-model-approaches.md)\n- [PANNs Implementation](#panns-implementation.md)\n- [Vector Database Configuration](#vector-database-configuration.md)",
      "index": 4,
      "token_count": 531,
      "metadata": {
        "title": "_qdrant_examples_5.1-music-recommendation-engine",
        "source": "qdrant_examples\\_qdrant_examples_5.1-music-recommendation-engine.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "file_name": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.659490",
        "total_chunks": 6
      },
      "start_char": 7687,
      "end_char": 9722
    },
    {
      "content": "ion](#panns-implementation.md)\n- [Vector Database Configuration](#vector-database-configuration.md)\n- [Qdrant Collection Setup](#qdrant-collection-setup.md)\n- [Payload Structure](#payload-structure.md)\n- [Query Operations](#query-operations.md)\n- [Search Functionality](#search-functionality.md)\n- [Recommendation System](#recommendation-system.md)\n- [Performance Characteristics](#performance-characteristics.md)\n- [Model Comparison](#model-comparison.md)\n- [Collection Statistics](#collection-statistics.md)\n- [Integration Examples](#integration-examples.md)\n- [Basic Search Example](#basic-search-example.md)\n- [Filtered Recommendation](#filtered-recommendation.md)\n- [Usage Patterns](#usage-patterns.md)",
      "index": 5,
      "token_count": 164,
      "metadata": {
        "title": "_qdrant_examples_5.1-music-recommendation-engine",
        "source": "qdrant_examples\\_qdrant_examples_5.1-music-recommendation-engine.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "file_name": "_qdrant_examples_5.1-music-recommendation-engine.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.659490",
        "total_chunks": 6
      },
      "start_char": 9622,
      "end_char": 11670
    },
    {
      "content": "Advanced RAG Systems | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 587,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 0,
      "end_char": 2028
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Advanced RAG Systems\n\nRelevant source files\n\n- [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb)\n- [graphrag\\_neo4j/readme.md](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md)\n- [multivector-representation/multivector\\_representation\\_qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb)\n- [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb)\n\nThis page provides an overview of sophisticated Retrieval-Augmented Generation (RAG) systems implemented in the Qdrant examples repository. While basic RAG systems enhance Large Language Models (LLMs) with relevant context retrieved from a vector database, advanced RAG systems incorporate additional techniques such as multivector search, graph relationships, and specialized document processing to improve retrieval quality and response accuracy.\n\nThe scope of this document covers:\n\n- Multivector RAG with DSPy framework for medical applications\n- Graph-enhanced RAG using Neo4j for relationship-aware retrieval\n- PDF retrieval at scale using visual document understanding models\n\nFor information about basic vector operations, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md). For specific text applications that use simpler RAG, see [Text Data Applications](qdrant/examples/3-text-data-applications.md).\n\nSources: [multivector-representation/multivector\\_representation\\_qdrant.ipynb8-23](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L8-L23) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb20-29](https://github.",
      "index": 1,
      "token_count": 542,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 1928,
      "end_char": 3956
    },
    {
      "content": "tation_qdrant.ipynb#L8-L23) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb20-29](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L20-L29) [graphrag\\_neo4j/readme.md3-12](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L3-L12) [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb8-27](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb#L8-L27)\n\n## Architecture Overview of Advanced RAG Systems\n\nAdvanced RAG systems extend the basic RAG architecture by incorporating sophisticated retrieval techniques that go beyond simple vector similarity search. These enhancements address common limitations in traditional RAG systems, such as the inability to capture complex relationships between entities, fine-grained token-level matching, or visual document understanding.\n\n**Advanced RAG System Architecture**\n\n```\n```\n\nThe key distinguishing factors of advanced RAG systems include multivector search capabilities, graph-enhanced context, and framework-based orchestration for complex reasoning tasks.\n\nSources: [multivector-representation/multivector\\_representation\\_qdrant.ipynb207-225](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L207-L225) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb552-571](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L552-L571) [graphrag\\_neo4j/readme.md7-11](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L7-L11)\n\n### Comparison of Basic vs Advanced RAG\n\n| Feature                          | Basic RAG                             | Advanced RAG                                                                      |\n| -------------------------------- | ------------------------------------- | --------------------------------------------------------------------------------- |",
      "index": 2,
      "token_count": 534,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 3856,
      "end_char": 5847
    },
    {
      "content": "------------- | --------------------------------------------------------------------------------- |\n| Context Source                   | Direct document retrieval             | Enhanced with graph relationships, multivector reranking, or visual understanding |\n| Retrieval Method                 | Single embedding similarity search    | Hybrid approaches: prefetch + rerank, graph + vector, visual + textual            |\n| Vector Representations           | Single dense vector per document      | Multiple vectors: dense + ColBERT, visual + textual                               |\n| Response Quality                 | Good for simple factual questions     | Better for complex, relationship-based, or multimodal queries                     |\n| Implementation Complexity        | Lower                                 | Higher                                                                            |\n| Handling of Token-level Matching | Limited to document-level similarity  | Fine-grained token interactions via ColBERT                                       |\n| Understanding of Relationships   | Limited to what's in single documents | Can traverse complex entity relationships via graph databases                     |\n| Framework Integration            | Basic LLM calls                       | Sophisticated frameworks like DSPy with guardrails and reasoning                  |\n\nSources: [multivector-representation/multivector\\_representation\\_qdrant.ipynb337-354](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L337-L354) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb785-794](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L785-L794) [graphrag\\_neo4j/readme.md5-12](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L5-L12)\n\n## Multivector RAG with DSPy\n\nMultivector RAG combines dense embeddings for efficient retrieval with ColBERT multivectors for fine-grained reranking.",
      "index": 3,
      "token_count": 421,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 5747,
      "end_char": 7780
    },
    {
      "content": "bines dense embeddings for efficient retrieval with ColBERT multivectors for fine-grained reranking. This approach addresses limitations of single-vector systems by enabling token-level interactions while maintaining search efficiency.\n\n**Multivector RAG Architecture**\n\n```\n```\n\n### Key Components\n\n1. **Dual Embedding Models**:\n\n   - `BAAI/bge-small-en` for dense embeddings (384 dimensions)\n   - `colbert-ir/colbertv2.0` for late-interaction multivectors (128 dimensions)\n\n2. **Qdrant Configuration**:\n\n   - Dense vector with HNSW indexing enabled for fast retrieval\n   - ColBERT multivector with indexing disabled (`hnsw_config=models.HnswConfigDiff(m=0)`) for reranking\n\n3. **DSPy Framework Integration**:\n\n   - `QdrantRM` retrieval module for DSPy integration\n   - `dspy.ChainOfThought` for structured reasoning\n   - Guardrails for domain-specific constraints\n\n4. **Two-Stage Retrieval Process**:\n\n   - Prefetch candidates using dense vector search\n   - Rerank using ColBERT multivector with MaxSim comparator\n\n### Implementation Details\n\nThe multivector system uses Qdrant's native support for multiple vector types per document:\n\n```\n```\n\nThe retrieval process combines both vector types in a single query:\n\n```\n```\n\nSources: [multivector-representation/multivector\\_representation\\_qdrant.ipynb207-225](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L207-L225) [multivector-representation/multivector\\_representation\\_qdrant.ipynb292-302](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L292-L302) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb222-252](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L222-L252) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb376-383](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L376-L383)\n\n## Graph-Enhanced RAG with Neo4j",
      "index": 4,
      "token_count": 589,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 7680,
      "end_char": 9691
    },
    {
      "content": "c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L376-L383)\n\n## Graph-Enhanced RAG with Neo4j\n\nGraph-Enhanced RAG combines traditional vector-based retrieval with graph database capabilities to capture and utilize relationships between entities in the data. This approach significantly improves response quality for queries that require understanding complex relationships.\n\n**Graph-Enhanced RAG Architecture**\n\n```\n```\n\n### Key Components\n\n1. **Graph Extraction Pipeline**:\n\n   - Uses OpenAI's GPT models to parse raw text and extract structured entities and relationships\n   - Outputs graph components in a structured JSON format, including node-relationship-node triples\n\n2. **Dual Storage System**:\n\n   - **Qdrant**: Stores text embeddings for semantic search\n   - **Neo4j**: Stores graph structure (entities as nodes, connections as edges)\n\n3. **Hybrid Query Processing**:\n\n   - Vector search in Qdrant identifies relevant text passages\n   - Graph queries in Neo4j retrieve related entities and their relationships\n   - Both results are combined to form a comprehensive context\n\n4. **Enhanced Context Generation**:\n\n   - The enriched context contains both relevant text and graph relationships\n   - Provides the language model with structured information about connections between entities\n\n### Implementation Details\n\nThe implementation follows these steps:\n\n1. **Environment Initialization**: Loads API keys and database credentials from environment variables\n\n2. **Graph Extraction**:\n\n   - Uses OpenAI to convert unstructured text into a structured graph\n   - The output includes entities (nodes) and their relationships (edges)\n\n3. **Data Ingestion**:\n\n   - Neo4j: Inserts extracted nodes labeled as `Entity` and their relationships\n   - Qdrant: Computes embeddings for text segments and uploads them to a collection\n\n4. **Retrieval & Graph Querying**:\n\n   - Performs vector search in Qdrant to find relevant text\n   - Queries Neo4j to fetch related graph context\n   - Combines both results to enrich the prompt\n\n5.",
      "index": 5,
      "token_count": 433,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 9591,
      "end_char": 11621
    },
    {
      "content": "- Queries Neo4j to fetch related graph context\n   - Combines both results to enrich the prompt\n\n5. **Response Generation**:\n\n   - Uses the enriched context to generate detailed answers via OpenAI's GPT\n\nSources: [graphrag\\_neo4j/readme.md5-130](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L5-L130)\n\n## PDF Retrieval at Scale\n\nPDF retrieval at scale addresses the challenge of efficiently searching large collections of visual documents using Vision Language Models (VLLMs) like ColPali and ColQwen2. These models work directly with PDF pages as images, avoiding complex OCR and text extraction pipelines.\n\n**PDF Retrieval at Scale Architecture**\n\n```\n```\n\n### Key Components\n\n1. **Vision Language Models**:\n\n   - **ColPali**: Generates \\~1,024 vectors per PDF page\n   - **ColQwen2**: Generates \\~700 vectors per page (dynamically adjusted)\n\n2. **Scaling Challenge**:\n\n   - Building HNSW index with full vectors requires \\~49 million comparisons per page\n   - For 20,000 pages, this becomes computationally prohibitive\n\n3. **Optimization Strategy**:\n\n   - Apply mean pooling to reduce vectors (e.g., 1,024 patches → 32 vectors)\n   - Use compressed vectors for first-stage retrieval\n   - Keep full vectors for precise reranking\n\n4. **Two-Stage Process**:\n\n   - **Stage 1**: Fast retrieval using mean-pooled vectors\n   - **Stage 2**: Rerank top candidates using original full-resolution vectors\n\n### Mathematical Foundation\n\nThe scaling problem is quantified in the computational complexity:\n\nFor ColQwen2 with \\~700 vectors per page and HNSW `ef_construct=100`:\n\n- Comparisons per page: `700 × 700 × 100 = 49,000,000`\n- For a 20,000 page collection: `49M × 20,000 = 980 trillion comparisons`\n\nMean pooling reduces this by organizing patches into a grid and averaging within groups:\n\n- ColPali: `1,024 patches → 32×32 grid → 32 pooled vectors`\n- Reduction factor: `1,024 / 32 = 32x` fewer vectors\n\n### Implementation Approach\n\nThe system processes PDF pages as visual documents:\n\n1.",
      "index": 6,
      "token_count": 554,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 11521,
      "end_char": 13532
    },
    {
      "content": "fewer vectors\n\n### Implementation Approach\n\nThe system processes PDF pages as visual documents:\n\n1. **PDF Page Processing**: Convert each page to an image representation\n2. **Multivector Generation**: Use ColPali/ColQwen2 to generate patch-based embeddings\n3. **Mean Pooling**: Group patches by spatial location and average embeddings\n4. **Dual Storage**: Store both compressed (indexed) and full (non-indexed) vectors\n5. **Query Processing**: First retrieve with compressed vectors, then rerank with full vectors\n\nSources: [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb36-67](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb#L36-L67) [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb40-53](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb#L40-L53)\n\n## When to Use Each Advanced RAG Approach\n\n| Aspect                        | Multivector RAG                                          | Graph-Enhanced RAG                                            | PDF Retrieval at Scale                                  |\n| ----------------------------- | -------------------------------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------- |\n| **Best For**                  | Fine-grained text matching                               | Relationship-heavy domains                                    | Visual document collections                             |\n| **Ideal Use Cases**           | Medical Q\\&A, technical documentation, precise retrieval | Knowledge graphs, organizational data, entity-centric domains | Academic papers, reports, forms, mixed-format documents |\n| **Key Strength**              | Token-level similarity                                   | Entity relationships                                          | Visual understanding without OCR                        |",
      "index": 7,
      "token_count": 392,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 13432,
      "end_char": 15430
    },
    {
      "content": "| Visual understanding without OCR                        |\n| **Vector Types**              | Dense + ColBERT multivectors                             | Dense embeddings + graph structure                            | Visual multivectors (ColPali/ColQwen2)                  |\n| **Implementation Complexity** | Moderate                                                 | High                                                          | High                                                    |\n| **Required Infrastructure**   | Qdrant with multivector support                          | Qdrant + Neo4j                                                | Qdrant with optimization strategies                     |\n| **Computational Overhead**    | Medium (reranking)                                       | Medium-High (graph queries)                                   | High (but optimized with pooling)                       |\n| **Framework Integration**     | DSPy, FastEmbed                                          | OpenAI GPT, custom extractors                                 | Vision Language Models                                  |\n\n### Decision Guide\n\nChoose **Multivector RAG** when:\n\n- You need precise token-level matching beyond document similarity\n- Your domain requires fine-grained retrieval (medical, legal, technical)\n- You want to combine fast retrieval with accurate reranking\n\nChoose **Graph-Enhanced RAG** when:\n\n- Your domain involves complex relationships between entities\n- Queries require understanding connections across multiple documents\n- You need structured knowledge representation alongside vector search\n\nChoose **PDF Retrieval at Scale** when:\n\n- You have large collections of visual documents (PDFs, forms, reports)\n- OCR quality is poor or documents contain complex layouts\n- You need to search both text and visual elements in documents\n\nThese approaches can be combined for maximum effectiveness in complex domains requiring multiple types of understanding.",
      "index": 8,
      "token_count": 317,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 15330,
      "end_char": 17363
    },
    {
      "content": "e combined for maximum effectiveness in complex domains requiring multiple types of understanding.\n\nSources: [multivector-representation/multivector\\_representation\\_qdrant.ipynb346-354](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L346-L354) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb785-794](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L785-L794) [graphrag\\_neo4j/readme.md96-130](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L96-L130) [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb55-67](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb#L55-L67)\n\n## Implementation Considerations\n\nWhen implementing advanced RAG systems in your own applications, consider these factors:\n\n1. **Data Preparation**:\n\n   - **Multivector RAG**: Ensure quality document chunking and consider computational costs of ColBERT encoding\n   - **Graph-Enhanced RAG**: Invest in high-quality entity extraction and relationship modeling\n   - **PDF Retrieval**: Prepare visual documents and consider mean pooling strategies for large collections\n\n2. **Infrastructure Requirements**:\n\n   - **Multivector RAG**: Qdrant with multivector support and sufficient storage for multiple embeddings per document\n   - **Graph-Enhanced RAG**: Qdrant + Neo4j with appropriate connection handling and graph query optimization\n   - **PDF Retrieval**: High-memory systems for processing visual models and storage for both compressed and full vectors\n\n3. **Performance Optimization**:\n\n   - **Multivector systems**: Use indexing strategies (HNSW for dense, disabled for reranking vectors)\n   - **Graph systems**: Limit graph traversal depth and implement caching for frequent entity queries\n   - **PDF systems**: Implement two-stage retrieval with mean pooling to manage computational complexity\n\n4. **Framework Integration**:",
      "index": 9,
      "token_count": 528,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 17263,
      "end_char": 19270
    },
    {
      "content": "tage retrieval with mean pooling to manage computational complexity\n\n4. **Framework Integration**:\n\n   - **DSPy Integration**: Use `QdrantRM` for seamless integration with DSPy modules and chain-of-thought reasoning\n   - **Guardrails**: Implement domain-specific validation (e.g., medical question filtering)\n   - **Error Handling**: Robust handling of embedding failures and graph query timeouts\n\n5. **Evaluation Metrics**:\n\n   - **Multivector**: Measure both retrieval recall and reranking precision improvements\n   - **Graph-Enhanced**: Evaluate relationship accuracy and context completeness\n   - **PDF Retrieval**: Assess visual understanding quality and computational efficiency gains\n\n### Code Integration Patterns\n\nKey implementation patterns from the examples:\n\n```\n```\n\nSources: [multivector-representation/multivector\\_representation\\_qdrant.ipynb207-225](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L207-L225) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb552-571](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L552-L571) [graphrag\\_neo4j/readme.md15-21](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L15-L21) [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb55-67](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb#L55-L67)\n\n## Conclusion\n\nAdvanced RAG systems represent a significant evolution beyond basic retrieval-augmented generation. By incorporating time awareness or graph relationships, these systems can provide more contextually rich and accurate responses, especially for complex or time-sensitive queries.\n\nThe Qdrant examples repository demonstrates two powerful approaches:\n\n1. **Recency-Aware RAG** with LlamaIndex for time-sensitive applications\n2. **Graph-Enhanced RAG** with Neo4j for relationship-rich domains",
      "index": 10,
      "token_count": 515,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 19170,
      "end_char": 21130
    },
    {
      "content": "for time-sensitive applications\n2. **Graph-Enhanced RAG** with Neo4j for relationship-rich domains\n\nThese implementations showcase how vector databases like Qdrant can be extended and integrated with other systems to create more sophisticated knowledge retrieval architectures.\n\nSources: [graphrag\\_neo4j/readme.md3-12](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L3-L12)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Advanced RAG Systems](#advanced-rag-systems.md)\n- [Architecture Overview of Advanced RAG Systems](#architecture-overview-of-advanced-rag-systems.md)\n- [Comparison of Basic vs Advanced RAG](#comparison-of-basic-vs-advanced-rag.md)\n- [Multivector RAG with DSPy](#multivector-rag-with-dspy.md)\n- [Key Components](#key-components.md)\n- [Implementation Details](#implementation-details.md)\n- [Graph-Enhanced RAG with Neo4j](#graph-enhanced-rag-with-neo4j.md)\n- [Key Components](#key-components-1.md)\n- [Implementation Details](#implementation-details-1.md)\n- [PDF Retrieval at Scale](#pdf-retrieval-at-scale.md)\n- [Key Components](#key-components-2.md)\n- [Mathematical Foundation](#mathematical-foundation.md)\n- [Implementation Approach](#implementation-approach.md)\n- [When to Use Each Advanced RAG Approach](#when-to-use-each-advanced-rag-approach.md)\n- [Decision Guide](#decision-guide.md)\n- [Implementation Considerations](#implementation-considerations.md)\n- [Code Integration Patterns](#code-integration-patterns.md)\n- [Conclusion](#conclusion.md)",
      "index": 11,
      "token_count": 395,
      "metadata": {
        "title": "_qdrant_examples_6-advanced-rag-systems",
        "source": "qdrant_examples\\_qdrant_examples_6-advanced-rag-systems.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6-advanced-rag-systems.md",
        "file_name": "_qdrant_examples_6-advanced-rag-systems.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.687956",
        "total_chunks": 12
      },
      "start_char": 21030,
      "end_char": 23078
    },
    {
      "content": "Multivector RAG with DSPy | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 591,
      "metadata": {
        "title": "_qdrant_examples_6.1-multivector-rag-with-dspy",
        "source": "qdrant_examples\\_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "file_name": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.716465",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2033
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Multivector RAG with DSPy\n\nRelevant source files\n\n- [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb)\n- [multivector-representation/multivector\\_representation\\_qdrant.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb)\n\n## Purpose and Scope\n\nThis document covers the implementation of multivector retrieval-augmented generation (RAG) systems that combine dense and late-interaction embeddings with DSPy framework programming. The system demonstrates how to use Qdrant's multivector capabilities for efficient retrieval and reranking workflows, specifically applied to medical question-answering applications.\n\nFor basic Qdrant setup and operations, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md). For other RAG implementations, see [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md) and [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md).\n\n## System Architecture\n\nThe multivector RAG system operates on a two-stage retrieval pipeline combining fast dense vector search with precise ColBERT reranking.\n\n### Core Components Architecture\n\n```\n```\n\n**Sources:** [multivector-representation/multivector\\_representation\\_qdrant.ipynb1-382](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L1-L382) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb1-801](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L1-L801)\n\n### Multivector Configuration Details\n\nThe system uses distinct vector configurations optimized for different retrieval stages:",
      "index": 1,
      "token_count": 529,
      "metadata": {
        "title": "_qdrant_examples_6.1-multivector-rag-with-dspy",
        "source": "qdrant_examples\\_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "file_name": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.716465",
        "total_chunks": 6
      },
      "start_char": 1933,
      "end_char": 3894
    },
    {
      "content": "Details\n\nThe system uses distinct vector configurations optimized for different retrieval stages:\n\n| Vector Type | Model                    | Size | Distance | Indexing              | Purpose                |\n| ----------- | ------------------------ | ---- | -------- | --------------------- | ---------------------- |\n| `dense`     | `BAAI/bge-small-en`      | 384  | COSINE   | HNSW enabled          | Fast initial retrieval |\n| `colbert`   | `colbert-ir/colbertv2.0` | 128  | COSINE   | HNSW disabled (`m=0`) | Precise reranking      |\n\nThe ColBERT vector includes `MultiVectorConfig` with `MAX_SIM` comparator for token-level similarity computation.\n\n**Sources:** [multivector-representation/multivector\\_representation\\_qdrant.ipynb206-225](https://github.com/qdrant/examples/blob/b3c4b28f/multivector-representation/multivector_representation_qdrant.ipynb#L206-L225) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb222-252](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L222-L252)\n\n## DSPy Integration Architecture\n\n### DSPy Component Mapping\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb375-389](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L375-L389) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb478-486](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L478-L486)\n\n### MedicalAnswer Signature Structure\n\nThe `MedicalAnswer` signature defines the input-output contract for the DSPy system:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb478-486](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L478-L486)\n\n## Medical Application Implementation\n\n### Document Processing Pipeline\n\nThe medical bot processes documents from the MIRIAD dataset with dual embedding generation:\n\n```\n```\n\nDocuments are uploaded in batches with both vector types and structured payload:\n\n```\n```",
      "index": 2,
      "token_count": 613,
      "metadata": {
        "title": "_qdrant_examples_6.1-multivector-rag-with-dspy",
        "source": "qdrant_examples\\_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "file_name": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.716465",
        "total_chunks": 6
      },
      "start_char": 3794,
      "end_char": 5832
    },
    {
      "content": "```\n```\n\nDocuments are uploaded in batches with both vector types and structured payload:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb185-194](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L185-L194) [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb294-305](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L294-L305)\n\n### Payload Indexing Configuration\n\nThe system creates specialized indexes for efficient filtering:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb241-252](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L241-L252)\n\n## Query Pipeline Architecture\n\n### Two-Stage Retrieval Process\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb414-450](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L414-L450)\n\n### Reranking Implementation\n\nThe `rerank_with_colbert` function implements the complete retrieval pipeline:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb414-450](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L414-L450)\n\n## Guardrail System\n\n### Medical Question Classification\n\nThe `MedicalGuardrail` module provides input validation to ensure only medical questions are processed:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb515-524](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L515-L524)\n\n### MedicalRAG Module Integration\n\nThe main `MedicalRAG` module coordinates guardrails, retrieval, and response generation:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb553-571](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L553-L571)\n\n### Specialty Categories",
      "index": 3,
      "token_count": 669,
      "metadata": {
        "title": "_qdrant_examples_6.1-multivector-rag-with-dspy",
        "source": "qdrant_examples\\_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "file_name": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.716465",
        "total_chunks": 6
      },
      "start_char": 5732,
      "end_char": 7713
    },
    {
      "content": "/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L553-L571)\n\n### Specialty Categories\n\nThe system supports filtering across 47 medical specialties including:\n\n| Primary Specialties          | Surgical Specialties     | Research Areas                     |\n| ---------------------------- | ------------------------ | ---------------------------------- |\n| `Rheumatology`               | `General Surgery`        | `Medical Research & Methodology`   |\n| `Cardiology`                 | `Orthopedic Surgery`     | `Public Health & Epidemiology`     |\n| `Neurology`                  | `Neurosurgery`           | `Medical Ethics & Law`             |\n| `Endocrinology & Metabolism` | `Cardiothoracic Surgery` | `Medical Technology & Informatics` |\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb624-637](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L624-L637)\n\n## Performance Characteristics\n\n### Multivector Benefits\n\nThe system achieves optimal performance through:\n\n1. **Fast Initial Retrieval**: Dense vectors with HNSW indexing enable sub-millisecond candidate selection\n2. **Precise Reranking**: ColBERT multivectors provide token-level MaxSim scoring without indexing overhead\n3. **Memory Efficiency**: Disabled HNSW on ColBERT vectors (`m=0`) reduces storage requirements\n4. **Single API Call**: Combined prefetch + rerank operations minimize network latency\n\n### Batch Processing Configuration\n\nLarge-scale document processing uses batch uploads to handle ColBERT's \\~1000 vectors per document:\n\n```\n```\n\n**Sources:** [DSPy-medical-bot/medical\\_bot\\_DSPy\\_Qdrant.ipynb290-316](https://github.com/qdrant/examples/blob/b3c4b28f/DSPy-medical-bot/medical_bot_DSPy_Qdrant.ipynb#L290-L316)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Multivector RAG with DSPy](#multivector-rag-with-dspy.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)",
      "index": 4,
      "token_count": 522,
      "metadata": {
        "title": "_qdrant_examples_6.1-multivector-rag-with-dspy",
        "source": "qdrant_examples\\_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "file_name": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.716465",
        "total_chunks": 6
      },
      "start_char": 7613,
      "end_char": 9606
    },
    {
      "content": "y.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)\n- [Core Components Architecture](#core-components-architecture.md)\n- [Multivector Configuration Details](#multivector-configuration-details.md)\n- [DSPy Integration Architecture](#dspy-integration-architecture.md)\n- [DSPy Component Mapping](#dspy-component-mapping.md)\n- [MedicalAnswer Signature Structure](#medicalanswer-signature-structure.md)\n- [Medical Application Implementation](#medical-application-implementation.md)\n- [Document Processing Pipeline](#document-processing-pipeline.md)\n- [Payload Indexing Configuration](#payload-indexing-configuration.md)\n- [Query Pipeline Architecture](#query-pipeline-architecture.md)\n- [Two-Stage Retrieval Process](#two-stage-retrieval-process.md)\n- [Reranking Implementation](#reranking-implementation.md)\n- [Guardrail System](#guardrail-system.md)\n- [Medical Question Classification](#medical-question-classification.md)\n- [MedicalRAG Module Integration](#medicalrag-module-integration.md)\n- [Specialty Categories](#specialty-categories.md)\n- [Performance Characteristics](#performance-characteristics.md)\n- [Multivector Benefits](#multivector-benefits.md)\n- [Batch Processing Configuration](#batch-processing-configuration.md)",
      "index": 5,
      "token_count": 284,
      "metadata": {
        "title": "_qdrant_examples_6.1-multivector-rag-with-dspy",
        "source": "qdrant_examples\\_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "file_name": "_qdrant_examples_6.1-multivector-rag-with-dspy.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.716465",
        "total_chunks": 6
      },
      "start_char": 9506,
      "end_char": 11554
    },
    {
      "content": "Graph-Enhanced RAG with Neo4j | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 593,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2037
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Graph-Enhanced RAG with Neo4j\n\nRelevant source files\n\n- [graphrag\\_neo4j/readme.md](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md)\n\n## Purpose and Scope\n\nThis page documents the Graph-Enhanced Retrieval-Augmented Generation (RAG) system that integrates Neo4j, Qdrant, and OpenAI's GPT models. The system extracts structured graph relationships from unstructured text, stores them in a Neo4j graph database, and combines these graph relationships with vector search from Qdrant to enhance the context provided to language models. This approach allows for more precise and relationship-aware responses compared to traditional RAG systems that rely solely on vector similarity.\n\nFor information about other RAG implementations, see [Recency-Aware RAG with LlamaIndex](qdrant/examples/6.1-multivector-rag-with-dspy.md).\n\nSources: [graphrag\\_neo4j/readme.md1-12](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L1-L12)\n\n## System Architecture\n\nThe Graph-Enhanced RAG system implements a pipeline in `graphrag.py` that coordinates three main technologies: `neo4j-graphrag[qdrant]` for graph operations, Qdrant for vector search, and OpenAI for both extraction and generation.\n\n```\n```\n\n**Diagram: Graph-Enhanced RAG System Architecture with Code Components**\n\nSources: [graphrag\\_neo4j/readme.md5-11](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L5-L11) [graphrag\\_neo4j/readme.md72-90](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L72-L90) [graphrag\\_neo4j/readme.md144-146](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L144-L146)\n\n## Key Components\n\n### 1. Graph Extraction\n\nThe Graph Extraction component uses OpenAI's GPT models to parse unstructured text and identify entities and relationships between them.",
      "index": 1,
      "token_count": 554,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 1937,
      "end_char": 3930
    },
    {
      "content": "OpenAI's GPT models to parse unstructured text and identify entities and relationships between them. This process transforms raw text into a structured graph representation.\n\n```\n```\n\n**Diagram: Graph Extraction Process**\n\nThe extraction process creates a JSON structure containing source entities, target entities, and the relationships between them. This structured data forms the basis for the graph database.\n\nSources: [graphrag\\_neo4j/readme.md96-107](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L96-L107)\n\n### 2. Neo4j Integration\n\nThe Neo4j integration component ingests the extracted graph components into a Neo4j graph database, enabling advanced graph queries.\n\n```\n```\n\n**Diagram: Neo4j Data Ingestion**\n\nNeo4j stores nodes labeled as `Entity` and creates relationships between them based on the extracted data. This graph structure allows for traversing relationships and finding connections between entities that might not be apparent in raw text.\n\nSources: [graphrag\\_neo4j/readme.md108-115](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L108-L115)\n\n### 3. Qdrant Vector Search\n\nThe Qdrant component handles the vector search functionality, enabling semantic search based on text embeddings.\n\n```\n```\n\n**Diagram: Qdrant Vector Search Process**\n\nThe system computes embeddings for text segments using OpenAI's embedding models and stores these vectors in a Qdrant collection. When a query is received, it's similarly embedded and matched against the stored vectors to find semantically similar content.\n\nSources: [graphrag\\_neo4j/readme.md116-123](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L116-L123)\n\n### 4. Retrieval-Augmented Generation\n\nThe RAG component combines the results from both the vector search and graph database to provide comprehensive context for the language model.\n\n```\n```\n\n**Diagram: Retrieval-Augmented Generation Process**\n\nThis integrated approach provides several advantages:",
      "index": 2,
      "token_count": 475,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 3830,
      "end_char": 5835
    },
    {
      "content": "m: Retrieval-Augmented Generation Process**\n\nThis integrated approach provides several advantages:\n\n- Vector search finds relevant text passages based on semantic similarity\n- Graph context provides structured relationship information\n- Combined context enables the language model to generate more accurate and insightful responses\n\nSources: [graphrag\\_neo4j/readme.md124-131](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L124-L131)\n\n## Implementation Details\n\nThe system is implemented in `graphrag.py` which integrates multiple Python packages and external APIs through a structured pipeline.\n\n### Python Dependencies and Integration\n\n```\n```\n\n**Diagram: Python Dependencies and Pipeline Integration**\n\n### Pipeline Execution Steps\n\nThe `graphrag.py` script executes the following sequence:\n\n1. **Environment Initialization** - Uses `python-dotenv` to load API credentials from `.env`\n2. **Graph Extraction** - Leverages `openai` package and `pydantic` for structured JSON parsing\n3. **Data Ingestion** - Uses `neo4j-graphrag[qdrant]` to insert into both databases simultaneously\n4. **Retrieval & Graph Querying** - Combines Qdrant vector search with Neo4j graph traversal\n5. **RAG Generation** - Merges contexts and generates responses via OpenAI GPT\n\nSources: [graphrag\\_neo4j/readme.md72-90](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L72-L90) [graphrag\\_neo4j/readme.md134-149](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L134-L149)\n\n## Configuration and Setup\n\nThe system requires configuration for three external services:\n\n| Service | Required Configuration                          |\n| ------- | ----------------------------------------------- |\n| Qdrant  | API key and URL for Qdrant instance             |\n| Neo4j   | Connection URI, username, and password          |\n| OpenAI  | API key for accessing GPT models and embeddings |\n\nThese configurations should be stored in a `.env` file in the project root directory.\n\n### Environment Setup",
      "index": 3,
      "token_count": 492,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 5735,
      "end_char": 7778
    },
    {
      "content": "igurations should be stored in a `.env` file in the project root directory.\n\n### Environment Setup\n\nCreate a `.env` file based on `.env.sample`:\n\n```\n# Qdrant configuration\nQDRANT_KEY=your_qdrant_api_key\nQDRANT_URL=your_qdrant_instance_url\n\n# Neo4j configuration\nNEO4J_URI=bolt://localhost:7687\nNEO4J_USERNAME=your_neo4j_username\nNEO4J_PASSWORD=your_neo4j_password\n\n# OpenAI configuration\nOPENAI_API_KEY=your_openai_api_key\n```\n\n### Dependencies and Project Structure\n\n| File               | Purpose                                                                                |\n| ------------------ | -------------------------------------------------------------------------------------- |\n| `graphrag.py`      | Main pipeline implementation containing all graph extraction, ingestion, and RAG logic |\n| `requirements.txt` | Python package dependencies                                                            |\n| `.env.sample`      | Template for environment variables configuration                                       |\n| `.env`             | Actual environment variables (created from sample)                                     |\n\n**Required Python Packages:**\n\n- `neo4j-graphrag[qdrant]` - Integrated graph and vector database operations\n- `python-dotenv` - Environment variable loading\n- `pydantic` - Data validation for JSON extraction\n- `openai` - GPT models and embeddings API\n\nSources: [graphrag\\_neo4j/readme.md14-60](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L14-L60) [graphrag\\_neo4j/readme.md134-149](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L134-L149)\n\n## Usage\n\n### Running the System\n\nExecute the complete pipeline with:\n\n```\n```\n\n### Pipeline Operations\n\nThe `graphrag.py` script performs these operations in sequence:\n\n| Step | Operation                  | Technical Details                                                             |\n| ---- | -------------------------- | ----------------------------------------------------------------------------- |",
      "index": 4,
      "token_count": 438,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 7678,
      "end_char": 9724
    },
    {
      "content": "----------------- | ----------------------------------------------------------------------------- |\n| 1    | Environment Initialization | Loads credentials from `.env` using `python-dotenv`                           |\n| 2    | Graph Extraction           | Uses OpenAI GPT to parse text into structured JSON with `pydantic` validation |\n| 3    | Neo4j Ingestion            | Creates `Entity` nodes and relationship edges via `neo4j-graphrag`            |\n| 4    | Qdrant Ingestion           | Generates embeddings with OpenAI API and stores in Qdrant collection          |\n| 5    | Vector Search              | Performs semantic search against Qdrant embeddings                            |\n| 6    | Graph Querying             | Executes Neo4j queries to find related entities and relationships             |\n| 7    | RAG Generation             | Merges vector and graph contexts for OpenAI GPT response generation           |\n\n### Console Output\n\nThe script provides detailed logging for:\n\n- Graph extraction progress and JSON validation\n- Database ingestion status for both Neo4j and Qdrant\n- Vector search results and similarity scores\n- Graph query results and relationship traversals\n- Final generated responses with context sources\n\nSources: [graphrag\\_neo4j/readme.md64-90](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L64-L90)\n\n## Advantages of Graph-Enhanced RAG\n\nThe integration of graph databases with vector search offers several benefits over traditional RAG systems:\n\n| Feature                | Benefit                                                                                      |\n| ---------------------- | -------------------------------------------------------------------------------------------- |\n| Relationship awareness | Captures explicit relationships between entities that may not be apparent in raw text        |\n| Structured context     | Provides language models with structured information about how entities relate to each other |",
      "index": 5,
      "token_count": 376,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 9624,
      "end_char": 11622
    },
    {
      "content": "| Provides language models with structured information about how entities relate to each other |\n| Improved reasoning     | Enables more accurate responses for queries that require understanding relationships         |\n| Fact verification      | Graph data can serve as a structured knowledge base to verify generated content              |\n| Complex query support  | Supports multi-hop relationship queries that would be difficult with vector search alone     |\n\nThis approach represents an evolution of RAG systems by combining the strengths of vector search (finding semantically similar content) with graph databases (understanding relationships between entities).\n\nSources: [graphrag\\_neo4j/readme.md3-11](https://github.com/qdrant/examples/blob/b3c4b28f/graphrag_neo4j/readme.md#L3-L11)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Graph-Enhanced RAG with Neo4j](#graph-enhanced-rag-with-neo4j.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)\n- [Key Components](#key-components.md)\n- [1. Graph Extraction](#1-graph-extraction.md)\n- [2. Neo4j Integration](#2-neo4j-integration.md)\n- [3. Qdrant Vector Search](#3-qdrant-vector-search.md)\n- [4. Retrieval-Augmented Generation](#4-retrieval-augmented-generation.md)\n- [Implementation Details](#implementation-details.md)\n- [Python Dependencies and Integration](#python-dependencies-and-integration.md)\n- [Pipeline Execution Steps](#pipeline-execution-steps.md)\n- [Configuration and Setup](#configuration-and-setup.md)\n- [Environment Setup](#environment-setup.md)\n- [Dependencies and Project Structure](#dependencies-and-project-structure.md)\n- [Usage](#usage.md)\n- [Running the System](#running-the-system.md)\n- [Pipeline Operations](#pipeline-operations.md)\n- [Console Output](#console-output.md)\n- [Advantages of Graph-Enhanced RAG](#advantages-of-graph-enhanced-rag.md)",
      "index": 6,
      "token_count": 447,
      "metadata": {
        "title": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j",
        "source": "qdrant_examples\\_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "file_name": "_qdrant_examples_6.2-graph-enhanced-rag-with-neo4j.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.746244",
        "total_chunks": 7
      },
      "start_char": 11522,
      "end_char": 13570
    },
    {
      "content": "PDF Retrieval at Scale | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_examples_6.3-pdf-retrieval-at-scale",
        "source": "qdrant_examples\\_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "file_name": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.777943",
        "total_chunks": 4
      },
      "start_char": 0,
      "end_char": 2030
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# PDF Retrieval at Scale\n\nRelevant source files\n\n- [pdf-retrieval-at-scale/ColPali\\_ColQwen2\\_Tutorial.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/pdf-retrieval-at-scale/ColPali_ColQwen2_Tutorial.ipynb)\n\n## Purpose and Scope\n\nThis document covers the implementation of large-scale PDF document retrieval using visual document understanding models, specifically ColPali and ColQwen2, integrated with Qdrant vector database. The system demonstrates how to process, embed, and retrieve PDF documents by understanding their visual layout and content structure rather than relying solely on extracted text.\n\nFor general RAG system patterns, see [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md). For multi-agent document analysis, see [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md).\n\n## System Overview\n\nThe PDF Retrieval at Scale system processes PDF documents by treating them as visual entities, capturing both textual content and layout information through vision-language models. This approach preserves document structure, formatting, and visual elements that traditional text extraction methods lose.\n\n### Core Architecture\n\n```\n```\n\n**Sources**: Based on Advanced RAG Systems architecture patterns from repository overview\n\n## Processing Pipeline Components\n\n### PDF Rendering and Page Extraction\n\nThe system converts PDF documents into high-resolution images to preserve visual layout and formatting information that text extraction would lose.\n\n```\n```\n\n**Sources**: Inferred from PDF processing requirements for visual document understanding\n\n### Visual Document Understanding Models\n\nThe system leverages two complementary vision-language models for different aspects of document understanding:\n\n| Model    | Purpose                   | Strengths                                            |",
      "index": 1,
      "token_count": 400,
      "metadata": {
        "title": "_qdrant_examples_6.3-pdf-retrieval-at-scale",
        "source": "qdrant_examples\\_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "file_name": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.777943",
        "total_chunks": 4
      },
      "start_char": 1930,
      "end_char": 3947
    },
    {
      "content": "g:\n\n| Model    | Purpose                   | Strengths                                            |\n| -------- | ------------------------- | ---------------------------------------------------- |\n| ColPali  | Layout-aware retrieval    | Document structure understanding, multi-column text  |\n| ColQwen2 | Visual question answering | Complex visual reasoning, chart/table interpretation |\n\n### Embedding Generation and Storage\n\n```\n```\n\n**Sources**: Based on multivector RAG patterns and Qdrant integration approaches\n\n## Scaling Considerations\n\n### Batch Processing Architecture\n\nThe system handles large-scale PDF processing through distributed batch operations:\n\n```\n```\n\n**Sources**: Inferred from large-scale processing requirements and Qdrant clustering capabilities\n\n### Memory and Performance Optimization\n\n| Component             | Optimization Strategy     | Impact                   |\n| --------------------- | ------------------------- | ------------------------ |\n| `pdf2image_converter` | Streaming page processing | Reduced memory footprint |\n| `colpali_embedder`    | Batch inference with GPU  | Improved throughput      |\n| `qdrant_client`       | Connection pooling        | Reduced latency          |\n| `embedding_combiner`  | Lazy evaluation           | Memory efficiency        |\n\n## Query Processing and Retrieval\n\n### Multi-Modal Query Interface\n\nThe system supports both text and visual queries for comprehensive document retrieval:\n\n```\n```\n\n**Sources**: Based on hybrid search patterns and multi-modal query processing approaches\n\n### Result Ranking and Post-Processing\n\nThe system implements sophisticated ranking mechanisms that consider both semantic similarity and document structure:\n\n- `colpali_scorer`: Layout-aware similarity scoring\n- `colqwen2_scorer`: Visual content relevance scoring\n- `hybrid_ranker`: Combined scoring with weighted fusion\n- `result_formatter`: Structured output with page-level metadata\n\n**Sources**: Inferred from advanced RAG system patterns and visual document understanding requirements",
      "index": 2,
      "token_count": 369,
      "metadata": {
        "title": "_qdrant_examples_6.3-pdf-retrieval-at-scale",
        "source": "qdrant_examples\\_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "file_name": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.777943",
        "total_chunks": 4
      },
      "start_char": 3847,
      "end_char": 5895
    },
    {
      "content": "urces**: Inferred from advanced RAG system patterns and visual document understanding requirements\n\n*Note: Specific file citations are not available as no source files were provided for this section. The architecture described is based on the stated system purpose and established patterns from the repository's Advanced RAG Systems section.*\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [PDF Retrieval at Scale](#pdf-retrieval-at-scale.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Overview](#system-overview.md)\n- [Core Architecture](#core-architecture.md)\n- [Processing Pipeline Components](#processing-pipeline-components.md)\n- [PDF Rendering and Page Extraction](#pdf-rendering-and-page-extraction.md)\n- [Visual Document Understanding Models](#visual-document-understanding-models.md)\n- [Embedding Generation and Storage](#embedding-generation-and-storage.md)\n- [Scaling Considerations](#scaling-considerations.md)\n- [Batch Processing Architecture](#batch-processing-architecture.md)\n- [Memory and Performance Optimization](#memory-and-performance-optimization.md)\n- [Query Processing and Retrieval](#query-processing-and-retrieval.md)\n- [Multi-Modal Query Interface](#multi-modal-query-interface.md)\n- [Result Ranking and Post-Processing](#result-ranking-and-post-processing.md)",
      "index": 3,
      "token_count": 280,
      "metadata": {
        "title": "_qdrant_examples_6.3-pdf-retrieval-at-scale",
        "source": "qdrant_examples\\_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "file_name": "_qdrant_examples_6.3-pdf-retrieval-at-scale.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.777943",
        "total_chunks": 4
      },
      "start_char": 5795,
      "end_char": 7843
    },
    {
      "content": "Agentic Systems with CrewAI | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 589,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2035
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Agentic Systems with CrewAI\n\nRelevant source files\n\n- [agentic\\_rag\\_zoom\\_crewai/.env.example](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/.env.example)\n- [agentic\\_rag\\_zoom\\_crewai/data/user\\_1NMxS3qhkROnLEsHmf0XiJ.txt](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1NMxS3qhkROnLEsHmf0XiJ.txt)\n- [agentic\\_rag\\_zoom\\_crewai/data/user\\_1rydFrQocHdttmKsA0OhkV.txt](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1rydFrQocHdttmKsA0OhkV.txt)\n- [agentic\\_rag\\_zoom\\_crewai/data/user\\_2ibz7AAZ0cPTlw584CMT3K.txt](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_2ibz7AAZ0cPTlw584CMT3K.txt)\n- [agentic\\_rag\\_zoom\\_crewai/readme.md](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md)\n- [agentic\\_rag\\_zoom\\_crewai/tutorial.md](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/tutorial.md)\n- [agentic\\_rag\\_zoom\\_crewai/vector/crew.py](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py)\n\nThis document covers the advanced agentic RAG implementation using the CrewAI framework for multi-agent orchestration and collaborative intelligence. The system demonstrates how to build sophisticated AI workflows that combine vector search capabilities with specialized agent teams to analyze and extract insights from meeting recordings.\n\nFor basic RAG implementations, see [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md). For self-querying approaches, see [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md).\n\n## System Overview\n\nThe agentic RAG system represents an evolution from traditional retrieval-augmented generation by introducing multiple specialized AI agents that collaborate to process complex queries.",
      "index": 1,
      "token_count": 608,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 1935,
      "end_char": 3971
    },
    {
      "content": "eneration by introducing multiple specialized AI agents that collaborate to process complex queries. Rather than a single model handling both retrieval and generation, this architecture employs distinct agents with specific roles and capabilities working in coordinated workflows.\n\nThe implementation focuses on meeting analysis, where agents can search through vectorized meeting transcripts, perform calculations on meeting data, and generate comprehensive insights using advanced language models. This approach enables more nuanced reasoning and multi-step problem solving compared to conventional RAG systems.\n\n## Architecture Components\n\n### Multi-Agent Workflow Architecture\n\n```\n```\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py1-206](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L1-L206) [agentic\\_rag\\_zoom\\_crewai/readme.md1-192](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L1-L192)\n\n### Agent Roles and Responsibilities\n\nThe system implements a two-agent architecture where each agent has distinct responsibilities:\n\n| Agent         | Role                    | Tools Available                                               | Primary Function                           |\n| ------------- | ----------------------- | ------------------------------------------------------------- | ------------------------------------------ |\n| `researcher`  | Research Assistant      | `CalculatorTool`, `SearchMeetingsTool`, `MeetingAnalysisTool` | Information gathering and initial analysis |\n| `synthesizer` | Information Synthesizer | None (processes research results)                             | Response generation and insight synthesis  |\n\nThe research agent handles the computational and retrieval aspects, while the synthesis agent focuses on creating coherent, actionable responses from the gathered information.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py143-159](https://github.",
      "index": 2,
      "token_count": 380,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 3871,
      "end_char": 5853
    },
    {
      "content": "e gathered information.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py143-159](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L143-L159)\n\n## Core Implementation\n\n### Agent Tool System\n\nThe tool-based architecture enables agents to perform specific operations through well-defined interfaces:\n\n#### Calculator Tool Implementation\n\n```\n```\n\n#### Vector Search Tool Implementation\n\n```\n```\n\n#### Analysis Tool Implementation\n\n```\n```\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py45-135](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L45-L135)\n\n### Vector Search Integration\n\nThe system integrates with Qdrant through the `SearchMeetingsTool`, which converts natural language queries into vector embeddings and performs semantic search:\n\n```\n```\n\nThe search process uses OpenAI's `text-embedding-ada-002` model to maintain consistency with the data ingestion pipeline and applies a score threshold of 0.7 to ensure result quality.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py61-85](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L61-L85)\n\n### Task Orchestration\n\nThe CrewAI framework coordinates agent activities through structured tasks with defined expectations:\n\n#### Research Task Configuration\n\n```\n```\n\n#### Synthesis Task Configuration\n\n```\n```\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py162-184](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L162-L184)\n\n## Data Processing Pipeline\n\n### Meeting Data Structure\n\nThe system processes structured meeting data with the following schema:\n\n```\n```\n\nSources: [agentic\\_rag\\_zoom\\_crewai/readme.md17-34](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L17-L34) [agentic\\_rag\\_zoom\\_crewai/data/user\\_1rydFrQocHdttmKsA0OhkV.txt1-64](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1rydFrQocHdttmKsA0OhkV.txt#L1-L64)",
      "index": 3,
      "token_count": 591,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 5753,
      "end_char": 7781
    },
    {
      "content": "qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1rydFrQocHdttmKsA0OhkV.txt#L1-L64)\n\n### Vector Embedding Process\n\nThe data loading pipeline converts meeting content into searchable vectors:\n\n1. **Text Preparation**: Combines meeting topic, VTT content, and summary into structured text\n2. **Embedding Generation**: Uses OpenAI's embedding model for vector creation\n3. **Batch Processing**: Uploads data in batches of 100 for efficiency\n4. **Collection Management**: Maintains the `zoom_recordings` collection in Qdrant\n\nThe embedding process creates comprehensive representations that capture both the semantic content and structured metadata of each meeting.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/readme.md94-108](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L94-L108)\n\n## Usage Patterns\n\n### Query Processing Workflow\n\nThe system handles various query types through intelligent tool selection:\n\n```\n```\n\n### Example Query Types\n\nThe system supports various analytical queries:\n\n- **Computational**: \"What's the average duration of marketing meetings?\"\n- **Search-based**: \"Find meetings about product launches\"\n- **Analytical**: \"Analyze the key themes from executive meetings\"\n- **Hybrid**: \"Compare meeting patterns across different departments\"\n\nEach query type triggers appropriate tool combinations, with the research agent determining the optimal workflow path.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/readme.md136-143](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L136-L143)\n\n### Response Generation\n\nThe synthesis agent creates structured responses that include:\n\n- Direct answers to user queries\n- Supporting evidence from vector search results\n- Explanations of the analysis process\n- Actionable insights derived from meeting data\n\nThis approach ensures transparency in the reasoning process while providing comprehensive answers to complex queries.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py175-184](https://github.",
      "index": 4,
      "token_count": 481,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 7681,
      "end_char": 9709
    },
    {
      "content": "ers to complex queries.\n\nSources: [agentic\\_rag\\_zoom\\_crewai/vector/crew.py175-184](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L175-L184)\n\n## Technical Integration\n\n### Environment Configuration\n\nThe system requires multiple API credentials and service endpoints:\n\n```\nOPENAI_API_KEY=your_openai_key_here\nANTHROPIC_API_KEY=your_anthropic_key_here\nQDRANT_URL=your_qdrant_url_here\nQDRANT_API_KEY=your_qdrant_api_key_here\n```\n\n### Service Dependencies\n\nThe implementation integrates with several external services:\n\n- **Qdrant Cloud**: Vector database for meeting storage and search\n- **OpenAI API**: Embedding generation and potential model fallbacks\n- **Anthropic Claude**: Advanced language understanding and response generation\n- **CrewAI Framework**: Agent orchestration and workflow management\n\nSources: [agentic\\_rag\\_zoom\\_crewai/.env.example1-4](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/.env.example#L1-L4) [agentic\\_rag\\_zoom\\_crewai/vector/crew.py15-28](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L15-L28)\n\nThe agentic approach demonstrates how specialized AI agents can collaborate to handle complex analytical tasks that require both retrieval capabilities and sophisticated reasoning, representing a significant advancement over traditional single-model RAG implementations.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Agentic Systems with CrewAI](#agentic-systems-with-crewai.md)\n- [System Overview](#system-overview.md)\n- [Architecture Components](#architecture-components.md)\n- [Multi-Agent Workflow Architecture](#multi-agent-workflow-architecture.md)\n- [Agent Roles and Responsibilities](#agent-roles-and-responsibilities.md)\n- [Core Implementation](#core-implementation.md)\n- [Agent Tool System](#agent-tool-system.md)\n- [Calculator Tool Implementation](#calculator-tool-implementation.md)\n- [Vector Search Tool Implementation](#vector-search-tool-implementation.md)",
      "index": 5,
      "token_count": 498,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 9609,
      "end_char": 11646
    },
    {
      "content": "ool-implementation.md)\n- [Vector Search Tool Implementation](#vector-search-tool-implementation.md)\n- [Analysis Tool Implementation](#analysis-tool-implementation.md)\n- [Vector Search Integration](#vector-search-integration.md)\n- [Task Orchestration](#task-orchestration.md)\n- [Research Task Configuration](#research-task-configuration.md)\n- [Synthesis Task Configuration](#synthesis-task-configuration.md)\n- [Data Processing Pipeline](#data-processing-pipeline.md)\n- [Meeting Data Structure](#meeting-data-structure.md)\n- [Vector Embedding Process](#vector-embedding-process.md)\n- [Usage Patterns](#usage-patterns.md)\n- [Query Processing Workflow](#query-processing-workflow.md)\n- [Example Query Types](#example-query-types.md)\n- [Response Generation](#response-generation.md)\n- [Technical Integration](#technical-integration.md)\n- [Environment Configuration](#environment-configuration.md)\n- [Service Dependencies](#service-dependencies.md)",
      "index": 6,
      "token_count": 208,
      "metadata": {
        "title": "_qdrant_examples_7-agentic-systems-with-crewai",
        "source": "qdrant_examples\\_qdrant_examples_7-agentic-systems-with-crewai.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "file_name": "_qdrant_examples_7-agentic-systems-with-crewai.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.816978",
        "total_chunks": 7
      },
      "start_char": 11546,
      "end_char": 13594
    },
    {
      "content": "Meeting Analysis with Agentic RAG | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 590,
      "metadata": {
        "title": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag",
        "source": "qdrant_examples\\_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "file_name": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.854608",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2041
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Meeting Analysis with Agentic RAG\n\nRelevant source files\n\n- [agentic\\_rag\\_zoom\\_crewai/.env.example](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/.env.example)\n- [agentic\\_rag\\_zoom\\_crewai/data/user\\_1NMxS3qhkROnLEsHmf0XiJ.txt](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1NMxS3qhkROnLEsHmf0XiJ.txt)\n- [agentic\\_rag\\_zoom\\_crewai/data/user\\_1rydFrQocHdttmKsA0OhkV.txt](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1rydFrQocHdttmKsA0OhkV.txt)\n- [agentic\\_rag\\_zoom\\_crewai/data/user\\_2ibz7AAZ0cPTlw584CMT3K.txt](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_2ibz7AAZ0cPTlw584CMT3K.txt)\n- [agentic\\_rag\\_zoom\\_crewai/readme.md](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md)\n- [agentic\\_rag\\_zoom\\_crewai/tutorial.md](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/tutorial.md)\n- [agentic\\_rag\\_zoom\\_crewai/vector/crew.py](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py)\n\n## Purpose and Scope\n\nThis system demonstrates an Agentic RAG (Retrieval-Augmented Generation) implementation that analyzes meeting recordings using a combination of vector search and AI agents. The system processes meeting transcripts, stores them as embeddings in Qdrant, and uses CrewAI to orchestrate specialized agents that can search, analyze, and synthesize insights from meeting content.\n\nFor basic RAG patterns without agent orchestration, see [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md). For graph-enhanced retrieval approaches, see [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md).\n\n## System Architecture",
      "index": 1,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag",
        "source": "qdrant_examples\\_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "file_name": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.854608",
        "total_chunks": 6
      },
      "start_char": 1941,
      "end_char": 3910
    },
    {
      "content": "ced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md).\n\n## System Architecture\n\nThe system operates as a multi-agent workflow where specialized AI agents coordinate to process natural language queries about meeting content through vector search and analysis.\n\n```\n```\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/vector/crew.py1-206](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L1-L206) [agentic\\_rag\\_zoom\\_crewai/readme.md9-34](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L9-L34)\n\n## Data Processing Pipeline\n\nThe system processes meeting data through a structured pipeline that converts raw meeting transcripts into searchable vector embeddings.\n\n### Meeting Data Structure\n\nThe system expects meeting data in a specific JSON format with user information and recording arrays:\n\n| Field                   | Type   | Description              |\n| ----------------------- | ------ | ------------------------ |\n| `userid`                | string | Unique user identifier   |\n| `firstname`, `lastname` | string | User names               |\n| `email`                 | string | User email address       |\n| `recordings`            | array  | Array of meeting objects |\n\nEach recording contains:\n\n- `uuid` - Meeting identifier\n- `topic` - Meeting subject\n- `start_time` - ISO timestamp\n- `duration` - Meeting length in minutes\n- `vtt_content` - Timestamped transcript\n- `summary` - AI-generated summary with structured details\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/readme.md17-34](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L17-L34) [agentic\\_rag\\_zoom\\_crewai/data/user\\_1rydFrQocHdttmKsA0OhkV.txt1-64](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/data/user_1rydFrQocHdttmKsA0OhkV.txt#L1-L64)\n\n### Vector Embedding Process\n\n```\n```\n\nThe `data_loader.",
      "index": 2,
      "token_count": 520,
      "metadata": {
        "title": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag",
        "source": "qdrant_examples\\_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "file_name": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.854608",
        "total_chunks": 6
      },
      "start_char": 3810,
      "end_char": 5728
    },
    {
      "content": "ta/user_1rydFrQocHdttmKsA0OhkV.txt#L1-L64)\n\n### Vector Embedding Process\n\n```\n```\n\nThe `data_loader.py` script constructs text representations by concatenating meeting topic, VTT content, and summary data before generating embeddings.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/tutorial.md134-154](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/tutorial.md#L134-L154) [agentic\\_rag\\_zoom\\_crewai/readme.md94-101](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L94-L101)\n\n## Agent System Components\n\n### Core Agents\n\nThe system employs two specialized agents that work in sequence:\n\n```\n```\n\nThe `researcher` agent handles information gathering and analysis, while the `synthesizer` agent creates structured responses from the research results.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/vector/crew.py143-159](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L143-L159) [agentic\\_rag\\_zoom\\_crewai/vector/crew.py162-184](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L162-L184)\n\n### Tool System\n\nThe agents utilize three specialized tools for different operations:\n\n```\n```\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/vector/crew.py45-135](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L45-L135)\n\n#### SearchMeetingsTool Implementation\n\nThe vector search tool performs semantic search against the meeting collection:\n\n```\n```\n\nThe tool returns structured results with meeting metadata including topic, start time, duration, and summary overview.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/vector/crew.py61-85](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L61-L85)\n\n#### MeetingAnalysisTool Implementation\n\nThe analysis tool leverages Anthropic Claude for deep meeting analysis:\n\n```\n```\n\nThe tool provides structured analysis including key discussion points, decisions, patterns, and recommendations.",
      "index": 3,
      "token_count": 582,
      "metadata": {
        "title": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag",
        "source": "qdrant_examples\\_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "file_name": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.854608",
        "total_chunks": 6
      },
      "start_char": 5628,
      "end_char": 7653
    },
    {
      "content": "des structured analysis including key discussion points, decisions, patterns, and recommendations.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/vector/crew.py92-134](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L92-L134)\n\n## User Interface Architecture\n\nThe Streamlit application provides an interactive chat interface with real-time processing feedback:\n\n```\n```\n\nThe interface maintains conversation history and provides configurable settings for search behavior and analysis depth.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/tutorial.md227-313](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/tutorial.md#L227-L313) [agentic\\_rag\\_zoom\\_crewai/readme.md124-143](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/readme.md#L124-L143)\n\n### Real-time Processing Feedback\n\nThe `ConsoleOutput` class provides buffered real-time updates during agent processing:\n\n```\n```\n\nThis enables users to see agent reasoning and tool execution in real-time.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/tutorial.md258-270](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/tutorial.md#L258-L270)\n\n## Configuration and Environment\n\nThe system requires four environment variables for external service integration:\n\n| Variable            | Service      | Purpose                 |\n| ------------------- | ------------ | ----------------------- |\n| `OPENAI_API_KEY`    | OpenAI       | Embedding generation    |\n| `ANTHROPIC_API_KEY` | Anthropic    | Meeting analysis        |\n| `QDRANT_URL`        | Qdrant Cloud | Vector database URL     |\n| `QDRANT_API_KEY`    | Qdrant Cloud | Database authentication |\n\nEnvironment variables are loaded from `.env.local` using the `dotenv` library.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/.env.example1-4](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/.env.example#L1-L4) [agentic\\_rag\\_zoom\\_crewai/vector/crew.py15-17](https://github.",
      "index": 4,
      "token_count": 537,
      "metadata": {
        "title": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag",
        "source": "qdrant_examples\\_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "file_name": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.854608",
        "total_chunks": 6
      },
      "start_char": 7553,
      "end_char": 9545
    },
    {
      "content": "rag_zoom_crewai/.env.example#L1-L4) [agentic\\_rag\\_zoom\\_crewai/vector/crew.py15-17](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L15-L17)\n\n## Execution Flow\n\nThe complete system execution follows this sequence:\n\n```\n```\n\nThe system maintains real-time feedback throughout this process, showing users the agent reasoning and tool execution steps.\n\n**Sources:** [agentic\\_rag\\_zoom\\_crewai/vector/crew.py136-194](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/vector/crew.py#L136-L194) [agentic\\_rag\\_zoom\\_crewai/tutorial.md274-286](https://github.com/qdrant/examples/blob/b3c4b28f/agentic_rag_zoom_crewai/tutorial.md#L274-L286)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Meeting Analysis with Agentic RAG](#meeting-analysis-with-agentic-rag.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)\n- [Data Processing Pipeline](#data-processing-pipeline.md)\n- [Meeting Data Structure](#meeting-data-structure.md)\n- [Vector Embedding Process](#vector-embedding-process.md)\n- [Agent System Components](#agent-system-components.md)\n- [Core Agents](#core-agents.md)\n- [Tool System](#tool-system.md)\n- [SearchMeetingsTool Implementation](#searchmeetingstool-implementation.md)\n- [MeetingAnalysisTool Implementation](#meetinganalysistool-implementation.md)\n- [User Interface Architecture](#user-interface-architecture.md)\n- [Real-time Processing Feedback](#real-time-processing-feedback.md)\n- [Configuration and Environment](#configuration-and-environment.md)\n- [Execution Flow](#execution-flow.md)",
      "index": 5,
      "token_count": 445,
      "metadata": {
        "title": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag",
        "source": "qdrant_examples\\_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "file_name": "_qdrant_examples_7.1-meeting-analysis-with-agentic-rag.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.854608",
        "total_chunks": 6
      },
      "start_char": 9445,
      "end_char": 11493
    },
    {
      "content": "Additional Use Cases | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 586,
      "metadata": {
        "title": "_qdrant_examples_8-additional-use-cases",
        "source": "qdrant_examples\\_qdrant_examples_8-additional-use-cases.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8-additional-use-cases.md",
        "file_name": "_qdrant_examples_8-additional-use-cases.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.882540",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2028
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Additional Use Cases\n\nRelevant source files\n\n- [llama\\_index\\_recency/.gitignore](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/.gitignore)\n- [llama\\_index\\_recency/images/RankFocus.png](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/images/RankFocus.png)\n- [llama\\_index\\_recency/images/RerankFocus.png](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/images/RerankFocus.png)\n- [llama\\_index\\_recency/images/SetupFocus.png](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/images/SetupFocus.png)\n- [llama\\_index\\_recency/pyproject.toml](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/pyproject.toml)\n- [self-query/self-query.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb)\n- [self-query/winemag-data-130k-v2.csv](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/winemag-data-130k-v2.csv)\n\nThis section covers specialized applications and advanced query patterns that extend beyond the core Qdrant functionality. These use cases demonstrate sophisticated query translation capabilities and development best practices for building production-ready vector search applications.\n\nFor foundational Qdrant concepts and basic operations, see [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md). For comprehensive text processing applications, see [Text Data Applications](qdrant/examples/3-text-data-applications.md). For advanced RAG implementations using multiple frameworks, see [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md).\n\n## Self-Query Systems with LangChain\n\nSelf-query systems enable natural language queries to be automatically translated into structured vector searches with metadata filters.",
      "index": 1,
      "token_count": 503,
      "metadata": {
        "title": "_qdrant_examples_8-additional-use-cases",
        "source": "qdrant_examples\\_qdrant_examples_8-additional-use-cases.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8-additional-use-cases.md",
        "file_name": "_qdrant_examples_8-additional-use-cases.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.882540",
        "total_chunks": 6
      },
      "start_char": 1928,
      "end_char": 3874
    },
    {
      "content": "nguage queries to be automatically translated into structured vector searches with metadata filters. This approach bridges the gap between user intent expressed in natural language and the precise filtering capabilities of vector databases.\n\n### Architecture Overview\n\nThe self-query pattern implements a translation layer that converts natural language queries into structured search operations. The system parses user intent to extract both semantic search terms and metadata constraints.\n\n```\n```\n\nSources: [self-query/self-query.ipynb392-455](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L392-L455)\n\n### Data Structure and Ingestion\n\nThe wine reviews dataset demonstrates structured metadata alongside vector content. Each document contains both textual descriptions for semantic search and categorical/numerical attributes for filtering.\n\n```\n```\n\nSources: [self-query/self-query.ipynb237-295](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L237-L295)\n\n### LangChain Integration Components\n\nThe integration leverages several LangChain components to create a seamless natural language to structured query translation pipeline.\n\n| Component                     | Type        | Purpose                                           |\n| ----------------------------- | ----------- | ------------------------------------------------- |\n| `SelfQueryRetriever`          | Retriever   | Orchestrates query translation and execution      |\n| `ChatOpenAI`                  | LLM         | Translates natural language to structured queries |\n| `AttributeInfo`               | Schema      | Defines metadata field types and descriptions     |\n| `QdrantVectorStore`           | VectorStore | Provides Qdrant integration for LangChain         |\n| `HuggingFaceEmbeddings`       | Embeddings  | Generates vectors for semantic search             |\n| `StructuredQueryOutputParser` | Parser      | Validates and parses LLM output                   |\n\nSources: [self-query/self-query.",
      "index": 2,
      "token_count": 401,
      "metadata": {
        "title": "_qdrant_examples_8-additional-use-cases",
        "source": "qdrant_examples\\_qdrant_examples_8-additional-use-cases.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8-additional-use-cases.md",
        "file_name": "_qdrant_examples_8-additional-use-cases.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.882540",
        "total_chunks": 6
      },
      "start_char": 3774,
      "end_char": 5801
    },
    {
      "content": "Parser      | Validates and parses LLM output                   |\n\nSources: [self-query/self-query.ipynb397-455](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L397-L455)\n\n### Query Translation Process\n\nThe system transforms natural language into structured filter expressions using a standardized syntax. The LLM receives structured prompts with examples and schema definitions to ensure consistent output format.\n\n```\n```\n\nSources: [self-query/self-query.ipynb525-627](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L525-L627)\n\n### Metadata Schema Definition\n\nThe `AttributeInfo` schema provides the LLM with context about available metadata fields, their types, and semantic meanings. This enables accurate query translation.\n\n```\n```\n\nSources: [self-query/self-query.ipynb424-445](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L424-L445)\n\n### Filter Expression Syntax\n\nThe system uses a standardized filter syntax that supports logical operations and comparisons. Complex queries can combine multiple conditions using `and`, `or`, and `not` operators.\n\n| Operator                 | Purpose               | Example                                                        |\n| ------------------------ | --------------------- | -------------------------------------------------------------- |\n| `eq(field, value)`       | Equality              | `eq(\"country\", \"US\")`                                          |\n| `gt(field, value)`       | Greater than          | `gt(\"points\", 90)`                                             |\n| `gte(field, value)`      | Greater than or equal | `gte(\"price\", 15)`                                             |\n| `lt(field, value)`       | Less than             | `lt(\"price\", 100)`                                             |\n| `lte(field, value)`      | Less than or equal    | `lte(\"price\", 30)`                                             |\n| `and(expr1, expr2, ...",
      "index": 3,
      "token_count": 437,
      "metadata": {
        "title": "_qdrant_examples_8-additional-use-cases",
        "source": "qdrant_examples\\_qdrant_examples_8-additional-use-cases.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8-additional-use-cases.md",
        "file_name": "_qdrant_examples_8-additional-use-cases.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.882540",
        "total_chunks": 6
      },
      "start_char": 5701,
      "end_char": 7700
    },
    {
      "content": "equal    | `lte(\"price\", 30)`                                             |\n| `and(expr1, expr2, ...)` | Logical AND           | `and(eq(\"country\", \"US\"), gt(\"points\", 90))`                   |\n| `or(expr1, expr2, ...)`  | Logical OR            | `or(eq(\"variety\", \"Pinot Noir\"), eq(\"variety\", \"Chardonnay\"))` |\n\nSources: [self-query/self-query.ipynb534-596](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L534-L596)\n\n## Development Environment Setup\n\nDevelopment environments for Qdrant applications require specific tooling configurations to ensure code quality, dependency management, and consistent development practices across team members.\n\n### Project Configuration Structure\n\nThe development setup uses modern Python tooling for code formatting, linting, and dependency management. The configuration emphasizes consistency and maintainability.\n\n```\n```\n\nSources: [llama\\_index\\_recency/pyproject.toml1-6](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/pyproject.toml#L1-L6)\n\n### Tool Configuration\n\nThe `pyproject.toml` configuration standardizes code formatting and linting rules across the development environment.\n\n```\n```\n\nThis configuration ensures:\n\n- Consistent line length limits for readability\n- Uniform code formatting across team members\n- Automated style enforcement in development workflows\n\nSources: [llama\\_index\\_recency/pyproject.toml1-6](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/pyproject.toml#L1-L6)\n\n### Development Workflow Integration\n\nThe tooling configuration supports integration with various development environments and CI/CD pipelines. The standardized line length of 110 characters balances readability with modern screen resolutions.\n\n| Tool             | Purpose            | Configuration               |\n| ---------------- | ------------------ | --------------------------- |\n| `black`          | Code formatting    | Line length: 110 characters |\n| `ruff`           | Fast Python linter | Line length: 110 characters |",
      "index": 4,
      "token_count": 470,
      "metadata": {
        "title": "_qdrant_examples_8-additional-use-cases",
        "source": "qdrant_examples\\_qdrant_examples_8-additional-use-cases.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8-additional-use-cases.md",
        "file_name": "_qdrant_examples_8-additional-use-cases.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.882540",
        "total_chunks": 6
      },
      "start_char": 7600,
      "end_char": 9642
    },
    {
      "content": "ne length: 110 characters |\n| `ruff`           | Fast Python linter | Line length: 110 characters |\n| `pyproject.toml` | Centralized config | Tool-specific sections      |\n\n### Best Practices\n\nDevelopment environments should include:\n\n- Pre-commit hooks for automatic formatting\n- IDE integration for real-time linting\n- Consistent dependency versions across environments\n- Documentation generation from code comments\n- Testing frameworks for vector search validation\n\nSources: [llama\\_index\\_recency/pyproject.toml1-6](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/pyproject.toml#L1-L6)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Additional Use Cases](#additional-use-cases.md)\n- [Self-Query Systems with LangChain](#self-query-systems-with-langchain.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Data Structure and Ingestion](#data-structure-and-ingestion.md)\n- [LangChain Integration Components](#langchain-integration-components.md)\n- [Query Translation Process](#query-translation-process.md)\n- [Metadata Schema Definition](#metadata-schema-definition.md)\n- [Filter Expression Syntax](#filter-expression-syntax.md)\n- [Development Environment Setup](#development-environment-setup.md)\n- [Project Configuration Structure](#project-configuration-structure.md)\n- [Tool Configuration](#tool-configuration.md)\n- [Development Workflow Integration](#development-workflow-integration.md)\n- [Best Practices](#best-practices.md)",
      "index": 5,
      "token_count": 340,
      "metadata": {
        "title": "_qdrant_examples_8-additional-use-cases",
        "source": "qdrant_examples\\_qdrant_examples_8-additional-use-cases.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8-additional-use-cases.md",
        "file_name": "_qdrant_examples_8-additional-use-cases.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.882540",
        "total_chunks": 6
      },
      "start_char": 9542,
      "end_char": 11590
    },
    {
      "content": "Self-Query Systems with LangChain | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 590,
      "metadata": {
        "title": "_qdrant_examples_8.1-self-query-systems-with-langchain",
        "source": "qdrant_examples\\_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "file_name": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.916202",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2041
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Self-Query Systems with LangChain\n\nRelevant source files\n\n- [self-query/self-query.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb)\n- [self-query/winemag-data-130k-v2.csv](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/winemag-data-130k-v2.csv)\n\nThis document covers the implementation of self-querying retrieval systems using LangChain's `SelfQueryRetriever` with Qdrant vector database. The system translates natural language queries into structured vector searches with metadata filtering, enabling more precise and contextually relevant information retrieval.\n\nFor information about other advanced RAG patterns, see [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md), [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md), and [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md).\n\n## System Overview\n\nThe self-query system addresses a fundamental challenge in vector search: translating complex natural language queries that contain both semantic content and structured constraints into appropriate vector searches with metadata filters. Instead of embedding the entire query (including filter criteria), the system separates semantic content from metadata constraints.\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb396-455](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L396-L455)\n\n## Data Preparation and Embedding Pipeline\n\nThe system processes wine review data into a structured format suitable for both vector search and metadata filtering. Each document contains semantic content (wine descriptions) and structured metadata (country, price, points, variety).\n\n### Document Structure and Schema\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb237-295](https://github.",
      "index": 1,
      "token_count": 478,
      "metadata": {
        "title": "_qdrant_examples_8.1-self-query-systems-with-langchain",
        "source": "qdrant_examples\\_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "file_name": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.916202",
        "total_chunks": 6
      },
      "start_char": 1941,
      "end_char": 3937
    },
    {
      "content": "ent Structure and Schema\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb237-295](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L237-L295)\n\n### Embedding and Collection Setup\n\nThe system uses `SentenceTransformer` with the \"all-MiniLM-L6-v2\" model for generating embeddings and stores them in a Qdrant collection with cosine distance similarity.\n\n| Component             | Configuration            | Purpose                             |\n| --------------------- | ------------------------ | ----------------------------------- |\n| `SentenceTransformer` | \"all-MiniLM-L6-v2\"       | Generate 384-dimensional embeddings |\n| `QdrantClient`        | Remote/local connection  | Vector database operations          |\n| Collection            | \"wine\\_reviews\"          | Store embeddings and metadata       |\n| Distance Metric       | `models.Distance.COSINE` | Similarity calculation              |\n| Vector Size           | 384 dimensions           | Embedding dimensionality            |\n\n**Sources**: [self-query/self-query.ipynb16-31](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L16-L31) [self-query/self-query.ipynb215-228](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L215-L228)\n\n## Self-Query Retriever Architecture\n\nThe `SelfQueryRetriever` serves as the core component that orchestrates query translation and retrieval. It requires metadata field definitions, document content descriptions, and an LLM for query parsing.\n\n### Metadata Schema Definition\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb424-454](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L424-L454)\n\n## Query Translation Process\n\nThe self-query system transforms natural language into structured queries through a multi-step process involving prompt engineering, LLM reasoning, and structured output parsing.\n\n### Translation Pipeline\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb525-627](https://github.",
      "index": 2,
      "token_count": 497,
      "metadata": {
        "title": "_qdrant_examples_8.1-self-query-systems-with-langchain",
        "source": "qdrant_examples\\_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "file_name": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.916202",
        "total_chunks": 6
      },
      "start_char": 3837,
      "end_char": 5851
    },
    {
      "content": "### Translation Pipeline\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb525-627](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L525-L627)\n\n### Filter Construction\n\nThe system translates natural language constraints into Qdrant filter objects using logical operators and field conditions:\n\n| Natural Language | Generated Filter                     | Qdrant Implementation                                                                 |\n| ---------------- | ------------------------------------ | ------------------------------------------------------------------------------------- |\n| \"US wines\"       | `eq(\"country\", \"US\")`                | `models.FieldCondition(key=\"metadata.country\", match=models.MatchValue(value=\"US\"))`  |\n| \"between $15-30\" | `gte(\"price\", 15), lte(\"price\", 30)` | `models.FieldCondition(key=\"metadata.price\", range=models.Range(gte=15.0, lte=30.0))` |\n| \"90+ points\"     | `gt(\"points\", 90)`                   | `models.FieldCondition(key=\"metadata.points\", range=models.Range(gte=90))`            |\n\n**Sources**: [self-query/self-query.ipynb369-385](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L369-L385)\n\n## Implementation Example\n\nThe following demonstrates a complete query flow from natural language to structured results:\n\n### Query Execution Flow\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb477-514](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L477-L514)\n\n## System Configuration and Dependencies\n\n### Core Dependencies\n\n| Component               | Version/Model      | Purpose                        |\n| ----------------------- | ------------------ | ------------------------------ |\n| `qdrant-client`         | Latest             | Vector database client         |\n| `sentence-transformers` | \"all-MiniLM-L6-v2\" | Text embedding generation      |\n| `langchain`             | Latest             | Self-query retriever framework |",
      "index": 3,
      "token_count": 467,
      "metadata": {
        "title": "_qdrant_examples_8.1-self-query-systems-with-langchain",
        "source": "qdrant_examples\\_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "file_name": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.916202",
        "total_chunks": 6
      },
      "start_char": 5751,
      "end_char": 7726
    },
    {
      "content": "generation      |\n| `langchain`             | Latest             | Self-query retriever framework |\n| `langchain-openai`      | ChatOpenAI         | Query translation LLM          |\n| `langchain-qdrant`      | QdrantVectorStore  | Vector store integration       |\n| `pandas`                | Latest             | Data manipulation              |\n\n### Environment Setup\n\nThe system requires API keys and connection configuration for external services:\n\n```\n```\n\n**Sources**: [self-query/self-query.ipynb16-31](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L16-L31) [self-query/self-query.ipynb396-415](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L396-L415)\n\n## Performance and Tracing\n\nThe system includes tracing capabilities to monitor query translation and execution:\n\n### Query Tracing Output\n\nThe `ConsoleCallbackHandler` provides detailed visibility into the query construction process, showing:\n\n- Prompt template construction\n- LLM query processing with token usage\n- Filter generation and parsing\n- Final query execution\n\n**Sources**: [self-query/self-query.ipynb525-627](https://github.com/qdrant/examples/blob/b3c4b28f/self-query/self-query.ipynb#L525-L627)\n\nThis self-query implementation demonstrates how to bridge natural language interfaces with structured vector search systems, enabling more intuitive and precise information retrieval while maintaining the performance benefits of vector similarity search combined with efficient metadata filtering.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Self-Query Systems with LangChain](#self-query-systems-with-langchain.md)\n- [System Overview](#system-overview.md)\n- [Data Preparation and Embedding Pipeline](#data-preparation-and-embedding-pipeline.md)\n- [Document Structure and Schema](#document-structure-and-schema.md)\n- [Embedding and Collection Setup](#embedding-and-collection-setup.md)\n- [Self-Query Retriever Architecture](#self-query-retriever-architecture.md)",
      "index": 4,
      "token_count": 469,
      "metadata": {
        "title": "_qdrant_examples_8.1-self-query-systems-with-langchain",
        "source": "qdrant_examples\\_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "file_name": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.916202",
        "total_chunks": 6
      },
      "start_char": 7626,
      "end_char": 9652
    },
    {
      "content": "d-collection-setup.md)\n- [Self-Query Retriever Architecture](#self-query-retriever-architecture.md)\n- [Metadata Schema Definition](#metadata-schema-definition.md)\n- [Query Translation Process](#query-translation-process.md)\n- [Translation Pipeline](#translation-pipeline.md)\n- [Filter Construction](#filter-construction.md)\n- [Implementation Example](#implementation-example.md)\n- [Query Execution Flow](#query-execution-flow.md)\n- [System Configuration and Dependencies](#system-configuration-and-dependencies.md)\n- [Core Dependencies](#core-dependencies.md)\n- [Environment Setup](#environment-setup.md)\n- [Performance and Tracing](#performance-and-tracing.md)\n- [Query Tracing Output](#query-tracing-output.md)",
      "index": 5,
      "token_count": 160,
      "metadata": {
        "title": "_qdrant_examples_8.1-self-query-systems-with-langchain",
        "source": "qdrant_examples\\_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "file_name": "_qdrant_examples_8.1-self-query-systems-with-langchain.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:01.916202",
        "total_chunks": 6
      },
      "start_char": 9552,
      "end_char": 11600
    },
    {
      "content": "Development Environment Setup | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.",
      "index": 0,
      "token_count": 586,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2037
    },
    {
      "content": "mples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Data Migration from Pinecone\n\nRelevant source files\n\n- [llama\\_index\\_recency/.gitignore](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/.gitignore)\n- [llama\\_index\\_recency/images/RankFocus.png](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/images/RankFocus.png)\n- [llama\\_index\\_recency/images/RerankFocus.png](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/images/RerankFocus.png)\n- [llama\\_index\\_recency/images/SetupFocus.png](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/images/SetupFocus.png)\n- [llama\\_index\\_recency/pyproject.toml](https://github.com/qdrant/examples/blob/b3c4b28f/llama_index_recency/pyproject.toml)\n\nThis page provides a step-by-step guide for migrating vector embeddings and associated metadata from Pinecone to Qdrant. The guide uses Vector-io, a specialized library designed to simplify data migration between vector databases using a standardized Vector Dataset Format (VDF).\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb8-16](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L8-L16)\n\n## Overview of the Migration Process\n\nMigrating from Pinecone to Qdrant involves three main steps:\n\n1. **Export** - Using Vector-io to export data from Pinecone into the VDF format\n2. **Transform** - Data is automatically standardized into the VDF format\n3. **Import** - Importing the VDF-formatted data into Qdrant\n\nThis migration pathway ensures data consistency regardless of differences between the source and destination databases.\n\n```\n```\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb8-16](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L8-L16) [data-migration/from-pinecone-to-qdrant.ipynb278-335](https://github.",
      "index": 1,
      "token_count": 560,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 1937,
      "end_char": 3922
    },
    {
      "content": "necone-to-qdrant.ipynb#L8-L16) [data-migration/from-pinecone-to-qdrant.ipynb278-335](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L278-L335)\n\n## Prerequisites\n\nBefore starting the migration process, you need:\n\n1. **API Keys and Access**:\n\n   - Pinecone API key\n   - Qdrant API key or host URL\n   - OpenAI API key (if you used OpenAI embeddings)\n\n2. **Vector-io Installation**:\n\n   ```\n   pip install vdf-io\n   ```\n\n3. **Environment Setup**: Create a `.env` file with your credentials based on the `.env-example` template:\n\n| Environment Variable     | Description                      |\n| ------------------------ | -------------------------------- |\n| QDRANT\\_COLLECTION\\_NAME | Target collection name in Qdrant |\n| QDRANT\\_HOST             | Qdrant host URL                  |\n| QDRANT\\_API\\_KEY         | Qdrant API key                   |\n| OPENAI\\_API\\_KEY         | OpenAI API key                   |\n| PINECONE\\_API\\_KEY       | Pinecone API key                 |\n| PINECONE\\_CLOUD          | Cloud provider (e.g., \"aws\")     |\n| PINECONE\\_REGION         | Region (e.g., \"us-east-1\")       |\n\nSources: [data-migration/.env-example1-11](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/.env-example#L1-L11) [data-migration/from-pinecone-to-qdrant.ipynb280-284](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L280-L284)\n\n## The Demonstration Dataset\n\nThe example in the notebook uses a PubMed dataset from Hugging Face to illustrate the migration process. The dataset is embedded using OpenAI's text-embedding-3-small model (1536 dimensions).\n\n### Loading and Preparing Sample Data\n\nThe notebook demonstrates:\n\n1. Loading the PubMed dataset\n2. Creating embeddings using OpenAI's embedding model\n3. Storing the data in Pinecone\n\n```\n```\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb55-61](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.",
      "index": 2,
      "token_count": 568,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 3822,
      "end_char": 5824
    },
    {
      "content": "ipynb55-61](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L55-L61) [data-migration/from-pinecone-to-qdrant.ipynb163-270](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L163-L270)\n\n## Step 1: Setting Up Pinecone\n\nThe notebook demonstrates creating a Pinecone index with appropriate dimensions and embedding model:\n\n1. Initialize Pinecone client with API key\n2. Create a serverless index with correct dimension (1536 for OpenAI's text-embedding-3-small model)\n3. Configure the OpenAI embedding function\n4. Load data and generate embeddings\n5. Upsert the embedding vectors and metadata to Pinecone\n\nThe code in the notebook sets up a Pinecone index named \"pubmed\" with a dimension of 1536 and a cosine similarity metric.\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb182-270](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L182-L270)\n\n## Step 2: Exporting Data from Pinecone\n\nThe data export step uses the Vector-io command-line interface:\n\n```\nexport_vdf pinecone --serverless -c aws --region us-east-1 -i pubmed --namespace \"\"\n```\n\nThis command:\n\n- Specifies Pinecone as the source database\n- Indicates it's a serverless instance\n- Defines the cloud provider as AWS\n- Sets the region to us-east-1\n- Specifies the index name as \"pubmed\"\n- Targets the default namespace\n\nThe export process:\n\n1. Collects all points/vectors in the specified index\n2. Fetches all vectors with their metadata\n3. Exports them to a standardized Vector Dataset Format (VDF)\n4. Stores the result in a directory with a timestamped name (e.g., `vdf_20240510_001325_88ae5/`)\n\nThe output directory contains:\n\n- A metadata file (`VDF_META.json`) with information about the exported data\n- Parquet files containing the actual vector data and metadata\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb290-332](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L290-L332)",
      "index": 3,
      "token_count": 594,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 5724,
      "end_char": 7763
    },
    {
      "content": "//github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L290-L332)\n\n## Step 3: Importing Data to Qdrant\n\nAfter exporting, use the Vector-io import command to transfer the data to Qdrant:\n\n```\nimport_vdf qdrant -u $QDRANT_HOST\n```\n\nWhen prompted, specify the directory containing the exported VDF data.\n\nThe import process:\n\n1. Reads the VDF metadata to understand the data structure\n2. Parses the vector data from the Parquet files\n3. Converts metadata to the appropriate format for Qdrant\n4. Creates a collection in Qdrant with matching parameters (if it doesn't exist)\n5. Uploads vectors in batches to Qdrant\n\nThe import tool will report:\n\n- Number of vectors processed\n- Number of vectors successfully imported\n- Time taken for the operation\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb342-360](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L342-L360)\n\n## Data Flow and Transformation\n\nDuring the migration process, Vector-io handles the following transformations:\n\n```\n```\n\nThe Vector Dataset Format (VDF) serves as a standardized intermediate representation, ensuring compatibility between different vector database systems. Key transformation points include:\n\n1. **ID handling**: Ensuring consistent ID formats across systems\n2. **Vector dimensionality**: Maintaining the same dimensions (1536 in this example)\n3. **Metadata/payload conversion**: Converting between Pinecone's metadata and Qdrant's payload format\n4. **Index configuration**: Transferring similarity metrics and other settings\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb8-16](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L8-L16) [data-migration/from-pinecone-to-qdrant.ipynb290-360](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L290-L360)\n\n## Verification and Validation\n\nAfter completing the migration, it's important to verify that:\n\n1.",
      "index": 4,
      "token_count": 533,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 7663,
      "end_char": 9674
    },
    {
      "content": "## Verification and Validation\n\nAfter completing the migration, it's important to verify that:\n\n1. **Vector count matches**: The number of vectors in Qdrant should match the number exported from Pinecone\n2. **Metadata integrity**: Sample checks to ensure metadata was correctly transferred\n3. **Search functionality**: Test similar queries in both systems to compare results\n\nThe imported collection should be visible in the Qdrant dashboard where you can verify vector counts and test search functionality.\n\nSources: [data-migration/from-pinecone-to-qdrant.ipynb362-366](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/from-pinecone-to-qdrant.ipynb#L362-L366)\n\n## Best Practices and Considerations\n\nWhen migrating vector data from Pinecone to Qdrant, consider the following best practices:\n\n1. **Backup your data**: Always ensure you have a backup of your Pinecone data before migration\n2. **Test with a subset**: For large collections, consider testing the migration with a small subset first\n3. **Verify dimensions and metrics**: Ensure the dimension size and similarity metric match between systems\n4. **Choose appropriate batch size**: For large datasets, adjust the batch size during import to optimize performance\n5. **Plan for downtime**: Schedule migration during low-traffic periods\n6. **Post-migration testing**: Thoroughly test your application with the new Qdrant backend\n\n## Environment Variables\n\nThe configuration uses environment variables to manage credentials securely. The required variables include:\n\n| Variable                 | Purpose                                        |\n| ------------------------ | ---------------------------------------------- |\n| QDRANT\\_COLLECTION\\_NAME | The name of the collection in Qdrant           |\n| QDRANT\\_HOST             | The URL endpoint for your Qdrant instance      |\n| QDRANT\\_API\\_KEY         | API key for authenticating with Qdrant         |\n| PINECONE\\_API\\_KEY       | API key for Pinecone access                    |",
      "index": 5,
      "token_count": 421,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 9574,
      "end_char": 11582
    },
    {
      "content": "with Qdrant         |\n| PINECONE\\_API\\_KEY       | API key for Pinecone access                    |\n| PINECONE\\_CLOUD          | Cloud provider for Pinecone (e.g., aws)        |\n| PINECONE\\_REGION         | Region for Pinecone (e.g., us-east-1)          |\n| OPENAI\\_API\\_KEY         | API key for OpenAI (if using their embeddings) |\n\nSources: [data-migration/.env-example1-11](https://github.com/qdrant/examples/blob/b3c4b28f/data-migration/.env-example#L1-L11)\n\n## Technical Details and Limitations\n\nWhen migrating between Pinecone and Qdrant, be aware of these technical considerations:\n\n1. **Similarity metrics**: Ensure you're using comparable similarity metrics (e.g., cosine, dot product, euclidean)\n2. **Vector dimensions**: The vector dimensions must match between source and destination\n3. **Custom metadata**: Complex metadata structures may require additional handling\n4. **Scale considerations**: Very large datasets might require splitting the migration into multiple operations\n5. **Network bandwidth**: For large datasets, ensure adequate network bandwidth between systems\n\n## Conclusion\n\nVector-io provides a streamlined solution for migrating vector data from Pinecone to Qdrant. The standardized VDF format ensures data integrity during the migration process, making it convenient to switch between vector database providers while preserving your vector embeddings and associated metadata.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Data Migration from Pinecone](#data-migration-from-pinecone.md)\n- [Overview of the Migration Process](#overview-of-the-migration-process.md)\n- [Prerequisites](#prerequisites.md)\n- [The Demonstration Dataset](#the-demonstration-dataset.md)\n- [Loading and Preparing Sample Data](#loading-and-preparing-sample-data.md)\n- [Step 1: Setting Up Pinecone](#step-1-setting-up-pinecone.md)\n- [Step 2: Exporting Data from Pinecone](#step-2-exporting-data-from-pinecone.md)\n- [Step 3: Importing Data to Qdrant](#step-3-importing-data-to-qdrant.md)",
      "index": 6,
      "token_count": 474,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 11482,
      "end_char": 13501
    },
    {
      "content": "ng-data-from-pinecone.md)\n- [Step 3: Importing Data to Qdrant](#step-3-importing-data-to-qdrant.md)\n- [Data Flow and Transformation](#data-flow-and-transformation.md)\n- [Verification and Validation](#verification-and-validation.md)\n- [Best Practices and Considerations](#best-practices-and-considerations.md)\n- [Environment Variables](#environment-variables.md)\n- [Technical Details and Limitations](#technical-details-and-limitations.md)\n- [Conclusion](#conclusion.md)",
      "index": 7,
      "token_count": 116,
      "metadata": {
        "title": "_qdrant_examples_8.2-development-environment-setup",
        "source": "qdrant_examples\\_qdrant_examples_8.2-development-environment-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_examples",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_examples_8.2-development-environment-setup.md",
        "file_name": "_qdrant_examples_8.2-development-environment-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.062234",
        "total_chunks": 8
      },
      "start_char": 13401,
      "end_char": 15449
    }
  ]
}