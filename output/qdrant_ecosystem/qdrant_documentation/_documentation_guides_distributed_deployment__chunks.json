{
  "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
  "source_repo": "qdrant_documentation",
  "total_chunks": 41,
  "chunks": [
    {
      "content": "Distributed Deployment - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Getting Started\n\n[What is Qdrant?](https://qdrant.tech/documentation/overview/)\n\n- [Understanding Vector Search in Qdrant](https://qdrant.tech/documentation/overview/vector-search/)\n\n[Local Quickstart](https://qdrant.tech/documentation/quickstart/)\n\n[API & SDKs](https://qdrant.tech/documentation/interfaces/)\n\n[Qdrant Web UI](https://qdrant.tech/documentation/web-ui/)\n\n### User Manual\n\n[Concepts](https://qdrant.tech/documentation/concepts/)\n\n- [Collections](https://qdrant.tech/documentation/concepts/collections/)\n- [Points](https://qdrant.tech/documentation/concepts/points/)\n- [Vectors](https://qdrant.tech/documentation/concepts/vectors/)\n- [Payload](https://qdrant.tech/documentation/concepts/payload/)\n- [Search](https://qdrant.tech/documentation/concepts/search/)\n- [Explore](https://qdrant.tech/documentation/concepts/explore/)\n- [Hybrid Queries](https://qdrant.tech/documentation/concepts/hybrid-queries/)\n- [Filtering](https://qdrant.tech/documentation/concepts/filtering/)\n- [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/)\n- [Storage](https://qdrant.tech/documentation/concepts/storage/)\n- [Indexing](https://qdrant.tech/documentation/concepts/indexing/)\n- [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/)\n\n[Guides](https://qdrant.tech/documentation/guides/installation/)\n\n- [Installation](https://qdrant.",
      "index": 0,
      "token_count": 528,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 0,
      "end_char": 2033
    },
    {
      "content": "[Guides](https://qdrant.tech/documentation/guides/installation/)\n\n- [Installation](https://qdrant.tech/documentation/guides/installation/)\n- [Administration](https://qdrant.tech/documentation/guides/administration/)\n- [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/)\n- [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/)\n- [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/)\n- [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/)\n- [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/)\n- [Quantization](https://qdrant.tech/documentation/guides/quantization/)\n- [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/)\n- [Configuration](https://qdrant.tech/documentation/guides/configuration/)\n- [Security](https://qdrant.tech/documentation/guides/security/)\n- [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/)\n- [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/)\n\n### Ecosystem\n\n[FastEmbed](https://qdrant.tech/documentation/fastembed/)\n\n- [Quickstart](https://qdrant.tech/documentation/fastembed/fastembed-quickstart/)\n- [FastEmbed & Qdrant](https://qdrant.tech/documentation/fastembed/fastembed-semantic-search/)\n- [Working with miniCOIL](https://qdrant.tech/documentation/fastembed/fastembed-minicoil/)\n- [Working with SPLADE](https://qdrant.tech/documentation/fastembed/fastembed-splade/)\n- [Working with ColBERT](https://qdrant.tech/documentation/fastembed/fastembed-colbert/)\n- [Reranking with FastEmbed](https://qdrant.tech/documentation/fastembed/fastembed-rerankers/)\n\n[Qdrant MCP Server](https://github.com/qdrant/mcp-server-qdrant)\n\n### Tutorials\n\n[Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/)\n\n- [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/)\n- [Build a Neural Search Service](https://qdrant.",
      "index": 1,
      "token_count": 507,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 1933,
      "end_char": 3937
    },
    {
      "content": "ocumentation/beginner-tutorials/search-beginners/)\n- [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/)\n- [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/)\n- [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/)\n\n[Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/)\n\n- [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/)\n- [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)\n- [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/)\n- [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/)\n- [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/)\n\n[Using the Database](https://qdrant.tech/documentation/database-tutorials/)\n\n- [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/)\n- [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/)\n- [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/)\n- [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/)\n- [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/)\n- [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/)\n- [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/)\n\n### Support\n\n[FAQ](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)\n\n- [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)",
      "index": 2,
      "token_count": 472,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 3837,
      "end_char": 5852
    },
    {
      "content": "fundamentals/)\n\n- [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)\n- [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/)\n\n[Release Notes](https://github.com/qdrant/qdrant/releases)\n\n### Getting Started\n\n[What is Qdrant?](https://qdrant.tech/documentation/overview/)\n\n- [Understanding Vector Search in Qdrant](https://qdrant.tech/documentation/overview/vector-search/)\n\n[Local Quickstart](https://qdrant.tech/documentation/quickstart/)\n\n[API & SDKs](https://qdrant.tech/documentation/interfaces/)\n\n[Qdrant Web UI](https://qdrant.tech/documentation/web-ui/)\n\n### User Manual\n\n[Concepts](https://qdrant.tech/documentation/concepts/)\n\n- [Collections](https://qdrant.tech/documentation/concepts/collections/)\n- [Points](https://qdrant.tech/documentation/concepts/points/)\n- [Vectors](https://qdrant.tech/documentation/concepts/vectors/)\n- [Payload](https://qdrant.tech/documentation/concepts/payload/)\n- [Search](https://qdrant.tech/documentation/concepts/search/)\n- [Explore](https://qdrant.tech/documentation/concepts/explore/)\n- [Hybrid Queries](https://qdrant.tech/documentation/concepts/hybrid-queries/)\n- [Filtering](https://qdrant.tech/documentation/concepts/filtering/)\n- [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/)\n- [Storage](https://qdrant.tech/documentation/concepts/storage/)\n- [Indexing](https://qdrant.tech/documentation/concepts/indexing/)\n- [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/)\n\n[Guides](https://qdrant.tech/documentation/guides/installation/)\n\n- [Installation](https://qdrant.tech/documentation/guides/installation/)\n- [Administration](https://qdrant.tech/documentation/guides/administration/)\n- [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/)\n- [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/)\n- [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/)\n- [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/)",
      "index": 3,
      "token_count": 514,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 5752,
      "end_char": 7800
    },
    {
      "content": "n/guides/optimize/)\n- [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/)\n- [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/)\n- [Quantization](https://qdrant.tech/documentation/guides/quantization/)\n- [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/)\n- [Configuration](https://qdrant.tech/documentation/guides/configuration/)\n- [Security](https://qdrant.tech/documentation/guides/security/)\n- [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/)\n- [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/)\n\n### Ecosystem\n\n[FastEmbed](https://qdrant.tech/documentation/fastembed/)\n\n- [Quickstart](https://qdrant.tech/documentation/fastembed/fastembed-quickstart/)\n- [FastEmbed & Qdrant](https://qdrant.tech/documentation/fastembed/fastembed-semantic-search/)\n- [Working with miniCOIL](https://qdrant.tech/documentation/fastembed/fastembed-minicoil/)\n- [Working with SPLADE](https://qdrant.tech/documentation/fastembed/fastembed-splade/)\n- [Working with ColBERT](https://qdrant.tech/documentation/fastembed/fastembed-colbert/)\n- [Reranking with FastEmbed](https://qdrant.tech/documentation/fastembed/fastembed-rerankers/)\n\n[Qdrant MCP Server](https://github.com/qdrant/mcp-server-qdrant)\n\n### Tutorials\n\n[Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/)\n\n- [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/)\n- [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/)\n- [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/)\n- [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/)\n\n[Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/)\n\n- [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.",
      "index": 4,
      "token_count": 501,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 7700,
      "end_char": 9697
    },
    {
      "content": "nced-tutorials/)\n\n- [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/)\n- [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)\n- [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/)\n- [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/)\n- [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/)\n\n[Using the Database](https://qdrant.tech/documentation/database-tutorials/)\n\n- [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/)\n- [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/)\n- [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/)\n- [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/)\n- [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/)\n- [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/)\n- [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/)\n\n### Support\n\n[FAQ](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)\n\n- [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/)\n- [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/)\n\n[Release Notes](https://github.com/qdrant/qdrant/releases)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- [Guides](https://qdrant.tech/documentation/guides/)\n-\n- Distributed Deployment\n\n# Distributed deployment\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.",
      "index": 5,
      "token_count": 465,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 9597,
      "end_char": 11560
    },
    {
      "content": "yment\n\n# Distributed deployment\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode. In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.\n\n## How many Qdrant nodes should I run?\n\nThe ideal number of Qdrant nodes depends on how much you value cost-saving, resilience, and performance/scalability in relation to each other.\n\n- **Prioritizing cost-saving**: If cost is most important to you, run a single Qdrant node. This is not recommended for production environments. Drawbacks:\n\n  - Resilience: Users will experience downtime during node restarts, and recovery is not possible unless you have backups or snapshots.\n  - Performance: Limited to the resources of a single server.\n\n- **Prioritizing resilience**: If resilience is most important to you, run a Qdrant cluster with three or more nodes and two or more shard replicas. Clusters with three or more nodes and replication can perform all operations even while one node is down. Additionally, they gain performance benefits from load-balancing and they can recover from the permanent loss of one node without the need for backups or snapshots (but backups are still strongly recommended). This is most recommended for production environments. Drawbacks:\n\n  - Cost: Larger clusters are more costly than smaller clusters, which is the only drawback of this configuration.\n\n- **Balancing cost, resilience, and performance**: Running a two-node Qdrant cluster with replicated shards allows the cluster to respond to most read/write requests even when one node is down, such as during maintenance events. Having two nodes also means greater performance than a single-node cluster while still being cheaper than a three-node cluster. Drawbacks:\n\n  - Resilience (uptime): The cluster cannot perform operations on collections when one node is down. Those operations require >50% of nodes to be running, so this is only possible in a 3+ node cluster.",
      "index": 6,
      "token_count": 408,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 11460,
      "end_char": 13485
    },
    {
      "content": "Those operations require >50% of nodes to be running, so this is only possible in a 3+ node cluster. Since creating, editing, and deleting collections are usually rare operations, many users find this drawback to be negligible.\n  - Resilience (data integrity): If the data on one of the two nodes is permanently lost or corrupted, it cannot be recovered aside from snapshots or backups. Only 3+ node clusters can recover from the permanent loss of a single node since recovery operations require >50% of the cluster to be healthy.\n  - Cost: Replicating your shards requires storing two copies of your data.\n  - Performance: The maximum performance of a Qdrant cluster increases as you add more nodes.\n\nIn summary, single-node clusters are best for non-production workloads, replicated 3+ node clusters are the gold standard, and replicated 2-node clusters strike a good balance.\n\n## Enabling distributed mode in self-hosted Qdrant\n\nTo enable distributed deployment - enable the cluster mode in the [configuration](https://qdrant.tech/documentation/guides/configuration/) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n```yaml\ncluster:\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n  enabled: true\n  # Configuration of the inter-cluster communication\n  p2p:\n    # Port for internal communication between peers\n    port: 6335\n\n  # Configuration related to distributed consensus algorithm\n  consensus:\n    # How frequently peers should ping each other.\n    # Setting this parameter to lower value will allow consensus\n    # to detect disconnected node earlier, but too frequent\n    # tick period may create significant network and CPU overhead.\n    # We encourage you NOT to change this parameter unless you know what you are doing.\n    tick_period_ms: 100\n```\n\nBy default, Qdrant will use port `6335` for its internal communication. All peers should be accessible on this port from within the cluster, but make sure to isolate this port from outside access, as it might be used to perform write operations.",
      "index": 7,
      "token_count": 445,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 13385,
      "end_char": 15428
    },
    {
      "content": "ke sure to isolate this port from outside access, as it might be used to perform write operations.\n\nAdditionally, you must provide the `--uri` flag to the first peer so it can tell other nodes how it should be reached:\n\n```bash\n./qdrant --uri 'http://qdrant_node_1:6335'\n```\n\nSubsequent peers in a cluster must know at least one node of the existing cluster to synchronize through it with the rest of the cluster.\n\nTo do this, they need to be provided with a bootstrap URL:\n\n```bash\n./qdrant --bootstrap 'http://qdrant_node_1:6335'\n```\n\nThe URL of the new peers themselves will be calculated automatically from the IP address of their request. But it is also possible to provide them individually using the `--uri` argument.\n\n```text\nUSAGE:\n    qdrant [OPTIONS]\n\nOPTIONS:\n        --bootstrap <URI>\n            Uri of the peer to bootstrap from in case of multi-peer deployment. If not specified -\n            this peer will be considered as a first in a new deployment\n\n        --uri <URI>\n            Uri of this peer. Other peers should be able to reach it by this uri.\n\n            This value has to be supplied if this is the first peer in a new deployment.\n\n            In case this is not the first peer and it bootstraps the value is optional. If not\n            supplied then qdrant will take internal grpc port from config and derive the IP address\n            of this peer on bootstrap peer (receiving side)\n```\n\nAfter a successful synchronization you can observe the state of the cluster through the [REST API](https://api.qdrant.tech/master/api-reference/distributed/cluster-status):\n\n```http\nGET /cluster\n```\n\nExample result:\n\n```json\n{\n  \"result\": {\n    \"status\": \"enabled\",\n    \"peer_id\": 11532566549086892000,\n    \"peers\": {\n      \"9834046559507417430\": {\n        \"uri\": \"http://172.18.0.3:6335/\"\n      },\n      \"11532566549086892528\": {\n        \"uri\": \"http://qdrant_node_1:6335/\"\n      }\n    },\n    \"raft_info\": {\n      \"term\": 1,\n      \"commit\": 4,\n      \"pending_operations\": 1,\n      \"leader\": 11532566549086892000,",
      "index": 8,
      "token_count": 570,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 15328,
      "end_char": 17365
    },
    {
      "content": "\"term\": 1,\n      \"commit\": 4,\n      \"pending_operations\": 1,\n      \"leader\": 11532566549086892000,\n      \"role\": \"Leader\"\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 5.731e-06\n}\n```\n\nNote that enabling distributed mode does not automatically replicate your data. See the section on [making use of a new distributed Qdrant cluster](#making-use-of-a-new-distributed-qdrant-cluster.md) for the next steps.\n\n## Enabling distributed mode in Qdrant Cloud\n\nFor best results, first ensure your cluster is running Qdrant v1.7.4 or higher. Older versions of Qdrant do support distributed mode, but improvements in v1.7.4 make distributed clusters more resilient during outages.\n\nIn the [Qdrant Cloud console](https://cloud.qdrant.io/), click “Scale Up” to increase your cluster size to >1. Qdrant Cloud configures the distributed mode settings automatically.\n\nAdditionally, Qdrant Cloud also offers the ability to automatically rebalance and to reshard your collections, which is not available in self-hosted Qdrant. See the [Resharding](https://qdrant.tech/documentation/cloud/cluster-scaling/#resharding) and [Shard Rebalancing](https://qdrant.tech/documentation/cloud/configure-cluster/#shard-rebalancing) sections in for more details.\n\nAfter the scale-up process completes, you will have a new empty node running alongside your existing node(s). To replicate data into this new empty node, see the next section.\n\n## Making use of a new distributed Qdrant cluster\n\nWhen you enable distributed mode and scale up to two or more nodes, your data does not move to the new node automatically; it starts out empty. To make use of your new empty node, do one of the following:\n\n- Create a new replicated collection by setting the [replication\\_factor](#replication-factor.md) to 2 or more and setting the [number of shards](#choosing-the-right-number-of-shards.md) to a multiple of your number of nodes.",
      "index": 9,
      "token_count": 470,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 17265,
      "end_char": 19151
    },
    {
      "content": "[number of shards](#choosing-the-right-number-of-shards.md) to a multiple of your number of nodes.\n- If you have an existing collection which does not contain enough shards for each node, you must create a new collection as described in the previous bullet point.\n- If you already have enough shards for each node, and you merely need to replicate your data, follow the directions for [creating new shard replicas](#creating-new-shard-replicas.md).\n- If you already have enough shards for each node, and your data is already replicated, you can move data (without replicating it) onto the new node(s) by [moving shards](#moving-shards.md).\n\n## Raft\n\nQdrant uses the [Raft](https://raft.github.io/) consensus protocol to maintain consistency regarding the cluster topology and the collections structure.\n\nOperations on points, on the other hand, do not go through the consensus infrastructure. Qdrant is not intended to have strong transaction guarantees, which allows it to perform point operations with low overhead. In practice, it means that Qdrant does not guarantee atomic distributed updates but allows you to wait until the [operation is complete](https://qdrant.tech/documentation/concepts/points/#awaiting-result) to see the results of your writes.\n\nOperations on collections, on the contrary, are part of the consensus which guarantees that all operations are durable and eventually executed by all nodes. In practice it means that a majority of nodes agree on what operations should be applied before the service will perform them.\n\nPractically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\nYou may use the cluster [REST API](https://api.qdrant.tech/master/api-reference/distributed/cluster-status) to check the state of the consensus.\n\n## Sharding\n\nA Collection in Qdrant is made of one or more shards. A shard is an independent store of points which is able to perform all operations provided by collections.",
      "index": 10,
      "token_count": 422,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 19051,
      "end_char": 21095
    },
    {
      "content": "d is an independent store of points which is able to perform all operations provided by collections. There are two methods of distributing points across shards:\n\n- **Automatic sharding**: Points are distributed among shards by using a [consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing) algorithm, so that shards are managing non-intersecting subsets of points. This is the default behavior.\n\n- **User-defined sharding**: *Available as of v1.7.0* - Each point is uploaded to a specific shard, so that operations can hit only the shard or shards they need. Even with this distribution, shards still ensure having non-intersecting subsets of points. [See more…](#user-defined-sharding.md)\n\nEach node knows where all parts of the collection are stored through the [consensus protocol](#raft.md), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n### Choosing the right number of shards\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster when the collection was created. The `shard_number` cannot be changed without recreating the collection.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 300,\n      \"distance\": \"Cosine\"\n    },\n    \"shard_number\": 6\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n    shard_number=6,\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n    vectors: {\n        size: 300,\n        distance: \"Cosine\",\n    },\n    shard_number: 6,\n});\n```\n\n```rust",
      "index": 11,
      "token_count": 469,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 20995,
      "end_char": 22971
    },
    {
      "content": "tors: {\n        size: 300,\n        distance: \"Cosine\",\n    },\n    shard_number: 6,\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{CreateCollectionBuilder, Distance, VectorParamsBuilder};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(300, Distance::Cosine))\n            .shard_number(6),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(300)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setShardNumber(6)\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n\tcollectionName: \"{collection_name}\",\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\tshardNumber: 6\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n\tCollectionName: \"{collection_name}\",\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{",
      "index": 12,
      "token_count": 497,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 22871,
      "end_char": 24916
    },
    {
      "content": "CollectionName: \"{collection_name}\",\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n\t\tSize:     300,\n\t\tDistance: qdrant.Distance_Cosine,\n\t}),\n\tShardNumber: qdrant.PtrOf(uint32(6)),\n})\n```\n\nTo ensure all nodes in your cluster are evenly utilized, the number of shards must be a multiple of the number of nodes you are currently running in your cluster.\n\n> Aside: Advanced use cases such as multitenancy may require an uneven distribution of shards. See [Multitenancy](https://qdrant.tech/articles/multitenancy/).\n\nWe recommend creating at least 2 shards per node to allow future expansion without having to re-shard. [Resharding](#resharding.md) is possible when using our cloud offering, but should be avoided if hosting elsewhere as it would require creating a new collection.\n\nIf you anticipate a lot of growth, we recommend 12 shards since you can expand from 1 node up to 2, 3, 6, and 12 nodes without having to re-shard. Having more than 12 shards in a small cluster may not be worth the performance overhead.\n\nShards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n### Resharding\n\n*Available as of v1.13.0 in Cloud*\n\nResharding allows you to change the number of shards in your existing collections if you’re hosting with our [Cloud](https://qdrant.tech/documentation/cloud-intro/) offering.\n\nResharding can change the number of shards both up and down, without having to recreate the collection from scratch.\n\nPlease refer to the [Resharding](https://qdrant.tech/documentation/cloud/cluster-scaling/#resharding) section in our cloud documentation for more details.\n\n### Moving shards\n\n*Available as of v0.9.0*\n\nQdrant allows moving shards between nodes in the cluster and removing nodes from the cluster.",
      "index": 13,
      "token_count": 459,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 24816,
      "end_char": 26811
    },
    {
      "content": ".9.0*\n\nQdrant allows moving shards between nodes in the cluster and removing nodes from the cluster. This functionality unlocks the ability to dynamically scale the cluster size without downtime. It also allows you to upgrade or migrate nodes without downtime.\n\nIf your cluster is running in Qdrant Cloud, shards are balanced across the cluster nodes automatically. For more information see the [Configuring Cloud Clusters](https://qdrant.tech/documentation/cloud/configure-cluster/#shard-rebalancing) and [Cloud Cluster Scaling](https://qdrant.tech/documentation/cloud/cluster-scaling/) documentation.\n\nQdrant provides the information regarding the current shard distribution in the cluster with the [Collection Cluster info API](https://api.qdrant.tech/master/api-reference/distributed/collection-cluster-info).\n\nUse the [Update collection cluster setup API](https://api.qdrant.tech/master/api-reference/distributed/update-collection-cluster) to initiate the shard transfer:\n\n```http\nPOST /collections/{collection_name}/cluster\n{\n    \"move_shard\": {\n        \"shard_id\": 0,\n        \"from_peer_id\": 381894127,\n        \"to_peer_id\": 467122995\n    }\n}\n```\n\nYou likely want to select a specific [shard transfer method](#shard-transfer-method.md) to get desired performance and guarantees.\n\nAfter the transfer is initiated, the service will process it based on the used [transfer method](#shard-transfer-method.md) keeping both shards in sync. Once the transfer is completed, the old shard is deleted from the source node.\n\nIn case you want to downscale the cluster, you can move all shards away from a peer and then remove the peer using the [remove peer API](https://api.qdrant.tech/master/api-reference/distributed/remove-peer).\n\n```http\nDELETE /cluster/peer/{peer_id}\n```\n\nAfter that, Qdrant will exclude the node from the consensus, and the instance will be ready for shutdown.\n\n### User-defined sharding\n\n*Available as of v1.7.0*\n\nQdrant allows you to specify the shard for each point individually.",
      "index": 14,
      "token_count": 447,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 26711,
      "end_char": 28711
    },
    {
      "content": "rding\n\n*Available as of v1.7.0*\n\nQdrant allows you to specify the shard for each point individually. This feature is useful if you want to control the shard placement of your data, so that operations can hit only the subset of shards they actually need. In big clusters, this can significantly improve the performance of operations that do not require the whole collection to be scanned.\n\nA clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.\n\nTo enable user-defined sharding, set `sharding_method` to `custom` during collection creation:\n\n```http\nPUT /collections/{collection_name}\n{\n    \"shard_number\": 1,\n    \"sharding_method\": \"custom\"\n    // ... other collection parameters\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    shard_number=1,\n    sharding_method=models.ShardingMethod.CUSTOM,\n    # ... other collection parameters\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n    shard_number: 1,\n    sharding_method: \"custom\",\n    // ... other collection parameters\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    CreateCollectionBuilder, Distance, ShardingMethod, VectorParamsBuilder,\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(300, Distance::Cosine))\n            .shard_number(1)\n            .sharding_method(ShardingMethod::Custom.into()),\n    )\n    .await?;\n```\n\n```java\nimport static io.qdrant.client.ShardKeyFactory.shardKey;\n\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.",
      "index": 15,
      "token_count": 493,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 28611,
      "end_char": 30649
    },
    {
      "content": "ant.client.ShardKeyFactory.shardKey;\n\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.ShardingMethod;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            // ... other collection parameters\n            .setShardNumber(1)\n            .setShardingMethod(ShardingMethod.Custom)\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n\tcollectionName: \"{collection_name}\",\n\t// ... other collection parameters\n\tshardNumber: 1,\n\tshardingMethod: ShardingMethod.Custom\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n\tCollectionName: \"{collection_name}\",\n\t// ... other collection parameters\n\tShardNumber:    qdrant.PtrOf(uint32(1)),\n\tShardingMethod: qdrant.ShardingMethod_Custom.Enum(),\n})\n```\n\nIn this mode, the `shard_number` means the number of shards per shard key, where points will be distributed evenly. For example, if you have 10 shard keys and a collection config with these settings:\n\n```json\n{\n    \"shard_number\": 1,\n    \"sharding_method\": \"custom\",\n    \"replication_factor\": 2\n}\n```\n\nThen you will have `1 * 10 * 2 = 20` total physical shards in the collection.\n\nPhysical shards require a large amount of resources, so make sure your custom sharding key has a low cardinality.\n\nFor large cardinality keys, it is recommended to use [partition by payload](https://qdrant.tech/documentation/guides/multiple-partitions/#partition-by-payload) instead.",
      "index": 16,
      "token_count": 501,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 30549,
      "end_char": 32540
    },
    {
      "content": "load](https://qdrant.tech/documentation/guides/multiple-partitions/#partition-by-payload) instead.\n\nNow you need to create custom shards ([API reference](https://api.qdrant.tech/api-reference/distributed/create-shard-key#request)):\n\n```http\nPUT /collections/{collection_name}/shards\n{\n  \"shard_key\": \"{shard_key}\"\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_shard_key(\"{collection_name}\", \"{shard_key}\")\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createShardKey(\"{collection_name}\", {\n    shard_key: \"{shard_key}\"\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    CreateShardKeyBuilder, CreateShardKeyRequestBuilder\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_shard_key(\n        CreateShardKeyRequestBuilder::new(\"{collection_name}\")\n            .request(CreateShardKeyBuilder::default().shard_key(\"{shard_key\".to_string())),\n    )\n    .await?;\n```\n\n```java\nimport static io.qdrant.client.ShardKeyFactory.shardKey;\n\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateShardKey;\nimport io.qdrant.client.grpc.Collections.CreateShardKeyRequest;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient.createShardKeyAsync(CreateShardKeyRequest.newBuilder()\n                .setCollectionName(\"{collection_name}\")\n                .setRequest(CreateShardKey.newBuilder()\n                                .setShardKey(shardKey(\"{shard_key}\"))\n                                .build())\n                .build()).get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateShardKeyAsync(\n    \"{collection_name}\",",
      "index": 17,
      "token_count": 514,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 32440,
      "end_char": 34425
    },
    {
      "content": "= new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateShardKeyAsync(\n    \"{collection_name}\",\n    new CreateShardKey { ShardKey = new ShardKey { Keyword = \"{shard_key}\", } }\n    );\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateShardKey(context.Background(), \"{collection_name}\", &qdrant.CreateShardKey{\n\tShardKey: qdrant.NewShardKey(\"{shard_key}\"),\n})\n```\n\nTo specify the shard for each point, you need to provide the `shard_key` field in the upsert request:\n\n```http\nPUT /collections/{collection_name}/points\n{\n    \"points\": [\n        {\n            \"id\": 1111,\n            \"vector\": [0.1, 0.2, 0.3]\n        },\n    ]\n    \"shard_key\": \"user_1\"\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.upsert(\n    collection_name=\"{collection_name}\",\n    points=[\n        models.PointStruct(\n            id=1111,\n            vector=[0.1, 0.2, 0.3],\n        ),\n    ],\n    shard_key_selector=\"user_1\",\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.upsert(\"{collection_name}\", {\n    points: [\n        {\n            id: 1111,\n            vector: [0.1, 0.2, 0.3],\n        },\n    ],\n    shard_key: \"user_1\",\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{PointStruct, UpsertPointsBuilder};\nuse qdrant_client::Payload;\n\nclient\n    .upsert_points(\n        UpsertPointsBuilder::new(\n            \"{collection_name}\",\n            vec![PointStruct::new(\n                111,\n                vec![0.1, 0.2, 0.3],\n                Payload::default(),\n            )],\n        )\n        .shard_key_selector(\"user_1\".to_string()),\n    )\n    .await?;\n```\n\n```java\nimport java.util.List;\n\nimport static io.qdrant.client.PointIdFactory.id;\nimport static io.qdrant.client.ShardKeySelectorFactory.shardKeySelector;\nimport static io.qdrant.client.VectorsFactory.vectors;",
      "index": 18,
      "token_count": 594,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 34325,
      "end_char": 36374
    },
    {
      "content": "t.ShardKeySelectorFactory.shardKeySelector;\nimport static io.qdrant.client.VectorsFactory.vectors;\n\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Points.PointStruct;\nimport io.qdrant.client.grpc.Points.UpsertPoints;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .upsertAsync(\n        UpsertPoints.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .addAllPoints(\n                List.of(\n                    PointStruct.newBuilder()\n                        .setId(id(111))\n                        .setVectors(vectors(0.1f, 0.2f, 0.3f))\n                        .build()))\n            .setShardKeySelector(shardKeySelector(\"user_1\"))\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.UpsertAsync(\n\tcollectionName: \"{collection_name}\",\n\tpoints: new List<PointStruct>\n\t{\n\t\tnew() { Id = 111, Vectors = new[] { 0.1f, 0.2f, 0.3f } }\n\t},\n\tshardKeySelector: new ShardKeySelector { ShardKeys = { new List<ShardKey> { \"user_1\" } } }\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.Upsert(context.Background(), &qdrant.UpsertPoints{\n\tCollectionName: \"{collection_name}\",\n\tPoints: []*qdrant.PointStruct{\n\t\t{\n\t\t\tId:      qdrant.NewIDNum(111),\n\t\t\tVectors: qdrant.NewVectors(0.1, 0.2, 0.3),\n\t\t},\n\t},\n\tShardKeySelector: &qdrant.ShardKeySelector{\n\t\tShardKeys: []*qdrant.ShardKey{\n\t\t\tqdrant.NewShardKey(\"user_1\"),\n\t\t},\n\t},\n})\n```\n\nUsing the same point ID across multiple shard keys is **not supported\\*** and should be avoided.**\\*** When using custom sharding, IDs are only enforced to be unique within a shard key. This means that you can have multiple points with the same ID, if they have different shard keys.",
      "index": 19,
      "token_count": 576,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 36274,
      "end_char": 38263
    },
    {
      "content": "y. This means that you can have multiple points with the same ID, if they have different shard keys. This is a limitation of the current implementation, and is an anti-pattern that should be avoided because it can create scenarios of points with the same ID to have different contents. In the future, we plan to add a global ID uniqueness check.\n\nNow you can target the operations to specific shard(s) by specifying the `shard_key` on any operation you do. Operations that do not specify the shard key will be executed on **all** shards.\n\nAnother use-case would be to have shards that track the data chronologically, so that you can do more complex itineraries like uploading live data in one shard and archiving it once a certain age has passed.\n\n### Shard transfer method\n\n*Available as of v1.7.0*\n\nThere are different methods for transferring a shard, such as moving or replicating, to another node. Depending on what performance and guarantees you’d like to have and how you’d like to manage your cluster, you likely want to choose a specific method. Each method has its own pros and cons. Which is fastest depends on the size and state of a shard.\n\nAvailable shard transfer methods are:\n\n- `stream_records`: *(default)* transfer by streaming just its records to the target node in batches.\n- `snapshot`: transfer including its index and quantized data by utilizing a [snapshot](https://qdrant.tech/documentation/concepts/snapshots/) automatically.\n- `wal_delta`: *(auto recovery default)* transfer by resolving [WAL](https://qdrant.tech/documentation/concepts/storage/#versioning) difference; the operations that were missed.\n\nEach has pros, cons and specific requirements, some of which are:\n\n| Method:          | Stream records                               | Snapshot                                    | WAL delta                                 |\n| ---------------- | -------------------------------------------- | ------------------------------------------- | ----------------------------------------- |\n| **Version**      | v0.8.",
      "index": 20,
      "token_count": 395,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 38163,
      "end_char": 40204
    },
    {
      "content": "--------------------------- | ----------------------------------------- |\n| **Version**      | v0.8.0+                                      | v1.7.0+                                     | v1.8.0+                                   |\n| **Target**       | New/existing shard                           | New/existing shard                          | Existing shard                            |\n| **Connectivity** | Internal gRPC API (6335)                     | REST API (6333) Internal gRPC API (6335)    | Internal gRPC API (6335)                  |\n| **HNSW index**   | Doesn’t transfer, will reindex on target.    | Does transfer, immediately ready on target. | Doesn’t transfer, may index on target.    |\n| **Quantization** | Doesn’t transfer, will requantize on target. | Does transfer, immediately ready on target. | Doesn’t transfer, may quantize on target. |\n| **Ordering**     | Unordered updates on target[1](#fn:1.md)     | Ordered updates on target[2](#fn:2.md)      | Ordered updates on target[2](#fn:2.md)    |\n| **Disk space**   | No extra required                            | Extra required for snapshot on both nodes   | No extra required                         |\n\nTo select a shard transfer method, specify the `method` like:\n\n```http\nPOST /collections/{collection_name}/cluster\n{\n    \"move_shard\": {\n        \"shard_id\": 0,\n        \"from_peer_id\": 381894127,\n        \"to_peer_id\": 467122995,\n        \"method\": \"snapshot\"\n    }\n}\n```\n\nThe `stream_records` transfer method is the simplest available. It simply transfers all shard records in batches to the target node until it has transferred all of them, keeping both shards in sync. It will also make sure the transferred shard indexing process is keeping up before performing a final switch. The method has two common disadvantages: 1. It does not transfer index or quantization data, meaning that the shard has to be optimized again on the new node, which can be very expensive. 2. The ordering guarantees are `weak`[1](#fn:1.md), which is not suitable for some applications.",
      "index": 21,
      "token_count": 482,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 40104,
      "end_char": 42148
    },
    {
      "content": "ve. 2. The ordering guarantees are `weak`[1](#fn:1.md), which is not suitable for some applications. Because it is so simple, it’s also very robust, making it a reliable choice if the above cons are acceptable in your use case. If your cluster is unstable and out of resources, it’s probably best to use the `stream_records` transfer method, because it is unlikely to fail.\n\nThe `snapshot` transfer method utilizes [snapshots](https://qdrant.tech/documentation/concepts/snapshots/) to transfer a shard. A snapshot is created automatically. It is then transferred and restored on the target node. After this is done, the snapshot is removed from both nodes. While the snapshot/transfer/restore operation is happening, the source node queues up all new operations. All queued updates are then sent in order to the target shard to bring it into the same state as the source. There are two important benefits: 1. It transfers index and quantization data, so that the shard does not have to be optimized again on the target node, making them immediately available. This way, Qdrant ensures that there will be no degradation in performance at the end of the transfer. Especially on large shards, this can give a huge performance improvement. 2. The ordering guarantees can be `strong`[2](#fn:2.md), required for some applications.\n\nThe `wal_delta` transfer method only transfers the difference between two shards. More specifically, it transfers all operations that were missed to the target shard. The [WAL](https://qdrant.tech/documentation/concepts/storage/#versioning) of both shards is used to resolve this. There are two benefits: 1. It will be very fast because it only transfers the difference rather than all data. 2. The ordering guarantees can be `strong`[2](#fn:2.md), required for some applications. Two disadvantages are: 1. It can only be used to transfer to a shard that already exists on the other node. 2. Applicability is limited because the WALs normally don’t hold more than 64MB of recent operations.",
      "index": 22,
      "token_count": 446,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 42048,
      "end_char": 44064
    },
    {
      "content": ". Applicability is limited because the WALs normally don’t hold more than 64MB of recent operations. But that should be enough for a node that quickly restarts, to upgrade for example. If a delta cannot be resolved, this method automatically falls back to `stream_records` which equals transferring the full shard.\n\nThe `stream_records` method is currently used as default. This may change in the future. As of Qdrant 1.9.0 `wal_delta` is used for automatic shard replications to recover dead shards.\n\n## Replication\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.\n\n### Replication factor\n\nWhen you create a collection, you can control how many shard replicas you’d like to store by changing the `replication_factor`. By default, `replication_factor` is set to “1”, meaning no additional copy is maintained automatically. The default can be changed in the [Qdrant configuration](https://qdrant.tech/documentation/guides/configuration/#configuration-options). You can change that by setting the `replication_factor` when you create a collection.\n\nThe `replication_factor` can be updated for an existing collection, but the effect of this depends on how you’re running Qdrant. If you’re hosting the open source version of Qdrant yourself, changing the replication factor after collection creation doesn’t do anything. You can manually [create](#creating-new-shard-replicas.md) or drop shard replicas to achieve your desired replication factor. In Qdrant Cloud (including Hybrid Cloud, Private Cloud) your shards will automatically be replicated or dropped to match your configured replication factor.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n        \"size\": 300,\n        \"distance\": \"Cosine\"\n    },\n    \"shard_number\": 6,\n    \"replication_factor\": 2\n}\n```\n\n```python",
      "index": 23,
      "token_count": 440,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 43964,
      "end_char": 45997
    },
    {
      "content": "\"distance\": \"Cosine\"\n    },\n    \"shard_number\": 6,\n    \"replication_factor\": 2\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n    shard_number=6,\n    replication_factor=2,\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 300,\n    distance: \"Cosine\",\n  },\n  shard_number: 6,\n  replication_factor: 2,\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{CreateCollectionBuilder, Distance, VectorParamsBuilder};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(300, Distance::Cosine))\n            .shard_number(6)\n            .replication_factor(2),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(300)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setShardNumber(6)",
      "index": 24,
      "token_count": 493,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 45897,
      "end_char": 47936
    },
    {
      "content": ".build())\n                    .build())\n            .setShardNumber(6)\n            .setReplicationFactor(2)\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n\tcollectionName: \"{collection_name}\",\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\tshardNumber: 6,\n\treplicationFactor: 2\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n\tCollectionName: \"{collection_name}\",\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n\t\tSize:     300,\n\t\tDistance: qdrant.Distance_Cosine,\n\t}),\n\tShardNumber:       qdrant.PtrOf(uint32(6)),\n\tReplicationFactor: qdrant.PtrOf(uint32(2)),\n})\n```\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.\n\nSince a replication factor of “2” would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n### Creating new shard replicas\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://api.qdrant.tech/master/api-reference/distributed/update-collection-cluster). This is usually only necessary if you run Qdrant open-source. In Qdrant Cloud shard replication is handled and updated automatically, matching the configured `replication_factor`.\n\nA replica can be added on a specific peer by specifying the peer from which to replicate.\n\n```http\nPOST /collections/{collection_name}/cluster\n{\n    \"replicate_shard\": {\n        \"shard_id\": 0,\n        \"from_peer_id\": 381894127,\n        \"to_peer_id\": 467122995\n    }\n}\n```\n\nYou likely want to select a specific [shard transfer method](#shard-transfer-method.",
      "index": 25,
      "token_count": 513,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 47836,
      "end_char": 49865
    },
    {
      "content": "95\n    }\n}\n```\n\nYou likely want to select a specific [shard transfer method](#shard-transfer-method.md) to get desired performance and guarantees.\n\nAnd a replica can be removed on a specific peer.\n\n```http\nPOST /collections/{collection_name}/cluster\n{\n    \"drop_replica\": {\n        \"shard_id\": 0,\n        \"peer_id\": 381894127\n    }\n}\n```\n\nKeep in mind that a collection must contain at least one active replica of a shard.\n\n### Error handling\n\nReplicas can be in different states:\n\n- Active: healthy and ready to serve traffic\n- Dead: unhealthy and not ready to serve traffic\n- Partial: currently under resynchronization before activation\n\nA replica is marked as dead if it does not respond to internal healthchecks or if it fails to serve traffic.\n\nA dead replica will not receive traffic from other peers and might require a manual intervention if it does not recover automatically.\n\nThis mechanism ensures data consistency and availability if a subset of the replicas fail during an update operation.\n\n### Node Failure Recovery\n\nSometimes hardware malfunctions might render some nodes of the Qdrant cluster unrecoverable. No system is immune to this.\n\nBut several recovery scenarios allow qdrant to stay available for requests and even avoid performance degradation. Let’s walk through them from best to worst.\n\n**Recover with replicated collection**\n\nIf the number of failed nodes is less than the replication factor of the collection, then your cluster should still be able to perform read, search and update queries.\n\nNow, if the failed node restarts, consensus will trigger the replication process to update the recovering node with the newest updates it has missed.\n\nIf the failed node never restarts, you can recover the lost shards if you have a 3+ node cluster. You cannot recover lost shards in smaller clusters because recovery operations go through [raft](#raft.md) which requires >50% of the nodes to be healthy.\n\n**Recreate node with replicated collections**",
      "index": 26,
      "token_count": 412,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 49765,
      "end_char": 51741
    },
    {
      "content": "md) which requires >50% of the nodes to be healthy.\n\n**Recreate node with replicated collections**\n\nIf a node fails and it is impossible to recover it, you should exclude the dead node from the consensus and create an empty node.\n\nTo exclude failed nodes from the consensus, use [remove peer](https://api.qdrant.tech/master/api-reference/distributed/remove-peer) API. Apply the `force` flag if necessary.\n\nWhen you create a new node, make sure to attach it to the existing cluster by specifying `--bootstrap` CLI parameter with the URL of any of the running cluster nodes.\n\nOnce the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation. Use the [Replicate Shard Operation](https://api.qdrant.tech/master/api-reference/distributed/update-collection-cluster) to create another copy of the shard on the newly connected node.\n\nIt’s worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery. Building a completely automatic process of collection scaling would require control over the cluster machines themself. Check out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.\n\n**Recover from snapshot**\n\nIf there are no copies of data in the cluster, it is still possible to recover from a snapshot.\n\nFollow the same steps to detach failed node and create a new one in the cluster:\n\n- To exclude failed nodes from the consensus, use [remove peer](https://api.qdrant.tech/master/api-reference/distributed/remove-peer) API. Apply the `force` flag if necessary.\n- Create a new node, making sure to attach it to the existing cluster by specifying the `--bootstrap` CLI parameter with the URL of any of the running cluster nodes.\n\nSnapshot recovery, used in single-node deployment, is different from cluster one. Consensus manages all metadata about all collections and does not require snapshots to recover it.",
      "index": 27,
      "token_count": 414,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 51641,
      "end_char": 53689
    },
    {
      "content": ". Consensus manages all metadata about all collections and does not require snapshots to recover it. But you can use snapshots to recover missing shards of the collections.\n\nUse the [Collection Snapshot Recovery API](https://qdrant.tech/documentation/concepts/snapshots/#recover-in-cluster-deployment) to do it. The service will download the specified snapshot of the collection and recover shards with data from it.\n\nOnce all shards of the collection are recovered, the collection will become operational again.\n\n### Temporary node failure\n\nIf properly configured, running Qdrant in distributed mode can make your cluster resistant to outages when one node fails temporarily.\n\nHere is how differently-configured Qdrant clusters respond:\n\n- 1-node clusters: All operations time out or fail for up to a few minutes. It depends on how long it takes to restart and load data from disk.\n- 2-node clusters where shards ARE NOT replicated: All operations will time out or fail for up to a few minutes. It depends on how long it takes to restart and load data from disk.\n- 2-node clusters where all shards ARE replicated to both nodes: All requests except for operations on collections continue to work during the outage.\n- 3+-node clusters where all shards are replicated to at least 2 nodes: All requests continue to work during the outage.\n\n## Consistency guarantees\n\nBy default, Qdrant focuses on availability and maximum throughput of search operations. For the majority of use cases, this is a preferable trade-off.\n\nDuring the normal state of operation, it is possible to search and modify data from any peers in the cluster.\n\nBefore responding to the client, the peer handling the request dispatches all operations according to the current topology in order to keep the data synchronized across the cluster.\n\n- reads are using a partial fan-out strategy to optimize latency and availability\n- writes are executed in parallel on all active sharded replicas",
      "index": 28,
      "token_count": 383,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 53589,
      "end_char": 55547
    },
    {
      "content": "optimize latency and availability\n- writes are executed in parallel on all active sharded replicas\n\nHowever, in some cases, it is necessary to ensure additional guarantees during possible hardware instabilities, mass concurrent updates of same documents, etc.\n\nQdrant provides a few options to control consistency guarantees:\n\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\n- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.\n- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.\n\n### Write consistency factor\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to 1 by default. It can be configured at the collection’s creation or when updating the collection parameters.\n\nThis value can range from 1 to the number of replicas you have for each shard.\n\n```http",
      "index": 29,
      "token_count": 379,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 55447,
      "end_char": 57477
    },
    {
      "content": "arameters.\n\nThis value can range from 1 to the number of replicas you have for each shard.\n\n```http\nPUT /collections/{collection_name}\n{\n    \"vectors\": {\n        \"size\": 300,\n        \"distance\": \"Cosine\"\n    },\n    \"shard_number\": 6,\n    \"replication_factor\": 2,\n    \"write_consistency_factor\": 2\n}\n```\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n    shard_number=6,\n    replication_factor=2,\n    write_consistency_factor=2,\n)\n```\n\n```typescript\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\nconst client = new QdrantClient({ host: \"localhost\", port: 6333 });\n\nclient.createCollection(\"{collection_name}\", {\n  vectors: {\n    size: 300,\n    distance: \"Cosine\",\n  },\n  shard_number: 6,\n  replication_factor: 2,\n  write_consistency_factor: 2,\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{CreateCollectionBuilder, Distance, VectorParamsBuilder};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .create_collection(\n        CreateCollectionBuilder::new(\"{collection_name}\")\n            .vectors_config(VectorParamsBuilder::new(300, Distance::Cosine))\n            .shard_number(6)\n            .replication_factor(2)\n            .write_consistency_factor(2),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Collections.CreateCollection;\nimport io.qdrant.client.grpc.Collections.Distance;\nimport io.qdrant.client.grpc.Collections.VectorParams;\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient\n    .createCollectionAsync(\n        CreateCollection.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(",
      "index": 30,
      "token_count": 526,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 57377,
      "end_char": 59410
    },
    {
      "content": "ion.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .setVectorsConfig(\n                VectorsConfig.newBuilder()\n                    .setParams(\n                        VectorParams.newBuilder()\n                            .setSize(300)\n                            .setDistance(Distance.Cosine)\n                            .build())\n                    .build())\n            .setShardNumber(6)\n            .setReplicationFactor(2)\n            .setWriteConsistencyFactor(2)\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.CreateCollectionAsync(\n\tcollectionName: \"{collection_name}\",\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\tshardNumber: 6,\n\treplicationFactor: 2,\n\twriteConsistencyFactor: 2\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.CreateCollection(context.Background(), &qdrant.CreateCollection{\n\tCollectionName: \"{collection_name}\",\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n\t\tSize:     300,\n\t\tDistance: qdrant.Distance_Cosine,\n\t}),\n\tShardNumber:            qdrant.PtrOf(uint32(6)),\n\tReplicationFactor:      qdrant.PtrOf(uint32(2)),\n\tWriteConsistencyFactor: qdrant.PtrOf(uint32(2)),\n})\n```\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`. In this case, the client is expected to send the operation again to ensure a consistent state is reached.\n\nSetting the `write_consistency_factor` to a lower value may allow accepting writes even if there are unresponsive nodes. Unresponsive nodes are marked as dead and will automatically be recovered once available to ensure data consistency.\n\nThe configuration of the `write_consistency_factor` is important for adjusting the cluster’s behavior when some nodes go offline due to restarts, upgrades, or failures.",
      "index": 31,
      "token_count": 473,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 59310,
      "end_char": 61349
    },
    {
      "content": "djusting the cluster’s behavior when some nodes go offline due to restarts, upgrades, or failures.\n\nBy default, the cluster continues to accept updates as long as at least one replica of each shard is online. However, this behavior means that once an offline replica is restored, it will require additional synchronization with the rest of the cluster. In some cases, this synchronization can be resource-intensive and undesirable.\n\nSetting the `write_consistency_factor` to match the replication factor modifies the cluster’s behavior so that unreplicated updates are rejected, preventing the need for extra synchronization.\n\nIf the update is applied to enough replicas - according to the `write_consistency_factor` - the update will return a successful status. Any replicas that failed to apply the update will be temporarily disabled and are automatically recovered to keep data consistency. If the update could not be applied to enough replicas, it’ll return an error and may be partially applied. The user must submit the operation again to ensure data consistency.\n\nFor asynchronous updates and injection pipelines capable of handling errors and retries, this strategy might be preferable.\n\n### Read consistency\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result is consistent across cluster nodes.\n\n- `all` will query all nodes and return points, which present on all of them\n- `majority` will query all nodes and return points, which present on the majority of them\n- `quorum` will query randomly selected majority of nodes and return points, which present on all of them\n- `1`/`2`/`3`/etc - will query specified number of randomly selected nodes and return points which present on all of them\n- default `consistency` is `1`\n\n```http\nPOST /collections/{collection_name}/points/query?consistency=majority\n{\n    \"query\": [0.2, 0.1, 0.9, 0.7],\n    \"filter\": {\n        \"must\": [\n            {\n                \"key\": \"city\",\n                \"match\": {\n                    \"value\": \"London\"",
      "index": 32,
      "token_count": 427,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 61249,
      "end_char": 63292
    },
    {
      "content": "{\n                \"key\": \"city\",\n                \"match\": {\n                    \"value\": \"London\"\n                }\n            }\n        ]\n    },\n    \"params\": {\n        \"hnsw_ef\": 128,\n        \"exact\": false\n    },\n    \"limit\": 3\n}\n```\n\n```python\nclient.query_points(\n    collection_name=\"{collection_name}\",\n    query=[0.2, 0.1, 0.9, 0.7],\n    query_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"city\",\n                match=models.MatchValue(\n                    value=\"London\",\n                ),\n            )\n        ]\n    ),\n    search_params=models.SearchParams(hnsw_ef=128, exact=False),\n    limit=3,\n    consistency=\"majority\",\n)\n```\n\n```typescript\nclient.query(\"{collection_name}\", {\n    query: [0.2, 0.1, 0.9, 0.7],\n    filter: {\n        must: [{ key: \"city\", match: { value: \"London\" } }],\n    },\n    params: {\n        hnsw_ef: 128,\n        exact: false,\n    },\n    limit: 3,\n    consistency: \"majority\",\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    read_consistency::Value, Condition, Filter, QueryPointsBuilder, ReadConsistencyType,\n    SearchParamsBuilder,\n};\nuse qdrant_client::{Qdrant, QdrantError};\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .query(\n        QueryPointsBuilder::new(\"{collection_name}\")\n            .query(vec![0.2, 0.1, 0.9, 0.7])\n            .limit(3)\n            .filter(Filter::must([Condition::matches(\n                \"city\",\n                \"London\".to_string(),\n            )]))\n            .params(SearchParamsBuilder::default().hnsw_ef(128).exact(false))\n            .read_consistency(Value::Type(ReadConsistencyType::Majority.into())),\n    )\n    .await?;\n```\n\n```java\nimport io.qdrant.client.QdrantClient;\nimport io.qdrant.client.QdrantGrpcClient;\nimport io.qdrant.client.grpc.Points.Filter;\nimport io.qdrant.client.grpc.Points.QueryPoints;\nimport io.qdrant.client.grpc.Points.ReadConsistency;\nimport io.qdrant.client.grpc.Points.ReadConsistencyType;\nimport io.qdrant.client.grpc.Points.SearchParams;",
      "index": 33,
      "token_count": 541,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 63192,
      "end_char": 65230
    },
    {
      "content": "o.qdrant.client.grpc.Points.ReadConsistencyType;\nimport io.qdrant.client.grpc.Points.SearchParams;\n\nimport static io.qdrant.client.QueryFactory.nearest;\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\nQdrantClient client =\n    new QdrantClient(QdrantGrpcClient.newBuilder(\"localhost\", 6334, false).build());\n\nclient.queryAsync(\n        QueryPoints.newBuilder()\n                .setCollectionName(\"{collection_name}\")\n                .setFilter(Filter.newBuilder().addMust(matchKeyword(\"city\", \"London\")).build())\n                .setQuery(nearest(.2f, 0.1f, 0.9f, 0.7f))\n                .setParams(SearchParams.newBuilder().setHnswEf(128).setExact(false).build())\n                .setLimit(3)\n                .setReadConsistency(\n                        ReadConsistency.newBuilder().setType(ReadConsistencyType.Majority).build())\n                .build())\n        .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\nusing static Qdrant.Client.Grpc.Conditions;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.QueryAsync(\n\tcollectionName: \"{collection_name}\",\n\tquery: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },\n\tfilter: MatchKeyword(\"city\", \"London\"),\n\tsearchParams: new SearchParams { HnswEf = 128, Exact = false },\n\tlimit: 3,\n\treadConsistency: new ReadConsistency { Type = ReadConsistencyType.Majority }\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.Query(context.Background(), &qdrant.QueryPoints{\n\tCollectionName: \"{collection_name}\",\n\tQuery:          qdrant.NewQuery(0.2, 0.1, 0.9, 0.7),\n\tFilter: &qdrant.Filter{\n\t\tMust: []*qdrant.Condition{\n\t\t\tqdrant.NewMatch(\"city\", \"London\"),\n\t\t},\n\t},\n\tParams: &qdrant.SearchParams{\n\t\tHnswEf: qdrant.PtrOf(uint64(128)),\n\t},\n\tLimit:           qdrant.PtrOf(uint64(3)),\n\tReadConsistency: qdrant.NewReadConsistencyType(qdrant.ReadConsistencyType_Majority),\n})\n```\n\n### Write ordering",
      "index": 34,
      "token_count": 567,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 65130,
      "end_char": 67120
    },
    {
      "content": "cy: qdrant.NewReadConsistencyType(qdrant.ReadConsistencyType_Majority),\n})\n```\n\n### Write ordering\n\nWrite `ordering` can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same `ordering`) are performed and observed sequentially.\n\n- `weak` *(default)* ordering does not provide any additional guarantees, so write operations can be freely reordered.\n- `medium` ordering serializes all write operations through a dynamically elected leader, which might cause minor inconsistencies in case of leader change.\n- `strong` ordering serializes all write operations through the permanent leader, which provides strong consistency, but write operations may be unavailable if the leader is down.\n\nSome [shard transfer methods](#shard-transfer-method.md) may affect ordering guarantees.\n\n```http\nPUT /collections/{collection_name}/points?ordering=strong\n{\n    \"batch\": {\n        \"ids\": [1, 2, 3],\n        \"payloads\": [\n            {\"color\": \"red\"},\n            {\"color\": \"green\"},\n            {\"color\": \"blue\"}\n        ],\n        \"vectors\": [\n            [0.9, 0.1, 0.1],\n            [0.1, 0.9, 0.1],\n            [0.1, 0.1, 0.9]\n        ]\n    }\n}\n```\n\n```python\nclient.upsert(\n    collection_name=\"{collection_name}\",\n    points=models.Batch(\n        ids=[1, 2, 3],\n        payloads=[\n            {\"color\": \"red\"},\n            {\"color\": \"green\"},\n            {\"color\": \"blue\"},\n        ],\n        vectors=[\n            [0.9, 0.1, 0.1],\n            [0.1, 0.9, 0.1],\n            [0.1, 0.1, 0.9],\n        ],\n    ),\n    ordering=models.WriteOrdering.STRONG,\n)\n```\n\n```typescript\nclient.upsert(\"{collection_name}\", {\n  batch: {\n    ids: [1, 2, 3],\n    payloads: [{ color: \"red\" }, { color: \"green\" }, { color: \"blue\" }],\n    vectors: [\n      [0.9, 0.1, 0.1],\n      [0.1, 0.9, 0.1],\n      [0.1, 0.1, 0.9],\n    ],\n  },\n  ordering: \"strong\",\n});\n```\n\n```rust\nuse qdrant_client::qdrant::{\n    PointStruct, UpsertPointsBuilder, WriteOrdering, WriteOrderingType\n};",
      "index": 35,
      "token_count": 568,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 67020,
      "end_char": 69052
    },
    {
      "content": "qdrant_client::qdrant::{\n    PointStruct, UpsertPointsBuilder, WriteOrdering, WriteOrderingType\n};\nuse qdrant_client::Qdrant;\n\nlet client = Qdrant::from_url(\"http://localhost:6334\").build()?;\n\nclient\n    .upsert_points(\n        UpsertPointsBuilder::new(\n            \"{collection_name}\",\n            vec![\n                PointStruct::new(1, vec![0.9, 0.1, 0.1], [(\"color\", \"red\".into())]),\n                PointStruct::new(2, vec![0.1, 0.9, 0.1], [(\"color\", \"green\".into())]),\n                PointStruct::new(3, vec![0.1, 0.1, 0.9], [(\"color\", \"blue\".into())]),\n            ],\n        )\n        .ordering(WriteOrdering {\n            r#type: WriteOrderingType::Strong.into(),\n        }),\n    )\n    .await?;\n```\n\n```java\nimport java.util.List;\nimport java.util.Map;\n\nimport static io.qdrant.client.PointIdFactory.id;\nimport static io.qdrant.client.ValueFactory.value;\nimport static io.qdrant.client.VectorsFactory.vectors;\n\nimport io.qdrant.client.grpc.Points.PointStruct;\nimport io.qdrant.client.grpc.Points.UpsertPoints;\nimport io.qdrant.client.grpc.Points.WriteOrdering;\nimport io.qdrant.client.grpc.Points.WriteOrderingType;\n\nclient\n    .upsertAsync(\n        UpsertPoints.newBuilder()\n            .setCollectionName(\"{collection_name}\")\n            .addAllPoints(\n                List.of(\n                    PointStruct.newBuilder()\n                        .setId(id(1))\n                        .setVectors(vectors(0.9f, 0.1f, 0.1f))\n                        .putAllPayload(Map.of(\"color\", value(\"red\")))\n                        .build(),\n                    PointStruct.newBuilder()\n                        .setId(id(2))\n                        .setVectors(vectors(0.1f, 0.9f, 0.1f))\n                        .putAllPayload(Map.of(\"color\", value(\"green\")))\n                        .build(),\n                    PointStruct.newBuilder()\n                        .setId(id(3))\n                        .setVectors(vectors(0.1f, 0.1f, 0.94f))\n                        .putAllPayload(Map.of(\"color\", value(\"blue\")))\n                        .build()))",
      "index": 36,
      "token_count": 511,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 68952,
      "end_char": 71000
    },
    {
      "content": ".putAllPayload(Map.of(\"color\", value(\"blue\")))\n                        .build()))\n            .setOrdering(WriteOrdering.newBuilder().setType(WriteOrderingType.Strong).build())\n            .build())\n    .get();\n```\n\n```csharp\nusing Qdrant.Client;\nusing Qdrant.Client.Grpc;\n\nvar client = new QdrantClient(\"localhost\", 6334);\n\nawait client.UpsertAsync(\n\tcollectionName: \"{collection_name}\",\n\tpoints: new List<PointStruct>\n\t{\n\t\tnew()\n\t\t{\n\t\t\tId = 1,\n\t\t\tVectors = new[] { 0.9f, 0.1f, 0.1f },\n\t\t\tPayload = { [\"color\"] = \"red\" }\n\t\t},\n\t\tnew()\n\t\t{\n\t\t\tId = 2,\n\t\t\tVectors = new[] { 0.1f, 0.9f, 0.1f },\n\t\t\tPayload = { [\"color\"] = \"green\" }\n\t\t},\n\t\tnew()\n\t\t{\n\t\t\tId = 3,\n\t\t\tVectors = new[] { 0.1f, 0.1f, 0.9f },\n\t\t\tPayload = { [\"color\"] = \"blue\" }\n\t\t}\n\t},\n\tordering: WriteOrderingType.Strong\n);\n```\n\n```go\nimport (\n\t\"context\"\n\n\t\"github.com/qdrant/go-client/qdrant\"\n)\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\tHost: \"localhost\",\n\tPort: 6334,\n})\n\nclient.Upsert(context.Background(), &qdrant.UpsertPoints{\n\tCollectionName: \"{collection_name}\",\n\tPoints: []*qdrant.PointStruct{\n\t\t{\n\t\t\tId:      qdrant.NewIDNum(1),\n\t\t\tVectors: qdrant.NewVectors(0.9, 0.1, 0.1),\n\t\t\tPayload: qdrant.NewValueMap(map[string]any{\"color\": \"red\"}),\n\t\t},\n\t\t{\n\t\t\tId:      qdrant.NewIDNum(2),\n\t\t\tVectors: qdrant.NewVectors(0.1, 0.9, 0.1),\n\t\t\tPayload: qdrant.NewValueMap(map[string]any{\"color\": \"green\"}),\n\t\t},\n\t\t{\n\t\t\tId:      qdrant.NewIDNum(3),\n\t\t\tVectors: qdrant.NewVectors(0.1, 0.1, 0.9),\n\t\t\tPayload: qdrant.NewValueMap(map[string]any{\"color\": \"blue\"}),\n\t\t},\n\t},\n\tOrdering: &qdrant.WriteOrdering{\n\t\tType: qdrant.WriteOrderingType_Strong,\n\t},\n})\n```\n\n## Listener mode\n\nThis is an experimental feature, its behavior may change in the future.\n\nIn some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations. There are several scenarios where this can be useful:\n\n- Listener option can be used to store data in a separate node, which can be used for backup purposes or to store data for a long time.",
      "index": 37,
      "token_count": 673,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 70900,
      "end_char": 72942
    },
    {
      "content": "re data in a separate node, which can be used for backup purposes or to store data for a long time.\n- Listener node can be used to synchronize data into another region, while still performing search operations in the local region.\n\nTo enable listener mode, set `node_type` to `Listener` in the config file:\n\n```yaml\nstorage:\n  node_type: \"Listener\"\n```\n\nListener node will not participate in search operations, but will still accept write operations and will store the data in the local storage.\n\nAll shards, stored on the listener node, will be converted to the `Listener` state.\n\nAdditionally, all write requests sent to the listener node will be processed with `wait=false` option, which means that the write oprations will be considered successful once they are written to WAL. This mechanism should allow to minimize upsert latency in case of parallel snapshotting.\n\n## Consensus Checkpointing\n\nConsensus checkpointing is a technique used in Raft to improve performance and simplify log management by periodically creating a consistent snapshot of the system state. This snapshot represents a point in time where all nodes in the cluster have reached agreement on the state, and it can be used to truncate the log, reducing the amount of data that needs to be stored and transferred between nodes.\n\nFor example, if you attach a new node to the cluster, it should replay all the log entries to catch up with the current state. In long-running clusters, this can take a long time, and the log can grow very large.\n\nTo prevent this, one can use a special checkpointing mechanism, that will truncate the log and create a snapshot of the current state.\n\nTo use this feature, simply call the `/cluster/recover` API on required node:\n\n```http\nPOST /cluster/recover\n```\n\nThis API can be triggered on any non-leader node, it will send a request to the current consensus leader to create a snapshot. The leader will in turn send the snapshot back to the requesting node for application.",
      "index": 38,
      "token_count": 405,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 72842,
      "end_char": 74825
    },
    {
      "content": "a snapshot. The leader will in turn send the snapshot back to the requesting node for application.\n\nIn some cases, this API can be used to recover from an inconsistent cluster state by forcing a snapshot creation.\n\n---\n\n1. Weak ordering for updates: All records are streamed to the target node in order. New updates are received on the target node in parallel, while the transfer of records is still happening. We therefore have `weak` ordering, regardless of what [ordering](#write-ordering.md) is used for updates. [↩︎](#fnref:1.md) [↩︎](#fnref1:1.md)\n\n2. Strong ordering for updates: A snapshot of the shard is created, it is transferred and recovered on the target node. That ensures the state of the shard is kept consistent. New updates are queued on the source node, and transferred in order to the target node. Updates therefore have the same [ordering](#write-ordering.md) as the user selects, making `strong` ordering possible. [↩︎](#fnref:2.md) [↩︎](#fnref1:2.md) [↩︎](#fnref2:2.md) [↩︎](#fnref3:2.md)\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/guides/distributed_deployment.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Distributed deployment](#distributed-deployment.md)\n\n  - [How many Qdrant nodes should I run?](#how-many-qdrant-nodes-should-i-run.md)\n\n  - [Enabling distributed mode in self-hosted Qdrant](#enabling-distributed-mode-in-self-hosted-qdrant.md)\n\n  - [Enabling distributed mode in Qdrant Cloud](#enabling-distributed-mode-in-qdrant-cloud.md)\n\n  - [Making use of a new distributed Qdrant cluster](#making-use-of-a-new-distributed-qdrant-cluster.md)\n\n  - [Raft](#raft.md)\n\n  - [Sharding](#sharding.md)\n\n    - [Choosing the right number of shards](#choosing-the-right-number-of-shards.md)\n    - [Resharding](#resharding.md)\n    - [Moving shards](#moving-shards.md)",
      "index": 39,
      "token_count": 550,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 74725,
      "end_char": 76758
    },
    {
      "content": "ht-number-of-shards.md)\n    - [Resharding](#resharding.md)\n    - [Moving shards](#moving-shards.md)\n    - [User-defined sharding](#user-defined-sharding.md)\n    - [Shard transfer method](#shard-transfer-method.md)\n\n  - [Replication](#replication.md)\n\n    - [Replication factor](#replication-factor.md)\n    - [Creating new shard replicas](#creating-new-shard-replicas.md)\n    - [Error handling](#error-handling.md)\n    - [Node Failure Recovery](#node-failure-recovery.md)\n    - [Temporary node failure](#temporary-node-failure.md)\n\n  - [Consistency guarantees](#consistency-guarantees.md)\n\n    - [Write consistency factor](#write-consistency-factor.md)\n    - [Read consistency](#read-consistency.md)\n    - [Write ordering](#write-ordering.md)\n\n  - [Listener mode](#listener-mode.md)\n\n  - [Consensus Checkpointing](#consensus-checkpointing.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/guides/distributed_deployment.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
      "index": 40,
      "token_count": 360,
      "metadata": {
        "title": "_documentation_guides_distributed_deployment_",
        "source": "qdrant_documentation\\documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_guides_distributed_deployment",
        "category": "guides",
        "file_path": "documentation_guides_distributed_deployment\\_documentation_guides_distributed_deployment_.md",
        "file_name": "_documentation_guides_distributed_deployment_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.446891",
        "total_chunks": 41
      },
      "start_char": 76658,
      "end_char": 78706
    }
  ]
}