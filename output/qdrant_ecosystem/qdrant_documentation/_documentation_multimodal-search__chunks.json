{
  "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
  "source_repo": "qdrant_documentation",
  "total_chunks": 14,
  "chunks": [
    {
      "content": "Multilingual & Multimodal RAG with LlamaIndex - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.",
      "index": 0,
      "token_count": 548,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 0,
      "end_char": 2013
    },
    {
      "content": "- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)",
      "index": 1,
      "token_count": 537,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 1913,
      "end_char": 3938
    },
    {
      "content": "ocumentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)",
      "index": 2,
      "token_count": 547,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 3838,
      "end_char": 5869
    },
    {
      "content": "rameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)",
      "index": 3,
      "token_count": 528,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 5769,
      "end_char": 7788
    },
    {
      "content": "g with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.",
      "index": 4,
      "token_count": 509,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 7688,
      "end_char": 9724
    },
    {
      "content": ".tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)",
      "index": 5,
      "token_count": 536,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 9624,
      "end_char": 11664
    },
    {
      "content": "h/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)",
      "index": 6,
      "token_count": 542,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 11564,
      "end_char": 13598
    },
    {
      "content": "entation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.",
      "index": 7,
      "token_count": 554,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 13498,
      "end_char": 15541
    },
    {
      "content": "PT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.",
      "index": 8,
      "token_count": 504,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 15441,
      "end_char": 17444
    },
    {
      "content": "les/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- Multilingual & Multimodal RAG with LlamaIndex\n\n# Multilingual & Multimodal Search with LlamaIndex\n\n| Time: 15 min | Level: Beginner | Output: [GitHub](https://github.com/qdrant/examples/blob/master/multimodal-search/Multimodal_Search_with_LlamaIndex.ipynb) | [](https://githubtocolab.com/qdrant/examples/blob/master/multimodal-search/Multimodal_Search_with_LlamaIndex.ipynb) |\n| ------------ | --------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |\n\n## Overview\n\nWe often understand and share information more effectively when combining different types of data. For example, the taste of comfort food can trigger childhood memories. We might describe a song with just ‚Äúpam pam clap‚Äù sounds. Instead of writing paragraphs. Sometimes, we may use emojis and stickers to express how we feel or to share complex ideas.\n\nModalities of data such as **text**, **images**, **video** and **audio** in various combinations form valuable use cases for Semantic Search applications.",
      "index": 9,
      "token_count": 405,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 17344,
      "end_char": 19305
    },
    {
      "content": "** and **audio** in various combinations form valuable use cases for Semantic Search applications.\n\nVector databases, being **modality-agnostic**, are perfect for building these applications.\n\nIn this simple tutorial, we are working with two simple modalities: **image** and **text** data. However, you can create a Semantic Search application with any combination of modalities if you choose the right embedding model to bridge the **semantic gap**.\n\n> The **semantic gap** refers to the difference between low-level features (aka brightness) and high-level concepts (aka cuteness).\n\nFor example, the [vdr-2b-multi-v1 model](https://huggingface.co/llamaindex/vdr-2b-multi-v1) from LlamaIndex is designed for multilingual embedding, particularly effective for visual document retrieval across multiple languages and domains. It allows for searching and querying visually rich multilingual documents without the need for OCR or other data extraction pipelines.\n\n## Setup\n\nFirst, install the required libraries `qdrant-client` and `llama-index-embeddings-huggingface`.\n\n```bash\npip install qdrant-client llama-index-embeddings-huggingface\n```\n\n## Dataset\n\nTo make the demonstration simple, we created a tiny dataset of images and their captions for you.\n\nImages can be downloaded from [here](https://github.com/qdrant/examples/tree/master/multimodal-search/images). It‚Äôs **important** to place them in the same folder as your code/notebook, in the folder named `images`.\n\n## Vectorize data\n\n`LlamaIndex`‚Äôs `vdr-2b-multi-v1` model supports cross-lingual retrieval, allowing for effective searches across languages and domains. It encodes document page screenshots into dense single-vector representations, eliminating the need for OCR and other complex data extraction processes.\n\nLet‚Äôs embed the images and their captions in the **shared embedding space**.\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nmodel = HuggingFaceEmbedding(\n    model_name=\"llamaindex/vdr-2b-multi-v1\",",
      "index": 10,
      "token_count": 436,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 19205,
      "end_char": 21215
    },
    {
      "content": "rt HuggingFaceEmbedding\n\nmodel = HuggingFaceEmbedding(\n    model_name=\"llamaindex/vdr-2b-multi-v1\",\n    device=\"cpu\",  # \"mps\" for mac, \"cuda\" for nvidia GPUs\n    trust_remote_code=True,\n)\n\ndocuments = [\n    {\"caption\": \"An image about plane emergency safety.\", \"image\": \"images/image-1.png\"},\n    {\"caption\": \"An image about airplane components.\", \"image\": \"images/image-2.png\"},\n    {\"caption\": \"An image about COVID safety restrictions.\", \"image\": \"images/image-3.png\"},\n    {\"caption\": \"An confidential image about UFO sightings.\", \"image\": \"images/image-4.png\"},\n    {\"caption\": \"An image about unusual footprints on Aralar 2011.\", \"image\": \"images/image-5.png\"},\n]\n\ntext_embeddings = model.get_text_embedding_batch([doc[\"caption\"] for doc in documents])\nimage_embeddings = model.get_image_embedding_batch([doc[\"image\"] for doc in documents])\n```\n\n## Upload data to Qdrant\n\n1. **Create a client object for Qdrant**.\n\n```python\nfrom qdrant_client import QdrantClient, models\n\n# docker run -p 6333:6333 qdrant/qdrant\nclient = QdrantClient(url=\"http://localhost:6333/\")\n```\n\n2. **Create a new collection for the images with captions**.\n\n```python\nCOLLECTION_NAME = \"llama-multi\"\n\nif not client.collection_exists(COLLECTION_NAME):\n    client.create_collection(\n        collection_name=COLLECTION_NAME,\n        vectors_config={\n            \"image\": models.VectorParams(size=len(image_embeddings[0]), distance=models.Distance.COSINE),\n            \"text\": models.VectorParams(size=len(text_embeddings[0]), distance=models.Distance.COSINE),\n        }\n    )\n```\n\n3. **Upload our images with captions to the Collection**.\n\n```python\nclient.upload_points(\n    collection_name=COLLECTION_NAME,\n    points=[\n        models.PointStruct(\n            id=idx,\n            vector={\n                \"text\": text_embeddings[idx],\n                \"image\": image_embeddings[idx],\n            },\n            payload=doc\n        )\n        for idx, doc in enumerate(documents)\n    ]\n)\n```\n\n## Search\n\n### Text-to-Image",
      "index": 11,
      "token_count": 491,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 21115,
      "end_char": 23115
    },
    {
      "content": "c\n        )\n        for idx, doc in enumerate(documents)\n    ]\n)\n```\n\n## Search\n\n### Text-to-Image\n\nLet‚Äôs see what image we will get to the query ‚Äú*Adventures on snow hills*‚Äù.\n\n```python\nfrom PIL import Image\n\nfind_image = model.get_query_embedding(\"Adventures on snow hills\")\n\nImage.open(client.query_points(\n    collection_name=COLLECTION_NAME,\n    query=find_image,\n    using=\"image\",\n    with_payload=[\"image\"],\n    limit=1\n).points[0].payload['image'])\n```\n\nLet‚Äôs also run the same query in Italian and compare the results.\n\n### Multilingual Search\n\nNow, let‚Äôs do a multilingual search using an Italian query:\n\n```python\nImage.open(client.query_points(\n    collection_name=COLLECTION_NAME,\n    query=model.get_query_embedding(\"Avventure sulle colline innevate\"),\n    using=\"image\",\n    with_payload=[\"image\"],\n    limit=1\n).points[0].payload['image'])\n```\n\n**Response:**\n\n### Image-to-Text\n\nNow, let‚Äôs do a reverse search with the following image:\n\n```python\nclient.query_points(\n    collection_name=COLLECTION_NAME,\n    query=model.get_image_embedding(\"images/image-2.png\"),  \n    # Now we are searching only among text vectors with our image query\n    using=\"text\",\n    with_payload=[\"caption\"],\n    limit=1\n).points[0].payload['caption']\n```\n\n**Response:**\n\n```text\n'An image about plane emergency safety.'\n```\n\n## Next steps\n\nUse cases of even just Image & Text Multimodal Search are countless: E-Commerce, Media Management, Content Recommendation, Emotion Recognition Systems, Biomedical Image Retrieval, Spoken Sign Language Transcription, etc.\n\nImagine a scenario: a user wants to find a product similar to a picture they have, but they also have specific textual requirements, like ‚Äú*in beige colour*‚Äù. You can search using just texts or images and combine their embeddings in a **late fusion manner** (summing and weighting might work surprisingly well).\n\nMoreover, using [Discovery Search](https://qdrant.",
      "index": 12,
      "token_count": 448,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 23015,
      "end_char": 24935
    },
    {
      "content": "ing and weighting might work surprisingly well).\n\nMoreover, using [Discovery Search](https://qdrant.tech/articles/discovery-search/) with both modalities, you can provide users with information that is impossible to retrieve unimodally!\n\nJoin our [Discord community](https://qdrant.to/discord), where we talk about vector search and similarity learning, experiment, and have fun!\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! üôè\n\nWe are sorry to hear that. üòî You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/multimodal-search.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Multilingual & Multimodal Search with LlamaIndex](#multilingual--multimodal-search-with-llamaindex.md)\n\n  - [Overview](#overview.md)\n\n  - [Setup](#setup.md)\n\n  - [Dataset](#dataset.md)\n\n  - [Vectorize data](#vectorize-data.md)\n\n  - [Upload data to Qdrant](#upload-data-to-qdrant.md)\n\n  - [Search](#search.md)\n\n    - [Text-to-Image](#text-to-image.md)\n    - [Multilingual Search](#multilingual-search.md)\n    - [Image-to-Text](#image-to-text.md)\n\n  - [Next steps](#next-steps.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/multimodal-search.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n¬© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
      "index": 13,
      "token_count": 453,
      "metadata": {
        "title": "_documentation_multimodal-search_",
        "source": "qdrant_documentation\\documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_multimodal-search",
        "category": "multimodal-search",
        "file_path": "documentation_multimodal-search\\_documentation_multimodal-search_.md",
        "file_name": "_documentation_multimodal-search_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:32.913888",
        "total_chunks": 14
      },
      "start_char": 24835,
      "end_char": 26883
    }
  ]
}