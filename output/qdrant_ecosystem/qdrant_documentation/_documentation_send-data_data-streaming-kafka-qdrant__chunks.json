{
  "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
  "source_repo": "qdrant_documentation",
  "total_chunks": 17,
  "chunks": [
    {
      "content": "How to Setup Seamless Data Streaming with Kafka and Qdrant - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.",
      "index": 0,
      "token_count": 548,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 0,
      "end_char": 2026
    },
    {
      "content": "- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)",
      "index": 1,
      "token_count": 537,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 1926,
      "end_char": 3951
    },
    {
      "content": "ocumentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)",
      "index": 2,
      "token_count": 547,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 3851,
      "end_char": 5882
    },
    {
      "content": "rameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)",
      "index": 3,
      "token_count": 528,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 5782,
      "end_char": 7801
    },
    {
      "content": "g with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.",
      "index": 4,
      "token_count": 509,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 7701,
      "end_char": 9737
    },
    {
      "content": ".tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)",
      "index": 5,
      "token_count": 536,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 9637,
      "end_char": 11677
    },
    {
      "content": "h/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)",
      "index": 6,
      "token_count": 542,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 11577,
      "end_char": 13611
    },
    {
      "content": "entation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.",
      "index": 7,
      "token_count": 554,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 13511,
      "end_char": 15554
    },
    {
      "content": "PT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.",
      "index": 8,
      "token_count": 504,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 15454,
      "end_char": 17457
    },
    {
      "content": "les/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- [Send data](https://qdrant.tech/documentation/send-data/)\n-\n- How to Setup Seamless Data Streaming with Kafka and Qdrant\n\n# Setup Data Streaming with Kafka via Confluent\n\n**Author:** [M K Pavan Kumar](https://www.linkedin.com/in/kameshwara-pavan-kumar-mantha-91678b21/) , research scholar at [IIITDM, Kurnool](https://iiitk.ac.in). Specialist in hallucination mitigation techniques and RAG methodologies. • [GitHub](https://github.com/pavanjava) • [Medium](https://medium.com/@manthapavankumar11)\n\n## Introduction\n\nThis guide will walk you through the detailed steps of installing and setting up the [Qdrant Sink Connector](https://github.com/qdrant/qdrant-kafka), building the necessary infrastructure, and creating a practical playground application. By the end of this article, you will have a deep understanding of how to leverage this powerful integration to streamline your data workflows, ultimately enhancing the performance and capabilities of your data-driven real-time semantic search and RAG applications.\n\nIn this example, original data will be sourced from Azure Blob Storage and MongoDB.\n\nFigure 1: [Real time Change Data Capture (CDC)](https://www.confluent.io/learn/change-data-capture/) with Kafka and Qdrant.\n\n## The Architecture:",
      "index": 9,
      "token_count": 475,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 17357,
      "end_char": 19392
    },
    {
      "content": "(https://www.confluent.io/learn/change-data-capture/) with Kafka and Qdrant.\n\n## The Architecture:\n\n## Source Systems\n\nThe architecture begins with the **source systems**, represented by MongoDB and Azure Blob Storage. These systems are vital for storing and managing raw data. MongoDB, a popular NoSQL database, is known for its flexibility in handling various data formats and its capability to scale horizontally. It is widely used for applications that require high performance and scalability. Azure Blob Storage, on the other hand, is Microsoft’s object storage solution for the cloud. It is designed for storing massive amounts of unstructured data, such as text or binary data. The data from these sources is extracted using **source connectors**, which are responsible for capturing changes in real-time and streaming them into Kafka.\n\n## Kafka\n\nAt the heart of this architecture lies **Kafka**, a distributed event streaming platform capable of handling trillions of events a day. Kafka acts as a central hub where data from various sources can be ingested, processed, and distributed to various downstream systems. Its fault-tolerant and scalable design ensures that data can be reliably transmitted and processed in real-time. Kafka’s capability to handle high-throughput, low-latency data streams makes it an ideal choice for real-time data processing and analytics. The use of **Confluent** enhances Kafka’s functionalities, providing additional tools and services for managing Kafka clusters and stream processing.\n\n## Qdrant\n\nThe processed data is then routed to **Qdrant**, a highly scalable vector search engine designed for similarity searches. Qdrant excels at managing and searching through high-dimensional vector data, which is essential for applications involving machine learning and AI, such as recommendation systems, image recognition, and natural language processing. The **Qdrant Sink Connector** for Kafka plays a pivotal role here, enabling seamless integration between Kafka and Qdrant.",
      "index": 10,
      "token_count": 382,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 19292,
      "end_char": 21311
    },
    {
      "content": "ector** for Kafka plays a pivotal role here, enabling seamless integration between Kafka and Qdrant. This connector allows for the real-time ingestion of vector data into Qdrant, ensuring that the data is always up-to-date and ready for high-performance similarity searches.\n\n## Integration and Pipeline Importance\n\nThe integration of these components forms a powerful and efficient data streaming pipeline. The **Qdrant Sink Connector** ensures that the data flowing through Kafka is continuously ingested into Qdrant without any manual intervention. This real-time integration is crucial for applications that rely on the most current data for decision-making and analysis. By combining the strengths of MongoDB and Azure Blob Storage for data storage, Kafka for data streaming, and Qdrant for vector search, this pipeline provides a robust solution for managing and processing large volumes of data in real-time. The architecture’s scalability, fault-tolerance, and real-time processing capabilities are key to its effectiveness, making it a versatile solution for modern data-driven applications.\n\n## Installation of Confluent Kafka Platform\n\nTo install the Confluent Kafka Platform (self-managed locally), follow these 3 simple steps:\n\n**Download and Extract the Distribution Files:**\n\n- Visit [Confluent Installation Page](https://www.confluent.io/installation/).\n- Download the distribution files (tar, zip, etc.).\n- Extract the downloaded file using:\n\n```bash\ntar -xvf confluent-<version>.tar.gz\n```\n\nor\n\n```bash\nunzip confluent-<version>.zip\n```\n\n**Configure Environment Variables:**\n\n```bash\n# Set CONFLUENT_HOME to the installation directory:\nexport CONFLUENT_HOME=/path/to/confluent-<version>\n\n# Add Confluent binaries to your PATH\nexport PATH=$CONFLUENT_HOME/bin:$PATH\n```\n\n**Run Confluent Platform Locally:**\n\n```bash\n# Start the Confluent Platform services:\nconfluent local start\n# Stop the Confluent Platform services:\nconfluent local stop\n```\n\n## Installation of Qdrant:",
      "index": 11,
      "token_count": 425,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 21211,
      "end_char": 23200
    },
    {
      "content": "start\n# Stop the Confluent Platform services:\nconfluent local stop\n```\n\n## Installation of Qdrant:\n\nTo install and run Qdrant (self-managed locally), you can use Docker, which simplifies the process. First, ensure you have Docker installed on your system. Then, you can pull the Qdrant image from Docker Hub and run it with the following commands:\n\n```bash\ndocker pull qdrant/qdrant\ndocker run -p 6334:6334 -p 6333:6333 qdrant/qdrant\n```\n\nThis will download the Qdrant image and start a Qdrant instance accessible at `http://localhost:6333`. For more detailed instructions and alternative installation methods, refer to the [Qdrant installation documentation](https://qdrant.tech/documentation/quick-start/).\n\n## Installation of Qdrant-Kafka Sink Connector:\n\nTo install the Qdrant Kafka connector using [Confluent Hub](https://www.confluent.io/hub/), you can utilize the straightforward `confluent-hub install` command. This command simplifies the process by eliminating the need for manual configuration file manipulations. To install the Qdrant Kafka connector version 1.1.0, execute the following command in your terminal:\n\n```bash\n confluent-hub install qdrant/qdrant-kafka:1.1.0\n```\n\nThis command downloads and installs the specified connector directly from Confluent Hub into your Confluent Platform or Kafka Connect environment. The installation process ensures that all necessary dependencies are handled automatically, allowing for a seamless integration of the Qdrant Kafka connector with your existing setup. Once installed, the connector can be configured and managed using the Confluent Control Center or the Kafka Connect REST API, enabling efficient data streaming between Kafka and Qdrant without the need for intricate manual setup.\n\n*Figure 2: Local Confluent platform showing the Source and Sink connectors after installation.*\n\nEnsure the configuration of the connector once it’s installed as below. keep in mind that your `key.converter` and `value.",
      "index": 12,
      "token_count": 446,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 23100,
      "end_char": 25070
    },
    {
      "content": "on of the connector once it’s installed as below. keep in mind that your `key.converter` and `value.converter` are very important for kafka to safely deliver the messages from topic to qdrant.\n\n```bash\n{\n  \"name\": \"QdrantSinkConnectorConnector_0\",\n  \"config\": {\n    \"value.converter.schemas.enable\": \"false\",\n    \"name\": \"QdrantSinkConnectorConnector_0\",\n    \"connector.class\": \"io.qdrant.kafka.QdrantSinkConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"topics\": \"topic_62,qdrant_kafka.docs\",\n    \"errors.deadletterqueue.topic.name\": \"dead_queue\",\n    \"errors.deadletterqueue.topic.replication.factor\": \"1\",\n    \"qdrant.grpc.url\": \"http://localhost:6334\",\n    \"qdrant.api.key\": \"************\"\n  }\n}\n```\n\n## Installation of MongoDB\n\nFor the Kafka to connect MongoDB as source, your MongoDB instance should be running in a `replicaSet` mode. below is the `docker compose` file which will spin a single node `replicaSet` instance of MongoDB.\n\n```bash\nversion: \"3.8\"\n\nservices:\n  mongo1:\n    image: mongo:7.0\n    command: [\"--replSet\", \"rs0\", \"--bind_ip_all\", \"--port\", \"27017\"]\n    ports:\n      - 27017:27017\n    healthcheck:\n      test: echo \"try { rs.status() } catch (err) { rs.initiate({_id:'rs0',members:[{_id:0,host:'host.docker.internal:27017'}]}) }\" | mongosh --port 27017 --quiet\n      interval: 5s\n      timeout: 30s\n      start_period: 0s\n      start_interval: 1s\n      retries: 30\n    volumes:\n      - \"mongo1_data:/data/db\"\n      - \"mongo1_config:/data/configdb\"\n\nvolumes:\n  mongo1_data:\n  mongo1_config:\n```\n\nSimilarly, install and configure source connector as below.\n\n```bash\nconfluent-hub install mongodb/kafka-connect-mongodb:latest\n```\n\nAfter installing the `MongoDB` connector, connector configuration should look like this:\n\n```bash\n{\n  \"name\": \"MongoSourceConnectorConnector_0\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.",
      "index": 13,
      "token_count": 571,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 24970,
      "end_char": 27013
    },
    {
      "content": "\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"connection.uri\": \"mongodb://127.0.0.1:27017/?replicaSet=rs0&directConnection=true\",\n    \"database\": \"qdrant_kafka\",\n    \"collection\": \"docs\",\n    \"publish.full.document.only\": \"true\",\n    \"topic.namespace.map\": \"{\\\"*\\\":\\\"qdrant_kafka.docs\\\"}\",\n    \"copy.existing\": \"true\"\n  }\n}\n```\n\n## Playground Application\n\nAs the infrastructure set is completely done, now it’s time for us to create a simple application and check our setup. the objective of our application is the data is inserted to Mongodb and eventually it will get ingested into Qdrant also using [Change Data Capture (CDC)](https://www.confluent.io/learn/change-data-capture/).\n\n`requirements.txt`\n\n```bash\nfastembed==0.3.1\npymongo==4.8.0\nqdrant_client==1.10.1\n```\n\n`project_root_folder/main.py`\n\nThis is just sample code. Nevertheless it can be extended to millions of operations based on your use case.\n\n```python\nfrom pymongo import MongoClient\nfrom utils.app_utils import create_qdrant_collection\nfrom fastembed import TextEmbedding\n\ncollection_name: str = 'test'\nembed_model_name: str = 'snowflake/snowflake-arctic-embed-s'\n```\n\n```python\n# Step 0: create qdrant_collection\ncreate_qdrant_collection(collection_name=collection_name, embed_model=embed_model_name)\n\n# Step 1: Connect to MongoDB\nclient = MongoClient('mongodb://127.0.0.1:27017/?replicaSet=rs0&directConnection=true')\n\n# Step 2: Select Database\ndb = client['qdrant_kafka']\n\n# Step 3: Select Collection\ncollection = db['docs']\n\n# Step 4: Create a Document to Insert\n\ndescription = \"qdrant is a high available vector search engine\"\nembedding_model = TextEmbedding(model_name=embed_model_name)\nvector = next(embedding_model.embed(documents=description)).tolist()\ndocument = {\n    \"collection_name\": collection_name,\n    \"id\": 1,\n    \"vector\": vector,\n    \"payload\": {\n        \"name\": \"qdrant\",",
      "index": 14,
      "token_count": 528,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 26913,
      "end_char": 28937
    },
    {
      "content": "me\": collection_name,\n    \"id\": 1,\n    \"vector\": vector,\n    \"payload\": {\n        \"name\": \"qdrant\",\n        \"description\": description,\n        \"url\": \"https://qdrant.tech/documentation\"\n    }\n}\n\n# Step 5: Insert the Document into the Collection\nresult = collection.insert_one(document)\n\n# Step 6: Print the Inserted Document's ID\nprint(\"Inserted document ID:\", result.inserted_id)\n```\n\n`project_root_folder/utils/app_utils.py`\n\n```python\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\", api_key=\"<YOUR_KEY>\")\ndimension_dict = {\"snowflake/snowflake-arctic-embed-s\": 384}\n\ndef create_qdrant_collection(collection_name: str, embed_model: str):\n\n    if not client.collection_exists(collection_name=collection_name):\n        client.create_collection(\n            collection_name=collection_name,\n            vectors_config=models.VectorParams(size=dimension_dict.get(embed_model), distance=models.Distance.COSINE)\n        )\n```\n\nBefore we run the application, below is the state of MongoDB and Qdrant databases.\n\nFigure 3: Initial state: no collection named `test` & `no data` in the `docs` collection of MongodDB.\n\nOnce you run the code the data goes into Mongodb and the CDC gets triggered and eventually Qdrant will receive this data.\n\nFigure 4: The test Qdrant collection is created automatically.\n\nFigure 5: Data is inserted into both MongoDB and Qdrant.\n\n## Conclusion:\n\nIn conclusion, the integration of **Kafka** with **Qdrant** using the **Qdrant Sink Connector** provides a seamless and efficient solution for real-time data streaming and processing. This setup not only enhances the capabilities of your data pipeline but also ensures that high-dimensional vector data is continuously indexed and readily available for similarity searches. By following the installation and setup guide, you can easily establish a robust data flow from your **source systems** like **MongoDB** and **Azure Blob Storage**, through **Kafka**, and into **Qdrant**.",
      "index": 15,
      "token_count": 451,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 28837,
      "end_char": 30838
    },
    {
      "content": "ource systems** like **MongoDB** and **Azure Blob Storage**, through **Kafka**, and into **Qdrant**. This architecture empowers modern applications to leverage real-time data insights and advanced search capabilities, paving the way for innovative data-driven solutions.\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/send-data/data-streaming-kafka-qdrant.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Setup Data Streaming with Kafka via Confluent](#setup-data-streaming-with-kafka-via-confluent.md)\n\n  - [Introduction](#introduction.md)\n  - [The Architecture:](#the-architecture.md)\n  - [Source Systems](#source-systems.md)\n  - [Kafka](#kafka.md)\n  - [Qdrant](#qdrant.md)\n  - [Integration and Pipeline Importance](#integration-and-pipeline-importance.md)\n  - [Installation of Confluent Kafka Platform](#installation-of-confluent-kafka-platform.md)\n  - [Installation of Qdrant:](#installation-of-qdrant.md)\n  - [Installation of Qdrant-Kafka Sink Connector:](#installation-of-qdrant-kafka-sink-connector.md)\n  - [Installation of MongoDB](#installation-of-mongodb.md)\n  - [Playground Application](#playground-application.md)\n  - [Conclusion:](#conclusion.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/send-data/data-streaming-kafka-qdrant.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
      "index": 16,
      "token_count": 495,
      "metadata": {
        "title": "_documentation_send-data_data-streaming-kafka-qdrant_",
        "source": "qdrant_documentation\\documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_documentation",
        "source_subdir": "documentation_send-data_data-streaming-kafka-qdrant",
        "category": "send-data",
        "file_path": "documentation_send-data_data-streaming-kafka-qdrant\\_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "file_name": "_documentation_send-data_data-streaming-kafka-qdrant_.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:33.760451",
        "total_chunks": 17
      },
      "start_char": 30738,
      "end_char": 32786
    }
  ]
}