{
  "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
  "source_repo": "qdrant_fastembed",
  "total_chunks": 8,
  "chunks": [
    {
      "content": "Performance Optimization | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2043
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Performance Optimization\n\nRelevant source files\n\n- [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb)\n- [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb)\n- [docs/examples/Hybrid\\_Search.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb)\n- [docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/qdrant/Retrieval_with_FastEmbed.ipynb)\n- [fastembed/text/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py)\n\nFastEmbed is designed for high performance and efficient embedding generation. This page covers the various optimization techniques implemented in the library and how to configure them to maximize performance for your specific use case.\n\nFor information about the overall architecture of FastEmbed, see [Architecture](qdrant/fastembed/4-architecture.md). For detailed information about ONNX Runtime integration, see [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md). For specific information about parallel processing implementation, see [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md).\n\n## Key Performance Features\n\nFastEmbed incorporates several key performance optimization techniques that allow it to generate embeddings significantly faster than traditional embedding methods:\n\n1. **ONNX Runtime Integration**: Accelerated inference through ONNX optimization\n2.",
      "index": 1,
      "token_count": 499,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 1943,
      "end_char": 3936
    },
    {
      "content": "edding methods:\n\n1. **ONNX Runtime Integration**: Accelerated inference through ONNX optimization\n2. **Parallel Processing**: Multi-core utilization for faster embedding generation\n3. **Lazy Loading**: Efficient memory management for multiple models\n4. **Batch Processing**: Optimized throughput through batched operations\n5. **Hardware Acceleration**: Support for both CPU and GPU (CUDA) execution\n6. **Model Caching**: Efficient model storage and retrieval\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py198-229](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L198-L229)\n\n## ONNX Runtime Integration\n\nFastEmbed uses ONNX Runtime as its inference engine, which provides significant performance improvements over traditional PyTorch or TensorFlow implementations. ONNX (Open Neural Network Exchange) is an open standard for machine learning model representation that enables model interoperability between different frameworks.\n\n### Benefits of ONNX Runtime\n\n- **Optimized inference**: ONNX Runtime includes various optimizations specific to the hardware and platform\n- **Reduced memory usage**: More efficient memory management compared to PyTorch/TensorFlow\n- **Cross-platform compatibility**: Same model works across different environments\n\n### Configuration Options\n\nWhen initializing an embedding model, you can configure ONNX Runtime execution parameters:\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py199-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L199-L246)\n\n## Parallel Processing\n\nFastEmbed implements data-parallel processing to distribute embedding workloads across multiple CPU cores or GPU devices, enabling significant speedups for large dataset processing.\n\n### How Parallel Processing Works in FastEmbed\n\n```\n```\n\n### Configuring Parallel Processing\n\nWhen calling the `embed()` method, you can specify the level of parallelism:\n\n```\n```\n\nThe `parallel` parameter accepts:\n\n- A positive integer: Use the specified number of workers",
      "index": 2,
      "token_count": 435,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 3836,
      "end_char": 5878
    },
    {
      "content": "`\n```\n\nThe `parallel` parameter accepts:\n\n- A positive integer: Use the specified number of workers\n- 0: Use all available cores\n- None: Don't use data parallel processing (use default ONNX Runtime threading)\n\nPerformance benchmarks from example notebooks demonstrate significant speedups using parallel processing:\n\n| Processing Mode | User Time | System Time | Wall Clock Time | Speedup |\n| --------------- | --------- | ----------- | --------------- | ------- |\n| Sequential      | 16min 23s | 31.7s       | 3min            | 1x      |\n| Parallel        | 6min 19s  | 22s         | 1min 37s        | \\~2x    |\n\nSources: [docs/examples/Hybrid\\_Search.ipynb726-739](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L726-L739)\n\n## Lazy Loading\n\nLazy loading allows you to instantiate models without immediately loading their weights into memory, which can be useful when working with multiple models or in resource-constrained environments.\n\n### How to Enable Lazy Loading\n\n```\n```\n\nWhen lazy loading is enabled, the model weights are only loaded when the first embedding request is made, reducing initial memory usage and startup time.\n\nSources: [fastembed/text/onnx\\_embedding.py221-222](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L221-L222) [fastembed/text/onnx\\_embedding.py255-258](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L255-L258)\n\n## Batch Processing\n\nFastEmbed optimizes throughput by processing inputs in batches, which enables more efficient hardware utilization and minimizes overhead.\n\n### Configuring Batch Size\n\nYou can adjust the batch size when calling the `embed()` method:\n\n```\n```\n\nOptimal batch size depends on:\n\n- Available memory\n- Input document length\n- Model size\n- Hardware characteristics\n\nFor large datasets, combining batch processing with parallel processing can provide the best performance.\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.",
      "index": 3,
      "token_count": 505,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 5778,
      "end_char": 7788
    },
    {
      "content": "n provide the best performance.\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292)\n\n## Hardware Acceleration\n\nFastEmbed supports both CPU and GPU execution, allowing you to leverage available hardware for maximum performance.\n\n### CPU Optimization\n\nWhen running on CPU, you can specify the number of threads to use:\n\n```\n```\n\n### GPU Acceleration\n\nTo use GPU acceleration (if available):\n\n```\n```\n\nWhen using multiple GPUs with `device_ids`, FastEmbed will automatically distribute the workload across the specified devices when parallel processing is enabled.\n\nSources: [fastembed/text/onnx\\_embedding.py202-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L202-L246)\n\n## Performance Comparison\n\nFastEmbed significantly outperforms traditional embedding libraries by combining ONNX optimization with efficient parallel processing.\n\n```\n```\n\n### Benchmark Results\n\nFrom comparing FastEmbed vs. Hugging Face Transformers (using the same BGE-small-en-v1.5 model):\n\n| Library                   | Avg Processing Time | Chars/Second | Relative Performance |\n| ------------------------- | ------------------- | ------------ | -------------------- |\n| Hugging Face Transformers | 0.047s              | \\~811        | 1x (baseline)        |\n| FastEmbed                 | 0.044s              | \\~871        | \\~1.07x faster       |\n\nFor large datasets, the performance gap widens significantly due to parallel processing capabilities:\n\n| Dataset Size  | Traditional Library | FastEmbed | Speedup |\n| ------------- | ------------------- | --------- | ------- |\n| Small (10s)   | 1x                  | 1-2x      | 1-2x    |\n| Medium (100s) | 1x                  | 2-3x      | 2-3x    |\n| Large (1000s) | 1x                  | 3-5x      | 3-5x    |\n\nSources: [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb245-278](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb#L245-L278)",
      "index": 4,
      "token_count": 538,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 7688,
      "end_char": 9737
    },
    {
      "content": "github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb#L245-L278)\n\n## Advanced Configuration Parameters\n\nThe following table summarizes the key parameters that affect performance:\n\n| Parameter    | Description                                        | Default | When to Modify                                     |\n| ------------ | -------------------------------------------------- | ------- | -------------------------------------------------- |\n| `threads`    | Number of threads for ONNX session                 | `None`  | When you want to control CPU thread usage          |\n| `providers`  | ONNX Runtime providers                             | `None`  | For custom execution providers                     |\n| `cuda`       | Enable CUDA for GPU acceleration                   | `False` | When GPU is available                              |\n| `device_ids` | GPU device IDs for data parallel processing        | `None`  | When using multiple GPUs                           |\n| `lazy_load`  | Load model on demand rather than at initialization | `False` | When working with multiple models                  |\n| `batch_size` | Number of documents processed per batch            | `256`   | Adjust based on available memory and document size |\n| `parallel`   | Number of worker processes for parallel processing | `None`  | Set to >1 or 0 for large datasets                  |\n\nSources: [fastembed/text/onnx\\_embedding.py199-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L199-L246) [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292)\n\n## Best Practices for Performance Optimization\n\n### Model Selection\n\n- Smaller models (like BGE-small) are faster but may be less accurate\n- Larger models provide better quality but require more resources\n- Select the smallest model that meets your quality requirements\n\n### Processing Configuration\n\n1.",
      "index": 5,
      "token_count": 433,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 9637,
      "end_char": 11637
    },
    {
      "content": "s\n- Select the smallest model that meets your quality requirements\n\n### Processing Configuration\n\n1. **Enable parallel processing for large datasets**\n\n   ```\n   ```\n\n2. **Optimize batch size**\n\n   ```\n   ```\n\n3. **Use GPU acceleration when available**\n\n   ```\n   ```\n\n4. **Combine strategies for maximum performance**\n\n   ```\n   ```\n\n### Performance Debugging\n\nIf you encounter performance issues:\n\n1. Try reducing batch size if you're running out of memory\n2. Reduce the number of parallel workers if you experience excessive CPU/GPU contention\n3. Try a smaller model if speed is more important than quality\n4. Enable lazy loading if working with multiple models to manage memory more efficiently\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292) [docs/examples/Hybrid\\_Search.ipynb726-739](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L726-L739)\n\n## Late Interaction Models and Performance\n\nLate interaction models like ColBERT require special consideration for performance optimization:\n\n```\n```\n\nDue to their computational requirements, consider using late interaction models like ColBERT in a two-stage pipeline:\n\n1. **First stage**: Use a faster dense embedding model to retrieve candidate documents (100-500)\n2. **Second stage**: Use the more resource-intensive late interaction model to rerank results for higher precision\n\nThis approach balances speed and accuracy, leveraging the strengths of both approaches.\n\nSources: [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb373-398](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb#L373-L398)\n\n## Summary\n\nFastEmbed offers multiple performance optimization techniques that can be combined to achieve significant speedups in embedding generation. By choosing the right configuration for your specific use case, you can achieve optimal performance while maintaining high-quality embeddings.",
      "index": 6,
      "token_count": 467,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 11537,
      "end_char": 13559
    },
    {
      "content": "specific use case, you can achieve optimal performance while maintaining high-quality embeddings.\n\nFor most cases, enabling parallel processing (`parallel=0`) with an appropriate batch size and GPU acceleration (if available) will provide the best balance of performance and resource utilization.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Performance Optimization](#performance-optimization.md)\n- [Key Performance Features](#key-performance-features.md)\n- [ONNX Runtime Integration](#onnx-runtime-integration.md)\n- [Benefits of ONNX Runtime](#benefits-of-onnx-runtime.md)\n- [Configuration Options](#configuration-options.md)\n- [Parallel Processing](#parallel-processing.md)\n- [How Parallel Processing Works in FastEmbed](#how-parallel-processing-works-in-fastembed.md)\n- [Configuring Parallel Processing](#configuring-parallel-processing.md)\n- [Lazy Loading](#lazy-loading.md)\n- [How to Enable Lazy Loading](#how-to-enable-lazy-loading.md)\n- [Batch Processing](#batch-processing.md)\n- [Configuring Batch Size](#configuring-batch-size.md)\n- [Hardware Acceleration](#hardware-acceleration.md)\n- [CPU Optimization](#cpu-optimization.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Performance Comparison](#performance-comparison.md)\n- [Benchmark Results](#benchmark-results.md)\n- [Advanced Configuration Parameters](#advanced-configuration-parameters.md)\n- [Best Practices for Performance Optimization](#best-practices-for-performance-optimization.md)\n- [Model Selection](#model-selection.md)\n- [Processing Configuration](#processing-configuration.md)\n- [Performance Debugging](#performance-debugging.md)\n- [Late Interaction Models and Performance](#late-interaction-models-and-performance.md)\n- [Summary](#summary.md)",
      "index": 7,
      "token_count": 383,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.763283",
        "total_chunks": 8
      },
      "start_char": 13459,
      "end_char": 15507
    }
  ]
}