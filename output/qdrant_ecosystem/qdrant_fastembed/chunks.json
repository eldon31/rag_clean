{
  "collection": "qdrant_ecosystem",
  "subdirectory": "qdrant_fastembed",
  "total_chunks": 187,
  "chunks": [
    {
      "content": "qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)",
      "index": 0,
      "token_count": 622,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2038
    },
    {
      "content": "d/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Overview\n\nRelevant source files\n\n- [.pre-commit-config.yaml](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml)\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [fastembed/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py)\n- [pyproject.toml](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml)\n- [tests/test\\_late\\_interaction\\_multimodal.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py)\n\nFastEmbed is a lightweight, fast Python library designed for generating high-quality embeddings from text and images. It focuses on performance optimization through ONNX Runtime integration, providing a more efficient alternative to traditional embedding libraries like PyTorch-based Sentence Transformers.\n\nThis overview introduces the core concepts, architecture, and components of FastEmbed. For installation instructions, see [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md).\n\nSources: [README.md1-14](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L1-L14) [pyproject.toml1-12](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L12)\n\n## Core Features\n\nFastEmbed offers several key advantages over other embedding libraries:\n\n1. **Lightweight**: Minimal external dependencies, making it suitable for resource-constrained environments like serverless functions\n\n2.",
      "index": 1,
      "token_count": 514,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 1938,
      "end_char": 3848
    },
    {
      "content": "dependencies, making it suitable for resource-constrained environments like serverless functions\n\n2. **Fast**: ONNX Runtime integration and data parallelism for efficient embedding generation, providing significant performance gains over PyTorch-based alternatives\n\n3. **Accurate**: Support for state-of-the-art embedding models that deliver performance comparable to or better than commercial options like OpenAI's Ada-002\n\n4. **Versatile**: Support for multiple embedding strategies including dense, sparse, late interaction, and multimodal approaches\n\n5. **GPU Acceleration**: Optional GPU support through the `fastembed-gpu` package\n\nSources: [README.md7-14](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L7-L14) [pyproject.toml13-34](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L13-L34) [docs/examples/FastEmbed\\_GPU.ipynb9-21](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L9-L21)\n\n## System Architecture\n\nFastEmbed is organized around a modular architecture with specialized classes for different embedding approaches and modalities.\n\n### Core Components Diagram\n\n```\n```\n\nSources: [fastembed/\\_\\_init\\_\\_.py1-22](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py#L1-L22)\n\n### Embedding Process Flow\n\n```\n```\n\nSources: [README.md28-190](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L190)\n\n## Core Components\n\nFastEmbed provides specialized classes for different embedding approaches, each optimized for specific use cases:\n\n| Component                            | Description                                                | Primary Use Cases                             |\n| ------------------------------------ | ---------------------------------------------------------- | --------------------------------------------- |\n| `TextEmbedding`                      | Dense text embeddings, supports various pooling strategies | Semantic search, document similarity          |",
      "index": 2,
      "token_count": 451,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 3748,
      "end_char": 5743
    },
    {
      "content": "t embeddings, supports various pooling strategies | Semantic search, document similarity          |\n| `SparseTextEmbedding`                | Sparse text embeddings (SPLADE, BM25, BM42)                | Hybrid search, traditional search integration |\n| `LateInteractionTextEmbedding`       | Token-level embeddings (ColBERT)                           | Advanced retrieval with token matching        |\n| `ImageEmbedding`                     | CLIP and similar image embeddings                          | Image search, visual similarity               |\n| `LateInteractionMultimodalEmbedding` | Multimodal token-level embeddings (ColPali)                | Document image search, multimodal retrieval   |\n| `TextCrossEncoder`                   | Text pair scoring for reranking                            | Search result refinement, question-answering  |\n\nSources: [README.md49-190](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L49-L190) [fastembed/\\_\\_init\\_\\_.py3-22](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py#L3-L22)\n\n### Text Embeddings\n\nThe `TextEmbedding` class is the most commonly used component, providing dense vector representations for text:\n\n```\n```\n\nKey features:\n\n- Default model is \"BAAI/bge-small-en-v1.5\", a performant English embedding model\n- Automatic model downloading and caching\n- Parallel processing for large batches of documents\n- Optional GPU acceleration with the `fastembed-gpu` package\n\nSources: [README.md28-47](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L47)\n\n### Sparse Text Embeddings\n\nThe `SparseTextEmbedding` class provides sparse vector representations:\n\n```\n```\n\nThese sparse embeddings are particularly useful for hybrid search approaches that combine traditional term-based retrieval with semantic search.\n\nSources: [README.md87-99](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L87-L99)\n\n### Late Interaction Models\n\nThe `LateInteractionTextEmbedding` class implements ColBERT-style embeddings with token-level representations:",
      "index": 3,
      "token_count": 481,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 5643,
      "end_char": 7690
    },
    {
      "content": "eractionTextEmbedding` class implements ColBERT-style embeddings with token-level representations:\n\n```\n```\n\nThese models produce a matrix of embeddings per document (one vector per token), enabling more sophisticated matching during retrieval.\n\nSources: [README.md119-136](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L119-L136)\n\n### Image Embeddings\n\nThe `ImageEmbedding` class provides embedding generation for images:\n\n```\n```\n\nThis class supports both file paths and PIL Image objects as input.\n\nSources: [README.md140-154](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L140-L154)\n\n### Multimodal Embeddings\n\nThe `LateInteractionMultimodalEmbedding` class enables token-level embeddings for both text and images:\n\n```\n```\n\nThis allows for sophisticated cross-modal retrieval between text queries and document images.\n\nSources: [README.md158-176](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L158-L176) [tests/test\\_late\\_interaction\\_multimodal.py1-83](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py#L1-L83)\n\n### Text Cross Encoder for Reranking\n\nThe `TextCrossEncoder` class provides text pair scoring for reranking search results:\n\n```\n```\n\nThis is useful for refining search results after initial retrieval.\n\nSources: [README.md180-207](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L180-L207)\n\n## Performance Optimization\n\nFastEmbed focuses on performance through several key optimizations:\n\n1. **ONNX Runtime**: Uses ONNX models for efficient inference without requiring PyTorch/TensorFlow\n2. **Parallel Processing**: Automatically distributes embedding generation across CPU cores\n3. **GPU Acceleration**: Optional GPU support through `fastembed-gpu` package\n4. **Model Caching**: Automatic downloading and caching of models\n5. **Batching**: Efficient batching of inputs for optimized throughput\n\nA simple benchmark comparing CPU vs GPU performance shows orders of magnitude improvement:\n\n```\nCPU execution time: 4.33s (500 documents)",
      "index": 4,
      "token_count": 522,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 7590,
      "end_char": 9639
    },
    {
      "content": "U performance shows orders of magnitude improvement:\n\n```\nCPU execution time: 4.33s (500 documents)\nGPU execution time: 43.4ms (500 documents)\n```\n\nSources: [docs/examples/FastEmbed\\_GPU.ipynb390-511](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L390-L511)\n\n## Integration with Qdrant\n\nFastEmbed is maintained by Qdrant and has native integration with the Qdrant vector database:\n\n```\n```\n\nFor more details on using FastEmbed with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nSources: [README.md232-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L232-L281)\n\n## Supported Models\n\nFastEmbed supports a wide range of embedding models:\n\n1. **Dense Text Models**: BGE embeddings, Sentence Transformers, CLIP text models\n2. **Sparse Text Models**: SPLADE, BM25, BM42\n3. **Late Interaction Models**: ColBERT, Jina ColBERT\n4. **Image Models**: CLIP vision models\n5. **Multimodal Models**: ColPali\n\nFor a complete list of supported models and their configuration details, see [Supported Models](qdrant/fastembed/6-supported-models.md).\n\nThe library also supports extending with custom models through API methods like `TextEmbedding.add_custom_model()`.\n\nSources: [README.md66-82](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L66-L82) [README.md196-207](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L196-L207)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Overview](#overview.md)\n- [Core Features](#core-features.md)\n- [System Architecture](#system-architecture.md)\n- [Core Components Diagram](#core-components-diagram.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Core Components](#core-components.md)\n- [Text Embeddings](#text-embeddings.md)\n- [Sparse Text Embeddings](#sparse-text-embeddings.md)\n- [Late Interaction Models](#late-interaction-models.md)\n- [Image Embeddings](#image-embeddings.md)\n- [Multimodal Embeddings](#multimodal-embeddings.md)",
      "index": 5,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 9539,
      "end_char": 11554
    },
    {
      "content": "md)\n- [Image Embeddings](#image-embeddings.md)\n- [Multimodal Embeddings](#multimodal-embeddings.md)\n- [Text Cross Encoder for Reranking](#text-cross-encoder-for-reranking.md)\n- [Performance Optimization](#performance-optimization.md)\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Supported Models](#supported-models.md)",
      "index": 6,
      "token_count": 92,
      "metadata": {
        "title": "_qdrant_fastembed",
        "source": "qdrant_fastembed\\_qdrant_fastembed.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed.md",
        "file_name": "_qdrant_fastembed.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.073548",
        "total_chunks": 7
      },
      "start_char": 11454,
      "end_char": 13502
    },
    {
      "content": "qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)",
      "index": 0,
      "token_count": 622,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2038
    },
    {
      "content": "d/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Overview\n\nRelevant source files\n\n- [.pre-commit-config.yaml](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml)\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [fastembed/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py)\n- [pyproject.toml](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml)\n- [tests/test\\_late\\_interaction\\_multimodal.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py)\n\nFastEmbed is a lightweight, fast Python library designed for generating high-quality embeddings from text and images. It focuses on performance optimization through ONNX Runtime integration, providing a more efficient alternative to traditional embedding libraries like PyTorch-based Sentence Transformers.\n\nThis overview introduces the core concepts, architecture, and components of FastEmbed. For installation instructions, see [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md).\n\nSources: [README.md1-14](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L1-L14) [pyproject.toml1-12](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L12)\n\n## Core Features\n\nFastEmbed offers several key advantages over other embedding libraries:\n\n1. **Lightweight**: Minimal external dependencies, making it suitable for resource-constrained environments like serverless functions\n\n2.",
      "index": 1,
      "token_count": 514,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 1938,
      "end_char": 3848
    },
    {
      "content": "dependencies, making it suitable for resource-constrained environments like serverless functions\n\n2. **Fast**: ONNX Runtime integration and data parallelism for efficient embedding generation, providing significant performance gains over PyTorch-based alternatives\n\n3. **Accurate**: Support for state-of-the-art embedding models that deliver performance comparable to or better than commercial options like OpenAI's Ada-002\n\n4. **Versatile**: Support for multiple embedding strategies including dense, sparse, late interaction, and multimodal approaches\n\n5. **GPU Acceleration**: Optional GPU support through the `fastembed-gpu` package\n\nSources: [README.md7-14](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L7-L14) [pyproject.toml13-34](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L13-L34) [docs/examples/FastEmbed\\_GPU.ipynb9-21](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L9-L21)\n\n## System Architecture\n\nFastEmbed is organized around a modular architecture with specialized classes for different embedding approaches and modalities.\n\n### Core Components Diagram\n\n```\n```\n\nSources: [fastembed/\\_\\_init\\_\\_.py1-22](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py#L1-L22)\n\n### Embedding Process Flow\n\n```\n```\n\nSources: [README.md28-190](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L190)\n\n## Core Components\n\nFastEmbed provides specialized classes for different embedding approaches, each optimized for specific use cases:\n\n| Component                            | Description                                                | Primary Use Cases                             |\n| ------------------------------------ | ---------------------------------------------------------- | --------------------------------------------- |\n| `TextEmbedding`                      | Dense text embeddings, supports various pooling strategies | Semantic search, document similarity          |",
      "index": 2,
      "token_count": 451,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 3748,
      "end_char": 5743
    },
    {
      "content": "t embeddings, supports various pooling strategies | Semantic search, document similarity          |\n| `SparseTextEmbedding`                | Sparse text embeddings (SPLADE, BM25, BM42)                | Hybrid search, traditional search integration |\n| `LateInteractionTextEmbedding`       | Token-level embeddings (ColBERT)                           | Advanced retrieval with token matching        |\n| `ImageEmbedding`                     | CLIP and similar image embeddings                          | Image search, visual similarity               |\n| `LateInteractionMultimodalEmbedding` | Multimodal token-level embeddings (ColPali)                | Document image search, multimodal retrieval   |\n| `TextCrossEncoder`                   | Text pair scoring for reranking                            | Search result refinement, question-answering  |\n\nSources: [README.md49-190](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L49-L190) [fastembed/\\_\\_init\\_\\_.py3-22](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py#L3-L22)\n\n### Text Embeddings\n\nThe `TextEmbedding` class is the most commonly used component, providing dense vector representations for text:\n\n```\n```\n\nKey features:\n\n- Default model is \"BAAI/bge-small-en-v1.5\", a performant English embedding model\n- Automatic model downloading and caching\n- Parallel processing for large batches of documents\n- Optional GPU acceleration with the `fastembed-gpu` package\n\nSources: [README.md28-47](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L28-L47)\n\n### Sparse Text Embeddings\n\nThe `SparseTextEmbedding` class provides sparse vector representations:\n\n```\n```\n\nThese sparse embeddings are particularly useful for hybrid search approaches that combine traditional term-based retrieval with semantic search.\n\nSources: [README.md87-99](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L87-L99)\n\n### Late Interaction Models\n\nThe `LateInteractionTextEmbedding` class implements ColBERT-style embeddings with token-level representations:",
      "index": 3,
      "token_count": 481,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 5643,
      "end_char": 7690
    },
    {
      "content": "eractionTextEmbedding` class implements ColBERT-style embeddings with token-level representations:\n\n```\n```\n\nThese models produce a matrix of embeddings per document (one vector per token), enabling more sophisticated matching during retrieval.\n\nSources: [README.md119-136](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L119-L136)\n\n### Image Embeddings\n\nThe `ImageEmbedding` class provides embedding generation for images:\n\n```\n```\n\nThis class supports both file paths and PIL Image objects as input.\n\nSources: [README.md140-154](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L140-L154)\n\n### Multimodal Embeddings\n\nThe `LateInteractionMultimodalEmbedding` class enables token-level embeddings for both text and images:\n\n```\n```\n\nThis allows for sophisticated cross-modal retrieval between text queries and document images.\n\nSources: [README.md158-176](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L158-L176) [tests/test\\_late\\_interaction\\_multimodal.py1-83](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py#L1-L83)\n\n### Text Cross Encoder for Reranking\n\nThe `TextCrossEncoder` class provides text pair scoring for reranking search results:\n\n```\n```\n\nThis is useful for refining search results after initial retrieval.\n\nSources: [README.md180-207](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L180-L207)\n\n## Performance Optimization\n\nFastEmbed focuses on performance through several key optimizations:\n\n1. **ONNX Runtime**: Uses ONNX models for efficient inference without requiring PyTorch/TensorFlow\n2. **Parallel Processing**: Automatically distributes embedding generation across CPU cores\n3. **GPU Acceleration**: Optional GPU support through `fastembed-gpu` package\n4. **Model Caching**: Automatic downloading and caching of models\n5. **Batching**: Efficient batching of inputs for optimized throughput\n\nA simple benchmark comparing CPU vs GPU performance shows orders of magnitude improvement:\n\n```\nCPU execution time: 4.33s (500 documents)",
      "index": 4,
      "token_count": 522,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 7590,
      "end_char": 9639
    },
    {
      "content": "U performance shows orders of magnitude improvement:\n\n```\nCPU execution time: 4.33s (500 documents)\nGPU execution time: 43.4ms (500 documents)\n```\n\nSources: [docs/examples/FastEmbed\\_GPU.ipynb390-511](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L390-L511)\n\n## Integration with Qdrant\n\nFastEmbed is maintained by Qdrant and has native integration with the Qdrant vector database:\n\n```\n```\n\nFor more details on using FastEmbed with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nSources: [README.md232-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L232-L281)\n\n## Supported Models\n\nFastEmbed supports a wide range of embedding models:\n\n1. **Dense Text Models**: BGE embeddings, Sentence Transformers, CLIP text models\n2. **Sparse Text Models**: SPLADE, BM25, BM42\n3. **Late Interaction Models**: ColBERT, Jina ColBERT\n4. **Image Models**: CLIP vision models\n5. **Multimodal Models**: ColPali\n\nFor a complete list of supported models and their configuration details, see [Supported Models](qdrant/fastembed/6-supported-models.md).\n\nThe library also supports extending with custom models through API methods like `TextEmbedding.add_custom_model()`.\n\nSources: [README.md66-82](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L66-L82) [README.md196-207](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L196-L207)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Overview](#overview.md)\n- [Core Features](#core-features.md)\n- [System Architecture](#system-architecture.md)\n- [Core Components Diagram](#core-components-diagram.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Core Components](#core-components.md)\n- [Text Embeddings](#text-embeddings.md)\n- [Sparse Text Embeddings](#sparse-text-embeddings.md)\n- [Late Interaction Models](#late-interaction-models.md)\n- [Image Embeddings](#image-embeddings.md)\n- [Multimodal Embeddings](#multimodal-embeddings.md)",
      "index": 5,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 9539,
      "end_char": 11554
    },
    {
      "content": "md)\n- [Image Embeddings](#image-embeddings.md)\n- [Multimodal Embeddings](#multimodal-embeddings.md)\n- [Text Cross Encoder for Reranking](#text-cross-encoder-for-reranking.md)\n- [Performance Optimization](#performance-optimization.md)\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Supported Models](#supported-models.md)",
      "index": 6,
      "token_count": 92,
      "metadata": {
        "title": "_qdrant_fastembed_1-overview",
        "source": "qdrant_fastembed\\_qdrant_fastembed_1-overview.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_1-overview.md",
        "file_name": "_qdrant_fastembed_1-overview.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.080121",
        "total_chunks": 7
      },
      "start_char": 11454,
      "end_char": 13502
    },
    {
      "content": "Development Guide | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_10-development-guide",
        "source": "qdrant_fastembed\\_qdrant_fastembed_10-development-guide.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_10-development-guide.md",
        "file_name": "_qdrant_fastembed_10-development-guide.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.086792",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2036
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Development Guide\n\nRelevant source files\n\n- [.github/workflows/python-tests.yml](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml)\n- [.pre-commit-config.yaml](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml)\n- [pyproject.toml](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml)\n\nThis guide outlines the process and best practices for contributing to FastEmbed. It covers the development environment setup, code organization, testing, and contribution workflow. For information about using FastEmbed in your applications, refer to the [Usage Examples](qdrant/fastembed/7-usage-examples.md).\n\n## Setting Up the Development Environment\n\nThis section explains how to set up your development environment to contribute to FastEmbed.\n\n### Prerequisites\n\n- Python 3.9 or higher\n- Git\n- Poetry (for dependency management)\n\n### Installation Steps for Development\n\n1. Clone the repository:\n\n   ```\n   ```\n\n2. Install dependencies with Poetry:\n\n   ```\n   ```\n\n3. Install pre-commit hooks:\n\n   ```\n   ```\n\nSources: [pyproject.toml1-64](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L64) [.pre-commit-config.yaml1-10](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml#L1-L10)\n\n## Project Structure\n\nFastEmbed follows a modular architecture with multiple embedding strategies and implementations. Understanding this structure is important for contributors.\n\n### Code Organization\n\n```\n```\n\n### From Interface to Implementation\n\n```\n```\n\nSources: System architecture diagrams\n\n## Development Workflow",
      "index": 1,
      "token_count": 486,
      "metadata": {
        "title": "_qdrant_fastembed_10-development-guide",
        "source": "qdrant_fastembed\\_qdrant_fastembed_10-development-guide.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_10-development-guide.md",
        "file_name": "_qdrant_fastembed_10-development-guide.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.086792",
        "total_chunks": 6
      },
      "start_char": 1936,
      "end_char": 3892
    },
    {
      "content": "terface to Implementation\n\n```\n```\n\nSources: System architecture diagrams\n\n## Development Workflow\n\nFollowing the proper development workflow ensures your contributions can be efficiently reviewed and merged.\n\n### Step 1: Create a Feature Branch\n\n```\n```\n\n### Step 2: Implement Your Changes\n\nWhen implementing features or fixes, ensure you:\n\n- Follow the code style guidelines\n- Add appropriate tests\n- Update documentation if necessary\n\n### Step 3: Run Tests Locally\n\n```\n```\n\n### Step 4: Submit a Pull Request\n\n- Push your branch to GitHub\n- Create a PR against the `main` branch\n- Provide a clear description of the changes\n\n### Continuous Integration\n\nFastEmbed uses GitHub Actions for CI, which automatically runs tests on:\n\n- Multiple Python versions (3.9 to 3.13)\n- Multiple operating systems (Linux, macOS, Windows)\n\n```\n```\n\nSources: [.github/workflows/python-tests.yml1-48](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml#L1-L48)\n\n## Code Style and Quality\n\nFastEmbed maintains strict code quality standards to ensure maintainability and readability.\n\n### Formatting and Linting\n\nFastEmbed uses Ruff for both formatting and linting:\n\n- Line length limit: 99 characters\n- Enforced through pre-commit hooks\n\n### Type Checking\n\nType hints are required for all function parameters and return values:\n\n- FastEmbed uses Pyright in strict mode\n- Type checking can be run with: `poetry run pyright`\n\n### Common Patterns\n\nWhen adding new features, follow these common patterns:\n\n1. **New Embedding Types**: Extend appropriate base classes\n2. **New Models**: Follow the OnnxModel pattern for consistent interface\n3. **Utilities**: Place in appropriate utility modules to ensure reusability\n\nSources: [pyproject.toml59-63](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L59-L63) [.pre-commit-config.yaml1-10](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml#L1-L10)\n\n## Testing Guidelines\n\nTests are crucial for maintaining FastEmbed's reliability.",
      "index": 2,
      "token_count": 483,
      "metadata": {
        "title": "_qdrant_fastembed_10-development-guide",
        "source": "qdrant_fastembed\\_qdrant_fastembed_10-development-guide.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_10-development-guide.md",
        "file_name": "_qdrant_fastembed_10-development-guide.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.086792",
        "total_chunks": 6
      },
      "start_char": 3792,
      "end_char": 5819
    },
    {
      "content": "nfig.yaml#L1-L10)\n\n## Testing Guidelines\n\nTests are crucial for maintaining FastEmbed's reliability. All new features and bug fixes should include appropriate tests.\n\n### Test Organization\n\nTests are organized to mirror the project structure:\n\n```\n```\n\n### Writing Tests\n\n1. **Unit Tests**: Focus on testing a single function or method in isolation\n2. **Integration Tests**: Test how components work together\n3. **Model Tests**: Verify model-specific functionality and correctness\n\n### Running Tests\n\n```\n```\n\nSources: [.github/workflows/python-tests.yml44-48](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml#L44-L48)\n\n## Documentation\n\nGood documentation is essential for both users and contributors.\n\n### Docstrings\n\nUse descriptive docstrings for all public classes and methods:\n\n- Include parameter descriptions\n- Specify return types and values\n- Provide examples where appropriate\n\n### API Documentation\n\nAPI documentation is generated from docstrings. When updating or adding new features, ensure the documentation is updated.\n\n### Building Documentation\n\nFastEmbed uses MkDocs with Material theme for documentation:\n\n```\n```\n\nSources: [pyproject.toml44-49](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L44-L49)\n\n## Adding New Models\n\nAdding a new embedding model to FastEmbed typically involves these steps:\n\n```\n```\n\n### Model Implementation Pattern\n\n1. Determine the embedding type (dense, sparse, late interaction)\n2. Extend the appropriate base class\n3. Implement the required methods for tokenization and inference\n4. Add model configuration to model registry if applicable\n\n### ONNX Export Considerations\n\nWhen adding support for new models:\n\n1. Ensure the model can be properly exported to ONNX format\n2. Test inference with ONNX Runtime\n3. Optimize the model if necessary for better performance\n\n## Dependency Management\n\nFastEmbed uses Poetry for dependency management. When adding new dependencies or updating existing ones:\n\n1. Use Poetry's commands to add dependencies:",
      "index": 3,
      "token_count": 439,
      "metadata": {
        "title": "_qdrant_fastembed_10-development-guide",
        "source": "qdrant_fastembed\\_qdrant_fastembed_10-development-guide.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_10-development-guide.md",
        "file_name": "_qdrant_fastembed_10-development-guide.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.086792",
        "total_chunks": 6
      },
      "start_char": 5719,
      "end_char": 7767
    },
    {
      "content": "adding new dependencies or updating existing ones:\n\n1. Use Poetry's commands to add dependencies:\n\n   ```\n   ```\n\n2. Update dependency constraints in pyproject.toml if necessary\n\n3. Test compatibility across Python versions\n\nSources: [pyproject.toml13-33](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L13-L33)\n\n## Release Process\n\nThe release process for FastEmbed follows these steps:\n\n1. **Version Bump**: Update version in pyproject.toml\n2. **Changelog Update**: Ensure CHANGELOG.md is updated with all changes\n3. **Documentation Update**: Make sure documentation is up-to-date\n4. **Create Release PR**: Submit a PR with these changes\n5. **Tag Release**: After PR is merged, tag the release\n6. **Publish Package**: The CI pipeline will publish to PyPI\n\n```\n```\n\nSources: [pyproject.toml1-5](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L5)\n\n## Troubleshooting Common Issues\n\nThis section covers common issues encountered during development and their solutions.\n\n### Environment Setup Issues\n\n- **Poetry Installation Problems**: Ensure you're using a compatible Python version\n- **Dependency Conflicts**: Try refreshing your virtual environment\n\n### Testing Issues\n\n- **Test Failures on Specific OS**: Check OS-specific dependencies\n- **Model Download Failures**: Ensure you have proper internet connectivity and HF\\_TOKEN if needed\n\n### Contribution Issues\n\n- **Pre-commit Hooks Failing**: Run pre-commit manually to see detailed errors\n- **CI Pipeline Failures**: Check the specific test that failed and reproduce locally\n\n## Conclusion\n\nThis development guide provides an overview of the contribution process for FastEmbed. By following these guidelines, you can help maintain and improve this high-performance embedding library.\n\nFor questions or help with contributions, please reach out through GitHub issues or pull requests.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Development Guide](#development-guide.md)",
      "index": 4,
      "token_count": 439,
      "metadata": {
        "title": "_qdrant_fastembed_10-development-guide",
        "source": "qdrant_fastembed\\_qdrant_fastembed_10-development-guide.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_10-development-guide.md",
        "file_name": "_qdrant_fastembed_10-development-guide.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.086792",
        "total_chunks": 6
      },
      "start_char": 7667,
      "end_char": 9662
    },
    {
      "content": "h this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Development Guide](#development-guide.md)\n- [Setting Up the Development Environment](#setting-up-the-development-environment.md)\n- [Prerequisites](#prerequisites.md)\n- [Installation Steps for Development](#installation-steps-for-development.md)\n- [Project Structure](#project-structure.md)\n- [Code Organization](#code-organization.md)\n- [From Interface to Implementation](#from-interface-to-implementation.md)\n- [Development Workflow](#development-workflow.md)\n- [Step 1: Create a Feature Branch](#step-1-create-a-feature-branch.md)\n- [Step 2: Implement Your Changes](#step-2-implement-your-changes.md)\n- [Step 3: Run Tests Locally](#step-3-run-tests-locally.md)\n- [Step 4: Submit a Pull Request](#step-4-submit-a-pull-request.md)\n- [Continuous Integration](#continuous-integration.md)\n- [Code Style and Quality](#code-style-and-quality.md)\n- [Formatting and Linting](#formatting-and-linting.md)\n- [Type Checking](#type-checking.md)\n- [Common Patterns](#common-patterns.md)\n- [Testing Guidelines](#testing-guidelines.md)\n- [Test Organization](#test-organization.md)\n- [Writing Tests](#writing-tests.md)\n- [Running Tests](#running-tests.md)\n- [Documentation](#documentation.md)\n- [Docstrings](#docstrings.md)\n- [API Documentation](#api-documentation.md)\n- [Building Documentation](#building-documentation.md)\n- [Adding New Models](#adding-new-models.md)\n- [Model Implementation Pattern](#model-implementation-pattern.md)\n- [ONNX Export Considerations](#onnx-export-considerations.md)\n- [Dependency Management](#dependency-management.md)\n- [Release Process](#release-process.md)\n- [Troubleshooting Common Issues](#troubleshooting-common-issues.md)\n- [Environment Setup Issues](#environment-setup-issues.md)\n- [Testing Issues](#testing-issues.md)\n- [Contribution Issues](#contribution-issues.md)\n- [Conclusion](#conclusion.md)",
      "index": 5,
      "token_count": 473,
      "metadata": {
        "title": "_qdrant_fastembed_10-development-guide",
        "source": "qdrant_fastembed\\_qdrant_fastembed_10-development-guide.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_10-development-guide.md",
        "file_name": "_qdrant_fastembed_10-development-guide.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.086792",
        "total_chunks": 6
      },
      "start_char": 9562,
      "end_char": 11610
    },
    {
      "content": "Installation and Setup | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_fastembed_2-installation-and-setup",
        "source": "qdrant_fastembed\\_qdrant_fastembed_2-installation-and-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_2-installation-and-setup.md",
        "file_name": "_qdrant_fastembed_2-installation-and-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.093794",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2041
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Installation and Setup\n\nRelevant source files\n\n- [.github/workflows/python-tests.yml](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml)\n- [.gitignore](https://github.com/qdrant/fastembed/blob/b785640b/.gitignore)\n- [.pre-commit-config.yaml](https://github.com/qdrant/fastembed/blob/b785640b/.pre-commit-config.yaml)\n- [LICENSE](https://github.com/qdrant/fastembed/blob/b785640b/LICENSE)\n- [experiments/02\\_SPLADE\\_to\\_ONNX.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/experiments/02_SPLADE_to_ONNX.ipynb)\n- [pyproject.toml](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml)\n\nThis page provides comprehensive instructions for installing and setting up FastEmbed, a high-performance embedding library. FastEmbed is designed for fast, light, and accurate embedding generation, with a focus on production environments. For an overview of the library's capabilities, see [Overview](qdrant/fastembed/1-overview.md).\n\n## System Requirements\n\nBefore installing FastEmbed, ensure your system meets the following requirements:\n\n| Requirement       | Details                                          |\n| ----------------- | ------------------------------------------------ |\n| Python Version    | Python 3.9 or newer                              |\n| Operating Systems | Windows, macOS, Linux (all supported)            |\n| Disk Space        | \\~600MB (varies depending on models downloaded)  |\n| RAM               | 1GB minimum (4GB+ recommended for larger models) |\n\nSources: [pyproject.toml13-14](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L13-L14)\n\n## Installation Methods\n\n### Using pip (Recommended)",
      "index": 1,
      "token_count": 525,
      "metadata": {
        "title": "_qdrant_fastembed_2-installation-and-setup",
        "source": "qdrant_fastembed\\_qdrant_fastembed_2-installation-and-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_2-installation-and-setup.md",
        "file_name": "_qdrant_fastembed_2-installation-and-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.093794",
        "total_chunks": 6
      },
      "start_char": 1941,
      "end_char": 3969
    },
    {
      "content": "tembed/blob/b785640b/pyproject.toml#L13-L14)\n\n## Installation Methods\n\n### Using pip (Recommended)\n\nThe simplest way to install FastEmbed is using pip:\n\n```\n```\n\nFor specific versions:\n\n```\n```\n\n### Using Poetry\n\nIf you use Poetry for dependency management:\n\n```\n```\n\n### From Source\n\nFor the latest development version or to contribute:\n\n```\n```\n\nOr with Poetry:\n\n```\n```\n\nSources: [pyproject.toml1-12](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L12) [.github/workflows/python-tests.yml37-42](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml#L37-L42)\n\n## Installation Flow\n\nThe following diagram illustrates the FastEmbed installation flow:\n\n```\n```\n\nSources: [pyproject.toml13-33](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L13-L33) [.github/workflows/python-tests.yml37-47](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml#L37-L47)\n\n## Dependencies\n\nFastEmbed automatically installs the following core dependencies:\n\n| Dependency       | Purpose              | Version Requirement                |\n| ---------------- | -------------------- | ---------------------------------- |\n| numpy            | Numerical operations | ≥1.21 (varies by Python version)   |\n| onnxruntime      | Model inference      | ≥1.17.0 (varies by Python version) |\n| tokenizers       | Text tokenization    | ≥0.15,<1.0                         |\n| huggingface-hub  | Model downloading    | ≥0.20,<1.0                         |\n| tqdm             | Progress bars        | ^4.66                              |\n| requests         | HTTP requests        | ^2.31                              |\n| loguru           | Logging              | ^0.7.2                             |\n| pillow           | Image processing     | ≥10.3.0,<12.0.0                    |\n| mmh3             | Hash functions       | ≥4.1.0,<6.0.0                      |\n| py-rust-stemmers | Text stemming        | ^0.1.0                             |",
      "index": 2,
      "token_count": 527,
      "metadata": {
        "title": "_qdrant_fastembed_2-installation-and-setup",
        "source": "qdrant_fastembed\\_qdrant_fastembed_2-installation-and-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_2-installation-and-setup.md",
        "file_name": "_qdrant_fastembed_2-installation-and-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.093794",
        "total_chunks": 6
      },
      "start_char": 3869,
      "end_char": 5881
    },
    {
      "content": "|\n| py-rust-stemmers | Text stemming        | ^0.1.0                             |\n\nThe version requirements ensure compatibility with different Python versions, including the latest 3.13.\n\nSources: [pyproject.toml15-33](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L15-L33)\n\n## Dependency Architecture\n\n```\n```\n\nSources: [pyproject.toml15-33](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L15-L33)\n\n## Verifying Installation\n\nAfter installation, you can verify that FastEmbed is working correctly:\n\n```\n```\n\nOn first use, FastEmbed will automatically download the default model (typically `BAAI/bge-small-en-v1.5`). The model files are stored in a local cache directory.\n\n## Model Cache Location\n\nFastEmbed stores downloaded models in a local cache directory:\n\n| Platform | Default Cache Location                                        |\n| -------- | ------------------------------------------------------------- |\n| Linux    | `~/.cache/fastembed`                                          |\n| macOS    | `~/Library/Caches/fastembed`                                  |\n| Windows  | `C:\\Users\\<username>\\AppData\\Local\\fastembed\\fastembed\\Cache` |\n\nYou can override this location by setting the `FASTEMBED_CACHE_PATH` environment variable:\n\n```\n```\n\n## Python Version Support\n\nFastEmbed supports different Python versions with appropriate dependency configurations:\n\n```\n```\n\nSources: [pyproject.toml15-25](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L15-L25)\n\n## Common Installation Issues\n\n| Issue                                                       | Solution                                                 |\n| ----------------------------------------------------------- | -------------------------------------------------------- |\n| `ImportError: DLL load failed while importing _onnxruntime` | Ensure Visual C++ Redistributable is installed (Windows) |",
      "index": 3,
      "token_count": 409,
      "metadata": {
        "title": "_qdrant_fastembed_2-installation-and-setup",
        "source": "qdrant_fastembed\\_qdrant_fastembed_2-installation-and-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_2-installation-and-setup.md",
        "file_name": "_qdrant_fastembed_2-installation-and-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.093794",
        "total_chunks": 6
      },
      "start_char": 5781,
      "end_char": 7717
    },
    {
      "content": "d failed while importing _onnxruntime` | Ensure Visual C++ Redistributable is installed (Windows) |\n| `ModuleNotFoundError: No module named 'fastembed'`          | Verify installation with `pip list \\| grep fastembed`    |\n| `Model download failed`                                     | Check internet connection and firewall settings          |\n| Slow model downloads                                        | Configure HuggingFace token for faster downloads         |\n\n## Using HuggingFace Authentication\n\nFor faster model downloads and access to gated models, configure a HuggingFace token:\n\n```\n```\n\n## Next Steps\n\nAfter successful installation, proceed to:\n\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md) to learn about the main embedding functionality\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md) for practical implementation examples\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md) to optimize for your specific use case\n\nSources: [pyproject.toml1-33](https://github.com/qdrant/fastembed/blob/b785640b/pyproject.toml#L1-L33) [.github/workflows/python-tests.yml37-47](https://github.com/qdrant/fastembed/blob/b785640b/.github/workflows/python-tests.yml#L37-L47)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Installation and Setup](#installation-and-setup.md)\n- [System Requirements](#system-requirements.md)\n- [Installation Methods](#installation-methods.md)\n- [Using pip (Recommended)](#using-pip-recommended.md)\n- [Using Poetry](#using-poetry.md)\n- [From Source](#from-source.md)\n- [Installation Flow](#installation-flow.md)\n- [Dependencies](#dependencies.md)\n- [Dependency Architecture](#dependency-architecture.md)\n- [Verifying Installation](#verifying-installation.md)\n- [Model Cache Location](#model-cache-location.md)\n- [Python Version Support](#python-version-support.md)\n- [Common Installation Issues](#common-installation-issues.md)\n- [Using HuggingFace Authentication](#using-huggingface-authentication.md)\n- [Next Steps](#next-steps.",
      "index": 4,
      "token_count": 480,
      "metadata": {
        "title": "_qdrant_fastembed_2-installation-and-setup",
        "source": "qdrant_fastembed\\_qdrant_fastembed_2-installation-and-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_2-installation-and-setup.md",
        "file_name": "_qdrant_fastembed_2-installation-and-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.093794",
        "total_chunks": 6
      },
      "start_char": 7617,
      "end_char": 9663
    },
    {
      "content": "[Using HuggingFace Authentication](#using-huggingface-authentication.md)\n- [Next Steps](#next-steps.md)",
      "index": 5,
      "token_count": 27,
      "metadata": {
        "title": "_qdrant_fastembed_2-installation-and-setup",
        "source": "qdrant_fastembed\\_qdrant_fastembed_2-installation-and-setup.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_2-installation-and-setup.md",
        "file_name": "_qdrant_fastembed_2-installation-and-setup.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.093794",
        "total_chunks": 6
      },
      "start_char": 9563,
      "end_char": 11611
    },
    {
      "content": "Core Embedding Classes | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2041
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Core Embedding Classes\n\nRelevant source files\n\n- [NOTICE](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE)\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [docs/examples/Supported\\_Models.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb)\n- [fastembed/embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/embedding.py)\n- [fastembed/late\\_interaction\\_multimodal/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/__init__.py)\n\nThis page provides an overview of the main embedding classes in FastEmbed, which serve as primary interfaces for generating embeddings from different types of data. Each class is designed for a specific embedding strategy, offering a balance of simplicity and performance through ONNX Runtime integration.\n\nFor detailed information about how to use each class with specific examples, see [Usage Examples](qdrant/fastembed/7-usage-examples.md).\n\n## Class Hierarchy\n\nThe FastEmbed library is built around a hierarchical architecture of embedding classes, providing specialized functionality for different embedding strategies and data types.\n\n```\n```\n\nSources: [fastembed/embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/embedding.py)\n\n## Overview of Core Classes\n\nThe core embedding classes in FastEmbed can be categorized based on the type of embeddings they generate and the modalities they support:",
      "index": 1,
      "token_count": 510,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 1941,
      "end_char": 3923
    },
    {
      "content": "can be categorized based on the type of embeddings they generate and the modalities they support:\n\n| Class                              | Embedding Type   | Input Type    | Primary Use Cases                             |\n| ---------------------------------- | ---------------- | ------------- | --------------------------------------------- |\n| TextEmbedding                      | Dense            | Text          | Semantic search, document retrieval           |\n| SparseTextEmbedding                | Sparse           | Text          | Lexical search, hybrid search                 |\n| LateInteractionTextEmbedding       | Token-level      | Text          | Precise text matching, complex queries        |\n| ImageEmbedding                     | Dense            | Images        | Image search, visual similarity               |\n| LateInteractionMultimodalEmbedding | Token-level      | Text & Images | Cross-modal search, visual question answering |\n| TextCrossEncoder                   | Relevance scores | Text pairs    | Result reranking, relevance judgment          |\n\nSources: [README.md49-156](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L49-L156)\n\n## Dense Text Embeddings with TextEmbedding\n\nThe `TextEmbedding` class is the primary interface for generating dense vector representations of text. It creates fixed-length vectors that capture semantic meaning.\n\n```\n```\n\n### Key Features\n\n- Supports various embedding models with different dimensions and language capabilities\n- Automatically handles model downloading and caching\n- Provides parallel processing capabilities for handling large datasets\n- Supports GPU acceleration through ONNX Runtime\n\n### Basic Usage Example\n\n```\n```\n\nFor detailed usage examples, see [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md).\n\nSources: [README.md29-47](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L29-L47) [docs/examples/Supported\\_Models.ipynb56-129](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.",
      "index": 2,
      "token_count": 423,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 3823,
      "end_char": 5862
    },
    {
      "content": "odels.ipynb56-129](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L56-L129)\n\n## Sparse Text Embeddings with SparseTextEmbedding\n\nThe `SparseTextEmbedding` class generates sparse vector representations where most values are zero, making them efficient for large vocabularies and well-suited for lexical matching.\n\n```\n```\n\n### Key Features\n\n- Returns sparse embeddings as pairs of indices (token IDs) and values (weights)\n- Supports both neural sparse models (SPLADE) and statistical approaches (BM25)\n- Complements dense embeddings for hybrid search scenarios\n- Some models (BM25, BM42) require IDF statistics for optimal performance\n\n### Basic Usage Example\n\n```\n```\n\nFor detailed usage examples, see [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md).\n\nSources: [README.md85-99](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L85-L99) [docs/examples/Supported\\_Models.ipynb370-413](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L370-L413)\n\n## Late Interaction Text Embeddings with LateInteractionTextEmbedding\n\nThe `LateInteractionTextEmbedding` class implements the ColBERT approach, which creates token-level embeddings to enable fine-grained matching between queries and documents.\n\n```\n```\n\n### Key Features\n\n- Generates a sequence of embeddings for each token rather than a single vector\n- Allows for more precise matching by comparing individual token representations\n- Maintains context through token-level embeddings\n- Typically used with specialized retrieval approaches\n\n### Basic Usage Example\n\n```\n```\n\nFor detailed usage examples, see [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md).\n\nSources: [README.md117-136](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L117-L136) [docs/examples/Supported\\_Models.ipynb492-600](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L492-L600)\n\n## Image Embeddings with ImageEmbedding",
      "index": 3,
      "token_count": 543,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 5762,
      "end_char": 7805
    },
    {
      "content": "/b785640b/docs/examples/Supported_Models.ipynb#L492-L600)\n\n## Image Embeddings with ImageEmbedding\n\nThe `ImageEmbedding` class generates vector representations of images, allowing for image similarity search and visual retrieval.\n\n```\n```\n\n### Key Features\n\n- Supports loading images from file paths or PIL Image objects\n- Automatically handles image preprocessing (resizing, normalization)\n- Integrates with vision-language models like CLIP\n- Returns fixed-length vector representations of images\n\n### Basic Usage Example\n\n```\n```\n\nFor detailed usage examples, see [Image Embedding](qdrant/fastembed/7.4-image-embedding.md).\n\nSources: [README.md138-155](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L138-L155) [docs/examples/Supported\\_Models.ipynb603-714](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L603-L714)\n\n## Multimodal Embeddings with LateInteractionMultimodalEmbedding\n\nThe `LateInteractionMultimodalEmbedding` class enables cross-modal retrieval through models that can embed both text and images in a compatible latent space.\n\n```\n```\n\n### Key Features\n\n- Supports both text and image inputs through separate embedding methods\n- Uses late interaction approach similar to ColBERT\n- Primarily implements the ColPali model for multimodal retrieval\n- Enables complex queries involving both textual and visual elements\n\n### Basic Usage Example\n\n```\n```\n\nSources: [README.md157-176](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L157-L176) [NOTICE16-19](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L16-L19) [fastembed/late\\_interaction\\_multimodal/\\_\\_init\\_\\_.py1-5](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/__init__.py#L1-L5)\n\n## Text Cross-Encoder with TextCrossEncoder\n\nThe `TextCrossEncoder` class implements cross-encoder models for reranking search results by directly scoring the relevance of query-document pairs.\n\n```\n```\n\n### Key Features",
      "index": 4,
      "token_count": 536,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 7705,
      "end_char": 9694
    },
    {
      "content": "arch results by directly scoring the relevance of query-document pairs.\n\n```\n```\n\n### Key Features\n\n- Returns relevance scores for query-document pairs instead of embeddings\n- Designed specifically for reranking results after initial retrieval\n- More accurate but computationally expensive compared to embedding similarity\n- Efficient implementation through ONNX Runtime\n\n### Basic Usage Example\n\n```\n```\n\nSources: [README.md178-208](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L178-L208) [docs/examples/Supported\\_Models.ipynb717-836](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L717-L836)\n\n## GPU Acceleration\n\nAll the core embedding classes support GPU acceleration through ONNX Runtime's CUDA execution provider, which can significantly improve performance, especially for batch processing.\n\n### Enabling GPU Support\n\nTo use GPU acceleration:\n\n1. Install the GPU-enabled version of FastEmbed:\n\n   ```\n   ```\n\n2. Specify the CUDA execution provider when initializing the embedding model:\n\n   ```\n   ```\n\nFor detailed GPU setup instructions and troubleshooting, see [FastEmbed GPU](qdrant/fastembed/8-performance-optimization.md).\n\nSources: [README.md210-230](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L210-L230) [docs/examples/FastEmbed\\_GPU.ipynb1-42](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L1-L42) [docs/examples/FastEmbed\\_GPU.ipynb90-108](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L90-L108)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Core Embedding Classes](#core-embedding-classes.md)\n- [Class Hierarchy](#class-hierarchy.md)\n- [Overview of Core Classes](#overview-of-core-classes.md)\n- [Dense Text Embeddings with TextEmbedding](#dense-text-embeddings-with-textembedding.md)\n- [Key Features](#key-features.md)\n- [Basic Usage Example](#basic-usage-example.md)",
      "index": 5,
      "token_count": 529,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 9594,
      "end_char": 11557
    },
    {
      "content": "xtembedding.md)\n- [Key Features](#key-features.md)\n- [Basic Usage Example](#basic-usage-example.md)\n- [Sparse Text Embeddings with SparseTextEmbedding](#sparse-text-embeddings-with-sparsetextembedding.md)\n- [Key Features](#key-features-1.md)\n- [Basic Usage Example](#basic-usage-example-1.md)\n- [Late Interaction Text Embeddings with LateInteractionTextEmbedding](#late-interaction-text-embeddings-with-lateinteractiontextembedding.md)\n- [Key Features](#key-features-2.md)\n- [Basic Usage Example](#basic-usage-example-2.md)\n- [Image Embeddings with ImageEmbedding](#image-embeddings-with-imageembedding.md)\n- [Key Features](#key-features-3.md)\n- [Basic Usage Example](#basic-usage-example-3.md)\n- [Multimodal Embeddings with LateInteractionMultimodalEmbedding](#multimodal-embeddings-with-lateinteractionmultimodalembedding.md)\n- [Key Features](#key-features-4.md)\n- [Basic Usage Example](#basic-usage-example-4.md)\n- [Text Cross-Encoder with TextCrossEncoder](#text-cross-encoder-with-textcrossencoder.md)\n- [Key Features](#key-features-5.md)\n- [Basic Usage Example](#basic-usage-example-5.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Enabling GPU Support](#enabling-gpu-support.md)",
      "index": 6,
      "token_count": 331,
      "metadata": {
        "title": "_qdrant_fastembed_3-core-embedding-classes",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3-core-embedding-classes.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3-core-embedding-classes.md",
        "file_name": "_qdrant_fastembed_3-core-embedding-classes.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.124160",
        "total_chunks": 7
      },
      "start_char": 11457,
      "end_char": 13505
    },
    {
      "content": "TextEmbedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_fastembed_3.1-textembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.1-textembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.1-textembedding.md",
        "file_name": "_qdrant_fastembed_3.1-textembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.143894",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2032
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# TextEmbedding\n\nRelevant source files\n\n- [docs/Getting Started.ipynb](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb>)\n- [docs/index.md](https://github.com/qdrant/fastembed/blob/b785640b/docs/index.md)\n- [fastembed/embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/embedding.py)\n- [fastembed/text/text\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py)\n- [tests/test\\_text\\_onnx\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_text_onnx_embeddings.py)\n\nThe `TextEmbedding` class is the primary entry point for generating dense vector representations (embeddings) from text in the FastEmbed library. It provides a unified interface to multiple underlying embedding implementations while maintaining high performance through ONNX Runtime integration.\n\nFor sparse text embeddings, see [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md), and for late interaction models, see [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md).\n\n## Class Architecture\n\n```\n```\n\nSources: [fastembed/text/text\\_embedding.py16-25](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L16-L25) [fastembed/text/text\\_embedding.py79-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L79-L130)\n\n## Initialization and Configuration\n\nThe `TextEmbedding` class can be initialized with various parameters to control model selection and runtime behavior:\n\n```\n```\n\nKey initialization parameters:",
      "index": 1,
      "token_count": 527,
      "metadata": {
        "title": "_qdrant_fastembed_3.1-textembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.1-textembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.1-textembedding.md",
        "file_name": "_qdrant_fastembed_3.1-textembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.143894",
        "total_chunks": 6
      },
      "start_char": 1932,
      "end_char": 3891
    },
    {
      "content": "rameters to control model selection and runtime behavior:\n\n```\n```\n\nKey initialization parameters:\n\n| Parameter    | Type                               | Default                  | Description                                |\n| ------------ | ---------------------------------- | ------------------------ | ------------------------------------------ |\n| `model_name` | str                                | \"BAAI/bge-small-en-v1.5\" | Model identifier                           |\n| `cache_dir`  | Optional\\[str]                     | None                     | Directory to store downloaded models       |\n| `threads`    | Optional\\[int]                     | None                     | Number of threads for ONNX Runtime         |\n| `providers`  | Optional\\[Sequence\\[OnnxProvider]] | None                     | ONNX Runtime providers                     |\n| `cuda`       | bool                               | False                    | Whether to use CUDA for acceleration       |\n| `device_ids` | Optional\\[list\\[int]]              | None                     | CUDA device IDs to use                     |\n| `lazy_load`  | bool                               | False                    | Whether to load the model only when needed |\n\nSources: [fastembed/text/text\\_embedding.py79-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L79-L130) [docs/Getting Started.ipynb68-86](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb#L68-L86>)\n\n## Embedding Generation Process\n\n```\n```\n\nSources: [fastembed/text/text\\_embedding.py111-126](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L111-L126) [fastembed/text/text\\_embedding.py131-153](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L131-L153)\n\n## Core Methods\n\n### Embedding Documents\n\nThe primary method for generating embeddings is `embed()`, which accepts documents and returns an iterator of embedding vectors:\n\n```\n```\n\nThe method signature is:\n\n```\n```",
      "index": 2,
      "token_count": 462,
      "metadata": {
        "title": "_qdrant_fastembed_3.1-textembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.1-textembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.1-textembedding.md",
        "file_name": "_qdrant_fastembed_3.1-textembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.143894",
        "total_chunks": 6
      },
      "start_char": 3791,
      "end_char": 5835
    },
    {
      "content": "ocuments and returns an iterator of embedding vectors:\n\n```\n```\n\nThe method signature is:\n\n```\n```\n\nParameters:\n\n- `documents`: A single text document or an iterable of documents\n- `batch_size`: Number of documents to process at once (default: 256)\n- `parallel`: Number of parallel processes to use (if > 0)\n\nSources: [fastembed/text/text\\_embedding.py131-153](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L131-L153) [docs/Getting Started.ipynb116-121](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb#L116-L121>)\n\n### Query and Passage Embedding\n\nFor retrieval tasks, `TextEmbedding` provides specialized methods for queries and passages:\n\n```\n```\n\nThese methods are particularly useful for models that have different embedding strategies for queries versus passages.\n\nSources: [fastembed/text/text\\_embedding.py155-180](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L155-L180) [docs/index.md27-36](https://github.com/qdrant/fastembed/blob/b785640b/docs/index.md#L27-L36)\n\n### Model Management\n\nThe `TextEmbedding` class provides methods to list supported models and add custom models:\n\n```\n```\n\nSources: [fastembed/text/text\\_embedding.py26-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding.py#L26-L77)\n\n## Advanced Usage Patterns\n\n### Batch Processing\n\nFor large datasets, you can process documents in batches to conserve memory:\n\n```\n```\n\n### Parallel Processing\n\nTo speed up embedding generation, you can use parallel processing:\n\n```\n```\n\nSources: [tests/test\\_text\\_onnx\\_embeddings.py119-139](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_text_onnx_embeddings.py#L119-L139)\n\n### Lazy Loading\n\nTo conserve memory when working with multiple models, you can use lazy loading, which only loads the model when it's first used:\n\n```\n```\n\nSources: [tests/test\\_text\\_onnx\\_embeddings.py142-158](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_text_onnx_embeddings.py#L142-L158)",
      "index": 3,
      "token_count": 585,
      "metadata": {
        "title": "_qdrant_fastembed_3.1-textembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.1-textembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.1-textembedding.md",
        "file_name": "_qdrant_fastembed_3.1-textembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.143894",
        "total_chunks": 6
      },
      "start_char": 5735,
      "end_char": 7781
    },
    {
      "content": "8](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_text_onnx_embeddings.py#L142-L158)\n\n## Supported Models\n\n`TextEmbedding` supports a wide range of models, including:\n\n| Model Category        | Examples                                                                                            |\n| --------------------- | --------------------------------------------------------------------------------------------------- |\n| BGE Embeddings        | BAAI/bge-small-en-v1.5, BAAI/bge-base-en, BAAI/bge-large-en-v1.5                                    |\n| Sentence Transformers | sentence-transformers/all-MiniLM-L6-v2, sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |\n| Jina AI Models        | jinaai/jina-embeddings-v2-base-en, jinaai/jina-embeddings-v3                                        |\n| Nomic AI Models       | nomic-ai/nomic-embed-text-v1, nomic-ai/nomic-embed-text-v1.5                                        |\n| Others                | thenlper/gte-large, mixedbread-ai/mxbai-embed-large-v1, snowflake/snowflake-arctic-embed models     |\n\nEach model produces embeddings of different dimensions and with different characteristics. The default model (BAAI/bge-small-en-v1.5) produces 384-dimensional vectors.\n\nSources: [tests/test\\_text\\_onnx\\_embeddings.py10-70](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_text_onnx_embeddings.py#L10-L70) [docs/Getting Started.ipynb150-162](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb#L150-L162>)\n\n## Integration with Vector Databases\n\n`TextEmbedding` is designed to work seamlessly with vector databases like Qdrant. For detailed examples, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\n```\n```\n\nSources: [docs/index.md39-75](https://github.com/qdrant/fastembed/blob/b785640b/docs/index.md#L39-L75)\n\n## Performance Considerations\n\nThe `TextEmbedding` class is designed for high performance through:\n\n1. **ONNX Runtime**: All models are converted to ONNX format for efficient inference\n2.",
      "index": 4,
      "token_count": 525,
      "metadata": {
        "title": "_qdrant_fastembed_3.1-textembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.1-textembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.1-textembedding.md",
        "file_name": "_qdrant_fastembed_3.1-textembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.143894",
        "total_chunks": 6
      },
      "start_char": 7681,
      "end_char": 9726
    },
    {
      "content": "ce through:\n\n1. **ONNX Runtime**: All models are converted to ONNX format for efficient inference\n2. **Batch Processing**: Documents are processed in batches for memory efficiency\n3. **Parallel Processing**: Multiple CPU cores can be used for faster processing\n4. **Lazy Loading**: Models are loaded only when needed\n5. **Model Quantization**: Some models have quantized versions for faster inference\n\nFor more details on optimizing performance, see [Performance Optimization](qdrant/fastembed/8-performance-optimization.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [TextEmbedding](#textembedding.md)\n- [Class Architecture](#class-architecture.md)\n- [Initialization and Configuration](#initialization-and-configuration.md)\n- [Embedding Generation Process](#embedding-generation-process.md)\n- [Core Methods](#core-methods.md)\n- [Embedding Documents](#embedding-documents.md)\n- [Query and Passage Embedding](#query-and-passage-embedding.md)\n- [Model Management](#model-management.md)\n- [Advanced Usage Patterns](#advanced-usage-patterns.md)\n- [Batch Processing](#batch-processing.md)\n- [Parallel Processing](#parallel-processing.md)\n- [Lazy Loading](#lazy-loading.md)\n- [Supported Models](#supported-models.md)\n- [Integration with Vector Databases](#integration-with-vector-databases.md)\n- [Performance Considerations](#performance-considerations.md)",
      "index": 5,
      "token_count": 307,
      "metadata": {
        "title": "_qdrant_fastembed_3.1-textembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.1-textembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.1-textembedding.md",
        "file_name": "_qdrant_fastembed_3.1-textembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.143894",
        "total_chunks": 6
      },
      "start_char": 9626,
      "end_char": 11674
    },
    {
      "content": "SparseTextEmbedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2038
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# SparseTextEmbedding\n\nRelevant source files\n\n- [fastembed/sparse/bm25.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py)\n- [fastembed/sparse/bm42.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py)\n- [fastembed/sparse/sparse\\_text\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py)\n- [fastembed/sparse/splade\\_pp.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py)\n- [tests/test\\_attention\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py)\n- [tests/test\\_sparse\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py)\n\n## Purpose and Scope\n\nThis document covers the `SparseTextEmbedding` class, which provides functionality for generating sparse text embeddings from documents and queries. Sparse embeddings represent text as high-dimensional, sparse vectors where each dimension corresponds to a specific token/term with an associated importance score. Unlike dense embeddings (covered in [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)), sparse embeddings explicitly capture lexical information and enable efficient hybrid search capabilities.\n\nThis page explains how to use the `SparseTextEmbedding` class and the different sparse embedding models it supports (SpladePP, Bm25, and Bm42).\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L130)\n\n## Overview",
      "index": 1,
      "token_count": 535,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 1938,
      "end_char": 3902
    },
    {
      "content": "om/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L130)\n\n## Overview\n\n`SparseTextEmbedding` serves as a unified interface to different sparse embedding models. It selects the appropriate implementation based on the model name provided, allowing users to easily switch between sparse embedding approaches without changing their code.\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L91)\n\n## Supported Models\n\n`SparseTextEmbedding` supports the following sparse embedding models:\n\n| Model                                   | Implementation | Description                                      | Use Case                                           |\n| --------------------------------------- | -------------- | ------------------------------------------------ | -------------------------------------------------- |\n| prithivida/Splade\\_PP\\_en\\_v1           | SpladePP       | Neural sparse embedding model based on SPLADE++  | General-purpose sparse embeddings for English      |\n| Qdrant/bm25                             | Bm25           | Traditional BM25 as sparse vectors               | Classic lexical search with IDF weighting          |\n| Qdrant/bm42-all-minilm-l6-v2-attentions | Bm42           | BM25-inspired model using transformer attentions | Better handling of short documents and rare tokens |\n\nYou can list all supported models with:\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py19-50](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L19-L50) [fastembed/sparse/splade\\_pp.py14-32](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L14-L32) [fastembed/sparse/bm25.py46-58](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L46-L58) [fastembed/sparse/bm42.py20-36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L20-L36)\n\n## Sparse vs.",
      "index": 2,
      "token_count": 523,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 3802,
      "end_char": 5839
    },
    {
      "content": "](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L20-L36)\n\n## Sparse vs. Dense Embeddings\n\n```\n```\n\n**Key differences:**\n\n- **Dense embeddings**: Fixed-size vectors where every dimension contains a value and the meaning of each dimension is not interpretable\n- **Sparse embeddings**: High-dimensional vectors (vocabulary size) where most values are zero and each dimension corresponds to a specific token\n\nSources: [tests/test\\_sparse\\_embeddings.py10-46](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L10-L46)\n\n## Embedding Process Flow\n\n```\n```\n\nSources: [fastembed/sparse/splade\\_pp.py37-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L37-L52) [fastembed/sparse/bm25.py61-146](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L61-L146) [fastembed/sparse/bm42.py39-251](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L39-L251)\n\n## Usage\n\n### Initialization\n\nTo create a `SparseTextEmbedding` instance, specify the model name and optional parameters:\n\n```\n```\n\nCommon initialization parameters:\n\n| Parameter   | Description                                 | Default               |\n| ----------- | ------------------------------------------- | --------------------- |\n| model\\_name | The name of the model to use                | Required              |\n| cache\\_dir  | Path to cache directory                     | System temp directory |\n| threads     | Number of threads to use for inference      | None (auto)           |\n| cuda        | Whether to use CUDA for inference           | False                 |\n| device\\_ids | List of device IDs for multi-GPU processing | None                  |\n| lazy\\_load  | Whether to load the model only when needed  | False                 |\n| parallel    | Number of parallel processes for embedding  | None                  |\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py52-91](https://github.",
      "index": 3,
      "token_count": 519,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 5739,
      "end_char": 7741
    },
    {
      "content": "None                  |\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py52-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L52-L91)\n\n### Document Embedding\n\nTo generate sparse embeddings for documents:\n\n```\n```\n\nThe `embed()` method accepts the following parameters:\n\n| Parameter   | Description                                                  | Default  |\n| ----------- | ------------------------------------------------------------ | -------- |\n| documents   | String or iterable of strings to embed                       | Required |\n| batch\\_size | Batch size for processing                                    | 256      |\n| parallel    | Number of parallel processes (None, int, or 0 for all cores) | None     |\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py93-115](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L93-L115)\n\n### Query Embedding\n\nFor search applications, you can generate embeddings for queries:\n\n```\n```\n\nNote that different sparse models may handle queries differently:\n\n- SpladePP treats queries the same as documents\n- Bm25 and Bm42 use simplified query representations (token hashing with value 1.0)\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py117-129](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L117-L129) [fastembed/sparse/bm25.py298-316](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L298-L316) [fastembed/sparse/bm42.py312-333](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L312-L333)\n\n## Model Details\n\n### SpladePP\n\nSpladePP (SPLADE++) implements a neural network approach to sparse embeddings. It:\n\n1. Tokenizes input text\n2. Processes through a transformer model\n3. Applies ReLU and log transformations to produce sparse representations\n4. Extracts non-zero values and their corresponding token indices\n\nThe resulting sparse embeddings have the following characteristics:",
      "index": 4,
      "token_count": 526,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 7641,
      "end_char": 9672
    },
    {
      "content": "r corresponding token indices\n\nThe resulting sparse embeddings have the following characteristics:\n\n- Non-zero values typically represent 0.1-5% of the vocabulary\n- Value magnitude indicates token importance\n- Each token dimension corresponds to a specific token in the vocabulary\n\nSources: [fastembed/sparse/splade\\_pp.py36-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L36-L52)\n\n### Bm25\n\nBm25 implements the classic BM25 ranking function as sparse embeddings. It:\n\n1. Tokenizes input text\n2. Applies stemming and removes stopwords\n3. Calculates term frequency for each token\n4. Applies BM25 formula to compute token importance\n5. Hashes tokens to create token IDs\n\nThe BM25 formula used is:\n\n```\nscore(q, d) = SUM[ IDF(q_i) * (f(q_i, d) * (k + 1)) / (f(q_i, d) + k * (1 - b + b * (|d| / avg_len))) ]\n```\n\nWhere:\n\n- IDF is computed on Qdrant's side using the `idf` modifier\n- f(q\\_i, d) is the term frequency\n- k and b are configurable parameters (defaults: k=1.2, b=0.75)\n- avg\\_len is the average document length (default: 256.0)\n\nNote: Bm25 supports multiple languages through different stemmers.\n\nSources: [fastembed/sparse/bm25.py61-146](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L61-L146) [fastembed/sparse/bm25.py264-291](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L264-L291)\n\n### Bm42\n\nBm42 is an extension of BM25 that leverages transformer attention weights. It:\n\n1. Tokenizes input text\n2. Processes through a transformer model to get attention weights\n3. Reconstructs subword tokens into words\n4. Applies stemming and removes stopwords\n5. Uses attention weights to evaluate token importance\n6. Hashes tokens to create token IDs\n\nKey advantages:\n\n- Works better with short documents where term frequency is less informative\n- Better handles rare tokens\n- Leverages semantic understanding from the transformer model\n\nSources: [fastembed/sparse/bm42.py39-250](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.",
      "index": 5,
      "token_count": 601,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 9572,
      "end_char": 11618
    },
    {
      "content": "embed/sparse/bm42.py39-250](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L39-L250)\n\n## Parallel Processing\n\nAll sparse embedding models support parallel processing for faster embedding generation with large datasets:\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py93-115](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L93-L115) [tests/test\\_sparse\\_embeddings.py96-124](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L96-L124)\n\n## Implementation Architecture\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L91) [fastembed/sparse/sparse\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_embedding_base.py)\n\n## Integration with Vector Databases\n\nSparse embeddings are particularly useful for hybrid search when combined with vector databases. For example, with Qdrant:\n\n```\n```\n\nNote: The Bm25 and Bm42 models are designed to be used with the `idf` modifier in Qdrant to fully implement the BM25/BM42 scoring function.\n\nSources: [fastembed/sparse/bm25.py62-74](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L62-L74) [fastembed/sparse/bm42.py52-56](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L52-L56)\n\n## Performance and Memory Considerations\n\nSparse embeddings offer several advantages over dense embeddings:\n\n1. **Storage efficiency** for large vocabularies since only non-zero values are stored\n2. **Interpretability** as each dimension corresponds to a specific token\n3. **Exact term matching** which can be important for specialized terms\n\nHowever, they also have some considerations:\n\n1. **Processing overhead** for generating sparse vectors (especially for neural models)\n2. **Index size** can be larger than dense embeddings in some cases\n3.",
      "index": 6,
      "token_count": 570,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 11518,
      "end_char": 13516
    },
    {
      "content": "especially for neural models)\n2. **Index size** can be larger than dense embeddings in some cases\n3. **Model selection** should match your data characteristics (e.g., document length)\n\nSources: [tests/test\\_sparse\\_embeddings.py52-94](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L52-L94) [tests/test\\_attention\\_embeddings.py10-96](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L10-L96)\n\n## Related Pages\n\n- For dense text embeddings, see [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- For late interaction embeddings, see [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- For more details on implementation, see [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- For integration with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [SparseTextEmbedding](#sparsetextembedding.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [Overview](#overview.md)\n- [Supported Models](#supported-models.md)\n- [Sparse vs. Dense Embeddings](#sparse-vs-dense-embeddings.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Usage](#usage.md)\n- [Initialization](#initialization.md)\n- [Document Embedding](#document-embedding.md)\n- [Query Embedding](#query-embedding.md)\n- [Model Details](#model-details.md)\n- [SpladePP](#spladepp.md)\n- [Bm25](#bm25.md)\n- [Bm42](#bm42.md)\n- [Parallel Processing](#parallel-processing.md)\n- [Implementation Architecture](#implementation-architecture.md)\n- [Integration with Vector Databases](#integration-with-vector-databases.md)\n- [Performance and Memory Considerations](#performance-and-memory-considerations.md)\n- [Related Pages](#related-pages.md)",
      "index": 7,
      "token_count": 501,
      "metadata": {
        "title": "_qdrant_fastembed_3.2-sparsetextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.2-sparsetextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "file_name": "_qdrant_fastembed_3.2-sparsetextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.175295",
        "total_chunks": 8
      },
      "start_char": 13416,
      "end_char": 15464
    },
    {
      "content": "LateInteractionTextEmbedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 622,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2047
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# LateInteractionTextEmbedding\n\nRelevant source files\n\n- [fastembed/late\\_interaction/colbert.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py)\n- [fastembed/late\\_interaction/jina\\_colbert.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py)\n- [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py)\n- [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py)\n- [fastembed/sparse/utils/tokenizer.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/utils/tokenizer.py)\n\n## Purpose and Scope\n\nThe `LateInteractionTextEmbedding` class provides access to late-interaction embedding models in FastEmbed. Unlike traditional dense embeddings that aggregate text into a single vector, late-interaction models preserve token-level embeddings, enabling more precise matching between queries and documents. This page documents the implementation, architecture, and usage of `LateInteractionTextEmbedding` and its associated components.\n\nFor information about dense text embeddings (single vector representation), see [TextEmbedding](qdrant/fastembed/3.1-textembedding.md). For sparse text embeddings, see [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md).\n\n## Overview\n\nLate-interaction models represent documents and queries as sequences of token-level embeddings rather than single vectors.",
      "index": 1,
      "token_count": 490,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 1947,
      "end_char": 3945
    },
    {
      "content": "s represent documents and queries as sequences of token-level embeddings rather than single vectors. These models defer the interaction between query and document tokens until retrieval time, allowing for more fine-grained matching that can capture local context more effectively than global embeddings.\n\n```\n```\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py107-119](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py#L107-L119) [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py30-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py#L30-L60)\n\n## Architecture\n\n`LateInteractionTextEmbedding` serves as a facade over specific late-interaction model implementations, currently supporting ColBERT and Jina ColBERT models.\n\n```\n```\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py14-15](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py#L14-L15) [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py8-9](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py#L8-L9) [fastembed/late\\_interaction/colbert.py39](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L39) [fastembed/late\\_interaction/jina\\_colbert.py21](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L21-L21)\n\n## Key Components\n\n### LateInteractionTextEmbedding\n\nThe main entry point class that users interact with to access late-interaction embedding functionality. It:\n\n- Provides a unified interface for various late-interaction model implementations\n- Delegates embedding operations to the appropriate model implementation based on the model name\n- Supports listing available models and their metadata",
      "index": 2,
      "token_count": 498,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 3845,
      "end_char": 5828
    },
    {
      "content": "odel implementation based on the model name\n- Supports listing available models and their metadata\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py14-120](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py#L14-L120)\n\n### LateInteractionTextEmbeddingBase\n\nThe abstract base class that defines the contract for late-interaction embedding implementations. It:\n\n- Inherits from `ModelManagement` to handle model downloading and caching\n- Defines abstract methods that implementations must provide (`embed`, `query_embed`)\n- Provides a default implementation for `passage_embed` that routes to `embed`\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py#L8-L60)\n\n### Specific Model Implementations\n\n#### Colbert\n\nImplementation for the ColBERT late-interaction model. It:\n\n- Handles tokenization and ONNX model loading\n- Processes documents and queries differently (with different marker tokens)\n- Performs token-level normalization for document embeddings\n- Supports query augmentation with mask tokens\n\nSources: [fastembed/late\\_interaction/colbert.py39-204](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L204)\n\n#### JinaColbert\n\nExtended implementation of ColBERT for Jina AI's variant. It:\n\n- Uses different marker token IDs\n- Applies special attention mask handling for queries\n- Supports longer context lengths (up to 8192 tokens)\n\nSources: [fastembed/late\\_interaction/jina\\_colbert.py21-48](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L21-L48)\n\n## Embedding Process\n\nThe late-interaction embedding process differs between queries and documents:\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.",
      "index": 3,
      "token_count": 501,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 5728,
      "end_char": 7752
    },
    {
      "content": "lbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65) [fastembed/late\\_interaction/colbert.py67-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L67-L77) [fastembed/late\\_interaction/colbert.py79-104](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L79-L104)\n\n## Supported Models\n\nThe `LateInteractionTextEmbedding` class currently supports the following models:\n\n| Model Name                            | Dimension | Description                                                                                   | License      | Size (GB) |\n| ------------------------------------- | --------- | --------------------------------------------------------------------------------------------- | ------------ | --------- |\n| colbert-ir/colbertv2.0                | 128       | Late interaction model                                                                        | MIT          | 0.44      |\n| answerdotai/answerai-colbert-small-v1 | 96        | Text embeddings, Unimodal (text), Multilingual (\\~100 languages), 512 input tokens truncation | Apache-2.0   | 0.13      |\n| jinaai/jina-colbert-v2                | 128       | Multilingual model with 8192 context length                                                   | cc-by-nc-4.0 | 2.24      |\n\nSources: [fastembed/late\\_interaction/colbert.py17-36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L17-L36) [fastembed/late\\_interaction/jina\\_colbert.py7-18](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L7-L18)\n\n## Usage\n\n### Basic Usage\n\n```\n```\n\n### Advanced Usage with Parameters\n\n```\n```\n\n## Implementation Details\n\n### Special Token Handling\n\nLate-interaction models like ColBERT use special marker tokens to differentiate between queries and documents:\n\n| Model        | Query Marker Token ID | Document Marker Token ID | Mask Token |",
      "index": 4,
      "token_count": 514,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 7652,
      "end_char": 9677
    },
    {
      "content": "es and documents:\n\n| Model        | Query Marker Token ID | Document Marker Token ID | Mask Token |\n| ------------ | --------------------- | ------------------------ | ---------- |\n| ColBERT      | 1                     | 2                        | \\[MASK]    |\n| Jina ColBERT | 250002                | 250003                   | \\<mask>    |\n\nThese markers are inserted at the beginning of the token sequence to inform the model about the input type.\n\nSources: [fastembed/late\\_interaction/colbert.py40-43](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L40-L43) [fastembed/late\\_interaction/jina\\_colbert.py22-25](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L22-L25)\n\n### Query Augmentation\n\nFor queries shorter than a minimum length (31 tokens for ColBERT), the implementation pads the query with mask tokens. This query augmentation technique improves retrieval performance by providing additional context signals.\n\nSources: [fastembed/late\\_interaction/colbert.py86-104](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L86-L104)\n\n### Token-level Normalization\n\nFor document embeddings, the implementation:\n\n1. Applies the attention mask to zero out padding and punctuation tokens\n2. Normalizes each token embedding to unit length (L2 normalization)\n\nThis ensures that similarity calculations during retrieval focus on meaningful tokens and are properly scaled.\n\nSources: [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65)\n\n### Parallel Processing\n\nFor large-scale embedding tasks, the implementation supports parallel processing using worker processes, each with its own ONNX runtime session. This significantly improves throughput when processing large document collections.\n\nSources: [fastembed/late\\_interaction/colbert.py211-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.",
      "index": 5,
      "token_count": 505,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 9577,
      "end_char": 11623
    },
    {
      "content": "ert.py211-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L211-L253) [fastembed/late\\_interaction/colbert.py256-263](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L256-L263)\n\n## Performance Considerations\n\n- Late-interaction models produce token-level embeddings, resulting in larger storage requirements compared to dense embeddings\n- The fine-grained matching requires specialized retrieval algorithms (not covered by this class, which handles only the embedding generation)\n- Setting appropriate batch sizes and enabling parallel processing can significantly improve throughput for large document collections\n\n## Related Components\n\nFor more information about other embedding approaches in FastEmbed:\n\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md) - Dense text embeddings\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md) - Sparse text embeddings\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md) - Late interaction for text and images\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [LateInteractionTextEmbedding](#lateinteractiontextembedding.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [Overview](#overview.md)\n- [Architecture](#architecture.md)\n- [Key Components](#key-components.md)\n- [LateInteractionTextEmbedding](#lateinteractiontextembedding-1.md)\n- [LateInteractionTextEmbeddingBase](#lateinteractiontextembeddingbase.md)\n- [Specific Model Implementations](#specific-model-implementations.md)\n- [Colbert](#colbert.md)\n- [JinaColbert](#jinacolbert.md)\n- [Embedding Process](#embedding-process.md)\n- [Supported Models](#supported-models.md)\n- [Usage](#usage.md)\n- [Basic Usage](#basic-usage.md)\n- [Advanced Usage with Parameters](#advanced-usage-with-parameters.md)\n- [Implementation Details](#implementation-details.md)\n- [Special Token Handling](#special-token-handling.md)\n- [Query Augmentation](#query-augmentation.md)",
      "index": 6,
      "token_count": 511,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 11523,
      "end_char": 13555
    },
    {
      "content": "[Special Token Handling](#special-token-handling.md)\n- [Query Augmentation](#query-augmentation.md)\n- [Token-level Normalization](#token-level-normalization.md)\n- [Parallel Processing](#parallel-processing.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Related Components](#related-components.md)",
      "index": 7,
      "token_count": 72,
      "metadata": {
        "title": "_qdrant_fastembed_3.3-lateinteractiontextembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "file_name": "_qdrant_fastembed_3.3-lateinteractiontextembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.209579",
        "total_chunks": 8
      },
      "start_char": 13455,
      "end_char": 15503
    },
    {
      "content": "ImageEmbedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_fastembed_3.4-imageembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.4-imageembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.4-imageembedding.md",
        "file_name": "_qdrant_fastembed_3.4-imageembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.233975",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2033
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# ImageEmbedding\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [fastembed/image/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py)\n\n## Purpose and Scope\n\nThe `ImageEmbedding` class provides a high-level interface for generating vector representations (embeddings) from images. These embeddings can be used for various tasks including image search, image similarity, and multimodal applications. The class offers a simplified API while leveraging ONNX Runtime for efficient inference.\n\nFor text embedding functionality, see [TextEmbedding](qdrant/fastembed/3.1-textembedding.md). For multimodal embedding that combines both text and images, see [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md).\n\nSources: [README.md137-156](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L137-L156)\n\n## Architecture Overview\n\n`ImageEmbedding` serves as an entry point class that abstracts the underlying implementation details of image embedding models. The class follows FastEmbed's pattern of providing a clean, user-friendly interface while leveraging optimized implementations underneath.\n\n### Class Hierarchy\n\n```\n```\n\nSources: [fastembed/image/onnx\\_embedding.py1-10](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L1-L10)\n\n## Supported Models\n\n`ImageEmbedding` supports several pre-trained models optimized for ONNX runtime:",
      "index": 1,
      "token_count": 512,
      "metadata": {
        "title": "_qdrant_fastembed_3.4-imageembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.4-imageembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.4-imageembedding.md",
        "file_name": "_qdrant_fastembed_3.4-imageembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.233975",
        "total_chunks": 6
      },
      "start_char": 1933,
      "end_char": 3924
    },
    {
      "content": "Supported Models\n\n`ImageEmbedding` supports several pre-trained models optimized for ONNX runtime:\n\n| Model                       | Dimensions | Description                              | Year | License    |\n| --------------------------- | ---------- | ---------------------------------------- | ---- | ---------- |\n| Qdrant/clip-ViT-B-32-vision | 512        | Multimodal (text & image)                | 2021 | MIT        |\n| Qdrant/resnet50-onnx        | 2048       | Unimodal (image only)                    | 2016 | Apache-2.0 |\n| Qdrant/Unicom-ViT-B-16      | 768        | Multimodal with detailed representations | 2023 | Apache-2.0 |\n| Qdrant/Unicom-ViT-B-32      | 512        | Multimodal (text & image)                | 2023 | Apache-2.0 |\n| jinaai/jina-clip-v1         | 768        | Multimodal (text & image)                | 2024 | Apache-2.0 |\n\nSources: [fastembed/image/onnx\\_embedding.py13-59](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L13-L59)\n\n## Basic Usage\n\nUsing `ImageEmbedding` is straightforward:\n\n```\n```\n\nSources: [README.md137-156](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L137-L156)\n\n## Embedding Process\n\nThe image embedding process consists of several steps:\n\n```\n```\n\nSources: [fastembed/image/onnx\\_embedding.py148-181](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L148-L181)\n\n## Class Initialization Parameters\n\nWhen initializing an `ImageEmbedding` instance, you can customize its behavior with several parameters:\n\n```\n```\n\n### Key Parameters\n\n- `model_name`: The name of the embedding model to use\n- `cache_dir`: Custom location for storing downloaded models\n- `providers`: ONNX runtime providers (e.g., \"CUDAExecutionProvider\" for GPU)\n- `cuda`: Enable CUDA for inference\n- `device_ids`: List of GPU device IDs for parallel processing\n\nSources: [fastembed/image/onnx\\_embedding.py63-96](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L63-L96)\n\n## GPU Acceleration",
      "index": 2,
      "token_count": 588,
      "metadata": {
        "title": "_qdrant_fastembed_3.4-imageembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.4-imageembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.4-imageembedding.md",
        "file_name": "_qdrant_fastembed_3.4-imageembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.233975",
        "total_chunks": 6
      },
      "start_char": 3824,
      "end_char": 5861
    },
    {
      "content": "com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L63-L96)\n\n## GPU Acceleration\n\n`ImageEmbedding` supports GPU acceleration through ONNX Runtime's CUDA Execution Provider. To use GPU acceleration:\n\n1. Install the GPU version of FastEmbed:\n\n   ```\n   ```\n\n2. Initialize the model with GPU support:\n\n   ```\n   ```\n\nSources: [docs/examples/FastEmbed\\_GPU.ipynb1-108](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L1-L108) [docs/examples/FastEmbed\\_GPU.ipynb184-229](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L184-L229)\n\n## Embedding Method Parameters\n\nThe `embed` method accepts several parameters to control the embedding process:\n\n```\n```\n\n### Key Parameters\n\n- `images`: List of image paths or image objects to embed\n\n- `batch_size`: Number of images to process in a single batch (higher values use more memory)\n\n- `parallel`: Number of parallel workers for data-parallel processing\n\n  - If > 1, that many workers will be used\n  - If 0, all available cores will be used\n  - If None, parallel processing is disabled\n\nSources: [fastembed/image/onnx\\_embedding.py148-169](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L148-L169)\n\n## Implementation Details\n\n### Embedding Process Flow\n\n```\n```\n\nSources: [fastembed/image/onnx\\_embedding.py148-181](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L148-L181) [fastembed/image/onnx\\_embedding.py125-136](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L125-L136) [fastembed/image/onnx\\_embedding.py187-197](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L187-L197)\n\n### Normalization\n\nAfter obtaining raw embeddings from the model, they are normalized to unit length to ensure consistent similarity calculations.\n\nSources: [fastembed/image/onnx\\_embedding.py196-197](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.",
      "index": 3,
      "token_count": 604,
      "metadata": {
        "title": "_qdrant_fastembed_3.4-imageembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.4-imageembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.4-imageembedding.md",
        "file_name": "_qdrant_fastembed_3.4-imageembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.233975",
        "total_chunks": 6
      },
      "start_char": 5761,
      "end_char": 7798
    },
    {
      "content": "bedding.py196-197](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L196-L197)\n\n## Integration with Qdrant\n\nFastEmbed's `ImageEmbedding` can be easily integrated with Qdrant vector database for image search applications:\n\n```\n```\n\nFor more detailed information on using FastEmbed with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nSources: [README.md232-280](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L232-L280)\n\n## Performance Considerations\n\n- **Batch Size**: Larger batch sizes generally improve throughput but increase memory usage.\n- **Parallel Processing**: For large datasets, enabling parallel processing (`parallel > 0`) can significantly improve performance.\n- **GPU Acceleration**: Using GPU acceleration can provide substantial speedups, especially for batch processing.\n- **Model Selection**: Different models offer different trade-offs between accuracy, embedding dimension, and speed.\n\nSources: [docs/examples/FastEmbed\\_GPU.ipynb398-512](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L398-L512)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [ImageEmbedding](#imageembedding.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Class Hierarchy](#class-hierarchy.md)\n- [Supported Models](#supported-models.md)\n- [Basic Usage](#basic-usage.md)\n- [Embedding Process](#embedding-process.md)\n- [Class Initialization Parameters](#class-initialization-parameters.md)\n- [Key Parameters](#key-parameters.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Embedding Method Parameters](#embedding-method-parameters.md)\n- [Key Parameters](#key-parameters-1.md)\n- [Implementation Details](#implementation-details.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Normalization](#normalization.md)\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Performance Considerations](#performance-considerations.md)",
      "index": 4,
      "token_count": 524,
      "metadata": {
        "title": "_qdrant_fastembed_3.4-imageembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.4-imageembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.4-imageembedding.md",
        "file_name": "_qdrant_fastembed_3.4-imageembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.233975",
        "total_chunks": 6
      },
      "start_char": 7698,
      "end_char": 9746
    },
    {
      "content": "(#integration-with-qdrant.md)\n- [Performance Considerations](#performance-considerations.md)",
      "index": 5,
      "token_count": 21,
      "metadata": {
        "title": "_qdrant_fastembed_3.4-imageembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.4-imageembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.4-imageembedding.md",
        "file_name": "_qdrant_fastembed_3.4-imageembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.233975",
        "total_chunks": 6
      },
      "start_char": 9646,
      "end_char": 11694
    },
    {
      "content": "LateInteractionMultimodalEmbedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)",
      "index": 0,
      "token_count": 610,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2014
    },
    {
      "content": "d-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# LateInteractionMultimodalEmbedding\n\nRelevant source files\n\n- [fastembed/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/__init__.py)\n- [fastembed/late\\_interaction\\_multimodal/colpali.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py)\n- [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py)\n- [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding_base.py)\n- [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py)\n- [tests/test\\_late\\_interaction\\_multimodal.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py)\n\n## Purpose and Overview\n\nThe `LateInteractionMultimodalEmbedding` class provides a unified interface for generating and working with late interaction embeddings across multiple modalities (specifically text and images). Late interaction embedding models produce token-level embeddings rather than single dense vectors, enabling more precise matching between queries and documents. The multimodal capability allows for cross-modal retrieval applications like searching images with text queries or vice versa.",
      "index": 1,
      "token_count": 513,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 1914,
      "end_char": 3901
    },
    {
      "content": "lows for cross-modal retrieval applications like searching images with text queries or vice versa.\n\nFor text-only late interaction embeddings, see [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md).\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py1-131](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L1-L131) [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py1-68](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding_base.py#L1-L68)\n\n## Architecture\n\nThe `LateInteractionMultimodalEmbedding` system is designed with a factory pattern that delegates to specialized implementations based on the selected model. Currently, the system supports the ColPali model, but the architecture allows for easy extension with additional multimodal late interaction models.\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py14-16](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L14-L16) [fastembed/late\\_interaction\\_multimodal/colpali.py34-131](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L34-L131) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py20-83](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L20-L83)\n\n## Embedding Process Flow\n\nThe late interaction multimodal embedding process uses specialized token-level encoders for both text and images, combining the power of late interaction with multimodal capabilities.\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py86-130](https://github.",
      "index": 2,
      "token_count": 529,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 3801,
      "end_char": 5786
    },
    {
      "content": "bed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py86-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L86-L130) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py86-223](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L86-L223)\n\n## Supported Models\n\nCurrently, the `LateInteractionMultimodalEmbedding` class supports the following model:\n\n| Model                    | Dimensions | Description                                                                            | License | Size   |\n| ------------------------ | ---------- | -------------------------------------------------------------------------------------- | ------- | ------ |\n| Qdrant/colpali-v1.3-fp16 | 128        | Text embeddings, Multimodal (text & image), English, 50 tokens query length truncation | MIT     | 6.5 GB |\n\nThe ColPali model is designed for late interaction retrieval between text and images, enabling efficient cross-modal search capabilities.\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py20-31](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L20-L31)\n\n## Implementation Details\n\n### ColPali\n\nThe ColPali implementation uses an ONNX-optimized model architecture that captures token-level embeddings for both text and images. It has several important components:\n\n1. **Text Processing**:\n\n   - Adds specific prefix and tokens to text queries\n   - Tokenizes text with a specialized tokenizer\n   - Preprocesses tokenized text for ONNX inference\n\n2. **Image Processing**:\n\n   - Uses a standardized image preprocessor\n   - Converts images to the expected format (3x448x448)\n   - Adds placeholders for text when processing images\n\n3. **Model Output**:\n\n   - Token-level embeddings rather than a single vector\n   - Preserves contextual information for more precise matching",
      "index": 3,
      "token_count": 482,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 5686,
      "end_char": 7700
    },
    {
      "content": "ddings rather than a single vector\n   - Preserves contextual information for more precise matching\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py34-190](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L34-L190)\n\n### Parallel Processing Support\n\nThe system supports parallel processing for both text and image embedding to improve throughput when dealing with large datasets:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py113-223](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L113-L223) [fastembed/late\\_interaction\\_multimodal/colpali.py275-300](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L275-L300)\n\n## Usage\n\n### Initializing the Embedder\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py54-84](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L54-L84)\n\n### Embedding Text\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py86-107](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L86-L107)\n\n### Embedding Images\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py109-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L109-L130) [tests/test\\_late\\_interaction\\_multimodal.py40-45](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_late_interaction_multimodal.py#L40-L45)\n\n## Key Parameters\n\n| Parameter    | Description                                        | Default                 |\n| ------------ | -------------------------------------------------- | ----------------------- |",
      "index": 4,
      "token_count": 580,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 7600,
      "end_char": 9628
    },
    {
      "content": "|\n| ------------ | -------------------------------------------------- | ----------------------- |\n| `model_name` | The name of the multimodal model to use            | Required                |\n| `cache_dir`  | Directory to cache downloaded models               | System temp directory   |\n| `threads`    | Number of threads for single ONNX session          | None (auto)             |\n| `cuda`       | Whether to use CUDA for inference                  | False                   |\n| `device_ids` | List of device IDs for parallel processing         | None                    |\n| `lazy_load`  | Whether to load the model on demand                | False                   |\n| `batch_size` | Number of items to process together                | 256 (text), 16 (images) |\n| `parallel`   | Number of worker processes for parallel processing | None                    |\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py54-84](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L54-L84) [fastembed/late\\_interaction\\_multimodal/colpali.py46-78](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L46-L78)\n\n## Notes on Token-Level Embeddings\n\nUnlike traditional dense embeddings that produce a single vector per input, late interaction models like ColPali produce token-level embeddings. This means:\n\n1. Each token in the text or image gets its own embedding vector\n2. The output shape is (batch\\_size, sequence\\_length, embedding\\_dimension)\n3. These token-level embeddings enable more fine-grained matching between queries and documents\n\nFor multimodal late interaction, this token-level approach allows precise matching between text tokens and image regions, enabling more accurate cross-modal retrieval.\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py129-160](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.",
      "index": 5,
      "token_count": 474,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 9528,
      "end_char": 11569
    },
    {
      "content": "60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L129-L160)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [LateInteractionMultimodalEmbedding](#lateinteractionmultimodalembedding.md)\n- [Purpose and Overview](#purpose-and-overview.md)\n- [Architecture](#architecture.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Supported Models](#supported-models.md)\n- [Implementation Details](#implementation-details.md)\n- [ColPali](#colpali.md)\n- [Parallel Processing Support](#parallel-processing-support.md)\n- [Usage](#usage.md)\n- [Initializing the Embedder](#initializing-the-embedder.md)\n- [Embedding Text](#embedding-text.md)\n- [Embedding Images](#embedding-images.md)\n- [Key Parameters](#key-parameters.md)\n- [Notes on Token-Level Embeddings](#notes-on-token-level-embeddings.md)",
      "index": 6,
      "token_count": 236,
      "metadata": {
        "title": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "file_name": "_qdrant_fastembed_3.5-lateinteractionmultimodalembedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.266166",
        "total_chunks": 7
      },
      "start_char": 11469,
      "end_char": 13517
    },
    {
      "content": "TextCrossEncoder | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2035
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# TextCrossEncoder\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [fastembed/rerank/cross\\_encoder/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/__init__.py)\n- [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py)\n- [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py)\n- [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder_base.py)\n\n## Purpose and Overview\n\nThe `TextCrossEncoder` class provides reranking functionality within the FastEmbed library. Unlike embedding models that convert text to vectors for similarity comparison, cross encoders directly score the relevance between a query and a document. This makes them particularly useful for improving search results by reranking candidate documents retrieved through initial vector similarity search.\n\nFor information about generating embeddings for retrieval, see [TextEmbedding](qdrant/fastembed/3.1-textembedding.md).\n\nSources: [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py15-164](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L15-L164) [README.md177-191](https://github.",
      "index": 1,
      "token_count": 559,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 1935,
      "end_char": 3947
    },
    {
      "content": "0b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L15-L164) [README.md177-191](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L177-L191)\n\n## Architecture\n\nThe `TextCrossEncoder` system follows a layered architecture that enables extensibility while maintaining a simple interface for users.\n\n```\n```\n\nThe key components are:\n\n1. **TextCrossEncoder**: The main entry point class that users interact with. It acts as a façade to the underlying implementations.\n\n2. **TextCrossEncoderBase**: An abstract base class that defines the interface for text cross encoders.\n\n3. **OnnxTextCrossEncoder**: The primary implementation that uses ONNX Runtime for efficient inference.\n\n4. **CustomTextCrossEncoder**: An implementation for user-defined custom models.\n\n5. **TextRerankerWorker/TextCrossEncoderWorker**: Worker classes for parallel processing of reranking tasks.\n\nSources: [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py15-164](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L15-L164) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py67-215](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py#L67-L215) [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder\\_base.py7-59](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder_base.py#L7-L59)\n\n## Reranking Process\n\nThe reranking process in `TextCrossEncoder` involves several steps from initialization to scoring documents:\n\n```\n```\n\nDuring reranking:\n\n1. The query and documents are combined into pairs\n2. These pairs are tokenized and processed in batches\n3. Each batch is run through the cross-encoder model\n4. The model outputs relevance scores for each query-document pair\n5. Scores are returned to the user, with higher scores indicating greater relevance\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py154-174](https://github.",
      "index": 2,
      "token_count": 535,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 3847,
      "end_char": 5852
    },
    {
      "content": "nce\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py154-174](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py#L154-L174) [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py86-99](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L86-L99)\n\n## Supported Models\n\nThe `TextCrossEncoder` class supports a variety of models optimized for reranking tasks:\n\n| Model Name                                | Description                                                                 | Size (GB) | License      |\n| ----------------------------------------- | --------------------------------------------------------------------------- | --------- | ------------ |\n| Xenova/ms-marco-MiniLM-L-6-v2             | MiniLM-L-6-v2 model optimized for re-ranking tasks                          | 0.08      | apache-2.0   |\n| Xenova/ms-marco-MiniLM-L-12-v2            | MiniLM-L-12-v2 model optimized for re-ranking tasks                         | 0.12      | apache-2.0   |\n| BAAI/bge-reranker-base                    | BGE reranker base model for cross-encoder re-ranking                        | 1.04      | mit          |\n| jinaai/jina-reranker-v1-tiny-en           | Blazing-fast re-ranking with 8K context length, fewer parameters than turbo | 0.13      | apache-2.0   |\n| jinaai/jina-reranker-v1-turbo-en          | Blazing-fast re-ranking with 8K context length                              | 0.15      | apache-2.0   |\n| jinaai/jina-reranker-v2-base-multilingual | Multi-lingual reranker with 1K context length and sliding window            | 1.11      | cc-by-nc-4.0 |\n\nYou can get a list of all supported models programmatically:\n\n```\n```\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py15-63](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py#L15-L63) [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py21-51](https://github.",
      "index": 3,
      "token_count": 558,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 5752,
      "end_char": 7796
    },
    {
      "content": "s_encoder.py#L15-L63) [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py21-51](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L21-L51)\n\n## Usage Examples\n\n### Basic Reranking\n\nThe most common use case is to rerank a list of documents based on a query:\n\n```\n```\n\n### Scoring Text Pairs\n\nYou can also directly score pairs of texts for relevance:\n\n```\n```\n\n### Batching and Parallelization\n\nFor large datasets, you can control the batch size and enable parallel processing:\n\n```\n```\n\nSources: [README.md177-191](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L177-L191) [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py86-132](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L86-L132)\n\n## GPU Acceleration\n\n`TextCrossEncoder` supports GPU acceleration through ONNX Runtime's CUDA execution provider. To enable GPU acceleration:\n\n1. Install the GPU version of FastEmbed:\n\n   ```\n   ```\n\n2. Specify the CUDA execution provider when initializing the cross encoder:\n\n   ```\n   ```\n\nAlternatively, you can use the `cuda` parameter:\n\n```\n```\n\nFor systems with multiple GPUs, you can specify which devices to use:\n\n```\n```\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py77-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py#L77-L130) [docs/examples/FastEmbed\\_GPU.ipynb9-194](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L9-L194)\n\n## Integration with Search Systems\n\n```\n```\n\nA typical integration of `TextCrossEncoder` in a search system involves:\n\n1. Using embeddings (e.g., `TextEmbedding`) to retrieve initial candidate documents based on vector similarity\n2. Applying `TextCrossEncoder` to rerank these candidates based on their relevance to the query\n3. Returning the reranked results to the user",
      "index": 4,
      "token_count": 551,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 7696,
      "end_char": 9653
    },
    {
      "content": "ese candidates based on their relevance to the query\n3. Returning the reranked results to the user\n\nThis two-stage approach combines the efficiency of vector search for initial retrieval with the accuracy of cross-encoders for final ranking.\n\nSources: [README.md177-191](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L177-L191)\n\n## Extending with Custom Models\n\nYou can add your own custom cross encoder models that aren't in the default supported list:\n\n```\n```\n\nWhen adding a custom model, you need to provide:\n\n- The model name identifier\n- The model file path within the model directory\n- The source of the model (typically Hugging Face repository)\n- Optional metadata like description, license, and size\n\nSources: [fastembed/rerank/cross\\_encoder/text\\_cross\\_encoder.py134-163](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/text_cross_encoder.py#L134-L163) [README.md193-208](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L193-L208)\n\n## Technical Implementation\n\nThe underlying implementation of `TextCrossEncoder` relies on the ONNX Runtime for efficient inference. When a reranking operation is performed:\n\n1. The model first tokenizes the input text pairs\n2. The tokenized inputs are processed by the ONNX model\n3. The model outputs relevance scores, which are then returned to the user\n\nUnlike embedding models that produce vectors, cross encoders directly output a scalar relevance score for each input pair. This makes them more accurate for ranking tasks but less efficient for large-scale retrieval, which is why they're typically used in combination with embedding models in a two-stage retrieval system.\n\nCurrently, parallel execution with multiple GPUs is not fully supported for cross encoders, as noted in the warning message in the code:\n\n> \"Parallel execution is currently not supported for cross encoders, only the first device will be used for inference\"\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py119-123](https://github.",
      "index": 5,
      "token_count": 489,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 9553,
      "end_char": 11590
    },
    {
      "content": "ce\"\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py119-123](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py#L119-L123) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_cross\\_encoder.py199-200](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_cross_encoder.py#L199-L200)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [TextCrossEncoder](#textcrossencoder.md)\n- [Purpose and Overview](#purpose-and-overview.md)\n- [Architecture](#architecture.md)\n- [Reranking Process](#reranking-process.md)\n- [Supported Models](#supported-models.md)\n- [Usage Examples](#usage-examples.md)\n- [Basic Reranking](#basic-reranking.md)\n- [Scoring Text Pairs](#scoring-text-pairs.md)\n- [Batching and Parallelization](#batching-and-parallelization.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Integration with Search Systems](#integration-with-search-systems.md)\n- [Extending with Custom Models](#extending-with-custom-models.md)\n- [Technical Implementation](#technical-implementation.md)",
      "index": 6,
      "token_count": 329,
      "metadata": {
        "title": "_qdrant_fastembed_3.6-textcrossencoder",
        "source": "qdrant_fastembed\\_qdrant_fastembed_3.6-textcrossencoder.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "file_name": "_qdrant_fastembed_3.6-textcrossencoder.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.404019",
        "total_chunks": 7
      },
      "start_char": 11490,
      "end_char": 13538
    },
    {
      "content": "Architecture | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.",
      "index": 0,
      "token_count": 623,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2049
    },
    {
      "content": "embed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Architecture\n\nRelevant source files\n\n- [fastembed/common/model\\_management.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py)\n- [fastembed/common/onnx\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py)\n- [fastembed/parallel\\_processor.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py)\n- [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py)\n- [fastembed/text/text\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py)\n\nThis page provides a detailed overview of FastEmbed's architecture, outlining the core components, their interactions, and the embedding generation process flow. The document covers the high-level design patterns, component responsibilities, and implementation details that enable FastEmbed's efficient embedding generation capabilities.\n\nFor information about model management specifics, see [Model Management](qdrant/fastembed/4.1-model-management.md). For details on ONNX Runtime integration, see [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md). For parallel processing implementation, see [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md).\n\n## Core Components Overview\n\nFastEmbed's architecture is built around several key components that work together to provide high-performance embedding generation:\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.",
      "index": 1,
      "token_count": 494,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 1949,
      "end_char": 3971
    },
    {
      "content": "_management.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py) [fastembed/common/onnx\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py) [fastembed/parallel\\_processor.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py) [fastembed/text/text\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py)\n\n## Model Management System\n\nThe model management system is responsible for model discovery, downloading, and caching. It ensures models are available locally for embedding operations, handling various sources (HuggingFace, Google Cloud Storage) and verifying model integrity.\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py24-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L24-L458)\n\nThe `ModelManagement` class provides methods for:\n\n- Listing supported models (`list_supported_models`)\n- Adding custom models (`add_custom_model`)\n- Downloading models from various sources (`download_model`)\n- Verifying model integrity through metadata validation\n\n## ONNX Runtime Integration\n\nFastEmbed leverages ONNX Runtime for optimized inference, providing significant performance improvements over traditional PyTorch/TensorFlow implementations.\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py19-137](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L19-L137) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-146](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L146)\n\nThe `OnnxModel` class:\n\n- Manages ONNX session creation with appropriate execution providers\n- Handles model loading, input preprocessing, and output post-processing\n- Supports CPU and GPU (CUDA) execution\n- Provides a generic interface for different types of embeddings\n\n## Parallel Processing Framework",
      "index": 2,
      "token_count": 494,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 3871,
      "end_char": 5861
    },
    {
      "content": "- Provides a generic interface for different types of embeddings\n\n## Parallel Processing Framework\n\nFastEmbed implements efficient parallel processing through a worker pool design, enabling multi-process execution of embedding tasks for improved throughput.\n\n```\n```\n\nSources: [fastembed/parallel\\_processor.py20-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L20-L253) [fastembed/common/onnx\\_model.py114-137](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L114-L137)\n\nThe parallel processing framework provides:\n\n- A worker pool that manages multiple processes for embedding computation\n- Process-safe queues for input/output communication\n- Work distribution and result collection mechanisms\n- Support for ordered results (maintaining input sequence order)\n- Device management for GPU acceleration\n\n## Embedding Process Flow\n\nThe following diagram illustrates the typical flow for generating embeddings in FastEmbed:\n\n```\n```\n\nSources: [fastembed/text/text\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py#L8-L60) [fastembed/common/onnx\\_model.py26-112](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L112) [fastembed/parallel\\_processor.py91-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L91-L253)\n\n## Class Hierarchy\n\nFastEmbed organizes its functionality through a clear class hierarchy:\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py24-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L24-L458) [fastembed/common/onnx\\_model.py26-137](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L137) [fastembed/parallel\\_processor.py26-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L26-L253) [fastembed/text/text\\_embedding\\_base.py8-60](https://github.",
      "index": 3,
      "token_count": 558,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 5761,
      "end_char": 7759
    },
    {
      "content": "tembed/parallel_processor.py#L26-L253) [fastembed/text/text\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py#L8-L60) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-170](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L170)\n\n## Implementation Details\n\n### Model Loading Process\n\nThe model loading process in FastEmbed follows these steps:\n\n1. Model description is retrieved from the supported models list\n2. Model is downloaded from the appropriate source (HuggingFace or GCS)\n3. Model files are verified for integrity through metadata validation\n4. ONNX session is created with the appropriate execution provider\n5. Additional resources (like tokenizers) are loaded\n\nKey implementations:\n\n- `download_model()` in `ModelManagement` [fastembed/common/model\\_management.py378-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L378-L458)\n- `_load_onnx_model()` in `OnnxModel` [fastembed/common/onnx\\_model.py46-106](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L46-L106)\n\n### Parallel Processing Implementation\n\nFastEmbed's parallel processing is implemented using Python's multiprocessing module:\n\n1. A worker pool is created with a specified number of processes\n2. Each worker process is initialized with the model and necessary resources\n3. Input data is batched and distributed through a queue\n4. Workers process batches in parallel\n5. Results are collected, reordered, and returned to the user\n\nKey implementations:\n\n- `ParallelWorkerPool` class [fastembed/parallel\\_processor.py91-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L91-L253)\n- `_worker()` function [fastembed/parallel\\_processor.py35-88](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L35-L88)\n- `ordered_map()` method [fastembed/parallel\\_processor.py142-151](https://github.",
      "index": 4,
      "token_count": 554,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 7659,
      "end_char": 9682
    },
    {
      "content": "essor.py#L35-L88)\n- `ordered_map()` method [fastembed/parallel\\_processor.py142-151](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L142-L151)\n\n### ONNX Provider Selection\n\nFastEmbed automatically selects the appropriate ONNX execution provider based on:\n\n| Provider         | Condition                     | Implementation                                 |\n| ---------------- | ----------------------------- | ---------------------------------------------- |\n| Custom Providers | Explicitly provided by user   | User-specified list of providers               |\n| CUDA             | `cuda=True` in initialization | CUDAExecutionProvider with optional device\\_id |\n| CPU              | Default fallback              | CPUExecutionProvider                           |\n\nKey implementation in `_load_onnx_model()` [fastembed/common/onnx\\_model.py46-106](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L46-L106)\n\n## Interface to Implementation Mapping\n\nThis table shows how the user-facing embedding classes map to their implementations:\n\n| User API Class                     | Base Class                   | Implementation Classes                                        |\n| ---------------------------------- | ---------------------------- | ------------------------------------------------------------- |\n| TextEmbedding                      | TextEmbeddingBase, OnnxModel | PooledEmbedding, PooledNormalizedEmbedding, CLIPOnnxEmbedding |\n| SparseTextEmbedding                | TextEmbeddingBase, OnnxModel | SpladePP, Bm25, Bm42                                          |\n| LateInteractionTextEmbedding       | TextEmbeddingBase, OnnxModel | Colbert, JinaColbert                                          |\n| ImageEmbedding                     | OnnxModel                    | OnnxImageEmbedding                                            |\n| LateInteractionMultimodalEmbedding | OnnxModel                    | ColPali                                                       |",
      "index": 5,
      "token_count": 385,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 9582,
      "end_char": 11623
    },
    {
      "content": "ng | OnnxModel                    | ColPali                                                       |\n| TextCrossEncoder                   | OnnxModel                    | OnnxTextCrossEncoder                                          |\n\nSources: [fastembed/text/text\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py#L8-L60) [fastembed/common/onnx\\_model.py26-137](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L137) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-170](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L170)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Architecture](#architecture.md)\n- [Core Components Overview](#core-components-overview.md)\n- [Model Management System](#model-management-system.md)\n- [ONNX Runtime Integration](#onnx-runtime-integration.md)\n- [Parallel Processing Framework](#parallel-processing-framework.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Class Hierarchy](#class-hierarchy.md)\n- [Implementation Details](#implementation-details.md)\n- [Model Loading Process](#model-loading-process.md)\n- [Parallel Processing Implementation](#parallel-processing-implementation.md)\n- [ONNX Provider Selection](#onnx-provider-selection.md)\n- [Interface to Implementation Mapping](#interface-to-implementation-mapping.md)",
      "index": 6,
      "token_count": 359,
      "metadata": {
        "title": "_qdrant_fastembed_4-architecture",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4-architecture.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4-architecture.md",
        "file_name": "_qdrant_fastembed_4-architecture.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.438559",
        "total_chunks": 7
      },
      "start_char": 11523,
      "end_char": 13571
    },
    {
      "content": "Model Management | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_4.1-model-management",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.1-model-management.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.1-model-management.md",
        "file_name": "_qdrant_fastembed_4.1-model-management.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.471027",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2035
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Model Management\n\nRelevant source files\n\n- [fastembed/common/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/__init__.py)\n- [fastembed/common/model\\_management.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py)\n- [fastembed/common/types.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/types.py)\n- [fastembed/common/utils.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/utils.py)\n- [fastembed/text/text\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py)\n\nThis document explains how FastEmbed manages embedding models, including downloading, caching, and verification processes. Model Management is a core subsystem responsible for ensuring models are efficiently retrieved and stored locally for embedding generation. For information about the actual embedding process, see [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md).\n\n## Overview\n\nThe Model Management subsystem provides a unified interface for:\n\n- Downloading models from different sources (HuggingFace Hub, Google Cloud Storage)\n- Caching models locally to avoid redundant downloads\n- Verifying the integrity of downloaded models\n- Supporting custom model registration\n\nIt serves as the foundation for all embedding classes in FastEmbed, enabling them to retrieve model files regardless of their source.\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py377-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L377-L458)\n\n## Architecture",
      "index": 1,
      "token_count": 492,
      "metadata": {
        "title": "_qdrant_fastembed_4.1-model-management",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.1-model-management.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.1-model-management.md",
        "file_name": "_qdrant_fastembed_4.1-model-management.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.471027",
        "total_chunks": 6
      },
      "start_char": 1935,
      "end_char": 3930
    },
    {
      "content": "om/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L377-L458)\n\n## Architecture\n\nThe Model Management system is designed as a generic base class that can handle different types of model descriptions. It integrates with the broader FastEmbed architecture as the foundation for embedding model access.\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py24-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L24-L458) [fastembed/text/text\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py#L8-L60)\n\n## Model Sources and Retrieval\n\nFastEmbed supports retrieving models from multiple sources:\n\n1. **HuggingFace Hub**: Primary source for many models, providing version control and metadata\n2. **Google Cloud Storage**: Alternative source, especially for optimized ONNX models\n\nThe system intelligently chooses the appropriate source based on model description and availability.\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py377-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L377-L458)\n\n### HuggingFace Download Process\n\nWhen downloading from HuggingFace, FastEmbed follows these steps:\n\n1. Check if files are already cached locally\n2. Verify files against metadata if present\n3. Download required files using `snapshot_download()`\n4. Collect and store metadata for future verification\n\nThe system uses a pattern-based approach to download only necessary files, reducing download size and time.\n\n```\n```\n\nSources: [fastembed/common/model\\_management.py132-288](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L132-L288)\n\n### Google Cloud Storage Download Process\n\nFor models hosted on Google Cloud Storage:\n\n1. Check if model is already cached locally\n2. Download the compressed model archive\n3. Decompress to temporary directory\n4. Move to final cache location\n\nThis process handles compressed archives (`.tar.",
      "index": 2,
      "token_count": 488,
      "metadata": {
        "title": "_qdrant_fastembed_4.1-model-management",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.1-model-management.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.1-model-management.md",
        "file_name": "_qdrant_fastembed_4.1-model-management.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.471027",
        "total_chunks": 6
      },
      "start_char": 3830,
      "end_char": 5858
    },
    {
      "content": "emporary directory\n4. Move to final cache location\n\nThis process handles compressed archives (`.tar.gz`) containing the ONNX model and supporting files.\n\nSources: [fastembed/common/model\\_management.py327-375](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L327-L375) [fastembed/common/model\\_management.py291-325](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L291-L325)\n\n## Caching Mechanism\n\nFastEmbed implements an efficient caching mechanism to avoid redundant downloads and improve performance.\n\n### Cache Directory Structure\n\n```\n[CACHE_DIR]/\n├── models--[ORGANIZATION]--[MODEL_NAME]/\n│   ├── files_metadata.json\n│   ├── model_file.onnx\n│   ├── config.json\n│   ├── tokenizer.json\n│   └── ...\n├── fast-[MODEL_NAME]/\n│   ├── model_file.onnx\n│   ├── config.json\n│   └── ...\n└── tmp/\n    └── (temporary files during download)\n```\n\nThe cache directory location is determined using the `define_cache_dir()` function, which checks:\n\n1. The `cache_dir` parameter provided to the embedding class\n2. The `FASTEMBED_CACHE_PATH` environment variable\n3. A default location in the system temporary directory\n\nSources: [fastembed/common/utils.py48-59](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/utils.py#L48-L59) [fastembed/common/model\\_management.py327-375](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L327-L375)\n\n## Verification Process\n\nTo ensure model integrity, FastEmbed verifies downloaded files against metadata:\n\n```\n```\n\nThe verification process checks:\n\n1. File existence\n2. File sizes\n3. Blob IDs (when online verification is possible)\n\nThis ensures that all model files are complete and uncorrupted.\n\nSources: [fastembed/common/model\\_management.py152-202](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L152-L202)\n\n## Model Description and Management\n\nModels in FastEmbed are represented by description objects that inherit from `BaseModelDescription`:",
      "index": 3,
      "token_count": 542,
      "metadata": {
        "title": "_qdrant_fastembed_4.1-model-management",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.1-model-management.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.1-model-management.md",
        "file_name": "_qdrant_fastembed_4.1-model-management.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.471027",
        "total_chunks": 6
      },
      "start_char": 5758,
      "end_char": 7807
    },
    {
      "content": "dels in FastEmbed are represented by description objects that inherit from `BaseModelDescription`:\n\n```\n```\n\nThese descriptions contain all necessary information to identify, download, and use a model, including:\n\n- Model identifier\n- Source locations (HuggingFace, GCS URL)\n- File names\n- Licensing information\n- Size information\n\nSources: [fastembed/common/model\\_management.py377-402](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L377-L402)\n\n## Implementation Details\n\nThe `ModelManagement` class is implemented as a generic class parameterized by a type variable `T` that extends `BaseModelDescription`. This allows different embedding classes to use specialized model descriptions while sharing common functionality.\n\n```\n```\n\nKey methods in `ModelManagement` include:\n\n1. `download_model()`: Main entry point for retrieving models\n2. `download_files_from_huggingface()`: Specialized for HuggingFace downloads\n3. `retrieve_model_gcs()`: Specialized for Google Cloud Storage downloads\n4. `_verify_files_from_metadata()`: Verifies downloaded files\n5. `_collect_file_metadata()`: Collects metadata for verification\n\nEach embedding class extends `ModelManagement` with specific type parameters and implementations of abstract methods like `_list_supported_models()`.\n\nSources: [fastembed/common/model\\_management.py24-67](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L24-L67) [fastembed/text/text\\_embedding\\_base.py8-19](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/text_embedding_base.py#L8-L19)\n\n## Error Handling and Retry Mechanism\n\nFastEmbed implements a robust retry mechanism for model downloads:\n\n1. First tries HuggingFace if available\n2. Falls back to Google Cloud Storage if HuggingFace fails\n3. Implements exponential backoff between retries\n4. Provides detailed error messages for troubleshooting\n\nThis ensures maximum availability even in case of temporary issues with one source.\n\nSources: [fastembed/common/model\\_management.",
      "index": 4,
      "token_count": 463,
      "metadata": {
        "title": "_qdrant_fastembed_4.1-model-management",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.1-model-management.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.1-model-management.md",
        "file_name": "_qdrant_fastembed_4.1-model-management.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.471027",
        "total_chunks": 6
      },
      "start_char": 7707,
      "end_char": 9754
    },
    {
      "content": "ity even in case of temporary issues with one source.\n\nSources: [fastembed/common/model\\_management.py412-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L412-L458)\n\n## Summary\n\nThe Model Management system in FastEmbed provides a comprehensive solution for handling embedding models through their lifecycle:\n\n1. **Model Retrieval**: Downloads models from multiple sources\n2. **Caching**: Stores models locally to improve performance\n3. **Verification**: Ensures model integrity\n4. **Abstraction**: Provides a unified interface for all embedding types\n\nThis system is a key component that enables FastEmbed to offer high-performance embeddings while maintaining a simple, user-friendly API.\n\nSources: [fastembed/common/model\\_management.py24-458](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/model_management.py#L24-L458)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Model Management](#model-management.md)\n- [Overview](#overview.md)\n- [Architecture](#architecture.md)\n- [Model Sources and Retrieval](#model-sources-and-retrieval.md)\n- [HuggingFace Download Process](#huggingface-download-process.md)\n- [Google Cloud Storage Download Process](#google-cloud-storage-download-process.md)\n- [Caching Mechanism](#caching-mechanism.md)\n- [Cache Directory Structure](#cache-directory-structure.md)\n- [Verification Process](#verification-process.md)\n- [Model Description and Management](#model-description-and-management.md)\n- [Implementation Details](#implementation-details.md)\n- [Error Handling and Retry Mechanism](#error-handling-and-retry-mechanism.md)\n- [Summary](#summary.md)",
      "index": 5,
      "token_count": 393,
      "metadata": {
        "title": "_qdrant_fastembed_4.1-model-management",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.1-model-management.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.1-model-management.md",
        "file_name": "_qdrant_fastembed_4.1-model-management.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.471027",
        "total_chunks": 6
      },
      "start_char": 9654,
      "end_char": 11702
    },
    {
      "content": "ONNX Runtime Integration | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2043
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# ONNX Runtime Integration\n\nRelevant source files\n\n- [fastembed/common/onnx\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py)\n- [fastembed/image/onnx\\_image\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py)\n- [fastembed/parallel\\_processor.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py)\n- [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py)\n- [fastembed/text/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py)\n- [fastembed/text/onnx\\_text\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py)\n\nThis document explains how FastEmbed leverages ONNX Runtime to achieve efficient inference for embedding generation. ONNX Runtime integration is a core component of FastEmbed's architecture that enables high-performance model execution across different hardware configurations and model types.\n\nFor information about parallel processing capabilities, see [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md).\n\n## Overview of ONNX in FastEmbed\n\nFastEmbed uses ONNX Runtime as its underlying inference engine for all embedding models. This provides substantial performance benefits over traditional PyTorch or TensorFlow implementations while maintaining compatibility with models originally trained in those frameworks.\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py26-109](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.",
      "index": 1,
      "token_count": 519,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 1943,
      "end_char": 3982
    },
    {
      "content": "onnx\\_model.py26-109](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L109) [fastembed/text/onnx\\_text\\_model.py17-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L17-L91) [fastembed/image/onnx\\_image\\_model.py21-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L21-L79) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L77)\n\n## ONNX Model Hierarchy\n\nFastEmbed implements a hierarchical structure for its ONNX-based models, with a common base class and specialized subclasses for different modalities.\n\n### Base Class: OnnxModel\n\nThe `OnnxModel` class serves as the foundation for all ONNX-based models in FastEmbed. It provides:\n\n- Generic ONNX session management\n- Provider configuration\n- Common input/output processing interfaces\n- Base methods for model loading and inference\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py26-112](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L112) [fastembed/text/onnx\\_text\\_model.py17-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L17-L91) [fastembed/image/onnx\\_image\\_model.py21-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L21-L79) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-146](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L146)\n\n## ONNX Session Configuration\n\nFastEmbed offers flexible configuration of ONNX Runtime sessions, enabling users to optimize for their specific hardware and performance requirements.\n\n### Provider Selection\n\nThe library allows specifying which ONNX Runtime execution providers to use:\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py46-106](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L46-L106)",
      "index": 2,
      "token_count": 626,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 3882,
      "end_char": 5929
    },
    {
      "content": "46-106](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L46-L106)\n\n### Session Options and Optimization\n\nFastEmbed applies several optimizations to the ONNX Runtime session:\n\n- Sets graph optimization level to `ORT_ENABLE_ALL`\n- Configures thread counts for intra-op and inter-op parallelism when specified\n- Validates providers against available execution providers in the runtime\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py86-95](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L86-L95)\n\n## Inference Pipeline\n\nThe ONNX-based inference pipeline in FastEmbed follows a consistent pattern across different model types, with modality-specific preprocessing and postprocessing steps.\n\n```\n```\n\nSources: [fastembed/text/onnx\\_text\\_model.py62-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L62-L90) [fastembed/image/onnx\\_image\\_model.py63-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L63-L79) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py66-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L66-L77)\n\n### Text Model Inference\n\nFor text models, the inference process includes:\n\n1. Tokenization of input text documents\n2. Conversion of tokens to input tensors (input\\_ids, attention\\_mask, etc.)\n3. Model inference via ONNX Runtime\n4. Post-processing of output embeddings (first token extraction, normalization)\n\n```\n```\n\nSources: [fastembed/text/onnx\\_text\\_model.py65-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L65-L90) [fastembed/text/onnx\\_embedding.py298-315](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L298-L315)\n\n### Image Model Inference\n\nFor image models, the inference process includes:\n\n1. Loading and preprocessing images\n2. Building ONNX input dictionary\n3. Model inference via ONNX Runtime\n4.",
      "index": 3,
      "token_count": 570,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 5829,
      "end_char": 7838
    },
    {
      "content": "ng and preprocessing images\n2. Building ONNX input dictionary\n3. Model inference via ONNX Runtime\n4. Reshaping and post-processing output embeddings\n\nSources: [fastembed/image/onnx\\_image\\_model.py63-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L63-L79)\n\n## GPU Acceleration\n\nFastEmbed provides support for GPU acceleration through ONNX Runtime's CUDA execution provider.\n\n### CUDA Configuration\n\nUsers can enable CUDA execution by:\n\n1. Setting the `cuda=True` parameter during model initialization\n2. Specifying `device_id` for particular GPU selection\n3. Providing multiple `device_ids` for parallel processing\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py58-73](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L58-L73)\n\n### Multi-GPU Support\n\nFor parallel processing across multiple GPUs, FastEmbed provides:\n\n1. Device ID specification through `device_ids` parameter\n2. Worker process allocation to specific GPUs\n3. Load balancing across available GPUs\n\n```\n```\n\nSources: [fastembed/parallel\\_processor.py120-126](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L120-L126)\n\n## Supported Models and Formats\n\nFastEmbed supports various ONNX-optimized embedding models with different dimensions and purposes.\n\n| Model                               | Dimension | Description               | Type  |\n| ----------------------------------- | --------- | ------------------------- | ----- |\n| BAAI/bge-small-en-v1.5              | 384       | English text embeddings   | Dense |\n| BAAI/bge-base-en-v1.5               | 768       | English text embeddings   | Dense |\n| BAAI/bge-large-en-v1.5              | 1024      | English text embeddings   | Dense |\n| snowflake/snowflake-arctic-embed-\\* | 384-1024  | English text embeddings   | Dense |\n| jinaai/jina-clip-v1                 | 768       | Multimodal (text & image) | Dense |\n| ... and others                      |           |                           |       |",
      "index": 4,
      "token_count": 511,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 7738,
      "end_char": 9772
    },
    {
      "content": "| Dense |\n| ... and others                      |           |                           |       |\n\nSources: [fastembed/text/onnx\\_embedding.py10-183](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L10-L183)\n\n## Implementation Details\n\n### Model Loading and Caching\n\nThe ONNX model loading process includes:\n\n1. Model download and caching\n2. Loading the ONNX model file\n3. Setting up the ONNX Runtime session with appropriate providers\n4. Loading supporting components (tokenizers, image processors)\n\nSources: [fastembed/text/onnx\\_embedding.py248-325](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L248-L325)\n\n### Lazy Loading\n\nFastEmbed supports lazy loading of ONNX models, which defers model loading until the first inference request. This is particularly useful for multi-GPU scenarios where models should be loaded in worker processes.\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py256-258](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L256-L258)\n\n### Error Handling\n\nFastEmbed includes error handling for ONNX Runtime provider configuration:\n\n1. Validation of requested providers against available providers\n2. Warning for CUDA provider failures\n3. Suggestion for CUDA 12.x compatibility\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py96-105](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L96-L105)\n\n## Integration with Embedding Classes\n\nThe ONNX Runtime integration is exposed through higher-level embedding classes that provide user-friendly interfaces for generating embeddings:\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py186-340](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L186-L340) [fastembed/common/onnx\\_model.py114-136](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L114-L136) [fastembed/parallel\\_processor.py26-34](https://github.",
      "index": 5,
      "token_count": 540,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 9672,
      "end_char": 11663
    },
    {
      "content": "0b/fastembed/common/onnx_model.py#L114-L136) [fastembed/parallel\\_processor.py26-34](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L26-L34)\n\n## Conclusion\n\nThe ONNX Runtime integration in FastEmbed provides significant performance benefits through:\n\n1. Hardware-specific optimizations via execution providers\n2. Efficient model loading and caching\n3. Support for parallel and distributed processing\n4. Consistent API across different model types and modalities\n\nBy leveraging ONNX Runtime's capabilities, FastEmbed achieves faster inference speeds compared to traditional embedding frameworks, making it suitable for production environments where performance is critical.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [ONNX Runtime Integration](#onnx-runtime-integration.md)\n- [Overview of ONNX in FastEmbed](#overview-of-onnx-in-fastembed.md)\n- [ONNX Model Hierarchy](#onnx-model-hierarchy.md)\n- [Base Class: OnnxModel](#base-class-onnxmodel.md)\n- [ONNX Session Configuration](#onnx-session-configuration.md)\n- [Provider Selection](#provider-selection.md)\n- [Session Options and Optimization](#session-options-and-optimization.md)\n- [Inference Pipeline](#inference-pipeline.md)\n- [Text Model Inference](#text-model-inference.md)\n- [Image Model Inference](#image-model-inference.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [CUDA Configuration](#cuda-configuration.md)\n- [Multi-GPU Support](#multi-gpu-support.md)\n- [Supported Models and Formats](#supported-models-and-formats.md)\n- [Implementation Details](#implementation-details.md)\n- [Model Loading and Caching](#model-loading-and-caching.md)\n- [Lazy Loading](#lazy-loading.md)\n- [Error Handling](#error-handling.md)\n- [Integration with Embedding Classes](#integration-with-embedding-classes.md)\n- [Conclusion](#conclusion.md)",
      "index": 6,
      "token_count": 445,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.500185",
        "total_chunks": 7
      },
      "start_char": 11563,
      "end_char": 13611
    },
    {
      "content": "Parallel Processing | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2038
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Parallel Processing\n\nRelevant source files\n\n- [fastembed/common/onnx\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py)\n- [fastembed/parallel\\_processor.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py)\n- [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py)\n- [fastembed/text/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py)\n\nFastEmbed provides a robust parallel processing system that enables efficient generation of embeddings for large datasets by distributing workloads across multiple CPU cores or GPU devices. This system significantly improves throughput when processing large batches of documents or images.\n\nFor information about the general architecture of FastEmbed, see [Architecture](qdrant/fastembed/4-architecture.md), and for details on ONNX integration, see [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md).\n\n## Parallel Processing Architecture\n\nFastEmbed's parallel processing is implemented through a worker pool architecture that manages multiple processes, each running its own instance of an embedding model. The core of this system is the `ParallelWorkerPool` class, which handles process creation, workload distribution, and result collection.\n\n```\n```\n\nSources: [fastembed/parallel\\_processor.py91-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L91-L253) [fastembed/text/onnx\\_embedding.py260-292](https://github.",
      "index": 1,
      "token_count": 501,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 1938,
      "end_char": 3928
    },
    {
      "content": "fastembed/parallel_processor.py#L91-L253) [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292)\n\n## Worker Interface and Implementation\n\nThe parallel processing system defines a `Worker` interface that all worker implementations must adhere to. Each embedding type in FastEmbed has its own worker implementation that handles the specific requirements for that type of embedding.\n\n```\n```\n\nSources: [fastembed/parallel\\_processor.py26-32](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L26-L32) [fastembed/common/onnx\\_model.py114-136](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L114-L136) [fastembed/text/onnx\\_embedding.py328-340](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L328-L340) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py148-169](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L148-L169)\n\n## Parallel Processing Flow\n\nWhen parallel processing is enabled, the following sequence of operations occurs:\n\n```\n```\n\nThe system uses two queues to manage this workflow:\n\n1. **Input Queue**: Sends batches of documents to worker processes\n2. **Output Queue**: Receives processed embeddings from worker processes\n\nThe `ParallelWorkerPool` class provides two mapping methods:\n\n- `semi_ordered_map()`: Returns results as soon as they are available (potentially out of order)\n- `ordered_map()`: Ensures results are returned in the same order as inputs\n\nSources: [fastembed/parallel\\_processor.py142-209](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L142-L209)\n\n## Enabling Parallel Processing\n\nTo utilize parallel processing in FastEmbed, you can set the `parallel` parameter when calling the `embed()` method:\n\n```\n```\n\nThe `parallel` parameter accepts the following values:\n\n- `None`: Disable parallel processing (default)",
      "index": 2,
      "token_count": 553,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 3828,
      "end_char": 5844
    },
    {
      "content": "`parallel` parameter accepts the following values:\n\n- `None`: Disable parallel processing (default)\n- `0`: Use all available CPU cores\n- `n` (where n > 0): Use n worker processes\n\nThis parameter is available in all embedding classes in FastEmbed, including `TextEmbedding`, `SparseTextEmbedding`, `LateInteractionTextEmbedding`, `ImageEmbedding`, and `TextCrossEncoder`.\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292)\n\n## GPU Support and Device Assignment\n\nFastEmbed's parallel processing system includes robust GPU support, allowing distribution of worker processes across multiple GPUs:\n\n```\n```\n\nTo utilize GPU acceleration with parallel processing, you can configure the embedding model as follows:\n\n```\n```\n\nIn this configuration:\n\n- The `cuda=True` parameter enables GPU acceleration\n- The `device_ids=[0, 1]` parameter specifies which GPUs to use\n- The `parallel=4` parameter creates 4 worker processes\n- Worker processes are assigned to GPUs in a round-robin fashion\n\nSources: [fastembed/parallel\\_processor.py120-126](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L120-L126) [fastembed/text/onnx\\_embedding.py200-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L200-L246)\n\n## Implementation Details\n\n### Worker Process Management\n\nThe `ParallelWorkerPool` class manages worker processes through several key methods:\n\n1. `start()`: Initializes worker processes and communication queues\n2. `ordered_map()`: Maps input items to worker processes and returns results in order\n3. `semi_ordered_map()`: Similar to `ordered_map()` but may return results out of order\n4. `check_worker_health()`: Monitors worker processes for failures\n5. `join_or_terminate()`: Handles emergency shutdown of worker processes\n6. `join()`: Waits for all worker processes to complete",
      "index": 3,
      "token_count": 483,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 5744,
      "end_char": 7685
    },
    {
      "content": "les emergency shutdown of worker processes\n6. `join()`: Waits for all worker processes to complete\n\nThe system includes robust error handling to detect and manage failures in worker processes, including timeouts for detecting hanging processes.\n\nSources: [fastembed/parallel\\_processor.py92-253](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L92-L253)\n\n### Worker Communication\n\nCommunication between the main process and worker processes occurs through multiprocessing queues:\n\n| Component     | Purpose                                                                       |\n| ------------- | ----------------------------------------------------------------------------- |\n| Input Queue   | Sends batches of documents to worker processes                                |\n| Output Queue  | Receives processed embeddings from worker processes                           |\n| Queue Signals | Special markers like `stop`, `confirm`, and `error` that control process flow |\n\nThe system uses a producer-consumer pattern where the main process produces items for the input queue and consumes results from the output queue, while worker processes do the opposite.\n\nSources: [fastembed/parallel\\_processor.py20-24](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L20-L24) [fastembed/parallel\\_processor.py35-89](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L35-L89)\n\n### Worker Process Creation\n\nWhen creating worker processes, FastEmbed handles several important considerations:\n\n1. **Start Method**: Uses `forkserver` (if available) or `spawn` to create new processes\n2. **Device Assignment**: Assigns GPU devices to workers in a round-robin fashion\n3. **Model Loading**: Each worker loads its own instance of the model\n4. **Synchronization**: Uses a shared counter to track active workers\n\nThis approach ensures that each worker process has its own isolated environment with access to the necessary resources.",
      "index": 4,
      "token_count": 411,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 7585,
      "end_char": 9597
    },
    {
      "content": "that each worker process has its own isolated environment with access to the necessary resources.\n\nSources: [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py117-118](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L117-L118) [fastembed/parallel\\_processor.py92-110](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L92-L110)\n\n## Performance Considerations\n\nWhen using parallel processing in FastEmbed, consider the following factors to optimize performance:\n\n- **Number of Workers**: Start with a number equal to your CPU cores or GPU count and adjust based on performance\n- **Batch Size**: Larger batch sizes typically improve throughput but increase memory usage\n- **GPU Memory**: Each worker loads a separate model instance, so ensure your GPUs have sufficient memory\n- **Process Creation Overhead**: There's an initial overhead to creating worker processes, so parallel processing is most beneficial for larger datasets\n- **Lazy Loading**: Consider using `lazy_load=True` when creating models with parallel processing to avoid loading the model multiple times\n\nTable of performance impacts:\n\n| Configuration        | Best For                                     | Considerations                         |\n| -------------------- | -------------------------------------------- | -------------------------------------- |\n| Single Process       | Small datasets, low latency requirements     | Limited throughput                     |\n| Multiple CPU Workers | Medium-sized datasets, no GPU available      | Higher throughput, increased CPU usage |\n| Multiple GPU Workers | Large datasets, high throughput requirements | Highest performance, requires GPU(s)   |\n\nSources: [fastembed/text/onnx\\_embedding.py199-257](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L199-L257)\n\n## Conclusion",
      "index": 5,
      "token_count": 409,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 9497,
      "end_char": 11411
    },
    {
      "content": "thub.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L199-L257)\n\n## Conclusion\n\nFastEmbed's parallel processing system provides a powerful mechanism for scaling embedding generation across multiple CPU cores or GPU devices. By distributing workloads and managing worker processes effectively, it achieves significant performance improvements for large datasets while maintaining a simple, consistent API.\n\nFor information on model management and caching, see [Model Management](qdrant/fastembed/4.1-model-management.md), or for details on how to use specific embedding types, refer to [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Parallel Processing](#parallel-processing.md)\n- [Parallel Processing Architecture](#parallel-processing-architecture.md)\n- [Worker Interface and Implementation](#worker-interface-and-implementation.md)\n- [Parallel Processing Flow](#parallel-processing-flow.md)\n- [Enabling Parallel Processing](#enabling-parallel-processing.md)\n- [GPU Support and Device Assignment](#gpu-support-and-device-assignment.md)\n- [Implementation Details](#implementation-details.md)\n- [Worker Process Management](#worker-process-management.md)\n- [Worker Communication](#worker-communication.md)\n- [Worker Process Creation](#worker-process-creation.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Conclusion](#conclusion.md)",
      "index": 6,
      "token_count": 310,
      "metadata": {
        "title": "_qdrant_fastembed_4.3-parallel-processing",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.3-parallel-processing.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.3-parallel-processing.md",
        "file_name": "_qdrant_fastembed_4.3-parallel-processing.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.525299",
        "total_chunks": 7
      },
      "start_char": 11311,
      "end_char": 13359
    },
    {
      "content": "Implementation Details | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2041
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Implementation Details\n\nRelevant source files\n\n- [fastembed/late\\_interaction/colbert.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py)\n- [fastembed/sparse/sparse\\_text\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py)\n- [fastembed/sparse/splade\\_pp.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py)\n- [fastembed/text/clip\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py)\n- [fastembed/text/pooled\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py)\n- [fastembed/text/pooled\\_normalized\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py)\n\nThis page provides an in-depth examination of how FastEmbed implements various embedding models. We focus on the core implementation approaches for dense, sparse, and late interaction embedding models. For high-level architecture information, see [Architecture](qdrant/fastembed/4-architecture.md), and for detailed model support, see [Supported Models](qdrant/fastembed/6-supported-models.md).\n\n## Implementation Architecture Overview\n\nFastEmbed uses a consistent implementation pattern across all embedding types, with specialized classes extending base classes that provide common functionality. The implementation is designed around ONNX Runtime for optimized inference.\n\n```\n```\n\nSources: [fastembed/text/pooled\\_embedding.py93-120](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.",
      "index": 1,
      "token_count": 512,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 1941,
      "end_char": 3942
    },
    {
      "content": "bedding.py93-120](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L93-L120) [fastembed/text/pooled\\_normalized\\_embedding.py127-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L127-L147) [fastembed/text/clip\\_embedding.py24-39](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py#L24-L39) [fastembed/sparse/splade\\_pp.py36-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L36-L52) [fastembed/late\\_interaction/colbert.py39-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L65)\n\n## Embedding Processing Flow\n\nAll embedding types follow a general embedding process flow, with variations in their pre-processing and post-processing steps:\n\n```\n```\n\nSources: [fastembed/text/pooled\\_embedding.py113-119](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L113-L119) [fastembed/text/pooled\\_normalized\\_embedding.py141-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L141-L147) [fastembed/sparse/splade\\_pp.py37-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L37-L52) [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65)\n\n## Dense Text Embeddings\n\nFastEmbed implements two primary dense text embedding approaches:\n\n### Pooled Embedding\n\nThe `PooledEmbedding` class provides basic mean pooling over token embeddings:\n\n1. Tokenize input text and create attention masks\n2. Run ONNX model to get token-level embeddings\n3. Apply mean pooling, weighted by attention mask\n\nThe core implementation centers around the mean pooling operation:\n\n```\n```\n\nSources: [fastembed/text/pooled\\_embedding.py93-119](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L93-L119)\n\n### Pooled Normalized Embedding",
      "index": 2,
      "token_count": 641,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 3842,
      "end_char": 5879
    },
    {
      "content": "tembed/blob/b785640b/fastembed/text/pooled_embedding.py#L93-L119)\n\n### Pooled Normalized Embedding\n\nThe `PooledNormalizedEmbedding` class extends `PooledEmbedding` with an additional normalization step:\n\n1. Perform mean pooling as in the base class\n2. Apply L2 normalization to the resulting embeddings\n\nThis implementation is particularly useful for models where cosine similarity is the preferred distance metric.\n\nSources: [fastembed/text/pooled\\_normalized\\_embedding.py127-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L127-L147)\n\n## Sparse Text Embeddings\n\n### SPLADE++ Implementation\n\nThe `SpladePP` class implements the SPLADE++ algorithm for sparse embeddings:\n\n1. Tokenize input text and run ONNX model\n2. Apply ReLU and log transformation to output scores\n3. Apply attention mask to handle variable-length inputs\n4. Take maximum value for each vocabulary term across all tokens\n5. Extract non-zero values and indices as sparse embedding\n\n```\n```\n\nThe resulting sparse embeddings contain only non-zero values and their corresponding indices, significantly reducing memory requirements.\n\nSources: [fastembed/sparse/splade\\_pp.py37-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L37-L52)\n\n## Late Interaction Models\n\n### ColBERT Implementation\n\nThe `Colbert` class implements the ColBERT late interaction approach:\n\n1. Different processing for queries and documents:\n\n   - For queries: Insert a query marker token and pad with mask tokens\n   - For documents: Insert a document marker token\n\n2. For document embeddings:\n\n   - Apply attention mask\n   - Zero out punctuation token embeddings\n   - Normalize token embeddings to unit length\n\n```\n```\n\nThe ColBERT implementation includes several specialized components:\n\n- Special marker tokens for queries (ID=1) and documents (ID=2)\n- Query augmentation with mask tokens to a minimum length\n- Excluding punctuation tokens from document embeddings\n- L2 normalization of per-token embeddings",
      "index": 3,
      "token_count": 467,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 5779,
      "end_char": 7816
    },
    {
      "content": "- Excluding punctuation tokens from document embeddings\n- L2 normalization of per-token embeddings\n\nSources: [fastembed/late\\_interaction/colbert.py39-204](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L204)\n\n## Implementation Optimizations\n\n### ONNX Runtime Integration\n\nAll embeddings in FastEmbed leverage ONNX Runtime for optimized inference:\n\n- Models are exported and optimized for the ONNX format\n- Support for CPU and GPU acceleration\n- Optimized operator fusion and graph optimization\n\n### Parallel Processing\n\nFastEmbed implements efficient parallel processing:\n\n- Worker processes handle subsets of data in parallel\n- Each implementation has its worker class (e.g., `PooledEmbeddingWorker`, `SpladePPEmbeddingWorker`, `ColbertEmbeddingWorker`)\n- Workers can be distributed across multiple GPUs\n\n```\n```\n\nSources: [fastembed/text/pooled\\_embedding.py122-134](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L122-L134) [fastembed/sparse/splade\\_pp.py169-181](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L169-L181) [fastembed/late\\_interaction/colbert.py252-263](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L252-L263)\n\n### Lazy Loading\n\nModels can be lazily loaded to optimize resource usage:\n\n1. Models are only loaded when needed for inference\n2. Particularly useful in multi-GPU setups where each worker loads its own model copy\n3. Controlled via the `lazy_load` parameter in model constructors\n\nThis optimization is implemented across all embedding classes through a common pattern:\n\n```\n```\n\nSources: [fastembed/text/pooled\\_embedding.py122-134](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L122-L134) [fastembed/sparse/splade\\_pp.py122-133](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L122-L133) [fastembed/late\\_interaction/colbert.py183-204](https://github.",
      "index": 4,
      "token_count": 570,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 7716,
      "end_char": 9728
    },
    {
      "content": "embed/sparse/splade_pp.py#L122-L133) [fastembed/late\\_interaction/colbert.py183-204](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L183-L204)\n\n## Model Registration System\n\nFastEmbed uses a registry system to maintain lists of supported models for each embedding type:\n\n```\n```\n\nEach embedding type registers supported models in class variables, along with model metadata such as:\n\n- Embedding dimension\n- Model description\n- License information\n- Model size\n- Source locations\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-68](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L68) [fastembed/sparse/splade\\_pp.py14-33](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L14-L33) [fastembed/text/pooled\\_embedding.py12-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L12-L90) [fastembed/text/pooled\\_normalized\\_embedding.py11-124](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L11-L124) [fastembed/late\\_interaction/colbert.py17-36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L17-L36)\n\n## Conclusion\n\nFastEmbed's implementation details showcase a well-designed architecture that balances flexibility, performance, and ease of use. The library provides specialized implementations for different embedding types while maintaining consistent interfaces and optimizations across the board. The use of ONNX Runtime, parallel processing, and other performance optimizations enables FastEmbed to deliver high-performance embedding generation for various applications.\n\nFor specific model support information, see [Supported Models](qdrant/fastembed/6-supported-models.md), and for usage examples, see [Usage Examples](qdrant/fastembed/7-usage-examples.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Implementation Details](#implementation-details.md)",
      "index": 5,
      "token_count": 540,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 9628,
      "end_char": 11670
    },
    {
      "content": "i\n\nEnter email to refresh\n\n### On this page\n\n- [Implementation Details](#implementation-details.md)\n- [Implementation Architecture Overview](#implementation-architecture-overview.md)\n- [Embedding Processing Flow](#embedding-processing-flow.md)\n- [Dense Text Embeddings](#dense-text-embeddings.md)\n- [Pooled Embedding](#pooled-embedding.md)\n- [Pooled Normalized Embedding](#pooled-normalized-embedding.md)\n- [Sparse Text Embeddings](#sparse-text-embeddings.md)\n- [SPLADE++ Implementation](#splade-implementation.md)\n- [Late Interaction Models](#late-interaction-models.md)\n- [ColBERT Implementation](#colbert-implementation.md)\n- [Implementation Optimizations](#implementation-optimizations.md)\n- [ONNX Runtime Integration](#onnx-runtime-integration.md)\n- [Parallel Processing](#parallel-processing.md)\n- [Lazy Loading](#lazy-loading.md)\n- [Model Registration System](#model-registration-system.md)\n- [Conclusion](#conclusion.md)",
      "index": 6,
      "token_count": 223,
      "metadata": {
        "title": "_qdrant_fastembed_5-implementation-details",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5-implementation-details.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5-implementation-details.md",
        "file_name": "_qdrant_fastembed_5-implementation-details.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.544739",
        "total_chunks": 7
      },
      "start_char": 11570,
      "end_char": 13618
    },
    {
      "content": "Dense Text Embeddings | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 622,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2040
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Dense Text Embeddings\n\nRelevant source files\n\n- [fastembed/text/clip\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py)\n- [fastembed/text/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py)\n- [fastembed/text/pooled\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py)\n- [fastembed/text/pooled\\_normalized\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py)\n\nDense text embeddings are fixed-size vector representations of text that capture semantic meaning in a continuous vector space. In FastEmbed, dense text embeddings are implemented through several specialized classes that leverage ONNX Runtime for high-performance inference. This page details the implementation of dense text embedding models in the FastEmbed library.\n\nFor information about sparse text embeddings, see [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md). For late interaction models, see [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md).\n\n## Architecture Overview\n\nFastEmbed implements dense text embeddings through a hierarchy of specialized classes, each handling different embedding strategies and post-processing techniques.\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py186-326](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L186-L326) [fastembed/text/pooled\\_embedding.py93-120](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L93-L120) [fastembed/text/pooled\\_normalized\\_embedding.",
      "index": 1,
      "token_count": 527,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 1940,
      "end_char": 3988
    },
    {
      "content": "b785640b/fastembed/text/pooled_embedding.py#L93-L120) [fastembed/text/pooled\\_normalized\\_embedding.py127-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L127-L147) [fastembed/text/clip\\_embedding.py24-40](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py#L24-L40)\n\n## Core Implementation Classes\n\nThe FastEmbed library implements dense text embeddings through four primary classes, each catering to different embedding strategies and post-processing techniques.\n\n### OnnxTextEmbedding\n\n`OnnxTextEmbedding` serves as the foundational class for ONNX-based text embeddings. It provides the base implementation for model loading, inference, and embedding generation.\n\nKey features:\n\n- Leverages ONNX Runtime for optimized inference\n- Supports parallel processing with multiple workers\n- Handles model downloading and caching\n- Implements a flexible post-processing pipeline\n\nThis class supports models like BGE, Snowflake Arctic, and MXBai embeddings.\n\nSources: [fastembed/text/onnx\\_embedding.py186-326](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L186-L326)\n\n### PooledEmbedding\n\n`PooledEmbedding` extends `OnnxTextEmbedding` and implements mean pooling for models that produce token-level embeddings. Mean pooling aggregates token embeddings weighted by attention mask values.\n\nKey features:\n\n- Implements mean pooling across token dimensions\n- Preserves semantic information from all tokens\n- Returns non-normalized embeddings\n\nThis class supports models like Nomic Embed, multilingual sentence transformers, and E5 models.\n\nSources: [fastembed/text/pooled\\_embedding.py93-120](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L93-L120)\n\n### PooledNormalizedEmbedding\n\n`PooledNormalizedEmbedding` extends `PooledEmbedding` and adds L2 normalization to the pooled embeddings.",
      "index": 2,
      "token_count": 477,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 3888,
      "end_char": 5824
    },
    {
      "content": "edNormalizedEmbedding` extends `PooledEmbedding` and adds L2 normalization to the pooled embeddings. Normalization ensures all embedding vectors have the same magnitude, which is particularly useful for cosine similarity comparisons.\n\nKey features:\n\n- Applies mean pooling like its parent class\n- Adds L2 normalization to the embeddings\n- Optimized for cosine similarity use cases\n\nThis class supports models like MiniLM, Jina Embeddings V2, and GTE models.\n\nSources: [fastembed/text/pooled\\_normalized\\_embedding.py127-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L127-L147)\n\n### CLIPOnnxEmbedding\n\n`CLIPOnnxEmbedding` extends `OnnxTextEmbedding` and is specialized for CLIP text encoder models. It handles the unique output format of CLIP models without additional pooling.\n\nKey features:\n\n- Specialized for CLIP text encoders\n- Works with multimodal embedding spaces\n- Compatible with corresponding image encoders\n\nThis class specifically supports the CLIP ViT-B-32 text encoder.\n\nSources: [fastembed/text/clip\\_embedding.py24-40](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py#L24-L40)\n\n## Embedding Process Flow\n\nThe process of generating dense text embeddings involves several steps, from input preprocessing to the final vector representation.\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py260-294](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L294) [fastembed/text/onnx\\_embedding.py306-315](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L306-L315) [fastembed/text/pooled\\_embedding.py113-119](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L113-L119) [fastembed/text/pooled\\_normalized\\_embedding.py141-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L141-L147) [fastembed/text/clip\\_embedding.py38-39](https://github.",
      "index": 3,
      "token_count": 554,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 5724,
      "end_char": 7721
    },
    {
      "content": "t/pooled_normalized_embedding.py#L141-L147) [fastembed/text/clip\\_embedding.py38-39](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py#L38-L39)\n\n## Post-Processing Techniques\n\nThe different embedding classes implement various post-processing techniques:\n\n1. **CLS Token Extraction**: `OnnxTextEmbedding` extracts the first token (CLS) embedding for models that encode sentence meaning in this special token.\n\n```\n```\n\n2. **Mean Pooling**: `PooledEmbedding` applies mean pooling across token embeddings, weighted by the attention mask.\n\n```\n```\n\n3. **Normalized Pooling**: `PooledNormalizedEmbedding` applies mean pooling followed by L2 normalization.\n\n```\n```\n\n4. **CLIP Processing**: `CLIPOnnxEmbedding` passes through the model output directly as it's already in the desired format.\n\nSources: [fastembed/text/onnx\\_embedding.py306-315](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L306-L315) [fastembed/text/pooled\\_embedding.py99-102](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L99-L102) [fastembed/text/pooled\\_normalized\\_embedding.py141-147](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L141-L147) [fastembed/text/clip\\_embedding.py38-39](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py#L38-L39)\n\n## Supported Models\n\nFastEmbed supports a wide range of dense text embedding models across different implementations:\n\n### OnnxTextEmbedding Models\n\n| Model                               | Dimension | Language | Context Length | License    |\n| ----------------------------------- | --------- | -------- | -------------- | ---------- |\n| BAAI/bge-small-en-v1.5              | 384       | English  | 512            | MIT        |\n| BAAI/bge-base-en-v1.5               | 768       | English  | 512            | MIT        |\n| BAAI/bge-large-en-v1.5              | 1024      | English  | 512            | MIT        |\n| BAAI/bge-small-zh-v1.",
      "index": 4,
      "token_count": 574,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 7621,
      "end_char": 9660
    },
    {
      "content": "-en-v1.5              | 1024      | English  | 512            | MIT        |\n| BAAI/bge-small-zh-v1.5              | 512       | Chinese  | 512            | MIT        |\n| mixedbread-ai/mxbai-embed-large-v1  | 1024      | English  | 512            | Apache-2.0 |\n| snowflake/snowflake-arctic-embed-\\* | 384-1024  | English  | 512-2048       | Apache-2.0 |\n| jinaai/jina-clip-v1                 | 768       | English  | -              | Apache-2.0 |\n\n### PooledEmbedding Models\n\n| Model                                            | Dimension | Language                   | Context Length | License    |\n| ------------------------------------------------ | --------- | -------------------------- | -------------- | ---------- |\n| nomic-ai/nomic-embed-text-v1.5                   | 768       | English                    | 8192           | Apache-2.0 |\n| sentence-transformers/paraphrase-multilingual-\\* | 384-768   | Multilingual (\\~50 langs)  | 384-512        | Apache-2.0 |\n| intfloat/multilingual-e5-large                   | 1024      | Multilingual (\\~100 langs) | 512            | MIT        |\n\n### PooledNormalizedEmbedding Models\n\n| Model                                  | Dimension | Language          | Context Length | License    |\n| -------------------------------------- | --------- | ----------------- | -------------- | ---------- |\n| sentence-transformers/all-MiniLM-L6-v2 | 384       | English           | 256            | Apache-2.0 |\n| jinaai/jina-embeddings-v2-base-en      | 768       | English           | 8192           | Apache-2.0 |\n| jinaai/jina-embeddings-v2-small-en     | 512       | English           | 8192           | Apache-2.0 |\n| jinaai/jina-embeddings-v2-base-\\*      | 768       | Various languages | 8192           | Apache-2.0 |\n| thenlper/gte-base                      | 768       | English           | 512            | MIT        |\n| thenlper/gte-large                     | 1024      | English           | 512            | MIT        |\n\n### CLIPOnnxEmbedding Models",
      "index": 5,
      "token_count": 576,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 9560,
      "end_char": 11568
    },
    {
      "content": "| 1024      | English           | 512            | MIT        |\n\n### CLIPOnnxEmbedding Models\n\n| Model                     | Dimension | Language | Context Length | License |\n| ------------------------- | --------- | -------- | -------------- | ------- |\n| Qdrant/clip-ViT-B-32-text | 512       | English  | 77             | MIT     |\n\nSources: [fastembed/text/onnx\\_embedding.py10-183](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L10-L183) [fastembed/text/pooled\\_embedding.py12-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L12-L90) [fastembed/text/pooled\\_normalized\\_embedding.py11-124](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L11-L124) [fastembed/text/clip\\_embedding.py8-21](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/clip_embedding.py#L8-L21)\n\n## Model Selection Guide\n\nWhen choosing a dense text embedding model, consider the following factors:\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py10-183](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L10-L183) [fastembed/text/pooled\\_embedding.py12-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_embedding.py#L12-L90) [fastembed/text/pooled\\_normalized\\_embedding.py11-124](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/pooled_normalized_embedding.py#L11-L124)\n\n## Integration with FastEmbed API\n\nThe dense text embedding classes are accessible through the main `TextEmbedding` class, which serves as the public API for all dense text embedding functionality in FastEmbed.\n\n```\n```\n\nInternally, `TextEmbedding` instantiates the appropriate implementation class (e.g., `OnnxTextEmbedding`, `PooledEmbedding`, etc.) based on the specified model name.\n\nFor more examples and detailed usage, see [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page",
      "index": 6,
      "token_count": 590,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 11468,
      "end_char": 13511
    },
    {
      "content": ".1-basic-text-embedding.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Dense Text Embeddings](#dense-text-embeddings.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Core Implementation Classes](#core-implementation-classes.md)\n- [OnnxTextEmbedding](#onnxtextembedding.md)\n- [PooledEmbedding](#pooledembedding.md)\n- [PooledNormalizedEmbedding](#poolednormalizedembedding.md)\n- [CLIPOnnxEmbedding](#cliponnxembedding.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Post-Processing Techniques](#post-processing-techniques.md)\n- [Supported Models](#supported-models.md)\n- [OnnxTextEmbedding Models](#onnxtextembedding-models.md)\n- [PooledEmbedding Models](#pooledembedding-models.md)\n- [PooledNormalizedEmbedding Models](#poolednormalizedembedding-models.md)\n- [CLIPOnnxEmbedding Models](#cliponnxembedding-models.md)\n- [Model Selection Guide](#model-selection-guide.md)\n- [Integration with FastEmbed API](#integration-with-fastembed-api.md)",
      "index": 7,
      "token_count": 262,
      "metadata": {
        "title": "_qdrant_fastembed_5.1-dense-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.1-dense-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.577103",
        "total_chunks": 8
      },
      "start_char": 13411,
      "end_char": 15459
    },
    {
      "content": "Sparse Text Embeddings | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2041
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Sparse Text Embeddings\n\nRelevant source files\n\n- [fastembed/sparse/bm25.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py)\n- [fastembed/sparse/bm42.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py)\n- [fastembed/sparse/sparse\\_text\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py)\n- [fastembed/sparse/splade\\_pp.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py)\n\nThis document explains the implementation and usage of sparse text embeddings in FastEmbed. Sparse embeddings represent text as high-dimensional vectors where most dimensions are zero, making them storage-efficient and well-suited for certain retrieval tasks. For information about dense text embeddings, see [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md).\n\n## Introduction to Sparse Embeddings\n\nSparse embeddings differ from dense embeddings in several key ways:\n\n- **Representation**: Sparse vectors have most dimensions set to zero, with only a small subset containing non-zero values\n- **Dimensionality**: Often much higher dimensional than dense embeddings (tens of thousands vs hundreds)\n- **Interpretability**: Each non-zero dimension typically corresponds to a specific token/word\n- **Storage**: More efficient to store as (index, value) pairs rather than full vectors\n- **Matching**: Often use similarity measures like BM25 rather than cosine distance\n\nFastEmbed implements three distinct approaches to sparse embeddings:\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-17](https://github.",
      "index": 1,
      "token_count": 503,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 1941,
      "end_char": 3943
    },
    {
      "content": "se embeddings:\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-17](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L17) [fastembed/sparse/sparse\\_embedding\\_base.py7-9](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_embedding_base.py#L7-L9)\n\n## Core Architecture\n\nThe `SparseTextEmbedding` class serves as the main entry point for generating sparse embeddings. It maintains a registry of implementation classes and delegates to the appropriate implementation based on the provided model name.\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py16-18](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L16-L18) [fastembed/sparse/splade\\_pp.py36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L36-L36) [fastembed/sparse/bm25.py61](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L61-L61) [fastembed/sparse/bm42.py39](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L39-L39)\n\n## Implemented Models\n\nFastEmbed offers three sparse embedding implementations with different characteristics:\n\n### 1. SpladePP\n\nSPLADE++ (SParse Lexical AnD Expansion) uses a transformer model to generate sparse embeddings that capture contextual meaning. It applies a log operation on ReLU-activated outputs to emphasize important tokens.\n\nKey features:\n\n- Neural-based approach using transformer models\n- Learns weights through contextual understanding of text\n- Creates high-quality sparse representations\n- Requires an ONNX model for inference\n\nImplementation highlights:\n\n```\n```\n\nSources: [fastembed/sparse/splade\\_pp.py36-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L36-L52)\n\n### 2. BM25\n\nBM25 (Best Matching 25) is a classic information retrieval algorithm implemented as sparse embeddings. It evaluates token importance based on term frequency and document length.\n\nKey features:",
      "index": 2,
      "token_count": 565,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 3843,
      "end_char": 5882
    },
    {
      "content": "eddings. It evaluates token importance based on term frequency and document length.\n\nKey features:\n\n- Traditional bag-of-words IR approach\n- Applies stemming and stopword removal\n- Language-aware with support for 18 languages\n- Requires no model weights (lightweight)\n\nThe BM25 formula used is:\n\n```\nscore(q, d) = SUM[ IDF(q_i) * (f(q_i, d) * (k + 1)) / (f(q_i, d) + k * (1 - b + b * (|d| / avg_len))) ]\n```\n\nImplementation highlights:\n\n```\n```\n\nSources: [fastembed/sparse/bm25.py61-266](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L61-L266) [fastembed/sparse/bm25.py25-44](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L25-L44)\n\n### 3. BM42\n\nBM42 is an extension of BM25 that addresses limitations with short documents by using transformer attention weights instead of token counts.\n\nKey features:\n\n- Hybrid approach combining BM25 with neural attention\n- Better handles short documents and rare tokens\n- Applies stemming and stopword removal\n- Named \"42\" as a reference to \"The Hitchhiker's Guide to the Galaxy\"\n\nImplementation highlights:\n\n```\n```\n\nSources: [fastembed/sparse/bm42.py39-218](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L39-L218)\n\n## Embedding Process Flow\n\nThe process of generating sparse embeddings differs between the three implementations, but follows a general pattern:\n\n```\n```\n\nSources: [fastembed/sparse/splade\\_pp.py37-52](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L37-L52) [fastembed/sparse/bm25.py251-262](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L251-L262) [fastembed/sparse/bm42.py220-250](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L220-L250)\n\n## Model Comparison\n\n| Feature          | SpladePP                         | BM25                          | BM42                     |\n| ---------------- | -------------------------------- | ----------------------------- | ------------------------ |",
      "index": 3,
      "token_count": 608,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 5782,
      "end_char": 7805
    },
    {
      "content": "--- | -------------------------------- | ----------------------------- | ------------------------ |\n| Approach         | Neural (Transformer)             | Statistical                   | Hybrid                   |\n| Model Size       | \\~532 MB                         | None (rule-based)             | \\~90 MB                  |\n| Strengths        | High quality for semantic search | Works well for keyword search | Good for short documents |\n| Token Importance | Learned weights                  | Term frequency                | Attention weights        |\n| Language Support | English                          | 18 languages                  | English                  |\n| IDF Requirement  | Yes                              | Yes                           | Yes                      |\n| Preprocessing    | Minimal                          | Stemming, stopwords           | Stemming, stopwords      |\n\nSources: [fastembed/sparse/splade\\_pp.py14-33](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L14-L33) [fastembed/sparse/bm25.py25-58](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L25-L58) [fastembed/sparse/bm42.py20-36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L20-L36)\n\n## Usage\n\nThe `SparseTextEmbedding` class provides a unified interface for all sparse embedding implementations:\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py52-129](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L52-L129)\n\n## IDF Requirement\n\nAll sparse embedding models in FastEmbed are designed to be used with inverse document frequency (IDF) weighting applied during search. When used with Qdrant vector database, this requires setting `\"modifier\": \"idf\"` in the sparse vector index configuration.\n\nIDF weights terms based on their rarity across the entire corpus, giving more importance to uncommon terms:\n\n```\nIDF(term) = log(N / df(term))\n```",
      "index": 4,
      "token_count": 468,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 7705,
      "end_char": 9695
    },
    {
      "content": "he entire corpus, giving more importance to uncommon terms:\n\n```\nIDF(term) = log(N / df(term))\n```\n\nWhere N is the total number of documents and df(term) is the number of documents containing the term.\n\nSources: [fastembed/sparse/bm25.py65-66](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L65-L66) [fastembed/sparse/bm42.py52-53](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L52-L53)\n\n## Implementation Details\n\n### SparseEmbedding Structure\n\nAll sparse embedding implementations produce objects of type `SparseEmbedding`, which contains:\n\n- `indices`: An array of token IDs (integers)\n- `values`: An array of weights corresponding to each token ID\n\nThis representation is compact and efficient for vector databases like Qdrant.\n\n### Model Selection\n\nThe `SparseTextEmbedding` class automatically selects the appropriate implementation based on the model name:\n\n```\n```\n\nSources: [fastembed/sparse/sparse\\_text\\_embedding.py73-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/sparse_text_embedding.py#L73-L91)\n\n### Parallel Processing\n\nAll sparse embedding implementations support parallel processing for faster document embedding. The `parallel` parameter in the `embed` method controls this behavior:\n\n- `None`: Uses single-process embedding with ONNX threading\n- `0`: Uses all available CPU cores\n- `> 1`: Uses the specified number of worker processes\n\nSources: [fastembed/sparse/splade\\_pp.py135-167](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/splade_pp.py#L135-L167) [fastembed/sparse/bm25.py156-198](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm25.py#L156-L198) [fastembed/sparse/bm42.py270-302](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/bm42.py#L270-L302)\n\n## Common Use Cases\n\n1. **Hybrid Search**: Combining sparse and dense embeddings for both keyword and semantic matching\n2. **Exact Keyword Matching**: When precise term matching is required\n3.",
      "index": 5,
      "token_count": 577,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 9595,
      "end_char": 11606
    },
    {
      "content": "yword and semantic matching\n2. **Exact Keyword Matching**: When precise term matching is required\n3. **Specialized Domain Search**: When specific terminology must be precisely matched\n4. **Long Document Search**: Sparse embeddings can better handle specific terms in very long documents\n\nFor hybrid search examples, see [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Sparse Text Embeddings](#sparse-text-embeddings.md)\n- [Introduction to Sparse Embeddings](#introduction-to-sparse-embeddings.md)\n- [Core Architecture](#core-architecture.md)\n- [Implemented Models](#implemented-models.md)\n- [1. SpladePP](#1-spladepp.md)\n- [2. BM25](#2-bm25.md)\n- [3. BM42](#3-bm42.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Model Comparison](#model-comparison.md)\n- [Usage](#usage.md)\n- [IDF Requirement](#idf-requirement.md)\n- [Implementation Details](#implementation-details.md)\n- [SparseEmbedding Structure](#sparseembedding-structure.md)\n- [Model Selection](#model-selection.md)\n- [Parallel Processing](#parallel-processing.md)\n- [Common Use Cases](#common-use-cases.md)",
      "index": 6,
      "token_count": 306,
      "metadata": {
        "title": "_qdrant_fastembed_5.2-sparse-text-embeddings",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "file_name": "_qdrant_fastembed_5.2-sparse-text-embeddings.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.609898",
        "total_chunks": 7
      },
      "start_char": 11506,
      "end_char": 13554
    },
    {
      "content": "Late Interaction Models | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2042
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Late Interaction Models\n\nRelevant source files\n\n- [fastembed/late\\_interaction/colbert.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py)\n- [fastembed/late\\_interaction/jina\\_colbert.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py)\n- [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py)\n- [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py)\n- [fastembed/sparse/utils/tokenizer.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/sparse/utils/tokenizer.py)\n\nThis page documents the Late Interaction Models in FastEmbed, which provide a specialized approach to text embeddings that differs from the standard dense embedding methods covered in [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md) and sparse embeddings described in [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md).\n\nLate interaction models generate token-level embeddings rather than a single vector per document, preserving word-level semantics for more precise matching during retrieval. This approach enables a finer-grained comparison between queries and documents at search time.\n\n## Overview of Late Interaction Models\n\nLate interaction models, like ColBERT, create separate embeddings for each token in a document or query rather than pooling them into a single vector.",
      "index": 1,
      "token_count": 487,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 1942,
      "end_char": 3931
    },
    {
      "content": "rate embeddings for each token in a document or query rather than pooling them into a single vector. This approach allows the model to perform \"late interaction\" - comparing query and document token embeddings at retrieval time rather than comparing single document/query vectors.\n\n```\n```\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py#L8-L60) [fastembed/late\\_interaction/colbert.py39-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L65)\n\n## Architecture\n\nThe late interaction models in FastEmbed are organized in a hierarchical structure:\n\n```\n```\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py8-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py#L8-L60) [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py14-119](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py#L14-L119) [fastembed/late\\_interaction/colbert.py39-255](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L255) [fastembed/late\\_interaction/jina\\_colbert.py21-58](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L21-L58)\n\nThe main classes are:\n\n1. **LateInteractionTextEmbeddingBase**: The abstract base class that defines the interface for late interaction models.\n2. **LateInteractionTextEmbedding**: A facade class that selects the appropriate implementation based on the model name.\n3. **Colbert**: Implementation of the ColBERT model.\n4. **JinaColbert**: Extension of the ColBERT model with Jina-specific enhancements.\n\n## Supported Models\n\nFastEmbed supports the following late interaction models:",
      "index": 2,
      "token_count": 510,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 3831,
      "end_char": 5758
    },
    {
      "content": "ific enhancements.\n\n## Supported Models\n\nFastEmbed supports the following late interaction models:\n\n| Model                                 | Dimension | Description                                                                                   | License      | Size (GB) |\n| ------------------------------------- | --------- | --------------------------------------------------------------------------------------------- | ------------ | --------- |\n| colbert-ir/colbertv2.0                | 128       | Late interaction model                                                                        | MIT          | 0.44      |\n| answerdotai/answerai-colbert-small-v1 | 96        | Text embeddings, Unimodal (text), Multilingual (\\~100 languages), 512 input tokens truncation | Apache-2.0   | 0.13      |\n| jinaai/jina-colbert-v2                | 128       | Multilingual model with context length of 8192                                                | CC-BY-NC-4.0 | 2.24      |\n\nSources: [fastembed/late\\_interaction/colbert.py17-36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L17-L36) [fastembed/late\\_interaction/jina\\_colbert.py7-18](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L7-L18)\n\n## Implementation Details\n\n### ColBERT Processing Flow\n\nThe ColBERT and JinaColBERT models implement a specific processing flow for queries and documents:\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65) [fastembed/late\\_interaction/colbert.py67-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L67-L77) [fastembed/late\\_interaction/colbert.py79-108](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L79-L108)\n\n### Key Implementation Features\n\n1. **Different Marker Tokens for Queries and Documents**:",
      "index": 3,
      "token_count": 507,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 5658,
      "end_char": 7647
    },
    {
      "content": "-L108)\n\n### Key Implementation Features\n\n1. **Different Marker Tokens for Queries and Documents**:\n\n   - Colbert uses `QUERY_MARKER_TOKEN_ID = 1` for queries and `DOCUMENT_MARKER_TOKEN_ID = 2` for documents\n   - JinaColbert uses `QUERY_MARKER_TOKEN_ID = 250002` for queries and `DOCUMENT_MARKER_TOKEN_ID = 250003` for documents\n\n2. **Query Augmentation**:\n\n   - Queries are padded with mask tokens to a minimum length (`MIN_QUERY_LENGTH = 31`) to improve performance\n\n3. **Document Token Processing**:\n\n   - Punctuation tokens are masked out (`skip_list`)\n   - Remaining token embeddings are L2-normalized\n\n4. **Worker Classes for Parallel Processing**:\n\n   - `ColbertEmbeddingWorker` and `JinaColbertEmbeddingWorker` enable parallel embedding generation\n\nSources: [fastembed/late\\_interaction/colbert.py39-44](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L44) [fastembed/late\\_interaction/jina\\_colbert.py22-25](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L22-L25) [fastembed/late\\_interaction/colbert.py86-104](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L86-L104) [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65) [fastembed/late\\_interaction/colbert.py256-263](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L256-L263)\n\n## Usage Example\n\nThe late interaction models can be used through the `LateInteractionTextEmbedding` class. The key difference from regular embedding models is the separate methods for query and document embedding:\n\n```\n```\n\nThe model provides:\n\n1. **embed()** - For embedding documents\n2. **query\\_embed()** - For embedding queries\n3. **passage\\_embed()** - Alternative name for embed() to maintain API consistency\n\nSources: [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py83-119](https://github.",
      "index": 4,
      "token_count": 580,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 7547,
      "end_char": 9560
    },
    {
      "content": "Sources: [fastembed/late\\_interaction/late\\_interaction\\_text\\_embedding.py83-119](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_text_embedding.py#L83-L119) [fastembed/late\\_interaction/late\\_interaction\\_embedding\\_base.py21-60](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/late_interaction_embedding_base.py#L21-L60)\n\n## Implementation Details: Colbert Class\n\nThe `Colbert` class is the central implementation of the ColBERT model in FastEmbed:\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py39-255](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L255) [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65) [fastembed/late\\_interaction/colbert.py67-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L67-L77) [fastembed/late\\_interaction/colbert.py79-108](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L79-L108) [fastembed/late\\_interaction/colbert.py239-249](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L239-L249)\n\nKey methods in the `Colbert` class:\n\n1. **\\_post\\_process\\_onnx\\_output()**: Different processing for documents (with masking and normalization) and queries\n2. **\\_preprocess\\_onnx\\_input()**: Adds marker tokens based on whether the input is a document or query\n3. **tokenize()**: Handles tokenization differently for documents and queries\n4. **\\_tokenize\\_query()**: Pads shorter queries with mask tokens for query augmentation\n5. **load\\_onnx\\_model()**: Initializes the ONNX model and sets up tokenization parameters\n6. **embed()** and **query\\_embed()**: Public interface methods for embedding documents and queries\n\n## JinaColbert Extensions\n\n`JinaColbert` extends the base `Colbert` class with specific enhancements:\n\n1. Different special token IDs:",
      "index": 5,
      "token_count": 601,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 9460,
      "end_char": 11489
    },
    {
      "content": "ert` extends the base `Colbert` class with specific enhancements:\n\n1. Different special token IDs:\n\n   - `QUERY_MARKER_TOKEN_ID = 250002`\n   - `DOCUMENT_MARKER_TOKEN_ID = 250003`\n   - `MASK_TOKEN = \"<mask>\"` (vs. \"\\[MASK]\" in Colbert)\n\n2. Modified preprocessing for queries:\n\n   - Sets all attention mask values to 1 for queries\n\n3. Support for longer contexts:\n\n   - Context length up to 8192 tokens (vs. the standard 512)\n\n4. Multilingual capabilities:\n\n   - Support for multiple languages vs. primarily English in the original ColBERT\n\nSources: [fastembed/late\\_interaction/jina\\_colbert.py21-48](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L21-L48)\n\n## Parallel Processing Support\n\nLate interaction models in FastEmbed support parallel processing for efficient embedding generation:\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py211-237](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L211-L237) [fastembed/late\\_interaction/colbert.py256-263](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L256-L263) [fastembed/late\\_interaction/jina\\_colbert.py51-58](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/jina_colbert.py#L51-L58)\n\nThe worker classes (`ColbertEmbeddingWorker` and `JinaColbertEmbeddingWorker`) handle embedding generation in separate processes, allowing for efficient utilization of multiple CPU cores or GPUs.\n\n## Conclusion\n\nLate interaction models in FastEmbed provide a powerful alternative to traditional dense embeddings. By generating token-level representations and postponing the interaction between query and document until search time, these models enable more precise matching and retrieval.\n\nThe implementation in FastEmbed offers efficient processing through ONNX runtime integration and parallel processing capabilities, making it suitable for production environments where performance is critical.\n\nDismiss\n\nRefresh this wiki",
      "index": 6,
      "token_count": 511,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 11389,
      "end_char": 13421
    },
    {
      "content": "it suitable for production environments where performance is critical.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Late Interaction Models](#late-interaction-models.md)\n- [Overview of Late Interaction Models](#overview-of-late-interaction-models.md)\n- [Architecture](#architecture.md)\n- [Supported Models](#supported-models.md)\n- [Implementation Details](#implementation-details.md)\n- [ColBERT Processing Flow](#colbert-processing-flow.md)\n- [Key Implementation Features](#key-implementation-features.md)\n- [Usage Example](#usage-example.md)\n- [Implementation Details: Colbert Class](#implementation-details-colbert-class.md)\n- [JinaColbert Extensions](#jinacolbert-extensions.md)\n- [Parallel Processing Support](#parallel-processing-support.md)\n- [Conclusion](#conclusion.md)",
      "index": 7,
      "token_count": 180,
      "metadata": {
        "title": "_qdrant_fastembed_5.3-late-interaction-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.3-late-interaction-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "file_name": "_qdrant_fastembed_5.3-late-interaction-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.641131",
        "total_chunks": 8
      },
      "start_char": 13321,
      "end_char": 15369
    },
    {
      "content": "Multimodal Models | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2036
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Multimodal Models\n\nRelevant source files\n\n- [fastembed/late\\_interaction\\_multimodal/colpali.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py)\n- [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py)\n- [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding_base.py)\n- [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py)\n\nThis document provides a detailed explanation of the multimodal embedding capabilities in FastEmbed. Multimodal models in FastEmbed enable the creation of embeddings from both text and image inputs within a compatible embedding space, making them ideal for cross-modal retrieval tasks. This page focuses specifically on the implementation of late interaction multimodal models, which process text and images separately but in alignment.\n\nFor information about text-only late interaction models, see [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md).\n\n## Architecture Overview\n\nThe multimodal embedding functionality in FastEmbed is built around the `LateInteractionMultimodalEmbedding` class, which serves as the main entry point for users.",
      "index": 1,
      "token_count": 480,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 1936,
      "end_char": 3878
    },
    {
      "content": "ound the `LateInteractionMultimodalEmbedding` class, which serves as the main entry point for users. This class is backed by specific implementations such as `ColPali`, which provide the actual embedding capabilities for both text and images.\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py14-16](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L14-L16) [fastembed/late\\_interaction\\_multimodal/colpali.py34-44](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L34-L44) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py20-44](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L20-L44)\n\n## Supported Models\n\nCurrently, FastEmbed supports the following multimodal model:\n\n| Model                    | Dimensions | Description                                                           | License | Size   |\n| ------------------------ | ---------- | --------------------------------------------------------------------- | ------- | ------ |\n| Qdrant/colpali-v1.3-fp16 | 128        | Text and image embeddings, English, 50 tokens query length truncation | MIT     | 6.5 GB |\n\nColPali is a multimodal embedding model that combines the strengths of ColBERT architecture with multimodal capabilities, allowing for effective cross-modal retrieval between text and images.\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py20-31](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L20-L31)\n\n## Implementation Details\n\n### Class Hierarchy\n\nThe multimodal embedding functionality in FastEmbed follows a clear inheritance hierarchy:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py10-67](https://github.",
      "index": 2,
      "token_count": 502,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 3778,
      "end_char": 5755
    },
    {
      "content": "ate\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py10-67](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding_base.py#L10-L67) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py20-82](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L20-L82) [fastembed/late\\_interaction\\_multimodal/colpali.py34-281](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L34-L281)\n\n### The ColPali Implementation\n\nThe `ColPali` class is the current implementation of multimodal embedding in FastEmbed. It implements both the `LateInteractionMultimodalEmbeddingBase` and `OnnxMultimodalModel` interfaces, providing functionality for:\n\n1. Text embedding using special tokens and processing\n2. Image embedding using a specialized vision encoder\n3. Unified embedding space for cross-modal retrieval\n\nKey implementation details include:\n\n- Special token handling for query prefixing: `QUERY_PREFIX = \"Query: \"` and `BOS_TOKEN = \"<s>\"`\n- Specialized preprocessing for multimodal inputs\n- Processing of text with image placeholders and images with text placeholders\n- Post-processing to ensure compatible embedding dimensions\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py35-45](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L35-L45) [fastembed/late\\_interaction\\_multimodal/colpali.py162-206](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L162-L206)\n\n## Embedding Process Flow\n\nThe embedding process for multimodal models follows a similar pattern for both text and images:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py86-224](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.",
      "index": 3,
      "token_count": 564,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 5655,
      "end_char": 7659
    },
    {
      "content": "thub.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L86-L224) [fastembed/late\\_interaction\\_multimodal/colpali.py162-173](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L162-L173) [fastembed/late\\_interaction\\_multimodal/colpali.py208-272](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L208-L272)\n\n## Key Components of Multimodal Processing\n\n### Text Preprocessing\n\nWhen embedding text, ColPali:\n\n1. Prefixes queries with `QUERY_PREFIX` and `BOS_TOKEN`\n2. Tokenizes the text\n3. Adds query marker tokens\n4. Adds empty image placeholders to ensure the model handles text-only input correctly\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py162-166](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L162-L166) [fastembed/late\\_interaction\\_multimodal/colpali.py172-186](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L172-L186)\n\n### Image Preprocessing\n\nWhen embedding images, ColPali:\n\n1. Processes the images using the image processor\n2. Adds empty text placeholders to ensure the model handles image-only input correctly\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py189-204](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L189-L204) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py162-174](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L162-L174)\n\n## Parallel Processing Support\n\nMultimodal embedding supports efficient parallel processing for large datasets:\n\n1. When `parallel` parameter is set, the system creates a `ParallelWorkerPool`\n2. For text embedding, it uses `ColPaliTextEmbeddingWorker` workers\n3. For image embedding, it uses `ColPaliImageEmbeddingWorker` workers\n4.",
      "index": 4,
      "token_count": 632,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 7559,
      "end_char": 9582
    },
    {
      "content": "extEmbeddingWorker` workers\n3. For image embedding, it uses `ColPaliImageEmbeddingWorker` workers\n4. Each worker processes a batch of inputs in parallel, significantly improving throughput\n\nSources: [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py152-160](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L152-L160) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py215-223](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L215-L223) [fastembed/late\\_interaction\\_multimodal/colpali.py283-300](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L283-L300)\n\n## Usage Example\n\nHere's how to use the multimodal embedding functionality:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py86-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L86-L130)\n\n## Integration with Vector Databases\n\nThe multimodal embeddings produced by FastEmbed are compatible with vector database systems like Qdrant. The embeddings from both text and images can be stored in the same collection, enabling cross-modal search (finding images with text queries or finding text with image queries).\n\nFor more information on integrating FastEmbed with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py22-30](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L22-L30)\n\n## Future Development\n\nThe architecture of FastEmbed's multimodal implementation is designed for extensibility. New multimodal models can be added by:\n\n1. Creating a new class that extends `LateInteractionMultimodalEmbeddingBase` and `OnnxMultimodalModel`\n2.",
      "index": 5,
      "token_count": 558,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 9482,
      "end_char": 11473
    },
    {
      "content": "ating a new class that extends `LateInteractionMultimodalEmbeddingBase` and `OnnxMultimodalModel`\n2. Implementing the required methods for text and image embedding\n3. Adding the new class to the `EMBEDDINGS_REGISTRY` in `LateInteractionMultimodalEmbedding`\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py15-16](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L15-L16) [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py66-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L66-L79)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Multimodal Models](#multimodal-models.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Supported Models](#supported-models.md)\n- [Implementation Details](#implementation-details.md)\n- [Class Hierarchy](#class-hierarchy.md)\n- [The ColPali Implementation](#the-colpali-implementation.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Key Components of Multimodal Processing](#key-components-of-multimodal-processing.md)\n- [Text Preprocessing](#text-preprocessing.md)\n- [Image Preprocessing](#image-preprocessing.md)\n- [Parallel Processing Support](#parallel-processing-support.md)\n- [Usage Example](#usage-example.md)\n- [Integration with Vector Databases](#integration-with-vector-databases.md)\n- [Future Development](#future-development.md)",
      "index": 6,
      "token_count": 402,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.676369",
        "total_chunks": 7
      },
      "start_char": 11373,
      "end_char": 13421
    },
    {
      "content": "Supported Models | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2035
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Supported Models\n\nRelevant source files\n\n- [NOTICE](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE)\n- [docs/examples/Supported\\_Models.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb)\n- [fastembed/late\\_interaction\\_multimodal/\\_\\_init\\_\\_.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/__init__.py)\n\nThis page provides a comprehensive list of all embedding models supported by FastEmbed, organized by model type. FastEmbed offers a diverse range of pre-trained models for various embedding tasks, including dense text embedding, sparse text embedding, late interaction models, image embedding, and cross-encoder reranking.\n\n## Model Types Overview\n\nFastEmbed supports five main types of embedding models, each with specific use cases and implementations.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb35-45](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L35-L45)\n\n## Dense Text Embedding Models\n\nDense text embedding models generate fixed-length vector representations of text, capturing semantic meaning. These models are typically used for semantic search and measuring text similarity.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb64-72](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L64-L72)\n\nFastEmbed supports a wide range of dense text embedding models, from small, efficient models to large, high-performance ones:",
      "index": 1,
      "token_count": 484,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 1935,
      "end_char": 3813
    },
    {
      "content": "ange of dense text embedding models, from small, efficient models to large, high-performance ones:\n\n| Model Size | Example Models                                                                       | Dimension | Size (GB)  | Primary Use                                          |\n| ---------- | ------------------------------------------------------------------------------------ | --------- | ---------- | ---------------------------------------------------- |\n| Small      | BAAI/bge-small-en-v1.5, jinaai/jina-embeddings-v2-small-en                           | 384-512   | 0.067-0.13 | Efficient semantic search with low resource usage    |\n| Medium     | BAAI/bge-base-en, nomic-ai/nomic-embed-text-v1.5, snowflake/snowflake-arctic-embed-m | 768       | 0.21-0.54  | Balanced performance and resource usage              |\n| Large      | BAAI/bge-large-en-v1.5, thenlper/gte-large, intfloat/multilingual-e5-large           | 1024      | 1.0-2.24   | Highest quality embeddings for critical applications |\n\nThe full list includes 25 dense text embedding models with diverse characteristics:\n\n- BGE models (small/base/large variants)\n- Snowflake Arctic models (xs/s/m/l variants)\n- Sentence Transformers models\n- Jina AI embedding models\n- Nomic AI embedding models\n- CLIP text models\n- Multilingual models (Chinese, German, etc.)\n\nThese models can be accessed through the `TextEmbedding` class using the `model_name` parameter:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb64-129](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L64-L129)\n\n## Sparse Text Embedding Models\n\nSparse embedding models generate high-dimensional, sparse vector representations where most values are zero. These models excel at lexical matching and are often used in hybrid search systems alongside dense embeddings.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb386-392](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L386-L392)",
      "index": 2,
      "token_count": 489,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 3713,
      "end_char": 5726
    },
    {
      "content": "(https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L386-L392)\n\nFastEmbed supports the following sparse text embedding models:\n\n| Model                                   | Vocab Size | Description                                          | Size (GB) | Requires IDF |\n| --------------------------------------- | ---------- | ---------------------------------------------------- | --------- | ------------ |\n| Qdrant/bm25                             | -          | BM25 as sparse embeddings                            | 0.01      | Yes          |\n| Qdrant/bm42-all-minilm-l6-v2-attentions | 30,522     | Light sparse embedding model using attention weights | 0.09      | Yes          |\n| prithivida/Splade\\_PP\\_en\\_v1           | 30,522     | SPLADE++ Model for sparse retrieval                  | 0.532     | No           |\n\nThese models can be accessed through the `SparseTextEmbedding` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb386-478](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L386-L478)\n\n## Late Interaction Text Embedding Models\n\nLate interaction models defer the computation of relevance scores until query time, preserving token-level representations instead of pooling them into a single vector. This approach provides higher precision at the cost of more complex retrieval.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb510-516](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L510-L516)\n\nFastEmbed supports the following late interaction text embedding models:\n\n| Model                                 | Dimension | Description                         | License      | Size (GB) |\n| ------------------------------------- | --------- | ----------------------------------- | ------------ | --------- |\n| answerdotai/answerai-colbert-small-v1 | 96        | Multilingual late interaction model | apache-2.0   | 0.13      |\n| colbert-ir/colbertv2.",
      "index": 3,
      "token_count": 477,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 5626,
      "end_char": 7631
    },
    {
      "content": "96        | Multilingual late interaction model | apache-2.0   | 0.13      |\n| colbert-ir/colbertv2.0                | 128       | Late interaction model              | mit          | 0.44      |\n| jinaai/jina-colbert-v2                | 128       | Enhanced ColBERT capabilities       | cc-by-nc-4.0 | 2.24      |\n\nThese models can be accessed through the `LateInteractionTextEmbedding` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb510-590](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L510-L590)\n\n## Image Embedding Models\n\nImage embedding models generate vector representations from images, enabling visual similarity search and multimodal applications.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb622-628](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L622-L628)\n\nFastEmbed supports the following image embedding models:\n\n| Model                       | Dimension | Description                     | License    | Size (GB) |\n| --------------------------- | --------- | ------------------------------- | ---------- | --------- |\n| Qdrant/resnet50-onnx        | 2048      | Image embeddings (2016)         | apache-2.0 | 0.10      |\n| Qdrant/clip-ViT-B-32-vision | 512       | CLIP vision embeddings          | mit        | 0.34      |\n| Qdrant/Unicom-ViT-B-32      | 512       | Unicom image embeddings         | apache-2.0 | 0.48      |\n| Qdrant/Unicom-ViT-B-16      | 768       | Higher detail Unicom embeddings | apache-2.0 | 0.82      |\n\nThese models can be accessed through the `ImageEmbedding` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb622-704](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L622-L704)\n\n## Multimodal Late Interaction Models\n\nFastEmbed also supports multimodal late interaction models, such as ColPali, which enable matching between text and images using token-level representations.\n\n```\n```",
      "index": 4,
      "token_count": 574,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 7531,
      "end_char": 9528
    },
    {
      "content": "ColPali, which enable matching between text and images using token-level representations.\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/\\_\\_init\\_\\_.py1-6](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/__init__.py#L1-L6) [NOTICE16-18](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L16-L18)\n\nCurrently, FastEmbed supports the following multimodal late interaction model:\n\n| Model               | Description                                               | License |\n| ------------------- | --------------------------------------------------------- | ------- |\n| vidore/colpali-v1.3 | Multimodal late interaction model for text-image matching | gemma   |\n\nThis model can be accessed through the `LateInteractionMultimodalEmbedding` class.\n\nSources: [NOTICE16-19](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L16-L19)\n\n## Cross-Encoder Models for Reranking\n\nCross-encoder models evaluate text pairs together rather than encoding them separately, providing more accurate relevance scores for reranking search results.\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb732-738](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L732-L738)\n\nFastEmbed supports the following cross-encoder models for reranking:\n\n| Model                                     | Description                   | License      | Size (GB) |\n| ----------------------------------------- | ----------------------------- | ------------ | --------- |\n| Xenova/ms-marco-MiniLM-L-6-v2             | MiniLM-L-6-v2 for reranking   | apache-2.0   | 0.08      |\n| Xenova/ms-marco-MiniLM-L-12-v2            | MiniLM-L-12-v2 for reranking  | apache-2.0   | 0.12      |\n| jinaai/jina-reranker-v1-tiny-en           | Fast reranker with 8K context | apache-2.0   | 0.13      |\n| jinaai/jina-reranker-v1-turbo-en          | Fast reranker with 8K context | apache-2.0   | 0.15      |",
      "index": 5,
      "token_count": 535,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 9428,
      "end_char": 11386
    },
    {
      "content": "aai/jina-reranker-v1-turbo-en          | Fast reranker with 8K context | apache-2.0   | 0.15      |\n| BAAI/bge-reranker-base                    | BGE reranker base model       | mit          | 1.04      |\n| jinaai/jina-reranker-v2-base-multilingual | Multilingual reranker         | cc-by-nc-4.0 | 1.11      |\n\nThese models can be accessed through the `TextCrossEncoder` class:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb732-826](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb#L732-L826)\n\n## Model Selection Guide\n\nWhen choosing a model for your application, consider these factors:\n\n```\n```\n\nSources: [docs/examples/Supported\\_Models.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Supported_Models.ipynb)\n\n## License Considerations\n\nWhen using FastEmbed models, be aware of licensing restrictions:\n\n- Most models are available under permissive licenses (MIT, Apache-2.0)\n\n- Some Jina AI models have non-commercial licenses (cc-by-nc-4.0):\n\n  - jinaai/jina-colbert-v2\n  - jinaai/jina-reranker-v2-base-multilingual\n  - jinaai/jina-embeddings-v3\n\n- The ColPali model (vidore/colpali-v1.3) is subject to the Gemma Terms of Use\n\nAlways check the license information before using a model in production applications, especially for commercial use.\n\nSources: [NOTICE1-23](https://github.com/qdrant/fastembed/blob/b785640b/NOTICE#L1-L23)\n\nFor more detailed information about using these models with FastEmbed, see the [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md) and [Usage Examples](qdrant/fastembed/7-usage-examples.md) sections.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Supported Models](#supported-models.md)\n- [Model Types Overview](#model-types-overview.md)\n- [Dense Text Embedding Models](#dense-text-embedding-models.md)\n- [Sparse Text Embedding Models](#sparse-text-embedding-models.md)\n- [Late Interaction Text Embedding Models](#late-interaction-text-embedding-models.md)",
      "index": 6,
      "token_count": 571,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 11286,
      "end_char": 13298
    },
    {
      "content": "g-models.md)\n- [Late Interaction Text Embedding Models](#late-interaction-text-embedding-models.md)\n- [Image Embedding Models](#image-embedding-models.md)\n- [Multimodal Late Interaction Models](#multimodal-late-interaction-models.md)\n- [Cross-Encoder Models for Reranking](#cross-encoder-models-for-reranking.md)\n- [Model Selection Guide](#model-selection-guide.md)\n- [License Considerations](#license-considerations.md)",
      "index": 7,
      "token_count": 109,
      "metadata": {
        "title": "_qdrant_fastembed_6-supported-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_6-supported-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_6-supported-models.md",
        "file_name": "_qdrant_fastembed_6-supported-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.811924",
        "total_chunks": 8
      },
      "start_char": 13198,
      "end_char": 15246
    },
    {
      "content": "Usage Examples | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_7-usage-examples",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7-usage-examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7-usage-examples.md",
        "file_name": "_qdrant_fastembed_7-usage-examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.845884",
        "total_chunks": 5
      },
      "start_char": 0,
      "end_char": 2033
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Usage Examples\n\nRelevant source files\n\n- [docs/Getting Started.ipynb](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb>)\n- [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb)\n- [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb)\n- [docs/examples/Hybrid\\_Search.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb)\n- [docs/index.md](https://github.com/qdrant/fastembed/blob/b785640b/docs/index.md)\n- [docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/qdrant/Retrieval_with_FastEmbed.ipynb)\n\nThis page provides practical examples of using FastEmbed for various embedding tasks. It demonstrates how to generate and work with embeddings for different use cases including basic text embedding, sparse and hybrid search, late interaction models, and more. For installation and setup information, see [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md).\n\n## Basic Text Embedding\n\nThe most common use case for FastEmbed is generating dense text embeddings using the `TextEmbedding` class.\n\n```\n```\n\nEach embedding is a numpy array with the default model's dimension (384 for BAAI/bge-small-en-v1.5):\n\n```\n```\n\n### Using Different Models\n\nFastEmbed supports various embedding models with different capabilities:\n\n```\n```\n\n### Query vs Passage Embedding\n\nFor retrieval tasks, it's recommended to use specific embedding methods for queries and passages:\n\n```\n```",
      "index": 1,
      "token_count": 543,
      "metadata": {
        "title": "_qdrant_fastembed_7-usage-examples",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7-usage-examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7-usage-examples.md",
        "file_name": "_qdrant_fastembed_7-usage-examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.845884",
        "total_chunks": 5
      },
      "start_char": 1933,
      "end_char": 3958
    },
    {
      "content": "ieval tasks, it's recommended to use specific embedding methods for queries and passages:\n\n```\n```\n\nSources: [docs/Getting Started.ipynb68-86](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb#L68-L86>) [docs/Getting Started.ipynb116-120](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb#L116-L120>) [docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb73-93](https://github.com/qdrant/fastembed/blob/b785640b/docs/qdrant/Retrieval_with_FastEmbed.ipynb#L73-L93) [docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb111-114](https://github.com/qdrant/fastembed/blob/b785640b/docs/qdrant/Retrieval_with_FastEmbed.ipynb#L111-L114)\n\n## Sparse and Hybrid Search\n\nFastEmbed supports sparse embeddings through the `SparseTextEmbedding` class, which is useful for hybrid search applications.\n\n### Sparse Embedding Generation\n\n```\n```\n\nSparse embeddings contain indices and values, representing token positions and their weights:\n\n```\n```\n\n### Hybrid Search with Qdrant\n\nCombining dense and sparse embeddings enables hybrid search using Qdrant:\n\n```\n```\n\nThe hybrid search workflow:\n\n```\n```\n\nSources: [docs/examples/Hybrid\\_Search.ipynb52-73](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L52-L73) [docs/examples/Hybrid\\_Search.ipynb442-470](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L442-L470) [docs/examples/Hybrid\\_Search.ipynb874-922](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L874-L922)\n\n## ColBERT and Late Interaction Models\n\nFastEmbed supports late interaction models through the `LateInteractionTextEmbedding` class, which enables more precise retrieval by preserving token-level interactions.\n\n### Understanding Late Interaction\n\nLate interaction models like ColBERT compute embeddings for each token in queries and documents, rather than pooling them into a single vector:\n\n```\n```\n\n### Using ColBERT Model\n\n```\n```\n\n### Maximal Similarity (MaxSim) Scoring",
      "index": 2,
      "token_count": 598,
      "metadata": {
        "title": "_qdrant_fastembed_7-usage-examples",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7-usage-examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7-usage-examples.md",
        "file_name": "_qdrant_fastembed_7-usage-examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.845884",
        "total_chunks": 5
      },
      "start_char": 3858,
      "end_char": 5882
    },
    {
      "content": "single vector:\n\n```\n```\n\n### Using ColBERT Model\n\n```\n```\n\n### Maximal Similarity (MaxSim) Scoring\n\nLate interaction models require a specific similarity computation called MaxSim:\n\n```\n```\n\nThe MaxSim operation workflow:\n\n```\n```\n\nSources: [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb72-74](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb#L72-L74) [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb168-205](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb#L168-L205) [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb280-309](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb#L280-L309)\n\n## Performance Comparison\n\nFastEmbed is designed to be faster than traditional embedding libraries by utilizing ONNX Runtime for inference and optimized model implementations.\n\n### Benchmarking with Hugging Face Transformers\n\nThe following benchmark compares FastEmbed with Hugging Face Transformers using the same model (BAAI/bge-small-en-v1.5):\n\n```\n```\n\nA typical benchmark with a set of 12 documents shows FastEmbed is around 10-20% faster than Hugging Face Transformers:\n\n| Framework       | Average (s) | Maximum (s) | Minimum (s) |\n| --------------- | ----------- | ----------- | ----------- |\n| HF Transformers | 0.047       | 0.066       | 0.043       |\n| FastEmbed       | 0.044       | 0.057       | 0.043       |\n\nFor larger document sets, the performance gap increases due to FastEmbed's parallel processing capabilities.\n\n### Parallelization Benefits\n\nFastEmbed uses data parallelism to speed up embedding generation, significantly reducing processing time for large datasets. In testing with sparse embeddings, parallelization reduced processing time by approximately 50-60% on a multi-core system.\n\nSources: [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb149-166](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb#L149-L166) [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.",
      "index": 3,
      "token_count": 571,
      "metadata": {
        "title": "_qdrant_fastembed_7-usage-examples",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7-usage-examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7-usage-examples.md",
        "file_name": "_qdrant_fastembed_7-usage-examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.845884",
        "total_chunks": 5
      },
      "start_char": 5782,
      "end_char": 7830
    },
    {
      "content": "s/examples/FastEmbed_vs_HF_Comparison.ipynb#L149-L166) [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb256-278](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb#L256-L278)\n\n## Integration with Qdrant\n\nFastEmbed integrates seamlessly with Qdrant for vector search and retrieval:\n\n```\n```\n\nThe integration workflow:\n\n```\n```\n\nSources: [docs/index.md40-74](https://github.com/qdrant/fastembed/blob/b785640b/docs/index.md#L40-L74)\n\n## Cross-Encoder Reranking\n\nThe `TextCrossEncoder` class can be used to rerank search results:\n\n```\n```\n\nThe reranking workflow:\n\n```\n```\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Usage Examples](#usage-examples.md)\n- [Basic Text Embedding](#basic-text-embedding.md)\n- [Using Different Models](#using-different-models.md)\n- [Query vs Passage Embedding](#query-vs-passage-embedding.md)\n- [Sparse and Hybrid Search](#sparse-and-hybrid-search.md)\n- [Sparse Embedding Generation](#sparse-embedding-generation.md)\n- [Hybrid Search with Qdrant](#hybrid-search-with-qdrant.md)\n- [ColBERT and Late Interaction Models](#colbert-and-late-interaction-models.md)\n- [Understanding Late Interaction](#understanding-late-interaction.md)\n- [Using ColBERT Model](#using-colbert-model.md)\n- [Maximal Similarity (MaxSim) Scoring](#maximal-similarity-maxsim-scoring.md)\n- [Performance Comparison](#performance-comparison.md)\n- [Benchmarking with Hugging Face Transformers](#benchmarking-with-hugging-face-transformers.md)\n- [Parallelization Benefits](#parallelization-benefits.md)\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Cross-Encoder Reranking](#cross-encoder-reranking.md)",
      "index": 4,
      "token_count": 474,
      "metadata": {
        "title": "_qdrant_fastembed_7-usage-examples",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7-usage-examples.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7-usage-examples.md",
        "file_name": "_qdrant_fastembed_7-usage-examples.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.845884",
        "total_chunks": 5
      },
      "start_char": 7730,
      "end_char": 9778
    },
    {
      "content": "Basic Text Embedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_7.1-basic-text-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.1-basic-text-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "file_name": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.867414",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2039
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Basic Text Embedding\n\nRelevant source files\n\n- [docs/Getting Started.ipynb](<https://github.com/qdrant/fastembed/blob/b785640b/docs/Getting Started.ipynb>)\n- [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb)\n- [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb)\n- [docs/examples/Hybrid\\_Search.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb)\n- [docs/index.md](https://github.com/qdrant/fastembed/blob/b785640b/docs/index.md)\n- [docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/qdrant/Retrieval_with_FastEmbed.ipynb)\n\nThis page covers the fundamentals of text embedding using FastEmbed's `TextEmbedding` class. Text embedding is the process of converting text into numerical vector representations that capture semantic meaning, enabling computers to understand and compare text documents mathematically.\n\nFor sparse embedding techniques, see [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md), and for late interaction embedding models, see [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md).\n\n## TextEmbedding Class Overview\n\nThe `TextEmbedding` class serves as the primary entry point for dense text embedding operations in FastEmbed. It provides a simple, high-performance interface to generate vector representations from text documents.\n\n```\n```\n\nSources: docs/Getting Started.ipynb, docs/index.md\n\n## Initialization and Basic Usage",
      "index": 1,
      "token_count": 543,
      "metadata": {
        "title": "_qdrant_fastembed_7.1-basic-text-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.1-basic-text-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "file_name": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.867414",
        "total_chunks": 6
      },
      "start_char": 1939,
      "end_char": 3970
    },
    {
      "content": "s.\n\n```\n```\n\nSources: docs/Getting Started.ipynb, docs/index.md\n\n## Initialization and Basic Usage\n\n### Initializing TextEmbedding\n\nYou can initialize the `TextEmbedding` class with either default settings or custom model parameters:\n\n```\n```\n\nThe initialization process requires minimal code:\n\n```\n```\n\nSources: docs/Getting Started.ipynb:68-80, docs/Getting Started.ipynb:185-187\n\n### Generating Embeddings\n\nOnce initialized, you can generate embeddings for a list of text documents:\n\n```\n```\n\nThe following example demonstrates basic embedding generation:\n\n```\n```\n\nSources: docs/Getting Started.ipynb:68-86, docs/Getting Started.ipynb:116-120\n\n## Understanding Embedding Output\n\nThe embeddings produced by `TextEmbedding` are NumPy arrays with dimensions determined by the model used. For example:\n\n| Model                          | Embedding Dimension |\n| ------------------------------ | ------------------- |\n| BAAI/bge-small-en-v1.5         | 384                 |\n| BAAI/bge-large-en-v1.5         | 1024                |\n| intfloat/multilingual-e5-large | 1024                |\n\nYou can process the embeddings in various ways:\n\n```\n```\n\nSources: docs/Getting Started.ipynb:98-120, docs/Getting Started.ipynb:140-143\n\n## Advanced Features\n\n### Query vs. Passage Embeddings\n\nFastEmbed distinguishes between query and passage embeddings to improve search relevance:\n\n```\n```\n\nUsing specialized methods for queries and documents significantly improves retrieval performance:\n\n```\n```\n\nSources: docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb:88-90, docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb:112, docs/Getting Started.ipynb:152-159\n\n### Format for Document Prefixes\n\nWhen performing retrieval tasks with the default model, you can add special prefixes to improve performance:\n\n- **Queries**: Add \"query:\" at the beginning of each query string\n- **Passages**: Add \"passage:\" at the beginning of each passage string",
      "index": 2,
      "token_count": 481,
      "metadata": {
        "title": "_qdrant_fastembed_7.1-basic-text-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.1-basic-text-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "file_name": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.867414",
        "total_chunks": 6
      },
      "start_char": 3870,
      "end_char": 5790
    },
    {
      "content": "inning of each query string\n- **Passages**: Add \"passage:\" at the beginning of each passage string\n\nThis is handled automatically by the `query_embed()` and `passage_embed()` methods, but can also be added manually when using the general `embed()` method.\n\nSources: docs/Getting Started.ipynb:152-159\n\n### Parallel Processing\n\nFastEmbed automatically leverages parallel processing for efficiency:\n\n```\n```\n\nThis parallel processing significantly reduces the time needed to generate embeddings for large datasets, providing much better performance compared to traditional embedding libraries.\n\nSources: docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb:252-278\n\n### Supported Models\n\nTextEmbedding supports various pre-trained models that can be specified during initialization:\n\n```\n```\n\nSome commonly used models include:\n\n- BAAI/bge-small-en-v1.5 (default)\n- BAAI/bge-large-en-v1.5\n- intfloat/multilingual-e5-large\n- sentence-transformers/all-mpnet-base-v2\n\nSources: docs/Getting Started.ipynb:159-162, docs/Getting Started.ipynb:185-187\n\n## Example Use Cases\n\n### Basic Retrieval\n\nOne common use case for text embeddings is semantic search:\n\n```\n```\n\nThis example shows how to perform basic retrieval using cosine similarity between the query and document embeddings.\n\nSources: docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb:111-123\n\n### Integration with Vector Databases\n\nFastEmbed integrates smoothly with vector databases like Qdrant:\n\n```\n```\n\nSources: docs/index.md:49-73\n\n## Performance Advantages\n\nFastEmbed is designed for high performance through several optimizations:\n\n1. **ONNX Runtime**: Uses optimized inference engine for speed\n2. **Quantized Models**: Reduced model size without significant accuracy loss\n3. **Parallel Processing**: Automatic multi-threading for batch operations\n\nThese optimizations make FastEmbed significantly faster than traditional implementations:\n\n| Metric             | FastEmbed | Hugging Face Transformers |\n| ------------------ | --------- | ------------------------- |\n| Avg Time per Batch | 0.",
      "index": 3,
      "token_count": 461,
      "metadata": {
        "title": "_qdrant_fastembed_7.1-basic-text-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.1-basic-text-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "file_name": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.867414",
        "total_chunks": 6
      },
      "start_char": 5690,
      "end_char": 7730
    },
    {
      "content": "nsformers |\n| ------------------ | --------- | ------------------------- |\n| Avg Time per Batch | 0.044s    | 0.047s                    |\n| Max Time           | 0.057s    | 0.066s                    |\n| Min Time           | 0.043s    | 0.043s                    |\n| Characters/Second  | \\~1200    | \\~1100                    |\n\nSources: docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb:274-345\n\n## Summary\n\nBasic text embedding with FastEmbed through the `TextEmbedding` class provides:\n\n1. A simple interface for generating high-quality vector representations of text\n2. Specialized methods for query and passage embedding to improve search relevance\n3. High performance through ONNX optimization and parallel processing\n4. Support for multiple pre-trained models\n5. Easy integration with vector databases and retrieval systems\n\nThese capabilities make FastEmbed an excellent choice for applications requiring semantic text understanding, including search, classification, clustering, and recommendation systems.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Basic Text Embedding](#basic-text-embedding.md)\n- [TextEmbedding Class Overview](#textembedding-class-overview.md)\n- [Initialization and Basic Usage](#initialization-and-basic-usage.md)\n- [Initializing TextEmbedding](#initializing-textembedding.md)\n- [Generating Embeddings](#generating-embeddings.md)\n- [Understanding Embedding Output](#understanding-embedding-output.md)\n- [Advanced Features](#advanced-features.md)\n- [Query vs. Passage Embeddings](#query-vs-passage-embeddings.md)\n- [Format for Document Prefixes](#format-for-document-prefixes.md)\n- [Parallel Processing](#parallel-processing.md)\n- [Supported Models](#supported-models.md)\n- [Example Use Cases](#example-use-cases.md)\n- [Basic Retrieval](#basic-retrieval.md)\n- [Integration with Vector Databases](#integration-with-vector-databases.md)\n- [Performance Advantages](#performance-advantages.md)\n- [Summary](#summary.md)",
      "index": 4,
      "token_count": 471,
      "metadata": {
        "title": "_qdrant_fastembed_7.1-basic-text-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.1-basic-text-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "file_name": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.867414",
        "total_chunks": 6
      },
      "start_char": 7630,
      "end_char": 9678
    },
    {
      "content": ")\n- [Summary](#summary.md)",
      "index": 5,
      "token_count": 9,
      "metadata": {
        "title": "_qdrant_fastembed_7.1-basic-text-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.1-basic-text-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "file_name": "_qdrant_fastembed_7.1-basic-text-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.867414",
        "total_chunks": 6
      },
      "start_char": 9578,
      "end_char": 11626
    },
    {
      "content": "Sparse and Hybrid Search | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_7.2-sparse-and-hybrid-search",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "file_name": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.890622",
        "total_chunks": 6
      },
      "start_char": 0,
      "end_char": 2043
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Sparse and Hybrid Search\n\nRelevant source files\n\n- [tests/test\\_attention\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py)\n- [tests/test\\_sparse\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py)\n\nThis page provides a comprehensive guide to implementing sparse embeddings and hybrid search using FastEmbed. While [TextEmbedding](qdrant/fastembed/3.1-textembedding.md) creates dense vector representations, sparse embeddings offer a different approach with unique advantages that complement dense embeddings in search applications.\n\n## Understanding Sparse Embeddings and Hybrid Search\n\nSparse embeddings represent text as high-dimensional, sparse vectors where most elements are zero. Unlike dense embeddings, sparse vectors only store the non-zero elements through indices and corresponding values.\n\n```\n```\n\nSources: [tests/test\\_sparse\\_embeddings.py10-47](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L10-L47)\n\n## Sparse Embedding Models in FastEmbed\n\nFastEmbed provides several sparse embedding models through the `SparseTextEmbedding` class:\n\n```\n```\n\nSources: [tests/test\\_sparse\\_embeddings.py6-7](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L6-L7) [tests/test\\_attention\\_embeddings.py6](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L6-L6)\n\n### Supported Models\n\nFastEmbed supports multiple types of sparse embedding models:\n\n1. **SPLADE Models**: Sparse Lexical and Expansion models (e.g., \"prithivida/Splade\\_PP\\_en\\_v1\")\n2.",
      "index": 1,
      "token_count": 498,
      "metadata": {
        "title": "_qdrant_fastembed_7.2-sparse-and-hybrid-search",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "file_name": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.890622",
        "total_chunks": 6
      },
      "start_char": 1943,
      "end_char": 3941
    },
    {
      "content": "1. **SPLADE Models**: Sparse Lexical and Expansion models (e.g., \"prithivida/Splade\\_PP\\_en\\_v1\")\n2. **BM25**: Classical lexical retrieval algorithm based on term frequency-inverse document frequency\n3. **BM42**: Attention-based sparse embeddings that combine neural and lexical features\n\nEach model produces sparse embeddings with different characteristics suitable for various search tasks.\n\nSources: [tests/test\\_sparse\\_embeddings.py52](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L52-L52) [tests/test\\_attention\\_embeddings.py10](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L10-L10)\n\n## Using Sparse Embeddings\n\n### Basic Usage\n\nThe `SparseTextEmbedding` class provides a straightforward interface for generating sparse embeddings:\n\n```\n```\n\nSources: [tests/test\\_sparse\\_embeddings.py57-65](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L57-L65)\n\n### Query vs. Passage Embeddings\n\nFor search applications, FastEmbed allows you to generate specialized embeddings for queries and passages:\n\n```\n```\n\nThis distinction is important for asymmetric search scenarios where queries and documents may need different processing.\n\nSources: [tests/test\\_sparse\\_embeddings.py84-86](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L84-L86)\n\n### Language Support and Special Characters\n\nBM25 models support multiple languages and handle special characters effectively:\n\n```\n```\n\nThe language parameter affects tokenization and stemming behavior, optimizing for language-specific features.\n\nSources: [tests/test\\_attention\\_embeddings.py104-111](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L104-L111) [tests/test\\_attention\\_embeddings.py125-144](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L125-L144)\n\n## Sparse Embedding Structure\n\nSparse embeddings in FastEmbed are represented with two components:\n\n1.",
      "index": 2,
      "token_count": 507,
      "metadata": {
        "title": "_qdrant_fastembed_7.2-sparse-and-hybrid-search",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "file_name": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.890622",
        "total_chunks": 6
      },
      "start_char": 3841,
      "end_char": 5868
    },
    {
      "content": "Sparse Embedding Structure\n\nSparse embeddings in FastEmbed are represented with two components:\n\n1. **Indices**: Integer array indicating which dimensions have non-zero values\n2. **Values**: Corresponding importance/weight for each non-zero dimension\n\n```\n```\n\nThis format allows for efficient storage and processing of high-dimensional sparse vectors.\n\nSources: [tests/test\\_sparse\\_embeddings.py10-47](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L10-L47)\n\n## Implementing Hybrid Search\n\nHybrid search combines the strengths of sparse and dense embeddings to improve search quality. Here's how to implement it:\n\n```\n```\n\n### Implementation Steps:\n\n1. Generate both sparse and dense embeddings for your document collection\n\n2. For each query, generate both sparse and dense query embeddings\n\n3. Perform separate searches with each embedding type\n\n4. Combine and re-rank the results using strategies like:\n\n   - Score normalization and weighted combination\n   - Reciprocal rank fusion\n   - Taking the union of top results from both methods\n\nSources: [tests/test\\_sparse\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py) [tests/test\\_attention\\_embeddings.py](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py)\n\n## Performance Optimization\n\nFastEmbed provides several performance optimizations for sparse embeddings:\n\n### Batch Processing\n\nProcess multiple documents efficiently:\n\n```\n```\n\nSources: [tests/test\\_sparse\\_embeddings.py98-102](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L98-L102)\n\n### Parallel Execution\n\nEnable parallel processing to utilize multiple CPU cores:\n\n```\n```\n\nSources: [tests/test\\_sparse\\_embeddings.py98-121](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L98-L121) [tests/test\\_attention\\_embeddings.py74-95](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L74-L95)\n\n### Lazy Loading",
      "index": 3,
      "token_count": 492,
      "metadata": {
        "title": "_qdrant_fastembed_7.2-sparse-and-hybrid-search",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "file_name": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.890622",
        "total_chunks": 6
      },
      "start_char": 5768,
      "end_char": 7815
    },
    {
      "content": "b.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L74-L95)\n\n### Lazy Loading\n\nDefer model loading until first use to conserve memory:\n\n```\n```\n\nSources: [tests/test\\_sparse\\_embeddings.py189-206](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L189-L206) [tests/test\\_attention\\_embeddings.py147-159](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_attention_embeddings.py#L147-L159)\n\n## BM25 Configuration\n\nThe BM25 model offers additional configuration options:\n\n### Stemming and Stopwords\n\n```\n```\n\nStemming reduces words to their root form, while stopword removal filters out common words. These techniques can significantly impact search quality.\n\nSources: [tests/test\\_sparse\\_embeddings.py136-186](https://github.com/qdrant/fastembed/blob/b785640b/tests/test_sparse_embeddings.py#L136-L186)\n\n## Summary\n\nSparse embeddings provide a complementary approach to dense embeddings, capturing lexical and token-level information that can improve search relevance. FastEmbed's `SparseTextEmbedding` class offers a flexible and efficient way to generate these embeddings with various models, including SPLADE, BM25, and BM42.\n\nHybrid search, combining sparse and dense approaches, leverages the strengths of both methods to deliver more comprehensive search results, particularly beneficial for complex search scenarios where both semantic and lexical matching are important.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Sparse and Hybrid Search](#sparse-and-hybrid-search.md)\n- [Understanding Sparse Embeddings and Hybrid Search](#understanding-sparse-embeddings-and-hybrid-search.md)\n- [Sparse Embedding Models in FastEmbed](#sparse-embedding-models-in-fastembed.md)\n- [Supported Models](#supported-models.md)\n- [Using Sparse Embeddings](#using-sparse-embeddings.md)\n- [Basic Usage](#basic-usage.md)\n- [Query vs. Passage Embeddings](#query-vs-passage-embeddings.md)\n- [Language Support and Special Characters](#language-support-and-special-characters.",
      "index": 4,
      "token_count": 506,
      "metadata": {
        "title": "_qdrant_fastembed_7.2-sparse-and-hybrid-search",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "file_name": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.890622",
        "total_chunks": 6
      },
      "start_char": 7715,
      "end_char": 9764
    },
    {
      "content": "embeddings.md)\n- [Language Support and Special Characters](#language-support-and-special-characters.md)\n- [Sparse Embedding Structure](#sparse-embedding-structure.md)\n- [Implementing Hybrid Search](#implementing-hybrid-search.md)\n- [Implementation Steps:](#implementation-steps.md)\n- [Performance Optimization](#performance-optimization.md)\n- [Batch Processing](#batch-processing.md)\n- [Parallel Execution](#parallel-execution.md)\n- [Lazy Loading](#lazy-loading.md)\n- [BM25 Configuration](#bm25-configuration.md)\n- [Stemming and Stopwords](#stemming-and-stopwords.md)\n- [Summary](#summary.md)",
      "index": 5,
      "token_count": 146,
      "metadata": {
        "title": "_qdrant_fastembed_7.2-sparse-and-hybrid-search",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "file_name": "_qdrant_fastembed_7.2-sparse-and-hybrid-search.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.890622",
        "total_chunks": 6
      },
      "start_char": 9664,
      "end_char": 11712
    },
    {
      "content": "ColBERT and Late Interaction | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 622,
      "metadata": {
        "title": "_qdrant_fastembed_7.3-colbert-and-late-interaction",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "file_name": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.924524",
        "total_chunks": 5
      },
      "start_char": 0,
      "end_char": 2047
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# ColBERT and Late Interaction\n\nRelevant source files\n\n- [fastembed/late\\_interaction/colbert.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py)\n\nThis page explains how to use FastEmbed's implementation of ColBERT for late interaction embeddings. Late interaction is a retrieval approach that defers token-to-token interaction between queries and documents until the final ranking stage, offering improved precision compared to traditional dense embeddings. For information about other embedding approaches, see [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md) or [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md).\n\n## What is ColBERT and Late Interaction?\n\nColBERT (Contextualized Late Interaction over BERT) is a retrieval model architecture that represents documents and queries as bags of contextualized embeddings rather than single vector representations. Unlike dense text embeddings that compress entire texts into single vectors, ColBERT preserves token-level information by:\n\n1. Embedding each token in a document or query separately\n2. Performing \"late interaction\" by computing fine-grained similarity between tokens only at retrieval time\n\nThis approach provides higher precision because it retains more context and semantic information, making it particularly effective for complex information retrieval tasks.\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py39-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L65)\n\n## ColBERT in FastEmbed Architecture",
      "index": 1,
      "token_count": 445,
      "metadata": {
        "title": "_qdrant_fastembed_7.3-colbert-and-late-interaction",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "file_name": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.924524",
        "total_chunks": 5
      },
      "start_char": 1947,
      "end_char": 3905
    },
    {
      "content": "blob/b785640b/fastembed/late_interaction/colbert.py#L39-L65)\n\n## ColBERT in FastEmbed Architecture\n\nFastEmbed implements ColBERT through the `LateInteractionTextEmbedding` entry point class, which uses the `Colbert` implementation class. This fits into the larger FastEmbed architecture as follows:\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py39-40](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L39-L40)\n\n## Supported Models\n\nFastEmbed currently supports these ColBERT models:\n\n| Model Name                            | Dimensions | Description                                                       | License    | Size (GB) |\n| ------------------------------------- | ---------- | ----------------------------------------------------------------- | ---------- | --------- |\n| colbert-ir/colbertv2.0                | 128        | Late interaction model                                            | MIT        | 0.44      |\n| answerdotai/answerai-colbert-small-v1 | 96         | Text embeddings, Multilingual (\\~100 languages), 512 input tokens | Apache 2.0 | 0.13      |\n\nSources: [fastembed/late\\_interaction/colbert.py17-36](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L17-L36)\n\n## Using ColBERT in FastEmbed\n\n### Basic Usage\n\nHere's how to use ColBERT for late interaction embeddings:\n\n```\n```\n\nThe key difference between ColBERT and standard dense embeddings is that:\n\n- Documents are embedded at indexing time\n- Queries are embedded at search time\n- The two never interact until the final similarity calculation\n\n### ColBERT Embedding Process\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py67-104](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L67-L104) [fastembed/late\\_interaction/colbert.py205-249](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L205-L249)\n\n## Implementation Details\n\n### Special Token Handling",
      "index": 2,
      "token_count": 514,
      "metadata": {
        "title": "_qdrant_fastembed_7.3-colbert-and-late-interaction",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "file_name": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.924524",
        "total_chunks": 5
      },
      "start_char": 3805,
      "end_char": 5820
    },
    {
      "content": "mbed/late_interaction/colbert.py#L205-L249)\n\n## Implementation Details\n\n### Special Token Handling\n\nColBERT uses different marker tokens for queries and documents to help the model distinguish their roles:\n\n- `QUERY_MARKER_TOKEN_ID = 1` - Added to queries\n- `DOCUMENT_MARKER_TOKEN_ID = 2` - Added to documents\n\nFor queries, ColBERT also pads shorter queries with mask tokens up to a minimum length (31 tokens) to improve retrieval performance through query augmentation.\n\nSources: [fastembed/late\\_interaction/colbert.py40-43](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L40-L43) [fastembed/late\\_interaction/colbert.py86-104](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L86-L104)\n\n### Document Embedding Post-Processing\n\nA key aspect of ColBERT is how document embeddings are post-processed:\n\n1. Punctuation and padding tokens are masked out (attention mask set to 0)\n2. The attention mask is applied to the model output\n3. Token embeddings are normalized to unit length\n\n```\n```\n\nSources: [fastembed/late\\_interaction/colbert.py45-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L45-L65)\n\n### Tokenization Process\n\nThe tokenization process differs between queries and documents:\n\n1. **Documents**: Standard tokenization with truncation\n2. **Queries**: Tokenized and padded with `[MASK]` tokens to a minimum length\n\nThe tokenizer also handles special considerations like truncation to avoid overflow after adding marker tokens.\n\nSources: [fastembed/late\\_interaction/colbert.py79-108](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L79-L108) [fastembed/late\\_interaction/colbert.py195-203](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L195-L203)\n\n## Performance Considerations\n\nThe ColBERT approach offers higher precision but comes with trade-offs:\n\n1. **Storage**: Requires more storage than dense embeddings (token-level vs.",
      "index": 3,
      "token_count": 545,
      "metadata": {
        "title": "_qdrant_fastembed_7.3-colbert-and-late-interaction",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "file_name": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.924524",
        "total_chunks": 5
      },
      "start_char": 5720,
      "end_char": 7764
    },
    {
      "content": "comes with trade-offs:\n\n1. **Storage**: Requires more storage than dense embeddings (token-level vs. single vector)\n2. **Computation**: Late interaction calculation is more computationally intensive at search time\n3. **Memory**: Uses more memory during search due to token-level operations\n\nFor optimal performance, FastEmbed implements ColBERT using ONNX Runtime for efficient inference and supports parallel processing for batch embedding.\n\nSources: [fastembed/late\\_interaction/colbert.py205-237](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction/colbert.py#L205-L237)\n\n## Working with Vector Databases\n\nWhen using ColBERT with vector databases like Qdrant, you'll need to store the token-level embeddings and implement the late interaction scoring during retrieval. This often requires custom extensions or specialized vector database support for ColBERT's retrieval mechanism.\n\nFor more information on integrating with Qdrant specifically, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [ColBERT and Late Interaction](#colbert-and-late-interaction.md)\n- [What is ColBERT and Late Interaction?](#what-is-colbert-and-late-interaction.md)\n- [ColBERT in FastEmbed Architecture](#colbert-in-fastembed-architecture.md)\n- [Supported Models](#supported-models.md)\n- [Using ColBERT in FastEmbed](#using-colbert-in-fastembed.md)\n- [Basic Usage](#basic-usage.md)\n- [ColBERT Embedding Process](#colbert-embedding-process.md)\n- [Implementation Details](#implementation-details.md)\n- [Special Token Handling](#special-token-handling.md)\n- [Document Embedding Post-Processing](#document-embedding-post-processing.md)\n- [Tokenization Process](#tokenization-process.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Working with Vector Databases](#working-with-vector-databases.md)",
      "index": 4,
      "token_count": 441,
      "metadata": {
        "title": "_qdrant_fastembed_7.3-colbert-and-late-interaction",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "file_name": "_qdrant_fastembed_7.3-colbert-and-late-interaction.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.924524",
        "total_chunks": 5
      },
      "start_char": 7664,
      "end_char": 9712
    },
    {
      "content": "Image Embedding | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 620,
      "metadata": {
        "title": "_qdrant_fastembed_7.4-image-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.4-image-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.4-image-embedding.md",
        "file_name": "_qdrant_fastembed_7.4-image-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.950028",
        "total_chunks": 5
      },
      "start_char": 0,
      "end_char": 2034
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Image Embedding\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n- [fastembed/image/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py)\n\nThis page documents the image embedding capabilities of the FastEmbed library. FastEmbed provides efficient mechanisms for generating vector representations (embeddings) from images using optimized ONNX models. For text embedding functionality, see [TextEmbedding](qdrant/fastembed/3.1-textembedding.md), and for multimodal embedding, see [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md).\n\n## Overview\n\nThe `ImageEmbedding` class is a high-level interface for generating embeddings from images. It leverages ONNX Runtime for efficient inference and supports various vision models. Image embeddings can be used for image search, similarity comparison, and as input to multimodal systems.\n\n```\n```\n\nSources: [fastembed/image/onnx\\_embedding.py62-65](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L62-L65)\n\n## Supported Models\n\nFastEmbed supports several vision models for image embedding, each with different characteristics, dimensions, and applications:\n\n| Model Name                  | Dimensions | Type       | Year | Description                                  |\n| --------------------------- | ---------- | ---------- | ---- | -------------------------------------------- |",
      "index": 1,
      "token_count": 471,
      "metadata": {
        "title": "_qdrant_fastembed_7.4-image-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.4-image-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.4-image-embedding.md",
        "file_name": "_qdrant_fastembed_7.4-image-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.950028",
        "total_chunks": 5
      },
      "start_char": 1934,
      "end_char": 3910
    },
    {
      "content": "----------------- | ---------- | ---------- | ---- | -------------------------------------------- |\n| Qdrant/clip-ViT-B-32-vision | 512        | Multimodal | 2021 | Image embeddings from CLIP model             |\n| Qdrant/resnet50-onnx        | 2048       | Unimodal   | 2016 | Image-only embeddings using ResNet50         |\n| Qdrant/Unicom-ViT-B-16      | 768        | Multimodal | 2023 | Detailed image embeddings, higher resolution |\n| Qdrant/Unicom-ViT-B-32      | 512        | Multimodal | 2023 | Efficient image embeddings                   |\n| jinaai/jina-clip-v1         | 768        | Multimodal | 2024 | Recent multimodal embedding model            |\n\n```\n```\n\nSources: [fastembed/image/onnx\\_embedding.py13-59](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L13-L59)\n\n## Image Embedding Process\n\nThe image embedding process in FastEmbed follows several steps from input to embedding generation:\n\n```\n```\n\nSources: [fastembed/image/onnx\\_embedding.py148-181](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L148-L181)\n\n## Basic Usage\n\nYou can generate image embeddings with just a few lines of code:\n\n```\n```\n\nThe `embed()` method returns a generator that yields numpy arrays, which can be converted to a list as shown above.\n\nSources: [README.md137-155](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L137-L155)\n\n## Configuration Options\n\nThe `ImageEmbedding` class provides several configuration options:\n\n```\n```\n\nKey parameters include:\n\n- `model_name`: Name of the model to use (required)\n- `cache_dir`: Directory to store downloaded models (optional)\n- `threads`: Number of threads for inference (optional)\n- `providers`: ONNX Runtime execution providers (optional)\n- `cuda`: Whether to use CUDA for inference (optional)\n- `device_ids`: List of device IDs for parallel processing (optional)\n- `lazy_load`: Whether to defer model loading until needed (optional)\n\nThe `embed()` method also accepts parameters:\n\n- `images`: Images to embed (required)",
      "index": 2,
      "token_count": 560,
      "metadata": {
        "title": "_qdrant_fastembed_7.4-image-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.4-image-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.4-image-embedding.md",
        "file_name": "_qdrant_fastembed_7.4-image-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.950028",
        "total_chunks": 5
      },
      "start_char": 3810,
      "end_char": 5859
    },
    {
      "content": "d (optional)\n\nThe `embed()` method also accepts parameters:\n\n- `images`: Images to embed (required)\n- `batch_size`: Number of images to process at once (default: 16)\n- `parallel`: Number of worker processes for parallelization (optional)\n\nSources: [fastembed/image/onnx\\_embedding.py63-96](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L63-L96) [fastembed/image/onnx\\_embedding.py148-166](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L148-L166)\n\n## GPU Acceleration\n\nFor faster image embedding, FastEmbed supports GPU acceleration through ONNX Runtime:\n\n1. Install with GPU support:\n\n   ```\n   ```\n\n2. Specify CUDA execution provider:\n\n   ```\n   ```\n\nGPU acceleration can significantly improve performance, especially for large batches of images.\n\nSources: [README.md210-230](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L210-L230) [docs/examples/FastEmbed\\_GPU.ipynb1-193](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L1-L193)\n\n## Integration with Qdrant\n\nFastEmbed's image embedding can be easily integrated with Qdrant vector database:\n\n```\n```\n\nSources: [README.md232-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L232-L281)\n\n## Implementation Details\n\nThe `OnnxImageEmbedding` class inherits from both `ImageEmbeddingBase` and `OnnxImageModel[NumpyArray]`. It manages model downloading, caching, and inference operations. The actual embedding generation is delegated to ONNX Runtime, which provides efficient execution on both CPU and GPU.\n\nFor parallel processing, the class utilizes `OnnxImageEmbeddingWorker`, which initializes separate model instances in worker processes to handle embedding generation in parallel.\n\nThe embedding vectors are normalized after model inference to ensure they have unit length, which is important for cosine similarity calculations in vector search.\n\nSources: [fastembed/image/onnx\\_embedding.py62-198](https://github.",
      "index": 3,
      "token_count": 526,
      "metadata": {
        "title": "_qdrant_fastembed_7.4-image-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.4-image-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.4-image-embedding.md",
        "file_name": "_qdrant_fastembed_7.4-image-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.950028",
        "total_chunks": 5
      },
      "start_char": 5759,
      "end_char": 7767
    },
    {
      "content": "calculations in vector search.\n\nSources: [fastembed/image/onnx\\_embedding.py62-198](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_embedding.py#L62-L198)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Image Embedding](#image-embedding.md)\n- [Overview](#overview.md)\n- [Supported Models](#supported-models.md)\n- [Image Embedding Process](#image-embedding-process.md)\n- [Basic Usage](#basic-usage.md)\n- [Configuration Options](#configuration-options.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Implementation Details](#implementation-details.md)",
      "index": 4,
      "token_count": 178,
      "metadata": {
        "title": "_qdrant_fastembed_7.4-image-embedding",
        "source": "qdrant_fastembed\\_qdrant_fastembed_7.4-image-embedding.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_7.4-image-embedding.md",
        "file_name": "_qdrant_fastembed_7.4-image-embedding.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.950028",
        "total_chunks": 5
      },
      "start_char": 7667,
      "end_char": 9715
    },
    {
      "content": "Performance Optimization | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 619,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 0,
      "end_char": 2043
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Performance Optimization\n\nRelevant source files\n\n- [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb)\n- [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb)\n- [docs/examples/Hybrid\\_Search.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb)\n- [docs/qdrant/Retrieval\\_with\\_FastEmbed.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/qdrant/Retrieval_with_FastEmbed.ipynb)\n- [fastembed/text/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py)\n\nFastEmbed is designed for high performance and efficient embedding generation. This page covers the various optimization techniques implemented in the library and how to configure them to maximize performance for your specific use case.\n\nFor information about the overall architecture of FastEmbed, see [Architecture](qdrant/fastembed/4-architecture.md). For detailed information about ONNX Runtime integration, see [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md). For specific information about parallel processing implementation, see [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md).\n\n## Key Performance Features\n\nFastEmbed incorporates several key performance optimization techniques that allow it to generate embeddings significantly faster than traditional embedding methods:\n\n1. **ONNX Runtime Integration**: Accelerated inference through ONNX optimization\n2.",
      "index": 1,
      "token_count": 499,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 1943,
      "end_char": 3936
    },
    {
      "content": "edding methods:\n\n1. **ONNX Runtime Integration**: Accelerated inference through ONNX optimization\n2. **Parallel Processing**: Multi-core utilization for faster embedding generation\n3. **Lazy Loading**: Efficient memory management for multiple models\n4. **Batch Processing**: Optimized throughput through batched operations\n5. **Hardware Acceleration**: Support for both CPU and GPU (CUDA) execution\n6. **Model Caching**: Efficient model storage and retrieval\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py198-229](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L198-L229)\n\n## ONNX Runtime Integration\n\nFastEmbed uses ONNX Runtime as its inference engine, which provides significant performance improvements over traditional PyTorch or TensorFlow implementations. ONNX (Open Neural Network Exchange) is an open standard for machine learning model representation that enables model interoperability between different frameworks.\n\n### Benefits of ONNX Runtime\n\n- **Optimized inference**: ONNX Runtime includes various optimizations specific to the hardware and platform\n- **Reduced memory usage**: More efficient memory management compared to PyTorch/TensorFlow\n- **Cross-platform compatibility**: Same model works across different environments\n\n### Configuration Options\n\nWhen initializing an embedding model, you can configure ONNX Runtime execution parameters:\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py199-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L199-L246)\n\n## Parallel Processing\n\nFastEmbed implements data-parallel processing to distribute embedding workloads across multiple CPU cores or GPU devices, enabling significant speedups for large dataset processing.\n\n### How Parallel Processing Works in FastEmbed\n\n```\n```\n\n### Configuring Parallel Processing\n\nWhen calling the `embed()` method, you can specify the level of parallelism:\n\n```\n```\n\nThe `parallel` parameter accepts:\n\n- A positive integer: Use the specified number of workers",
      "index": 2,
      "token_count": 435,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 3836,
      "end_char": 5878
    },
    {
      "content": "`\n```\n\nThe `parallel` parameter accepts:\n\n- A positive integer: Use the specified number of workers\n- 0: Use all available cores\n- None: Don't use data parallel processing (use default ONNX Runtime threading)\n\nPerformance benchmarks from example notebooks demonstrate significant speedups using parallel processing:\n\n| Processing Mode | User Time | System Time | Wall Clock Time | Speedup |\n| --------------- | --------- | ----------- | --------------- | ------- |\n| Sequential      | 16min 23s | 31.7s       | 3min            | 1x      |\n| Parallel        | 6min 19s  | 22s         | 1min 37s        | \\~2x    |\n\nSources: [docs/examples/Hybrid\\_Search.ipynb726-739](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L726-L739)\n\n## Lazy Loading\n\nLazy loading allows you to instantiate models without immediately loading their weights into memory, which can be useful when working with multiple models or in resource-constrained environments.\n\n### How to Enable Lazy Loading\n\n```\n```\n\nWhen lazy loading is enabled, the model weights are only loaded when the first embedding request is made, reducing initial memory usage and startup time.\n\nSources: [fastembed/text/onnx\\_embedding.py221-222](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L221-L222) [fastembed/text/onnx\\_embedding.py255-258](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L255-L258)\n\n## Batch Processing\n\nFastEmbed optimizes throughput by processing inputs in batches, which enables more efficient hardware utilization and minimizes overhead.\n\n### Configuring Batch Size\n\nYou can adjust the batch size when calling the `embed()` method:\n\n```\n```\n\nOptimal batch size depends on:\n\n- Available memory\n- Input document length\n- Model size\n- Hardware characteristics\n\nFor large datasets, combining batch processing with parallel processing can provide the best performance.\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.",
      "index": 3,
      "token_count": 505,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 5778,
      "end_char": 7788
    },
    {
      "content": "n provide the best performance.\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292)\n\n## Hardware Acceleration\n\nFastEmbed supports both CPU and GPU execution, allowing you to leverage available hardware for maximum performance.\n\n### CPU Optimization\n\nWhen running on CPU, you can specify the number of threads to use:\n\n```\n```\n\n### GPU Acceleration\n\nTo use GPU acceleration (if available):\n\n```\n```\n\nWhen using multiple GPUs with `device_ids`, FastEmbed will automatically distribute the workload across the specified devices when parallel processing is enabled.\n\nSources: [fastembed/text/onnx\\_embedding.py202-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L202-L246)\n\n## Performance Comparison\n\nFastEmbed significantly outperforms traditional embedding libraries by combining ONNX optimization with efficient parallel processing.\n\n```\n```\n\n### Benchmark Results\n\nFrom comparing FastEmbed vs. Hugging Face Transformers (using the same BGE-small-en-v1.5 model):\n\n| Library                   | Avg Processing Time | Chars/Second | Relative Performance |\n| ------------------------- | ------------------- | ------------ | -------------------- |\n| Hugging Face Transformers | 0.047s              | \\~811        | 1x (baseline)        |\n| FastEmbed                 | 0.044s              | \\~871        | \\~1.07x faster       |\n\nFor large datasets, the performance gap widens significantly due to parallel processing capabilities:\n\n| Dataset Size  | Traditional Library | FastEmbed | Speedup |\n| ------------- | ------------------- | --------- | ------- |\n| Small (10s)   | 1x                  | 1-2x      | 1-2x    |\n| Medium (100s) | 1x                  | 2-3x      | 2-3x    |\n| Large (1000s) | 1x                  | 3-5x      | 3-5x    |\n\nSources: [docs/examples/FastEmbed\\_vs\\_HF\\_Comparison.ipynb245-278](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb#L245-L278)",
      "index": 4,
      "token_count": 538,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 7688,
      "end_char": 9737
    },
    {
      "content": "github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_vs_HF_Comparison.ipynb#L245-L278)\n\n## Advanced Configuration Parameters\n\nThe following table summarizes the key parameters that affect performance:\n\n| Parameter    | Description                                        | Default | When to Modify                                     |\n| ------------ | -------------------------------------------------- | ------- | -------------------------------------------------- |\n| `threads`    | Number of threads for ONNX session                 | `None`  | When you want to control CPU thread usage          |\n| `providers`  | ONNX Runtime providers                             | `None`  | For custom execution providers                     |\n| `cuda`       | Enable CUDA for GPU acceleration                   | `False` | When GPU is available                              |\n| `device_ids` | GPU device IDs for data parallel processing        | `None`  | When using multiple GPUs                           |\n| `lazy_load`  | Load model on demand rather than at initialization | `False` | When working with multiple models                  |\n| `batch_size` | Number of documents processed per batch            | `256`   | Adjust based on available memory and document size |\n| `parallel`   | Number of worker processes for parallel processing | `None`  | Set to >1 or 0 for large datasets                  |\n\nSources: [fastembed/text/onnx\\_embedding.py199-246](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L199-L246) [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292)\n\n## Best Practices for Performance Optimization\n\n### Model Selection\n\n- Smaller models (like BGE-small) are faster but may be less accurate\n- Larger models provide better quality but require more resources\n- Select the smallest model that meets your quality requirements\n\n### Processing Configuration\n\n1.",
      "index": 5,
      "token_count": 433,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 9637,
      "end_char": 11637
    },
    {
      "content": "s\n- Select the smallest model that meets your quality requirements\n\n### Processing Configuration\n\n1. **Enable parallel processing for large datasets**\n\n   ```\n   ```\n\n2. **Optimize batch size**\n\n   ```\n   ```\n\n3. **Use GPU acceleration when available**\n\n   ```\n   ```\n\n4. **Combine strategies for maximum performance**\n\n   ```\n   ```\n\n### Performance Debugging\n\nIf you encounter performance issues:\n\n1. Try reducing batch size if you're running out of memory\n2. Reduce the number of parallel workers if you experience excessive CPU/GPU contention\n3. Try a smaller model if speed is more important than quality\n4. Enable lazy loading if working with multiple models to manage memory more efficiently\n\nSources: [fastembed/text/onnx\\_embedding.py260-292](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L260-L292) [docs/examples/Hybrid\\_Search.ipynb726-739](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/Hybrid_Search.ipynb#L726-L739)\n\n## Late Interaction Models and Performance\n\nLate interaction models like ColBERT require special consideration for performance optimization:\n\n```\n```\n\nDue to their computational requirements, consider using late interaction models like ColBERT in a two-stage pipeline:\n\n1. **First stage**: Use a faster dense embedding model to retrieve candidate documents (100-500)\n2. **Second stage**: Use the more resource-intensive late interaction model to rerank results for higher precision\n\nThis approach balances speed and accuracy, leveraging the strengths of both approaches.\n\nSources: [docs/examples/ColBERT\\_with\\_FastEmbed.ipynb373-398](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/ColBERT_with_FastEmbed.ipynb#L373-L398)\n\n## Summary\n\nFastEmbed offers multiple performance optimization techniques that can be combined to achieve significant speedups in embedding generation. By choosing the right configuration for your specific use case, you can achieve optimal performance while maintaining high-quality embeddings.",
      "index": 6,
      "token_count": 467,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 11537,
      "end_char": 13559
    },
    {
      "content": "specific use case, you can achieve optimal performance while maintaining high-quality embeddings.\n\nFor most cases, enabling parallel processing (`parallel=0`) with an appropriate batch size and GPU acceleration (if available) will provide the best balance of performance and resource utilization.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Performance Optimization](#performance-optimization.md)\n- [Key Performance Features](#key-performance-features.md)\n- [ONNX Runtime Integration](#onnx-runtime-integration.md)\n- [Benefits of ONNX Runtime](#benefits-of-onnx-runtime.md)\n- [Configuration Options](#configuration-options.md)\n- [Parallel Processing](#parallel-processing.md)\n- [How Parallel Processing Works in FastEmbed](#how-parallel-processing-works-in-fastembed.md)\n- [Configuring Parallel Processing](#configuring-parallel-processing.md)\n- [Lazy Loading](#lazy-loading.md)\n- [How to Enable Lazy Loading](#how-to-enable-lazy-loading.md)\n- [Batch Processing](#batch-processing.md)\n- [Configuring Batch Size](#configuring-batch-size.md)\n- [Hardware Acceleration](#hardware-acceleration.md)\n- [CPU Optimization](#cpu-optimization.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Performance Comparison](#performance-comparison.md)\n- [Benchmark Results](#benchmark-results.md)\n- [Advanced Configuration Parameters](#advanced-configuration-parameters.md)\n- [Best Practices for Performance Optimization](#best-practices-for-performance-optimization.md)\n- [Model Selection](#model-selection.md)\n- [Processing Configuration](#processing-configuration.md)\n- [Performance Debugging](#performance-debugging.md)\n- [Late Interaction Models and Performance](#late-interaction-models-and-performance.md)\n- [Summary](#summary.md)",
      "index": 7,
      "token_count": 383,
      "metadata": {
        "title": "_qdrant_fastembed_8-performance-optimization",
        "source": "qdrant_fastembed\\_qdrant_fastembed_8-performance-optimization.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_8-performance-optimization.md",
        "file_name": "_qdrant_fastembed_8-performance-optimization.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:02.980896",
        "total_chunks": 8
      },
      "start_char": 13459,
      "end_char": 15507
    },
    {
      "content": "Integration with Qdrant | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 622,
      "metadata": {
        "title": "_qdrant_fastembed_9-integration-with-qdrant",
        "source": "qdrant_fastembed\\_qdrant_fastembed_9-integration-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "file_name": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:03.015398",
        "total_chunks": 5
      },
      "start_char": 0,
      "end_char": 2042
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Integration with Qdrant\n\nRelevant source files\n\n- [README.md](https://github.com/qdrant/fastembed/blob/b785640b/README.md)\n- [docs/examples/FastEmbed\\_GPU.ipynb](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb)\n\n## Purpose and Scope\n\nThis document explains how to integrate the FastEmbed library with Qdrant, a vector database designed for efficient similarity search. FastEmbed generates embeddings, and Qdrant stores and searches them. This integration allows for seamless vector search capabilities in your applications, combining FastEmbed's efficient embedding generation with Qdrant's optimized vector storage and retrieval.\n\n## Overview of the Integration\n\nFastEmbed and Qdrant work together to provide an end-to-end solution for vector search. FastEmbed generates high-quality embeddings from your data, and Qdrant efficiently stores and searches these embeddings.\n\n```\n```\n\nSources: [README.md230-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L230-L281)\n\n## Installation\n\nTo use FastEmbed with Qdrant, you need to install the Qdrant client with FastEmbed support:\n\n```\n```\n\nOn some shells like zsh, you might need to use quotes:\n\n```\n```\n\nSources: [README.md236-246](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L236-L246)\n\n## Basic Usage\n\n### Initializing the Client\n\n```\n```\n\n### Adding Documents with Automatic Embedding\n\nThe integration provides a simplified workflow where Qdrant client handles embedding generation internally:\n\n```\n```\n\nCode example:\n\n```\n```\n\n### Searching for Similar Documents\n\n```\n```\n\nSources: [README.md247-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L247-L281)",
      "index": 1,
      "token_count": 541,
      "metadata": {
        "title": "_qdrant_fastembed_9-integration-with-qdrant",
        "source": "qdrant_fastembed\\_qdrant_fastembed_9-integration-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "file_name": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:03.015398",
        "total_chunks": 5
      },
      "start_char": 1942,
      "end_char": 3981
    },
    {
      "content": "Sources: [README.md247-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L247-L281)\n\n## Model Configuration\n\nBy default, the integration uses FastEmbed's default embedding model. You can customize this:\n\n```\n```\n\nSources: [README.md263-265](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L263-L265)\n\n## Integration Architecture\n\nThe following diagram illustrates how the integration works at a technical level:\n\n```\n```\n\nSources: [README.md230-281](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L230-L281)\n\n## Performance Considerations\n\n### GPU Acceleration\n\nIf you're working with large datasets or need real-time embedding generation, you can use GPU acceleration:\n\n1. Install with GPU support: `pip install qdrant-client[fastembed-gpu]`\n2. Ensure you have the proper CUDA drivers installed\n\nSee the [FastEmbed GPU documentation](qdrant/fastembed/8-performance-optimization.md) for more details on GPU setup and requirements.\n\nNote: GPU acceleration can significantly improve embedding generation performance, as shown in the comparison below:\n\n| Platform | Processing Time for 500 Documents |\n| -------- | --------------------------------- |\n| CPU      | \\~4.33 seconds                    |\n| GPU      | \\~43.4 milliseconds               |\n\nThis represents approximately a 100x speedup when using GPU acceleration.\n\nSources: [docs/examples/FastEmbed\\_GPU.ipynb20-33](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L20-L33) [docs/examples/FastEmbed\\_GPU.ipynb411-512](https://github.com/qdrant/fastembed/blob/b785640b/docs/examples/FastEmbed_GPU.ipynb#L411-L512)\n\n## Working with Different Embedding Types\n\nThe integration supports all embedding types available in FastEmbed:\n\n```\n```\n\n### Dense Text Embeddings\n\nWorks with standard dense vector collections in Qdrant.\n\n### Sparse Text Embeddings\n\nFor sparse embeddings (like SPLADE++), Qdrant stores them as sparse vectors.\n\n### Late Interaction Models",
      "index": 2,
      "token_count": 528,
      "metadata": {
        "title": "_qdrant_fastembed_9-integration-with-qdrant",
        "source": "qdrant_fastembed\\_qdrant_fastembed_9-integration-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "file_name": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:03.015398",
        "total_chunks": 5
      },
      "start_char": 3881,
      "end_char": 5868
    },
    {
      "content": "rse embeddings (like SPLADE++), Qdrant stores them as sparse vectors.\n\n### Late Interaction Models\n\nFor late interaction models (like ColBERT), Qdrant uses a multi-vector approach where multiple vectors represent a single document.\n\n### Image Embeddings\n\nImage embeddings are stored as dense vectors similar to text embeddings.\n\nSources: [README.md49-156](https://github.com/qdrant/fastembed/blob/b785640b/README.md#L49-L156)\n\n## Conclusion\n\nThe integration between FastEmbed and Qdrant provides a seamless way to implement vector search in your applications. By leveraging FastEmbed's efficient embedding generation and Qdrant's optimized vector storage and search capabilities, you can build powerful semantic search, recommendation systems, and other AI-powered applications.\n\nFor more information on specific embedding models, see [Supported Models](qdrant/fastembed/6-supported-models.md).\n\nFor examples of using different embedding types, see [Usage Examples](qdrant/fastembed/7-usage-examples.md).\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Integration with Qdrant](#integration-with-qdrant.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [Overview of the Integration](#overview-of-the-integration.md)\n- [Installation](#installation.md)\n- [Basic Usage](#basic-usage.md)\n- [Initializing the Client](#initializing-the-client.md)\n- [Adding Documents with Automatic Embedding](#adding-documents-with-automatic-embedding.md)\n- [Searching for Similar Documents](#searching-for-similar-documents.md)\n- [Model Configuration](#model-configuration.md)\n- [Integration Architecture](#integration-architecture.md)\n- [Performance Considerations](#performance-considerations.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [Working with Different Embedding Types](#working-with-different-embedding-types.md)\n- [Dense Text Embeddings](#dense-text-embeddings.md)\n- [Sparse Text Embeddings](#sparse-text-embeddings.md)\n- [Late Interaction Models](#late-interaction-models.md)\n- [Image Embeddings](#image-embeddings.md)",
      "index": 3,
      "token_count": 471,
      "metadata": {
        "title": "_qdrant_fastembed_9-integration-with-qdrant",
        "source": "qdrant_fastembed\\_qdrant_fastembed_9-integration-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "file_name": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:03.015398",
        "total_chunks": 5
      },
      "start_char": 5768,
      "end_char": 7811
    },
    {
      "content": "- [Late Interaction Models](#late-interaction-models.md)\n- [Image Embeddings](#image-embeddings.md)\n- [Conclusion](#conclusion.md)",
      "index": 4,
      "token_count": 36,
      "metadata": {
        "title": "_qdrant_fastembed_9-integration-with-qdrant",
        "source": "qdrant_fastembed\\_qdrant_fastembed_9-integration-with-qdrant.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "file_name": "_qdrant_fastembed_9-integration-with-qdrant.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T00:18:03.015398",
        "total_chunks": 5
      },
      "start_char": 7711,
      "end_char": 9759
    }
  ]
}