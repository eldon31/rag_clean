{
  "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
  "source_repo": "qdrant_fastembed",
  "total_chunks": 7,
  "chunks": [
    {
      "content": "ONNX Runtime Integration | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2043
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# ONNX Runtime Integration\n\nRelevant source files\n\n- [fastembed/common/onnx\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py)\n- [fastembed/image/onnx\\_image\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py)\n- [fastembed/parallel\\_processor.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py)\n- [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py)\n- [fastembed/text/onnx\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py)\n- [fastembed/text/onnx\\_text\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py)\n\nThis document explains how FastEmbed leverages ONNX Runtime to achieve efficient inference for embedding generation. ONNX Runtime integration is a core component of FastEmbed's architecture that enables high-performance model execution across different hardware configurations and model types.\n\nFor information about parallel processing capabilities, see [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md).\n\n## Overview of ONNX in FastEmbed\n\nFastEmbed uses ONNX Runtime as its underlying inference engine for all embedding models. This provides substantial performance benefits over traditional PyTorch or TensorFlow implementations while maintaining compatibility with models originally trained in those frameworks.\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py26-109](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.",
      "index": 1,
      "token_count": 519,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 1943,
      "end_char": 3982
    },
    {
      "content": "onnx\\_model.py26-109](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L109) [fastembed/text/onnx\\_text\\_model.py17-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L17-L91) [fastembed/image/onnx\\_image\\_model.py21-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L21-L79) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L77)\n\n## ONNX Model Hierarchy\n\nFastEmbed implements a hierarchical structure for its ONNX-based models, with a common base class and specialized subclasses for different modalities.\n\n### Base Class: OnnxModel\n\nThe `OnnxModel` class serves as the foundation for all ONNX-based models in FastEmbed. It provides:\n\n- Generic ONNX session management\n- Provider configuration\n- Common input/output processing interfaces\n- Base methods for model loading and inference\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py26-112](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L26-L112) [fastembed/text/onnx\\_text\\_model.py17-91](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L17-L91) [fastembed/image/onnx\\_image\\_model.py21-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L21-L79) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py21-146](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L21-L146)\n\n## ONNX Session Configuration\n\nFastEmbed offers flexible configuration of ONNX Runtime sessions, enabling users to optimize for their specific hardware and performance requirements.\n\n### Provider Selection\n\nThe library allows specifying which ONNX Runtime execution providers to use:\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py46-106](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L46-L106)",
      "index": 2,
      "token_count": 626,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 3882,
      "end_char": 5929
    },
    {
      "content": "46-106](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L46-L106)\n\n### Session Options and Optimization\n\nFastEmbed applies several optimizations to the ONNX Runtime session:\n\n- Sets graph optimization level to `ORT_ENABLE_ALL`\n- Configures thread counts for intra-op and inter-op parallelism when specified\n- Validates providers against available execution providers in the runtime\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py86-95](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L86-L95)\n\n## Inference Pipeline\n\nThe ONNX-based inference pipeline in FastEmbed follows a consistent pattern across different model types, with modality-specific preprocessing and postprocessing steps.\n\n```\n```\n\nSources: [fastembed/text/onnx\\_text\\_model.py62-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L62-L90) [fastembed/image/onnx\\_image\\_model.py63-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L63-L79) [fastembed/rerank/cross\\_encoder/onnx\\_text\\_model.py66-77](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/rerank/cross_encoder/onnx_text_model.py#L66-L77)\n\n### Text Model Inference\n\nFor text models, the inference process includes:\n\n1. Tokenization of input text documents\n2. Conversion of tokens to input tensors (input\\_ids, attention\\_mask, etc.)\n3. Model inference via ONNX Runtime\n4. Post-processing of output embeddings (first token extraction, normalization)\n\n```\n```\n\nSources: [fastembed/text/onnx\\_text\\_model.py65-90](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_text_model.py#L65-L90) [fastembed/text/onnx\\_embedding.py298-315](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L298-L315)\n\n### Image Model Inference\n\nFor image models, the inference process includes:\n\n1. Loading and preprocessing images\n2. Building ONNX input dictionary\n3. Model inference via ONNX Runtime\n4.",
      "index": 3,
      "token_count": 570,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 5829,
      "end_char": 7838
    },
    {
      "content": "ng and preprocessing images\n2. Building ONNX input dictionary\n3. Model inference via ONNX Runtime\n4. Reshaping and post-processing output embeddings\n\nSources: [fastembed/image/onnx\\_image\\_model.py63-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/image/onnx_image_model.py#L63-L79)\n\n## GPU Acceleration\n\nFastEmbed provides support for GPU acceleration through ONNX Runtime's CUDA execution provider.\n\n### CUDA Configuration\n\nUsers can enable CUDA execution by:\n\n1. Setting the `cuda=True` parameter during model initialization\n2. Specifying `device_id` for particular GPU selection\n3. Providing multiple `device_ids` for parallel processing\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py58-73](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L58-L73)\n\n### Multi-GPU Support\n\nFor parallel processing across multiple GPUs, FastEmbed provides:\n\n1. Device ID specification through `device_ids` parameter\n2. Worker process allocation to specific GPUs\n3. Load balancing across available GPUs\n\n```\n```\n\nSources: [fastembed/parallel\\_processor.py120-126](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L120-L126)\n\n## Supported Models and Formats\n\nFastEmbed supports various ONNX-optimized embedding models with different dimensions and purposes.\n\n| Model                               | Dimension | Description               | Type  |\n| ----------------------------------- | --------- | ------------------------- | ----- |\n| BAAI/bge-small-en-v1.5              | 384       | English text embeddings   | Dense |\n| BAAI/bge-base-en-v1.5               | 768       | English text embeddings   | Dense |\n| BAAI/bge-large-en-v1.5              | 1024      | English text embeddings   | Dense |\n| snowflake/snowflake-arctic-embed-\\* | 384-1024  | English text embeddings   | Dense |\n| jinaai/jina-clip-v1                 | 768       | Multimodal (text & image) | Dense |\n| ... and others                      |           |                           |       |",
      "index": 4,
      "token_count": 511,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 7738,
      "end_char": 9772
    },
    {
      "content": "| Dense |\n| ... and others                      |           |                           |       |\n\nSources: [fastembed/text/onnx\\_embedding.py10-183](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L10-L183)\n\n## Implementation Details\n\n### Model Loading and Caching\n\nThe ONNX model loading process includes:\n\n1. Model download and caching\n2. Loading the ONNX model file\n3. Setting up the ONNX Runtime session with appropriate providers\n4. Loading supporting components (tokenizers, image processors)\n\nSources: [fastembed/text/onnx\\_embedding.py248-325](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L248-L325)\n\n### Lazy Loading\n\nFastEmbed supports lazy loading of ONNX models, which defers model loading until the first inference request. This is particularly useful for multi-GPU scenarios where models should be loaded in worker processes.\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py256-258](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L256-L258)\n\n### Error Handling\n\nFastEmbed includes error handling for ONNX Runtime provider configuration:\n\n1. Validation of requested providers against available providers\n2. Warning for CUDA provider failures\n3. Suggestion for CUDA 12.x compatibility\n\n```\n```\n\nSources: [fastembed/common/onnx\\_model.py96-105](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L96-L105)\n\n## Integration with Embedding Classes\n\nThe ONNX Runtime integration is exposed through higher-level embedding classes that provide user-friendly interfaces for generating embeddings:\n\n```\n```\n\nSources: [fastembed/text/onnx\\_embedding.py186-340](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/text/onnx_embedding.py#L186-L340) [fastembed/common/onnx\\_model.py114-136](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/common/onnx_model.py#L114-L136) [fastembed/parallel\\_processor.py26-34](https://github.",
      "index": 5,
      "token_count": 540,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 9672,
      "end_char": 11663
    },
    {
      "content": "0b/fastembed/common/onnx_model.py#L114-L136) [fastembed/parallel\\_processor.py26-34](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/parallel_processor.py#L26-L34)\n\n## Conclusion\n\nThe ONNX Runtime integration in FastEmbed provides significant performance benefits through:\n\n1. Hardware-specific optimizations via execution providers\n2. Efficient model loading and caching\n3. Support for parallel and distributed processing\n4. Consistent API across different model types and modalities\n\nBy leveraging ONNX Runtime's capabilities, FastEmbed achieves faster inference speeds compared to traditional embedding frameworks, making it suitable for production environments where performance is critical.\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [ONNX Runtime Integration](#onnx-runtime-integration.md)\n- [Overview of ONNX in FastEmbed](#overview-of-onnx-in-fastembed.md)\n- [ONNX Model Hierarchy](#onnx-model-hierarchy.md)\n- [Base Class: OnnxModel](#base-class-onnxmodel.md)\n- [ONNX Session Configuration](#onnx-session-configuration.md)\n- [Provider Selection](#provider-selection.md)\n- [Session Options and Optimization](#session-options-and-optimization.md)\n- [Inference Pipeline](#inference-pipeline.md)\n- [Text Model Inference](#text-model-inference.md)\n- [Image Model Inference](#image-model-inference.md)\n- [GPU Acceleration](#gpu-acceleration.md)\n- [CUDA Configuration](#cuda-configuration.md)\n- [Multi-GPU Support](#multi-gpu-support.md)\n- [Supported Models and Formats](#supported-models-and-formats.md)\n- [Implementation Details](#implementation-details.md)\n- [Model Loading and Caching](#model-loading-and-caching.md)\n- [Lazy Loading](#lazy-loading.md)\n- [Error Handling](#error-handling.md)\n- [Integration with Embedding Classes](#integration-with-embedding-classes.md)\n- [Conclusion](#conclusion.md)",
      "index": 6,
      "token_count": 445,
      "metadata": {
        "title": "_qdrant_fastembed_4.2-onnx-runtime-integration",
        "source": "qdrant_fastembed\\_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "file_name": "_qdrant_fastembed_4.2-onnx-runtime-integration.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.551036",
        "total_chunks": 7
      },
      "start_char": 11563,
      "end_char": 13611
    }
  ]
}