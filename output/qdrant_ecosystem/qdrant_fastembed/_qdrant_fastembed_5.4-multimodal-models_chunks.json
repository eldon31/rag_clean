{
  "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\qdrant_ecosystem\\qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
  "source_repo": "qdrant_fastembed",
  "total_chunks": 7,
  "chunks": [
    {
      "content": "Multimodal Models | qdrant/fastembed | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/fastembed](https://github.com/qdrant/fastembed \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 20 April 2025 ([b78564](https://github.com/qdrant/fastembed/commits/b785640b))\n\n- [Overview](qdrant/fastembed/1-overview.md)\n- [Installation and Setup](qdrant/fastembed/2-installation-and-setup.md)\n- [Core Embedding Classes](qdrant/fastembed/3-core-embedding-classes.md)\n- [TextEmbedding](qdrant/fastembed/3.1-textembedding.md)\n- [SparseTextEmbedding](qdrant/fastembed/3.2-sparsetextembedding.md)\n- [LateInteractionTextEmbedding](qdrant/fastembed/3.3-lateinteractiontextembedding.md)\n- [ImageEmbedding](qdrant/fastembed/3.4-imageembedding.md)\n- [LateInteractionMultimodalEmbedding](qdrant/fastembed/3.5-lateinteractionmultimodalembedding.md)\n- [TextCrossEncoder](qdrant/fastembed/3.6-textcrossencoder.md)\n- [Architecture](qdrant/fastembed/4-architecture.md)\n- [Model Management](qdrant/fastembed/4.1-model-management.md)\n- [ONNX Runtime Integration](qdrant/fastembed/4.2-onnx-runtime-integration.md)\n- [Parallel Processing](qdrant/fastembed/4.3-parallel-processing.md)\n- [Implementation Details](qdrant/fastembed/5-implementation-details.md)\n- [Dense Text Embeddings](qdrant/fastembed/5.1-dense-text-embeddings.md)\n- [Sparse Text Embeddings](qdrant/fastembed/5.2-sparse-text-embeddings.md)\n- [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md)\n- [Multimodal Models](qdrant/fastembed/5.4-multimodal-models.md)\n- [Supported Models](qdrant/fastembed/6-supported-models.md)\n- [Usage Examples](qdrant/fastembed/7-usage-examples.md)\n- [Basic Text Embedding](qdrant/fastembed/7.1-basic-text-embedding.md)\n- [Sparse and Hybrid Search](qdrant/fastembed/7.2-sparse-and-hybrid-search.md)\n- [ColBERT and Late Interaction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.",
      "index": 0,
      "token_count": 621,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 0,
      "end_char": 2036
    },
    {
      "content": "ction](qdrant/fastembed/7.3-colbert-and-late-interaction.md)\n- [Image Embedding](qdrant/fastembed/7.4-image-embedding.md)\n- [Performance Optimization](qdrant/fastembed/8-performance-optimization.md)\n- [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md)\n- [Development Guide](qdrant/fastembed/10-development-guide.md)\n\nMenu\n\n# Multimodal Models\n\nRelevant source files\n\n- [fastembed/late\\_interaction\\_multimodal/colpali.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py)\n- [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py)\n- [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding_base.py)\n- [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py)\n\nThis document provides a detailed explanation of the multimodal embedding capabilities in FastEmbed. Multimodal models in FastEmbed enable the creation of embeddings from both text and image inputs within a compatible embedding space, making them ideal for cross-modal retrieval tasks. This page focuses specifically on the implementation of late interaction multimodal models, which process text and images separately but in alignment.\n\nFor information about text-only late interaction models, see [Late Interaction Models](qdrant/fastembed/5.3-late-interaction-models.md).\n\n## Architecture Overview\n\nThe multimodal embedding functionality in FastEmbed is built around the `LateInteractionMultimodalEmbedding` class, which serves as the main entry point for users.",
      "index": 1,
      "token_count": 480,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 1936,
      "end_char": 3878
    },
    {
      "content": "ound the `LateInteractionMultimodalEmbedding` class, which serves as the main entry point for users. This class is backed by specific implementations such as `ColPali`, which provide the actual embedding capabilities for both text and images.\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py14-16](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L14-L16) [fastembed/late\\_interaction\\_multimodal/colpali.py34-44](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L34-L44) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py20-44](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L20-L44)\n\n## Supported Models\n\nCurrently, FastEmbed supports the following multimodal model:\n\n| Model                    | Dimensions | Description                                                           | License | Size   |\n| ------------------------ | ---------- | --------------------------------------------------------------------- | ------- | ------ |\n| Qdrant/colpali-v1.3-fp16 | 128        | Text and image embeddings, English, 50 tokens query length truncation | MIT     | 6.5 GB |\n\nColPali is a multimodal embedding model that combines the strengths of ColBERT architecture with multimodal capabilities, allowing for effective cross-modal retrieval between text and images.\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py20-31](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L20-L31)\n\n## Implementation Details\n\n### Class Hierarchy\n\nThe multimodal embedding functionality in FastEmbed follows a clear inheritance hierarchy:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py10-67](https://github.",
      "index": 2,
      "token_count": 502,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 3778,
      "end_char": 5755
    },
    {
      "content": "ate\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding\\_base.py10-67](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding_base.py#L10-L67) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py20-82](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L20-L82) [fastembed/late\\_interaction\\_multimodal/colpali.py34-281](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L34-L281)\n\n### The ColPali Implementation\n\nThe `ColPali` class is the current implementation of multimodal embedding in FastEmbed. It implements both the `LateInteractionMultimodalEmbeddingBase` and `OnnxMultimodalModel` interfaces, providing functionality for:\n\n1. Text embedding using special tokens and processing\n2. Image embedding using a specialized vision encoder\n3. Unified embedding space for cross-modal retrieval\n\nKey implementation details include:\n\n- Special token handling for query prefixing: `QUERY_PREFIX = \"Query: \"` and `BOS_TOKEN = \"<s>\"`\n- Specialized preprocessing for multimodal inputs\n- Processing of text with image placeholders and images with text placeholders\n- Post-processing to ensure compatible embedding dimensions\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py35-45](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L35-L45) [fastembed/late\\_interaction\\_multimodal/colpali.py162-206](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L162-L206)\n\n## Embedding Process Flow\n\nThe embedding process for multimodal models follows a similar pattern for both text and images:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py86-224](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.",
      "index": 3,
      "token_count": 564,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 5655,
      "end_char": 7659
    },
    {
      "content": "thub.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L86-L224) [fastembed/late\\_interaction\\_multimodal/colpali.py162-173](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L162-L173) [fastembed/late\\_interaction\\_multimodal/colpali.py208-272](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L208-L272)\n\n## Key Components of Multimodal Processing\n\n### Text Preprocessing\n\nWhen embedding text, ColPali:\n\n1. Prefixes queries with `QUERY_PREFIX` and `BOS_TOKEN`\n2. Tokenizes the text\n3. Adds query marker tokens\n4. Adds empty image placeholders to ensure the model handles text-only input correctly\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py162-166](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L162-L166) [fastembed/late\\_interaction\\_multimodal/colpali.py172-186](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L172-L186)\n\n### Image Preprocessing\n\nWhen embedding images, ColPali:\n\n1. Processes the images using the image processor\n2. Adds empty text placeholders to ensure the model handles image-only input correctly\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py189-204](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L189-L204) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py162-174](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L162-L174)\n\n## Parallel Processing Support\n\nMultimodal embedding supports efficient parallel processing for large datasets:\n\n1. When `parallel` parameter is set, the system creates a `ParallelWorkerPool`\n2. For text embedding, it uses `ColPaliTextEmbeddingWorker` workers\n3. For image embedding, it uses `ColPaliImageEmbeddingWorker` workers\n4.",
      "index": 4,
      "token_count": 632,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 7559,
      "end_char": 9582
    },
    {
      "content": "extEmbeddingWorker` workers\n3. For image embedding, it uses `ColPaliImageEmbeddingWorker` workers\n4. Each worker processes a batch of inputs in parallel, significantly improving throughput\n\nSources: [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py152-160](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L152-L160) [fastembed/late\\_interaction\\_multimodal/onnx\\_multimodal\\_model.py215-223](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/onnx_multimodal_model.py#L215-L223) [fastembed/late\\_interaction\\_multimodal/colpali.py283-300](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L283-L300)\n\n## Usage Example\n\nHere's how to use the multimodal embedding functionality:\n\n```\n```\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py86-130](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L86-L130)\n\n## Integration with Vector Databases\n\nThe multimodal embeddings produced by FastEmbed are compatible with vector database systems like Qdrant. The embeddings from both text and images can be stored in the same collection, enabling cross-modal search (finding images with text queries or finding text with image queries).\n\nFor more information on integrating FastEmbed with Qdrant, see [Integration with Qdrant](qdrant/fastembed/9-integration-with-qdrant.md).\n\nSources: [fastembed/late\\_interaction\\_multimodal/colpali.py22-30](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/colpali.py#L22-L30)\n\n## Future Development\n\nThe architecture of FastEmbed's multimodal implementation is designed for extensibility. New multimodal models can be added by:\n\n1. Creating a new class that extends `LateInteractionMultimodalEmbeddingBase` and `OnnxMultimodalModel`\n2.",
      "index": 5,
      "token_count": 558,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 9482,
      "end_char": 11473
    },
    {
      "content": "ating a new class that extends `LateInteractionMultimodalEmbeddingBase` and `OnnxMultimodalModel`\n2. Implementing the required methods for text and image embedding\n3. Adding the new class to the `EMBEDDINGS_REGISTRY` in `LateInteractionMultimodalEmbedding`\n\nSources: [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py15-16](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L15-L16) [fastembed/late\\_interaction\\_multimodal/late\\_interaction\\_multimodal\\_embedding.py66-79](https://github.com/qdrant/fastembed/blob/b785640b/fastembed/late_interaction_multimodal/late_interaction_multimodal_embedding.py#L66-L79)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Multimodal Models](#multimodal-models.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Supported Models](#supported-models.md)\n- [Implementation Details](#implementation-details.md)\n- [Class Hierarchy](#class-hierarchy.md)\n- [The ColPali Implementation](#the-colpali-implementation.md)\n- [Embedding Process Flow](#embedding-process-flow.md)\n- [Key Components of Multimodal Processing](#key-components-of-multimodal-processing.md)\n- [Text Preprocessing](#text-preprocessing.md)\n- [Image Preprocessing](#image-preprocessing.md)\n- [Parallel Processing Support](#parallel-processing-support.md)\n- [Usage Example](#usage-example.md)\n- [Integration with Vector Databases](#integration-with-vector-databases.md)\n- [Future Development](#future-development.md)",
      "index": 6,
      "token_count": 402,
      "metadata": {
        "title": "_qdrant_fastembed_5.4-multimodal-models",
        "source": "qdrant_fastembed\\_qdrant_fastembed_5.4-multimodal-models.md",
        "chunk_method": "simple_fallback",
        "source_repo": "qdrant_fastembed",
        "source_subdir": "root",
        "category": "general",
        "file_path": "_qdrant_fastembed_5.4-multimodal-models.md",
        "file_name": "_qdrant_fastembed_5.4-multimodal-models.md",
        "collection": "qdrant_ecosystem",
        "processed_date": "2025-10-16T01:58:34.597064",
        "total_chunks": 7
      },
      "start_char": 11373,
      "end_char": 13421
    }
  ]
}