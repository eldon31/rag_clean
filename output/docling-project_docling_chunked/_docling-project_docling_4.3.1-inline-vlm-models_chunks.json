[
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:0",
    "content": "Inline VLM Models | docling-project/docling | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[docling-project/docling](https://github.com/docling-project/docling \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 12 October 2025 ([f7244a](https://github.com/docling-project/docling/commits/f7244a43))\n\n- [Overview](docling-project/docling/1-overview.md)\n- [Installation](docling-project/docling/1.1-installation.md)\n- [Quick Start](docling-project/docling/1.2-quick-start.md)\n- [Core Architecture](docling-project/docling/2-core-architecture.md)\n- [Document Conversion Flow](docling-project/docling/2.1-document-conversion-flow.md)\n- [DoclingDocument Data Model](docling-project/docling/2.2-doclingdocument-data-model.md)\n- [Configuration and Pipeline Options](docling-project/docling/2.3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 276,
      "char_count": 1005,
      "start_char": 0,
      "end_char": 1005
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:1",
    "content": "3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.4-format-detection-and-routing.md)\n- [Document Backends](docling-project/docling/3-document-backends.md)\n- [PDF Processing Backends](docling-project/docling/3.1-pdf-processing-backends.md)\n- [Office Document Backends](docling-project/docling/3.2-office-document-backends.md)\n- [Web and Markup Backends](docling-project/docling/3.3-web-and-markup-backends.md)\n- [AI/ML Models](docling-project/docling/4-aiml-models.md)\n- [OCR Models](docling-project/docling/4.1-ocr-models.md)\n- [Layout and Table Structure Models](docling-project/docling/4.2-layout-and-table-structure-models.md)\n- [Vision Language Models](docling-project/docling/4.3-vision-language-models.md)\n- [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md)\n- [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 283,
      "char_count": 987,
      "start_char": 905,
      "end_char": 1893
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:2",
    "content": ".2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)\n- [Processing Pipelines](docling-project/docling/5-processing-pipelines.md)\n- [Standard PDF Pipeline](docling-project/docling/5.1-standard-pdf-pipeline.md)\n- [Threaded PDF Pipeline](docling-project/docling/5.2-threaded-pdf-pipeline.md)\n- [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md)\n- [Extraction Pipeline](docling-project/docling/5.4-extraction-pipeline.md)\n- [ASR Pipeline](docling-project/docling/5.5-asr-pipeline.md)\n- [Base Pipeline Architecture](docling-project/docling/5.6-base-pipeline-architecture.md)\n- [Command Line Interface](docling-project/docling/6-command-line-interface.md)\n- [Document Conversion CLI](docling-project/docling/6.1-document-conversion-cli.md)\n- [Model Management CLI](docling-project/docling/6.2-model-management-cli.md)\n- [Python SDK](docling-project/docling/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 275,
      "char_count": 1003,
      "start_char": 1793,
      "end_char": 2797
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:3",
    "content": "ng/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)\n- [DocumentExtractor API](docling-project/docling/7.2-documentextractor-api.md)\n- [Usage Examples](docling-project/docling/7.3-usage-examples.md)\n- [Output and Integration](docling-project/docling/8-output-and-integration.md)\n- [Export Formats](docling-project/docling/8.1-export-formats.md)\n- [Document Chunking](docling-project/docling/8.2-document-chunking.md)\n- [Framework Integrations](docling-project/docling/8.3-framework-integrations.md)\n- [Development and Testing](docling-project/docling/9-development-and-testing.md)\n- [Testing Framework](docling-project/docling/9.1-testing-framework.md)\n- [Ground Truth Data](docling-project/docling/9.2-ground-truth-data.md)\n- [CI/CD and Development Workflow](docling-project/docling/9.3-cicd-and-development-workflow.md)\n- [Deployment](docling-project/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 270,
      "char_count": 998,
      "start_char": 2697,
      "end_char": 3696
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:4",
    "content": "/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)\n- [Model Artifacts Management](docling-project/docling/10.2-model-artifacts-management.md)\n\nMenu\n\n# Inline VLM Models\n\nRelevant source files\n\n- [README.md](https://github.com/docling-project/docling/blob/f7244a43/README.md)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 300,
      "char_count": 1019,
      "start_char": 3596,
      "end_char": 4616
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:5",
    "content": "https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docs/examples/minimal\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py)\n- [docs/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/index.md)\n- [docs/usage/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/index.md)\n- [docs/usage/mcp.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/mcp.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 309,
      "char_count": 1024,
      "start_char": 4516,
      "end_char": 5540
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:6",
    "content": "x.md)\n- [docs/usage/mcp.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/mcp.md)\n- [docs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)\n\nThis page documents the inline Vision Language Model (VLM) implementations in Docling. Inline VLM models run locally on the host machine, in contrast to API-based VLM models that connect to remote services. Three inference frameworks are supported: Hugging Face Transformers, MLX (for Apple Silicon acceleration), and vLLM (for optimized GPU inference).\n\nFor information about API-based VLM models that connect to remote services like Ollama or vLLM servers, see [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md). For general VLM integration concepts and configuration options, see [Vision Language Models](docling-project/docling/4.3-vision-language-models.md).\n\n## Architecture Overview",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 260,
      "char_count": 1024,
      "start_char": 5440,
      "end_char": 6465
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:7",
    "content": "Language Models](docling-project/docling/4.3-vision-language-models.md).\n\n## Architecture Overview\n\nThe inline VLM model system provides three specialized implementations sharing a common interface:\n\n```\n```\n\n**Sources:** [docling/models/base\\_model.py46-127](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L127) [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L376) [docling/models/vlm\\_models\\_inline/mlx\\_model.py33-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L33-L318) [docling/models/vlm\\_models\\_inline/vllm\\_model.py25-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L25-L301)\n\n## Configuration via InlineVlmOptions",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 288,
      "char_count": 931,
      "start_char": 6365,
      "end_char": 7299
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:8",
    "content": "43/docling/models/vlm_models_inline/vllm_model.py#L25-L301)\n\n## Configuration via InlineVlmOptions\n\nAll inline VLM models are configured through `InlineVlmOptions`, which specifies the model repository, inference framework, and generation parameters:\n\n| Parameter                   | Type                                               | Description                                                                                                |\n| --------------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| `repo_id`                   | `str`                                              | Hugging Face repository identifier (e.g., `\"ibm-granite/granite-docling-258M\"`)                            |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 119,
      "char_count": 833,
      "start_char": 7199,
      "end_char": 8033
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:9",
    "content": "ace repository identifier (e.g., `\"ibm-granite/granite-docling-258M\"`)                            |\n| `inference_framework`       | `InferenceFramework`                               | One of `TRANSFORMERS`, `MLX`, or `VLLM`                                                                    |\n| `transformers_model_type`   | `TransformersModelType`                            | Auto-loading class: `AUTOMODEL`, `AUTOMODEL_VISION2SEQ`, `AUTOMODEL_CAUSALLM`, `AUTOMODEL_IMAGETEXTTOTEXT` |\n| `transformers_prompt_style` | `TransformersPromptStyle`                          | Prompt formatting: `CHAT`, `RAW`, or `NONE`                                                                |\n| `response_format`           | `ResponseFormat`                                   | Expected output format: `DOCTAGS`, `MARKDOWN`, `HTML`, `OTSL`, or `PLAINTEXT`                              |\n| `torch_dtype`               | `Optional[str]`                                    | PyTorch dtype (e.g.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 215,
      "char_count": 980,
      "start_char": 7933,
      "end_char": 8913
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:10",
    "content": "orch_dtype`               | `Optional[str]`                                    | PyTorch dtype (e.g., `\"bfloat16\"`)                                                                         |\n| `max_new_tokens`            | `int`                                              | Maximum tokens to generate (default: `4096`)                                                               |\n| `temperature`               | `float`                                            | Sampling temperature (default: `0.0` for greedy)                                                           |\n| `scale`                     | `float`                                            | Image scaling factor (default: `2.0`)                                                                      |\n| `max_size`                  | `Optional[int]`                                    | Maximum image dimension                                                                                    |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 126,
      "char_count": 965,
      "start_char": 8813,
      "end_char": 9779
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:11",
    "content": "mage dimension                                                                                    |\n| `use_kv_cache`              | `bool`                                             | Enable key-value caching (default: `True`)                                                                 |\n| `stop_strings`              | `List[str]`                                        | Strings that trigger generation stop                                                                       |\n| `custom_stopping_criteria`  | `List[Union[StoppingCriteria, GenerationStopper]]` | Custom stopping logic                                                                                      |\n| `extra_generation_config`   | `Dict[str, Any]`                                   | Additional framework-specific generation parameters                                                        |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 103,
      "char_count": 875,
      "start_char": 9679,
      "end_char": 10555
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:12",
    "content": "l framework-specific generation parameters                                                        |\n| `extra_processor_kwargs`    | `Dict[str, Any]`                                   | Additional processor parameters                                                                            |\n| `quantized`                 | `bool`                                             | Enable quantization (default: `False`)                                                                     |\n| `load_in_8bit`              | `bool`                                             | Use 8-bit quantization (default: `True`)                                                                   |\n| `trust_remote_code`         | `bool`                                             | Allow remote code execution (default: `False`)                                                             |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 106,
      "char_count": 875,
      "start_char": 10455,
      "end_char": 11331
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:13",
    "content": "ote code execution (default: `False`)                                                             |\n| `revision`                  | `str`                                              | Model revision/branch (default: `\"main\"`)                                                                  |\n\n**Sources:** [docling/datamodel/pipeline\\_options\\_vlm\\_model.py54-89](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L54-L89)\n\n## Hugging Face Transformers Implementation\n\n### Model Loading and Initialization\n\n`HuggingFaceTransformersVlmModel` loads models using Transformers' auto-loading classes:\n\n```\n```\n\nThe model class is selected based on `transformers_model_type`:\n\n- `AUTOMODEL` → `AutoModel`\n- `AUTOMODEL_CAUSALLM` → `AutoModelForCausalLM`\n- `AUTOMODEL_VISION2SEQ` → `AutoModelForVision2Seq`\n- `AUTOMODEL_IMAGETEXTTOTEXT` → `AutoModelForImageTextToText`\n\nThe processor's tokenizer padding is configured with `padding_side = \"left\"` for batch processing.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 240,
      "char_count": 1017,
      "start_char": 11231,
      "end_char": 12250
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:14",
    "content": "The processor's tokenizer padding is configured with `padding_side = \"left\"` for batch processing.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-138](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L138)\n\n### Batch Inference Pipeline\n\nThe Transformers implementation processes images in batches:\n\n```\n```\n\n**Key Implementation Details:**\n\n1. **Image Normalization** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py209-224](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L209-L224): Converts numpy arrays to PIL RGB images\n2. **Prompt Handling** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py229-236](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L229-L236): Accepts single prompt string or list of prompts (one per image)\n3.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 282,
      "char_count": 995,
      "start_char": 12150,
      "end_char": 13145
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:15",
    "content": "transformers_model.py#L229-L236): Accepts single prompt string or list of prompts (one per image)\n3. **Processor Integration** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py240-256](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L240-L256): Handles both text and image preprocessing with automatic padding\n4. **Stopping Criteria** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L296): Supports `StopStringCriteria` and custom `GenerationStopper` instances\n5. **Token Trimming** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py343-344](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L343-L344): Removes input tokens from output sequences using attention mask",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "token_count": 283,
      "char_count": 967,
      "start_char": 13045,
      "end_char": 14014
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:16",
    "content": "_transformers_model.py#L343-L344): Removes input tokens from output sequences using attention mask\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py139-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L139-L376)\n\n### Stopping Criteria Handling\n\nThe Transformers implementation supports two types of stopping criteria:\n\n```\n```\n\nThe implementation distinguishes between:\n\n- **String-based stopping** via `StopStringCriteria` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py264-269](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L264-L269)\n- **GenerationStopper classes/instances** wrapped in `HFStoppingCriteriaWrapper` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py276-283](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L276-L283)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "token_count": 283,
      "char_count": 996,
      "start_char": 13914,
      "end_char": 14911
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:17",
    "content": "-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L276-L283)\n- **Native StoppingCriteria classes** instantiated with tokenizer [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py284-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L284-L287)\n- **StoppingCriteria instances** used directly [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py294-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L294-L296)\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L302)\n\n## MLX Implementation (Apple Silicon)\n\n### Architecture and Thread Safety\n\n`HuggingFaceMlxModel` uses the MLX framework for Apple Silicon acceleration with important thread safety considerations:\n\n```\n```",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "token_count": 299,
      "char_count": 1023,
      "start_char": 14811,
      "end_char": 15836
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:18",
    "content": "MLX framework for Apple Silicon acceleration with important thread safety considerations:\n\n```\n```\n\n**Critical Constraint:** MLX models are **not thread-safe**. All MLX inference operations are serialized using a global lock [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-30](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L30):\n\n```\n```\n\nThis means only one MLX model instance can perform inference at a time across the entire process.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-90](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L90)\n\n### Streaming Generation and Token Collection\n\nUnlike the Transformers implementation, MLX uses streaming generation:\n\n```\n```\n\n**Key Characteristics:**\n\n1. **No Batching** [docling/models/vlm\\_models\\_inline/mlx\\_model.py186-188](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 18,
      "token_count": 270,
      "char_count": 1016,
      "start_char": 15736,
      "end_char": 16752
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:19",
    "content": "https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L186-L188): Images are processed sequentially within the global lock\n2. **Token-Level Collection** [doclog/models/vlm\\_models\\_inline/mlx\\_model.py232-254](https://github.com/docling-project/docling/blob/f7244a43/doclog/models/vlm_models_inline/mlx_model.py#L232-L254): Each token includes text, token ID, and log probability\n3. **Early Stopping** [docling/models/vlm\\_models\\_inline/mlx\\_model.py258-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L258-L302): Stop strings and `GenerationStopper` instances are checked during streaming\n4. **Lookback Window** [docling/models/vlm\\_models\\_inline/mlx\\_model.py279-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L279-L287): Custom stoppers can specify how many recent characters to examine\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "token_count": 311,
      "char_count": 1023,
      "start_char": 16652,
      "end_char": 17675
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:20",
    "content": "how many recent characters to examine\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py149-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L149-L318)\n\n### Stopping Criteria Validation\n\nMLX enforces strict stopping criteria types [docling/models/vlm\\_models\\_inline/mlx\\_model.py75-89](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L75-L89):\n\n| Allowed                       | Not Allowed                  |\n| ----------------------------- | ---------------------------- |\n| `GenerationStopper` instances | `StoppingCriteria` instances |\n| `GenerationStopper` classes   | `StoppingCriteria` classes   |\n| Stop strings                  | -                            |\n\nIf Hugging Face `StoppingCriteria` is detected, a `ValueError` is raised with a clear message explaining that only `GenerationStopper` is supported for MLX.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "token_count": 251,
      "char_count": 1017,
      "start_char": 17575,
      "end_char": 18593
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:21",
    "content": "nerationStopper` is supported for MLX.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py75-89](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L75-L89)\n\n## vLLM Implementation\n\n### Configuration and Initialization\n\n`VllmVlmModel` provides GPU-optimized inference with strict separation of load-time and runtime parameters:\n\n```\n```\n\n**Parameter Allowlists:**\n\nThe implementation maintains two explicit allowlists [docling/models/vlm\\_models\\_inline/vllm\\_model.py32-80](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L32-L80):\n\n1. **`_VLLM_ENGINE_KEYS`** - Parameters passed to `LLM.__init__()` (load time)\n2. **`_VLLM_SAMPLING_KEYS`** - Parameters passed to `SamplingParams` (runtime)\n\nAny keys in `extra_generation_config` not in either allowlist trigger a warning and are ignored.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py82-174](https://github.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "token_count": 284,
      "char_count": 997,
      "start_char": 18493,
      "end_char": 19490
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:22",
    "content": "are ignored.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py82-174](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L82-L174)\n\n### Batch Inference with Multi-Modal Data\n\nvLLM processes images as multi-modal data in batch mode:\n\n```\n```\n\n**Key Features:**\n\n1. **True Batching** [docling/models/vlm\\_models\\_inline/vllm\\_model.py233-300](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L233-L300): vLLM processes all images in a single `generate()` call\n2. **Multi-Modal Data Format** [docling/models/vlm\\_models\\_inline/vllm\\_model.py277-280](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L277-L280): Images are passed via `multi_modal_data` dictionary with `\"image\"` key\n3. **Memory Limit** [docling/models/vlm\\_models\\_inline/vllm\\_model.py140](https://github.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "token_count": 304,
      "char_count": 948,
      "start_char": 19390,
      "end_char": 20338
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:23",
    "content": "age\"` key\n3. **Memory Limit** [docling/models/vlm\\_models\\_inline/vllm\\_model.py140](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L140-L140): `limit_mm_per_prompt={\"image\": 1}` restricts one image per prompt\n4. **GPU Memory Management** [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-151](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L151): Defaults to 30% GPU memory utilization to share with other models\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py175-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L175-L301)\n\n## Prompt Formatting\n\nAll inline VLM models share the `formulate_prompt()` method from `BaseVlmPageModel`:\n\n```\n```\n\n**Prompt Style Options:**\n\n| Style          | Behavior                               | Use Case                                            |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "token_count": 288,
      "char_count": 983,
      "start_char": 20238,
      "end_char": 21222
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:24",
    "content": "| Behavior                               | Use Case                                            |\n| -------------- | -------------------------------------- | --------------------------------------------------- |\n| `RAW`          | Returns user prompt unchanged          | Models that handle formatting internally            |\n| `NONE`         | Returns empty string                   | Models that don't need text prompts (e.g., GOT-OCR) |\n| `CHAT`         | Applies processor's chat template      | Standard instruction-following models               |\n| Custom (Phi-4) | Special formatting for specific models | Model-specific requirements                         |\n\n**Sources:** [docling/models/base\\_model.py85-126](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L85-L126)\n\n## Model Download and Caching\n\nAll inline VLM implementations inherit from `HuggingFaceModelDownloadMixin`:\n\n```\n```\n\nThe `repo_cache_folder` property converts slashes in `repo_id` to dashes (e.g.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "token_count": 202,
      "char_count": 1012,
      "start_char": 21122,
      "end_char": 22137
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:25",
    "content": "loadMixin`:\n\n```\n```\n\nThe `repo_cache_folder` property converts slashes in `repo_id` to dashes (e.g., `\"ibm-granite/granite-docling-258M\"` → `\"ibm-granite--granite-docling-258M\"`).\n\n**Sources:** [docling/models/utils/hf\\_model\\_download.py8-45](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py#L8-L45) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py86-88](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L86-L88)\n\n## Available Model Specifications\n\nDocling provides pre-configured model specifications in `vlm_model_specs`:\n\n### DocTags Output Models\n\n| Model Spec                    | Repository                                | Framework    | Devices   | Response Format |\n| ----------------------------- | ----------------------------------------- | ------------ | --------- | --------------- |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "token_count": 234,
      "char_count": 908,
      "start_char": 22037,
      "end_char": 22946
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:26",
    "content": "-------- | ----------------------------------------- | ------------ | --------- | --------------- |\n| `GRANITEDOCLING_TRANSFORMERS` | `ibm-granite/granite-docling-258M`        | Transformers | CPU, CUDA | DOCTAGS         |\n| `GRANITEDOCLING_VLLM`         | `ibm-granite/granite-docling-258M`        | vLLM         | CUDA      | DOCTAGS         |\n| `GRANITEDOCLING_MLX`          | `ibm-granite/granite-docling-258M-mlx`    | MLX          | MPS       | DOCTAGS         |\n| `SMOLDOCLING_TRANSFORMERS`    | `ds4sd/SmolDocling-256M-preview`          | Transformers | CPU, CUDA | DOCTAGS         |\n| `SMOLDOCLING_VLLM`            | `ds4sd/SmolDocling-256M-preview`          | vLLM         | CUDA      | DOCTAGS         |\n| `SMOLDOCLING_MLX`             | `ds4sd/SmolDocling-256M-preview-mlx-bf16` | MLX          | MPS       | DOCTAGS         |\n\n### Markdown Output Models\n\n| Model Spec                    | Repository                                  | Framework    | Devices        | Response Format |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 26,
      "token_count": 308,
      "char_count": 996,
      "start_char": 22846,
      "end_char": 23843
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:27",
    "content": "| Repository                                  | Framework    | Devices        | Response Format |\n| ----------------------------- | ------------------------------------------- | ------------ | -------------- | --------------- |\n| `GRANITE_VISION_TRANSFORMERS` | `ibm-granite/granite-vision-3.2-2b`         | Transformers | CPU, CUDA, MPS | MARKDOWN        |\n| `GRANITE_VISION_VLLM`         | `ibm-granite/granite-vision-3.2-2b`         | vLLM         | CUDA           | MARKDOWN        |\n| `PIXTRAL_12B_TRANSFORMERS`    | `mistral-community/pixtral-12b`             | Transformers | CPU, CUDA      | MARKDOWN        |\n| `PIXTRAL_12B_MLX`             | `mlx-community/pixtral-12b-bf16`            | MLX          | MPS            | MARKDOWN        |\n| `PHI4_TRANSFORMERS`           | `microsoft/Phi-4-multimodal-instruct`       | Transformers | CPU, CUDA      | MARKDOWN        |\n| `QWEN25_VL_3B_MLX`            | `mlx-community/Qwen2.5-VL-3B-Instruct-bf16` | MLX          | MPS            | MARKDOWN        |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 27,
      "token_count": 289,
      "char_count": 1007,
      "start_char": 23743,
      "end_char": 24753
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:28",
    "content": "| `mlx-community/Qwen2.5-VL-3B-Instruct-bf16` | MLX          | MPS            | MARKDOWN        |\n| `GOT2_TRANSFORMERS`           | `stepfun-ai/GOT-OCR-2.0-hf`                 | Transformers | CPU, CUDA      | MARKDOWN        |\n| `GEMMA3_12B_MLX`              | `mlx-community/gemma-3-12b-it-bf16`         | MLX          | MPS            | MARKDOWN        |\n| `GEMMA3_27B_MLX`              | `mlx-community/gemma-3-27b-it-bf16`         | MLX          | MPS            | MARKDOWN        |\n| `DOLPHIN_TRANSFORMERS`        | `ByteDance/Dolphin`                         | Transformers | CPU, CUDA, MPS | MARKDOWN        |\n\n### Plaintext Output Models\n\n| Model Spec                | Repository                            | Framework    | Devices   | Response Format |\n| ------------------------- | ------------------------------------- | ------------ | --------- | --------------- |\n| `SMOLVLM256_TRANSFORMERS` | `HuggingFaceTB/SmolVLM-256M-Instruct` | Transformers | CPU, CUDA | PLAINTEXT       |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 28,
      "token_count": 278,
      "char_count": 992,
      "start_char": 24653,
      "end_char": 25648
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:29",
    "content": "RANSFORMERS` | `HuggingFaceTB/SmolVLM-256M-Instruct` | Transformers | CPU, CUDA | PLAINTEXT       |\n| `SMOLVLM256_MLX`          | `moot20/SmolVLM-256M-Instruct-MLX`    | MLX          | MPS       | PLAINTEXT       |\n| `SMOLVLM256_VLLM`         | `HuggingFaceTB/SmolVLM-256M-Instruct` | vLLM         | CUDA      | PLAINTEXT       |\n\n### Extraction Models\n\n| Model Spec                   | Repository                | Framework    | Devices        | Response Format |\n| ---------------------------- | ------------------------- | ------------ | -------------- | --------------- |\n| `NU_EXTRACT_2B_TRANSFORMERS` | `numind/NuExtract-2.0-2B` | Transformers | CPU, CUDA, MPS | PLAINTEXT       |\n\n**Special Configuration Notes:**\n\n1. **GOT-OCR-2.0** uses `TransformersPromptStyle.NONE` and includes `extra_processor_kwargs={\"format\": True}`\n2. **Phi-4** requires `transformers<4.52.0` and uses `extra_generation_config={\"num_logits_to_keep\": 0}`\n3. **Dolphin** uses `TransformersPromptStyle.RAW` with a custom prompt format\n4.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 29,
      "token_count": 304,
      "char_count": 1017,
      "start_char": 25548,
      "end_char": 26565
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:30",
    "content": "ogits_to_keep\": 0}`\n3. **Dolphin** uses `TransformersPromptStyle.RAW` with a custom prompt format\n4. **GraniteDocling VLLM** uses `revision=\"untied\"` for compatibility with vLLM ≤0.10.2\n\n**Sources:** [docling/datamodel/vlm\\_model\\_specs.py1-303](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L1-L303)\n\n## Usage Examples\n\n### Basic Usage with Default Model\n\n```\n```\n\n### Selecting a Specific Model\n\n```\n```\n\n### Custom Model Configuration\n\n```\n```\n\n### Direct Image Processing\n\nAll inline VLM models support direct image processing via the `process_images()` method:\n\n```\n```\n\n**Sources:** [docs/examples/minimal\\_vlm\\_pipeline.py1-71](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py#L1-L71) [docs/usage/vision\\_models.md1-124](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L1-L124)\n\n## Performance Considerations\n\n### Framework Comparison",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 30,
      "token_count": 288,
      "char_count": 972,
      "start_char": 26465,
      "end_char": 27439
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:31",
    "content": "4a43/docs/usage/vision_models.md#L1-L124)\n\n## Performance Considerations\n\n### Framework Comparison\n\n| Framework        | Batching             | Thread Safety          | Best For                            |\n| ---------------- | -------------------- | ---------------------- | ----------------------------------- |\n| **Transformers** | ✅ Full batch support | ✅ Thread-safe          | General purpose, CPU/CUDA/MPS       |\n| **MLX**          | ❌ Sequential only    | ❌ Global lock required | Apple Silicon (fastest on M-series) |\n| **vLLM**         | ✅ Optimized batching | ✅ Thread-safe          | High-throughput GPU inference       |\n\n### Memory Management\n\n1. **Transformers**: Uses PyTorch's default memory management; consider `torch_dtype=\"bfloat16\"` for memory savings\n2. **MLX**: Automatically manages unified memory on Apple Silicon\n3. **vLLM**: Set `gpu_memory_utilization` (default 0.3) to reserve GPU memory for other models\n\n### Acceleration Options",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 31,
      "token_count": 220,
      "char_count": 961,
      "start_char": 27339,
      "end_char": 28302
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:32",
    "content": "memory_utilization` (default 0.3) to reserve GPU memory for other models\n\n### Acceleration Options\n\n- **Flash Attention 2**: Automatically enabled on CUDA devices when `accelerator_options.cuda_use_flash_attention2=True`\n- **Quantization**: Enable with `quantized=True` and `load_in_8bit=True` (Transformers and vLLM only)\n- **KV Cache**: Enabled by default with `use_kv_cache=True`; disable only if memory is constrained\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py123-128](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L123-L128) [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-155](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L155) [docs/usage/vision\\_models.md46-58](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L46-L58)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 32,
      "token_count": 294,
      "char_count": 999,
      "start_char": 28202,
      "end_char": 29201
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:33",
    "content": "/docs/usage/vision_models.md#L46-L58)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.\n\n### On this page\n\n- [Inline VLM Models](#inline-vlm-models.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Configuration via InlineVlmOptions](#configuration-via-inlinevlmoptions.md)\n- [Hugging Face Transformers Implementation](#hugging-face-transformers-implementation.md)\n- [Model Loading and Initialization](#model-loading-and-initialization.md)\n- [Batch Inference Pipeline](#batch-inference-pipeline.md)\n- [Stopping Criteria Handling](#stopping-criteria-handling.md)\n- [MLX Implementation (Apple Silicon)](#mlx-implementation-apple-silicon.md)\n- [Architecture and Thread Safety](#architecture-and-thread-safety.md)\n- [Streaming Generation and Token Collection](#streaming-generation-and-token-collection.md)\n- [Stopping Criteria Validation](#stopping-criteria-validation.md)\n- [vLLM Implementation](#vllm-implementation.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 33,
      "token_count": 230,
      "char_count": 978,
      "start_char": 29101,
      "end_char": 30080
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:34",
    "content": "eria Validation](#stopping-criteria-validation.md)\n- [vLLM Implementation](#vllm-implementation.md)\n- [Configuration and Initialization](#configuration-and-initialization.md)\n- [Batch Inference with Multi-Modal Data](#batch-inference-with-multi-modal-data.md)\n- [Prompt Formatting](#prompt-formatting.md)\n- [Model Download and Caching](#model-download-and-caching.md)\n- [Available Model Specifications](#available-model-specifications.md)\n- [DocTags Output Models](#doctags-output-models.md)\n- [Markdown Output Models](#markdown-output-models.md)\n- [Plaintext Output Models](#plaintext-output-models.md)\n- [Extraction Models](#extraction-models.md)\n- [Usage Examples](#usage-examples.md)\n- [Basic Usage with Default Model](#basic-usage-with-default-model.md)\n- [Selecting a Specific Model](#selecting-a-specific-model.md)\n- [Custom Model Configuration](#custom-model-configuration.md)\n- [Direct Image Processing](#direct-image-processing.md)\n- [Performance Considerations](#performance-considerations.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 34,
      "token_count": 239,
      "char_count": 1004,
      "start_char": 29980,
      "end_char": 30985
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.1-inline-vlm-models.md:chunk:35",
    "content": "essing](#direct-image-processing.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Framework Comparison](#framework-comparison.md)\n- [Memory Management](#memory-management.md)\n- [Acceleration Options](#acceleration-options.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.1-inline-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 35,
      "token_count": 54,
      "char_count": 246,
      "start_char": 30885,
      "end_char": 31909
    }
  }
]