[
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:0",
    "content": "API-Based VLM Models | docling-project/docling | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[docling-project/docling](https://github.com/docling-project/docling \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 12 October 2025 ([f7244a](https://github.com/docling-project/docling/commits/f7244a43))\n\n- [Overview](docling-project/docling/1-overview.md)\n- [Installation](docling-project/docling/1.1-installation.md)\n- [Quick Start](docling-project/docling/1.2-quick-start.md)\n- [Core Architecture](docling-project/docling/2-core-architecture.md)\n- [Document Conversion Flow](docling-project/docling/2.1-document-conversion-flow.md)\n- [DoclingDocument Data Model](docling-project/docling/2.2-doclingdocument-data-model.md)\n- [Configuration and Pipeline Options](docling-project/docling/2.3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 0,
      "token_count": 277,
      "char_count": 1008,
      "start_char": 0,
      "end_char": 1008
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:1",
    "content": "3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.4-format-detection-and-routing.md)\n- [Document Backends](docling-project/docling/3-document-backends.md)\n- [PDF Processing Backends](docling-project/docling/3.1-pdf-processing-backends.md)\n- [Office Document Backends](docling-project/docling/3.2-office-document-backends.md)\n- [Web and Markup Backends](docling-project/docling/3.3-web-and-markup-backends.md)\n- [AI/ML Models](docling-project/docling/4-aiml-models.md)\n- [OCR Models](docling-project/docling/4.1-ocr-models.md)\n- [Layout and Table Structure Models](docling-project/docling/4.2-layout-and-table-structure-models.md)\n- [Vision Language Models](docling-project/docling/4.3-vision-language-models.md)\n- [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md)\n- [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 1,
      "token_count": 283,
      "char_count": 987,
      "start_char": 908,
      "end_char": 1896
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:2",
    "content": ".2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)\n- [Processing Pipelines](docling-project/docling/5-processing-pipelines.md)\n- [Standard PDF Pipeline](docling-project/docling/5.1-standard-pdf-pipeline.md)\n- [Threaded PDF Pipeline](docling-project/docling/5.2-threaded-pdf-pipeline.md)\n- [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md)\n- [Extraction Pipeline](docling-project/docling/5.4-extraction-pipeline.md)\n- [ASR Pipeline](docling-project/docling/5.5-asr-pipeline.md)\n- [Base Pipeline Architecture](docling-project/docling/5.6-base-pipeline-architecture.md)\n- [Command Line Interface](docling-project/docling/6-command-line-interface.md)\n- [Document Conversion CLI](docling-project/docling/6.1-document-conversion-cli.md)\n- [Model Management CLI](docling-project/docling/6.2-model-management-cli.md)\n- [Python SDK](docling-project/docling/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 2,
      "token_count": 275,
      "char_count": 1003,
      "start_char": 1796,
      "end_char": 2800
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:3",
    "content": "ng/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)\n- [DocumentExtractor API](docling-project/docling/7.2-documentextractor-api.md)\n- [Usage Examples](docling-project/docling/7.3-usage-examples.md)\n- [Output and Integration](docling-project/docling/8-output-and-integration.md)\n- [Export Formats](docling-project/docling/8.1-export-formats.md)\n- [Document Chunking](docling-project/docling/8.2-document-chunking.md)\n- [Framework Integrations](docling-project/docling/8.3-framework-integrations.md)\n- [Development and Testing](docling-project/docling/9-development-and-testing.md)\n- [Testing Framework](docling-project/docling/9.1-testing-framework.md)\n- [Ground Truth Data](docling-project/docling/9.2-ground-truth-data.md)\n- [CI/CD and Development Workflow](docling-project/docling/9.3-cicd-and-development-workflow.md)\n- [Deployment](docling-project/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 3,
      "token_count": 270,
      "char_count": 998,
      "start_char": 2700,
      "end_char": 3699
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:4",
    "content": "/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)\n- [Model Artifacts Management](docling-project/docling/10.2-model-artifacts-management.md)\n\nMenu\n\n# API-Based VLM Models\n\nRelevant source files\n\n- [docling/datamodel/extraction.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/extraction.py)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/document\\_extractor.py](https://github.com/docling-project/docling/blob/f7244a43/docling/document_extractor.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 4,
      "token_count": 281,
      "char_count": 970,
      "start_char": 3599,
      "end_char": 4569
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:5",
    "content": "ing/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/nuextract\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/nuextract_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 5,
      "token_count": 298,
      "char_count": 1013,
      "start_char": 4469,
      "end_char": 5483
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:6",
    "content": "://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docling/pipeline/asr\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/asr_pipeline.py)\n- [docling/pipeline/base\\_extraction\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_extraction_pipeline.py)\n- [docling/pipeline/base\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_pipeline.py)\n- [docling/pipeline/extraction\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/extraction_vlm_pipeline.py)\n- [docling/pipeline/simple\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/simple_pipeline.py)\n- [docling/pipeline/threaded\\_standard\\_pdf\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/threaded_standard_pdf_pipeline.py)\n- [docling/pipeline/vlm\\_pipeline.py](https://github.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 6,
      "token_count": 289,
      "char_count": 1021,
      "start_char": 5383,
      "end_char": 6404
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:7",
    "content": "ng/pipeline/threaded_standard_pdf_pipeline.py)\n- [docling/pipeline/vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py)\n\n## Purpose and Scope\n\nThis page documents the API-based Vision Language Model (VLM) integration in Docling, which enables document processing using external VLM services via OpenAI-compatible HTTP APIs. API-based models connect to remote inference servers (e.g., Ollama, vLLM server, OpenAI) rather than loading models locally.\n\nFor locally-executed VLM models using Transformers, MLX, or vLLM frameworks, see [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md). For the broader VLM system architecture and pipeline integration, see [Vision Language Models](docling-project/docling/4.3-vision-language-models.md).\n\nAPI-based models are configured through `ApiVlmOptions` and executed by the `ApiVlmModel` class, which provides threaded request handling, streaming support, and early-abort capabilities through custom stopping c",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 7,
      "token_count": 237,
      "char_count": 1024,
      "start_char": 6304,
      "end_char": 7328
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:8",
    "content": "threaded request handling, streaming support, and early-abort capabilities through custom stopping criteria.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py1-102](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L1-L102) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py96-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L96-L112)\n\n---\n\n## System Architecture\n\n```\n```\n\n**Diagram: API-Based VLM Model Architecture**\n\nThe architecture separates configuration (`ApiVlmOptions`), execution (`ApiVlmModel`), and HTTP communication (`api_image_request` functions). The `ThreadPoolExecutor` enables concurrent processing of page batches, while the streaming path supports early termination via `GenerationStopper` instances.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py19-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L19-L101) [docling/pipeline/vlm\\_pipeline.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 8,
      "token_count": 273,
      "char_count": 1011,
      "start_char": 7228,
      "end_char": 8239
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:9",
    "content": "ect/docling/blob/f7244a43/docling/models/api_vlm_model.py#L19-L101) [docling/pipeline/vlm\\_pipeline.py66-73](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py#L66-L73)\n\n---\n\n## Configuration: ApiVlmOptions\n\nThe `ApiVlmOptions` class defines all parameters for connecting to and configuring an external VLM API:\n\n| Field                      | Type                           | Default                                      | Description                                      |\n| -------------------------- | ------------------------------ | -------------------------------------------- | ------------------------------------------------ |\n| `kind`                     | `Literal[\"api_model_options\"]` | `\"api_model_options\"`                        | Discriminator for option type                    |\n| `url`                      | `AnyUrl`                       | `http://localhost:11434/v1/chat/completions` | API endpoint URL (OpenAI-compatible)             |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 9,
      "token_count": 204,
      "char_count": 999,
      "start_char": 8139,
      "end_char": 9139
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:10",
    "content": "| `http://localhost:11434/v1/chat/completions` | API endpoint URL (OpenAI-compatible)             |\n| `headers`                  | `Dict[str, str]`               | `{}`                                         | HTTP headers (e.g., authorization)               |\n| `params`                   | `Dict[str, Any]`               | `{}`                                         | Model-specific parameters (e.g., `model` name)   |\n| `timeout`                  | `float`                        | `60`                                         | Request timeout in seconds                       |\n| `concurrency`              | `int`                          | `1`                                          | Number of concurrent page requests               |\n| `response_format`          | `ResponseFormat`               | —                                            | Expected response format (DOCTAGS/Markdown/HTML) |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 10,
      "token_count": 168,
      "char_count": 909,
      "start_char": 9039,
      "end_char": 9949
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:11",
    "content": "| —                                            | Expected response format (DOCTAGS/Markdown/HTML) |\n| `prompt`                   | `str`                          | —                                            | User prompt template                             |\n| `scale`                    | `float`                        | `2.0`                                        | Image scaling factor                             |\n| `max_size`                 | `Optional[int]`                | `None`                                       | Maximum image dimension                          |\n| `temperature`              | `float`                        | `0.0`                                        | Generation temperature                           |\n| `stop_strings`             | `List[str]`                    | `[]`                                         | Stop string tokens                               |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 11,
      "token_count": 130,
      "char_count": 909,
      "start_char": 9849,
      "end_char": 10759
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:12",
    "content": "| `[]`                                         | Stop string tokens                               |\n| `custom_stopping_criteria` | `List[GenerationStopper]`      | `[]`                                         | Early-abort logic instances                      |\n\n**Key Configuration Patterns:**\n\n```\n```\n\nThe `url` must point to an OpenAI-compatible `/v1/chat/completions` endpoint. The `params` dict is merged with runtime temperature settings and passed as the request body. The `concurrency` parameter controls the `ThreadPoolExecutor` worker count.\n\n**Sources:** [docling/datamodel/pipeline\\_options\\_vlm\\_model.py96-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L96-L112) [docling/datamodel/vlm\\_model\\_specs.py171-179](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L171-L179)\n\n---\n\n## Request Flow Sequence\n\n```\n```\n\n**Diagram: API VLM Request Flow**\n\nThe `ApiVlmModel.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 12,
      "token_count": 251,
      "char_count": 987,
      "start_char": 10659,
      "end_char": 11646
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:13",
    "content": "-L179)\n\n---\n\n## Request Flow Sequence\n\n```\n```\n\n**Diagram: API VLM Request Flow**\n\nThe `ApiVlmModel.__call__` method uses `ThreadPoolExecutor.map` to process pages concurrently. Each worker thread executes `_vlm_request`, which retrieves the page image, formats the prompt, and makes an HTTP request. If custom stopping criteria are configured, the streaming path (`api_image_request_streaming`) is used to enable early termination.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py43-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L43-L101)\n\n---\n\n## ApiVlmModel Implementation\n\n### Class Structure\n\nThe `ApiVlmModel` class implements `BasePageModel` and orchestrates API-based inference:\n\n```\n```\n\n**Initialization Validation:**\n\nThe constructor enforces the `enable_remote_services` flag to prevent accidental external connections:\n\n```\n```\n\nThis safety check requires explicit opt-in at the pipeline level before API requests are allowed.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 13,
      "token_count": 231,
      "char_count": 987,
      "start_char": 11546,
      "end_char": 12535
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:14",
    "content": "This safety check requires explicit opt-in at the pipeline level before API requests are allowed.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py20-41](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L20-L41)\n\n### Request Execution Pattern\n\nThe `_vlm_request` helper function processes a single page:\n\n1. **Validation:** Check `page._backend.is_valid()`\n2. **Image Extraction:** Call `page.get_image(scale, max_size)` and convert to RGB\n3. **Prompt Construction:** Use `vlm_options.build_prompt(page.parsed_page)`\n4. **Stopping Criteria Processing:** Instantiate any `GenerationStopper` classes\n5. **API Call:** Route to streaming or non-streaming based on `custom_stopping_criteria`\n6. **Response Decoding:** Apply `vlm_options.decode_response()`\n7. **Result Attachment:** Set `page.predictions.vlm_response`\n\n**Concurrency Control:**\n\n```\n```\n\nThe executor processes up to `concurrency` pages in parallel, with each thread making independent HTTP requests.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 14,
      "token_count": 241,
      "char_count": 1000,
      "start_char": 12435,
      "end_char": 13436
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:15",
    "content": "processes up to `concurrency` pages in parallel, with each thread making independent HTTP requests. This is essential for throughput when processing large documents.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py43-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L43-L101)\n\n---\n\n## Streaming and Early Abort\n\n### Streaming Request Flow\n\nWhen `custom_stopping_criteria` is non-empty, the model uses the streaming API path:\n\n```\n```\n\n### GenerationStopper Interface\n\nThe `GenerationStopper` protocol enables custom early-abort logic:\n\n```\n```\n\nStreaming requests check `should_stop()` after each token chunk arrives. This allows stopping generation when:\n\n- A specific pattern is detected (e.g., closing XML tag)\n- A confidence threshold is crossed\n- A maximum content length is reached\n\n**Sources:** [docling/models/api\\_vlm\\_model.py63-97](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L63-L97)\n\n---\n\n## Pipeline Integration",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 15,
      "token_count": 251,
      "char_count": 1015,
      "start_char": 13336,
      "end_char": 14354
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:16",
    "content": "oject/docling/blob/f7244a43/docling/models/api_vlm_model.py#L63-L97)\n\n---\n\n## Pipeline Integration\n\n### VlmPipeline Instantiation\n\nThe `VlmPipeline` detects `ApiVlmOptions` and instantiates `ApiVlmModel`:\n\n```\n```\n\nThis is the sole model in the `build_pipe` list, as API-based inference is end-to-end (no separate OCR, layout, or table models).\n\n**Sources:** [docling/pipeline/vlm\\_pipeline.py66-73](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py#L66-L73)\n\n### Page Processing\n\nThe pipeline's `initialize_page` method loads page backends, then `_apply_on_pages` iterates the `build_pipe`:\n\n```\n```\n\nFor `ApiVlmModel`, the `__call__` method internally uses the thread pool, so the outer iteration is straightforward. The model modifies `page.predictions.vlm_response` in-place and yields the updated pages.\n\n**Sources:** [docling/pipeline/base\\_pipeline.py189-195](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_pipeline.py#L189-L195)\n\n---",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 16,
      "token_count": 290,
      "char_count": 1013,
      "start_char": 14254,
      "end_char": 15269
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:17",
    "content": "github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_pipeline.py#L189-L195)\n\n---\n\n## Predefined API Configurations\n\nThe `docling/datamodel/vlm_model_specs.py` module provides ready-to-use configurations:\n\n### GRANITE\\_VISION\\_OLLAMA\n\n```\n```\n\nThis configuration targets a local Ollama server running the Granite Vision model. The `scale=1.0` uses original image resolution, and `timeout=120` allows longer processing for complex pages.\n\n**Usage Pattern:**\n\n```\n```\n\n**Sources:** [docling/datamodel/vlm\\_model\\_specs.py171-179](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L171-L179)\n\n---\n\n## API Request Format\n\n### OpenAI Chat Completions Schema\n\nThe API client sends requests to the `/v1/chat/completions` endpoint using the OpenAI-compatible schema:\n\n```\n```\n\nThe image is base64-encoded and included as a data URL. Additional parameters from `ApiVlmOptions.params` are merged into the request body.\n\n### Response Parsing\n\n**Non-Streaming Response:**",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 17,
      "token_count": 270,
      "char_count": 1020,
      "start_char": 15169,
      "end_char": 16191
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:18",
    "content": "tions.params` are merged into the request body.\n\n### Response Parsing\n\n**Non-Streaming Response:**\n\n```\n```\n\nThe `api_image_request` function extracts `choices[0].message.content`.\n\n**Streaming Response:**\n\nServer-Sent Events (SSE) format:\n\n```\ndata: {\"choices\": [{\"delta\": {\"content\": \"# \"}}]}\n\ndata: {\"choices\": [{\"delta\": {\"content\": \"Document\"}}]}\n\ndata: [DONE]\n```\n\nThe `api_image_request_streaming` function accumulates chunks until a stopper triggers or the stream completes.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py76-97](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L76-L97)\n\n---\n\n## Error Handling and Timeout\n\n### Request-Level Timeouts\n\nEach API request respects the `timeout` parameter:\n\n```\n```\n\nIf the server doesn't respond within `timeout` seconds, the request raises a timeout exception, which is caught by the pipeline's error handling.\n\n### Backend Validation\n\nBefore making API requests, the model validates the page backend:\n\n```\n```",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 18,
      "token_count": 250,
      "char_count": 1007,
      "start_char": 16091,
      "end_char": 17100
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:19",
    "content": "### Backend Validation\n\nBefore making API requests, the model validates the page backend:\n\n```\n```\n\nInvalid pages (e.g., corrupted PDFs) are returned unchanged, preventing unnecessary API calls.\n\n### Remote Services Flag\n\nThe `enable_remote_services` flag provides a safety gate:\n\n```\n```\n\nThis prevents accidental API calls in environments where external connections are forbidden or should be audited.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py28-49](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L28-L49)\n\n---\n\n## Comparison with Inline VLM Models\n\n| Aspect                | API-Based Models                        | Inline Models                                                            |\n| --------------------- | --------------------------------------- | ------------------------------------------------------------------------ |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 19,
      "token_count": 164,
      "char_count": 886,
      "start_char": 17000,
      "end_char": 17887
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:20",
    "content": "---------------------- | ------------------------------------------------------------------------ |\n| **Execution**         | Remote HTTP API                         | Local model loading (Transformers/MLX/vLLM)                              |\n| **Configuration**     | `ApiVlmOptions`                         | `InlineVlmOptions`                                                       |\n| **Model Class**       | `ApiVlmModel`                           | `HuggingFaceTransformersVlmModel`, `HuggingFaceMlxModel`, `VllmVlmModel` |\n| **Dependencies**      | HTTP client only                        | `transformers`, `torch`, `mlx`, `vllm`                                   |\n| **Concurrency**       | `ThreadPoolExecutor` (I/O bound)        | Model batching (compute bound)                                           |\n| **Device**            | N/A (server-side)                       | CPU/CUDA/MPS                                                             |",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 20,
      "token_count": 170,
      "char_count": 957,
      "start_char": 17787,
      "end_char": 18745
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:21",
    "content": "| CPU/CUDA/MPS                                                             |\n| **Artifacts**         | None (server manages)                   | Downloaded to `artifacts_path`                                           |\n| **Stopping Criteria** | `GenerationStopper` (streaming only)    | `StoppingCriteria` + `GenerationStopper`                                 |\n| **Use Case**          | Distributed inference, limited hardware | Local control, offline operation                                         |\n\n**Sources:** [docling/models/api\\_vlm\\_model.py19-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L19-L101) [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L376)\n\n---\n\n## Example: Custom Stopping Criteria\n\n### Implementing a GenerationStopper\n\n```\n```\n\n### Configuration with Stopper\n\n```\n```",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 21,
      "token_count": 235,
      "char_count": 986,
      "start_char": 18645,
      "end_char": 19656
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:22",
    "content": "g Criteria\n\n### Implementing a GenerationStopper\n\n```\n```\n\n### Configuration with Stopper\n\n```\n```\n\nWhen configured, the streaming API path is automatically selected, and generation terminates as soon as `</doctag>` appears in the output, saving tokens and reducing latency.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py63-74](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L63-L74) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py110-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L110-L112)\n\n---\n\n## Performance Considerations\n\n### Concurrency Tuning\n\nThe `concurrency` parameter controls parallel requests:\n\n- **Low concurrency (1-2):** Sequential processing, minimal server load\n- **Medium concurrency (4-8):** Balanced throughput for typical documents\n- **High concurrency (16+):** Maximum speed for large batches, requires server capacity\n\nOptimal settings depend on:\n\n1.",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 22,
      "token_count": 258,
      "char_count": 988,
      "start_char": 19556,
      "end_char": 20544
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:23",
    "content": "(16+):** Maximum speed for large batches, requires server capacity\n\nOptimal settings depend on:\n\n1. Server capacity (GPU count, batch size)\n2. Network latency and bandwidth\n3. Document complexity (larger images = longer inference)\n\n### Timeout Configuration\n\nAppropriate timeout values vary by model and document type:\n\n- **Simple text extraction:** 30-60 seconds\n- **Complex documents (tables, figures):** 120-300 seconds\n- **Large images (high resolution):** 300+ seconds\n\nInsufficient timeouts cause false failures; excessive timeouts delay error detection.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py36-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L36-L101)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.\n\n### On this page\n\n- [API-Based VLM Models](#api-based-vlm-models.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 23,
      "token_count": 253,
      "char_count": 979,
      "start_char": 20444,
      "end_char": 21425
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:24",
    "content": "s.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)\n- [Configuration: ApiVlmOptions](#configuration-apivlmoptions.md)\n- [Request Flow Sequence](#request-flow-sequence.md)\n- [ApiVlmModel Implementation](#apivlmmodel-implementation.md)\n- [Class Structure](#class-structure.md)\n- [Request Execution Pattern](#request-execution-pattern.md)\n- [Streaming and Early Abort](#streaming-and-early-abort.md)\n- [Streaming Request Flow](#streaming-request-flow.md)\n- [GenerationStopper Interface](#generationstopper-interface.md)\n- [Pipeline Integration](#pipeline-integration.md)\n- [VlmPipeline Instantiation](#vlmpipeline-instantiation.md)\n- [Page Processing](#page-processing.md)\n- [Predefined API Configurations](#predefined-api-configurations.md)\n- [GRANITE\\_VISION\\_OLLAMA](#granite_vision_ollama.md)\n- [API Request Format](#api-request-format.md)\n- [OpenAI Chat Completions Schema](#openai-chat-completions-schema.md)\n- [Response Parsing](#response-parsing.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 24,
      "token_count": 260,
      "char_count": 1002,
      "start_char": 21325,
      "end_char": 22328
    }
  },
  {
    "chunk_id": "docling-project_docling:_docling-project_docling_4.3.2-api-based-vlm-models.md:chunk:25",
    "content": "Completions Schema](#openai-chat-completions-schema.md)\n- [Response Parsing](#response-parsing.md)\n- [Error Handling and Timeout](#error-handling-and-timeout.md)\n- [Request-Level Timeouts](#request-level-timeouts.md)\n- [Backend Validation](#backend-validation.md)\n- [Remote Services Flag](#remote-services-flag.md)\n- [Comparison with Inline VLM Models](#comparison-with-inline-vlm-models.md)\n- [Example: Custom Stopping Criteria](#example-custom-stopping-criteria.md)\n- [Implementing a GenerationStopper](#implementing-a-generationstopper.md)\n- [Configuration with Stopper](#configuration-with-stopper.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Concurrency Tuning](#concurrency-tuning.md)\n- [Timeout Configuration](#timeout-configuration.md)",
    "metadata": {
      "source_file": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "source_collection": "docling-project_docling",
      "subdirectory": "root",
      "filename": "_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "file_extension": ".md",
      "chunk_index": 25,
      "token_count": 184,
      "char_count": 768,
      "start_char": 22228,
      "end_char": 23252
    }
  }
]