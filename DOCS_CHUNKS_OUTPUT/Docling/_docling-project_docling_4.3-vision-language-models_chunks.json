[
  {
    "text": "Vision Language Models | docling-project/docling | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[docling-project/docling](https://github.com/docling-project/docling \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 12 October 2025 ([f7244a](https://github.com/docling-project/docling/commits/f7244a43))",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 115,
      "character_count": 423,
      "created_at": "2025-10-16T17:42:16.977026",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 0,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [Overview](docling-project/docling/1-overview.md)\n- [Installation](docling-project/docling/1.1-installation.md)\n- [Quick Start](docling-project/docling/1.2-quick-start.md)\n- [Core Architecture](docling-project/docling/2-core-architecture.md)\n- [Document Conversion Flow](docling-project/docling/2.1-document-conversion-flow.md)\n- [DoclingDocument Data Model](docling-project/docling/2.2-doclingdocument-data-model.md)\n- [Configuration and Pipeline Options](docling-project/docling/2.3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.4-format-detection-and-routing.md)\n- [Document Backends](docling-project/docling/3-document-backends.md)\n- [PDF Processing Backends](docling-project/docling/3.1-pdf-processing-backends.md)\n- [Office Document Backends](docling-project/docling/3.2-office-document-backends.md)\n- [Web and Markup Backends](docling-project/docling/3.3-web-and-markup-backends.md)\n- [AI/ML Models](docling-project/docling/4-aiml-models.md)\n- [OCR Models](docling-project/docling/4.1-ocr-models.md)\n- [Layout and Table Structure Models](docling-project/docling/4.2-layout-and-table-structure-models.md)\n- [Vision Language Models](docling-project/docling/4.3-vision-language-models.md)\n- [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md)\n- [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)\n- [Processing Pipelines](docling-project/docling/5-processing-pipelines.md)\n- [Standard PDF Pipeline](docling-project/docling/5.1-standard-pdf-pipeline.md)\n- [Threaded PDF Pipeline](docling-project/docling/5.2-threaded-pdf-pipeline.md)\n- [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md)\n- [Extraction Pipeline](docling-project/docling/5.4-extraction-pipeline.md)\n- [ASR Pipeline](docling-project/docling/5.5-asr-pipeline.md)\n- [Base Pipeline Architecture](docling-project/docling/5.6-base-pipeline-architecture.md)\n- [Command Line Interface](docling-project/docling/6-command-line-interface.md)\n- [Document Conversion CLI](docling-project/docling/6.1-document-conversion-cli.md)\n- [Model Management CLI](docling-project/docling/6.2-model-management-cli.md)\n- [Python SDK](docling-project/docling/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)\n- [DocumentExtractor API](docling-project/docling/7.2-documentextractor-api.md)\n- [Usage Examples](docling-project/docling/7.3-usage-examples.md)\n- [Output and Integration](docling-project/docling/8-output-and-integration.md)\n- [Export Formats](docling-project/docling/8.1-export-formats.md)\n- [Document Chunking](docling-project/docling/8.2-document-chunking.md)\n- [Framework Integrations](docling-project/docling/8.3-framework-integrations.md)\n- [Development and Testing](docling-project/docling/9-development-and-testing.md)\n- [Testing Framework](docling-project/docling/9.1-testing-framework.md)\n- [Ground Truth Data](docling-project/docling/9.2-ground-truth-data.md)\n- [CI/CD and Development Workflow](docling-project/docling/9.3-cicd-and-development-workflow.md)\n- [Deployment](docling-project/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)\n- [Model Artifacts Management](docling-project/docling/10.2-model-artifacts-management.md)\n\nMenu\n\n# Vision Language Models\n\nRelevant source files",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 927,
      "character_count": 3421,
      "created_at": "2025-10-16T17:42:16.979502",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 1,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [README.md](https://github.com/docling-project/docling/blob/f7244a43/README.md)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docs/examples/minimal\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py)\n- [docs/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/index.md)\n- [docs/usage/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/index.md)\n- [docs/usage/mcp.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/mcp.md)\n- [docs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)\n\nVision Language Models (VLMs) in Docling enable end-to-end document understanding by processing document page images directly through multimodal AI models. Unlike the traditional pipeline approach that uses specialized models for layout, tables, and OCR, VLMs can perform document analysis in a single inference pass, generating structured output formats like DOCTAGS, Markdown, or HTML.\n\nThis page provides an overview of VLM integration in Docling, covering available model variants, response formats, and configuration options. For detailed implementation of inline VLM models (Transformers, MLX, vLLM), see [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md). For API-based VLM integration, see [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md). For pipeline-level VLM usage, see [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md).\n\nSources: [docling/models/base\\_model.py46-66](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L66) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py13-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L13-L32) [docs/usage/vision\\_models.md1-10](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L1-L10)\n\n## VLM Integration Architecture\n\nDocling provides a unified interface for VLM integration supporting both local model execution and external API services. The architecture separates model deployment strategy from the VLM capabilities exposed to pipelines.\n\n**Diagram: VLM Integration Architecture**\n\n```\n```\n\nThe architecture provides two key abstractions:\n\n- **`BaseVlmPageModel`**: Defines the interface for page-level VLM processing, requiring implementations to provide `__call__(conv_res, page_batch)` and `process_images(image_batch, prompt)` methods\n- **`BaseVlmOptions`**: Provides configuration for VLM behavior including prompts, scaling, temperature, and response format handling",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 1006,
      "character_count": 3936,
      "created_at": "2025-10-16T17:42:16.984029",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 2,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "Sources: [docling/models/base\\_model.py46-127](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L127) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py13-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L13-L32) [docling/pipeline/vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py)\n\n## Available VLM Model Variants\n\nDocling provides pre-configured specifications for popular VLM models, optimized for document understanding tasks. These are defined in `vlm_model_specs` and can be used directly or customized.\n\n**Diagram: VLM Model Variants and Frameworks**\n\n```\n```\n\n### GraniteDocling Models\n\nGraniteDocling is a specialized 258M parameter model trained for document understanding that outputs structured DOCTAGS format. It represents the recommended choice for document conversion in Docling.\n\n| Variant                          | Repo ID                                | Framework    | Devices   | Notes                          |\n| -------------------------------- | -------------------------------------- | ------------ | --------- | ------------------------------ |\n| **GRANITEDOCLING\\_TRANSFORMERS** | `ibm-granite/granite-docling-258M`     | Transformers | CPU, CUDA | Default for non-Apple hardware |\n| **GRANITEDOCLING\\_MLX**          | `ibm-granite/granite-docling-258M-mlx` | MLX          | MPS       | Optimized for Apple Silicon    |\n| **GRANITEDOCLING\\_VLLM**         | `ibm-granite/granite-docling-258M`     | vLLM         | CUDA      | High-throughput inference      |\n\nConfiguration example:\n\n```\n```\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py21-56](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L21-L56) [docs/usage/vision\\_models.md40-87](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L40-L87)\n\n### SmolDocling Models\n\nSmolDocling is another 256M parameter model designed for document understanding with DOCTAGS output. It provides an alternative to GraniteDocling with similar capabilities.\n\n| Variant                       | Repo ID                                   | Framework    | Devices   |\n| ----------------------------- | ----------------------------------------- | ------------ | --------- |\n| **SMOLDOCLING\\_TRANSFORMERS** | `ds4sd/SmolDocling-256M-preview`          | Transformers | CPU, CUDA |\n| **SMOLDOCLING\\_MLX**          | `ds4sd/SmolDocling-256M-preview-mlx-bf16` | MLX          | MPS       |\n| **SMOLDOCLING\\_VLLM**         | `ds4sd/SmolDocling-256M-preview`          | vLLM         | CUDA      |\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py58-97](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L58-L97)\n\n### General-Purpose VLM Models\n\nDocling supports general-purpose VLMs that output Markdown or HTML, suitable for document conversion when DOCTAGS-trained models are not required.\n\n| Model                  | Primary Output | Notable Features                                |\n| ---------------------- | -------------- | ----------------------------------------------- |\n| **Granite Vision 3.2** | Markdown       | IBM's 2B vision model, multi-framework support  |\n| **Pixtral 12B**        | Markdown       | Mistral's 12B multimodal model                  |\n| **Qwen2.5-VL**         | Markdown       | 3B parameter model with strong OCR capabilities |\n| **Phi-4**              | Markdown       | Microsoft's 14B multimodal model                |\n| **GOT-OCR 2.0**        | Markdown       | Specialized OCR model with format preservation  |\n\nExample configuration for Granite Vision:\n\n```\n```\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py143-245](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L143-L245) [docs/usage/vision\\_models.md46-58](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L46-L58)\n\n### Custom Model Configuration",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 1005,
      "character_count": 4067,
      "created_at": "2025-10-16T17:42:16.990747",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 3,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "Beyond pre-configured models, custom VLMs can be integrated by specifying `InlineVlmOptions` directly:\n\n```\n```\n\nSources: [docs/usage/vision\\_models.md88-113](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L88-L113) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py54-89](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L54-L89)\n\n## Response Formats\n\nVLM models support multiple output formats optimized for different document understanding tasks. The response format determines how the VLM structures its output and how Docling processes it into a `DoclingDocument`.\n\n**Diagram: Response Format Processing**\n\n```\n```\n\n### DOCTAGS Format\n\nDOCTAGS is an XML-based structured format designed specifically for document understanding. It provides the most accurate representation of document structure and is the recommended format for document conversion.\n\nExample DOCTAGS output:\n\n```\n```\n\nModels trained for DOCTAGS output include:\n\n- GraniteDocling (all variants)\n- SmolDocling (all variants)\n\nConfiguration:\n\n```\n```\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py27-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L27-L32) [docling/datamodel/vlm\\_model\\_specs.py22-37](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L22-L37)\n\n### Markdown Format\n\nMarkdown format outputs standard Markdown syntax, suitable for general-purpose document representation. This format is widely compatible with downstream tools and libraries.\n\nExample Markdown output:\n\n```\n```\n\nModels outputting Markdown include:\n\n- Granite Vision\n- Pixtral\n- Qwen2.5-VL\n- GOT-OCR 2.0\n\nConfiguration:\n\n```\n```\n\nSources: [docling/datamodel/vlm\\_model\\_specs.py144-157](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L144-L157) [docs/usage/vision\\_models.md4-9](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L4-L9)\n\n### HTML Format\n\nHTML format outputs HTML markup, preserving semantic document structure through HTML tags. This format is useful for web-based applications and rich document viewers.\n\nConfiguration:\n\n```\n```\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py27-32](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L27-L32)\n\n### Custom Response Processing\n\nThe `decode_response()` method in `BaseVlmOptions` allows custom post-processing of VLM outputs. This enables integration with models that return structured responses requiring transformation.\n\nExample implementation:\n\n```\n```\n\nThis pattern is used internally for specialized models like OlmOcr that return JSON-structured responses.\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py20-24](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L20-L24)\n\n## VLM Configuration Options\n\nVLM behavior is controlled through configuration classes that specify model selection, inference parameters, and processing options.\n\n**Diagram: VLM Configuration Hierarchy**\n\n```\n```\n\n### Core Configuration Parameters\n\n| Parameter            | Type             | Default        | Description                                |\n| -------------------- | ---------------- | -------------- | ------------------------------------------ |\n| **prompt**           | `str`            | Model-specific | Prompt text sent to VLM                    |\n| **scale**            | `float`          | `2.0`          | Image scaling factor for higher resolution |\n| **max\\_size**        | `Optional[int]`  | `None`         | Maximum image dimension (pixels)           |\n| **temperature**      | `float`          | `0.0`          | Sampling temperature (0.0 = deterministic) |\n| **response\\_format** | `ResponseFormat` | Required       | Expected output format                     |\n\n### Inline Model Parameters",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 927,
      "character_count": 4045,
      "created_at": "2025-10-16T17:42:17.000856",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 4,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "| Parameter                | Type                      | Purpose                                      |\n| ------------------------ | ------------------------- | -------------------------------------------- |\n| **repo\\_id**             | `str`                     | HuggingFace model repository identifier      |\n| **revision**             | `str`                     | Model version/branch (default: \"main\")       |\n| **inference\\_framework** | `InferenceFramework`      | Framework selection: MLX, TRANSFORMERS, VLLM |\n| **max\\_new\\_tokens**     | `int`                     | Maximum tokens to generate (default: 4096)   |\n| **stop\\_strings**        | `List[str]`               | Strings that trigger generation stop         |\n| **supported\\_devices**   | `List[AcceleratorDevice]` | Compatible hardware devices                  |\n\n### API Model Parameters\n\n| Parameter       | Type             | Purpose                             |\n| --------------- | ---------------- | ----------------------------------- |\n| **url**         | `AnyUrl`         | API endpoint URL                    |\n| **headers**     | `Dict[str, str]` | HTTP headers (e.g., authentication) |\n| **timeout**     | `float`          | Request timeout in seconds          |\n| **concurrency** | `int`            | Number of parallel API requests     |\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py13-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L13-L112)\n\n### Generation Control\n\nVLM generation behavior can be fine-tuned through stopping criteria and generation configuration:\n\n**Stop Strings**: Simple string-based stopping\n\n```\n```\n\n**Custom Stopping Criteria**: Programmatic stopping logic\n\n```\n```\n\n**Extra Generation Config**: Framework-specific parameters\n\n```\n```\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py78-82](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L78-L82) [docling/models/utils/generation\\_utils.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/generation_utils.py)\n\n## Prompt Construction and Formatting\n\nVLM prompts are constructed through the `build_prompt()` method, which can be customized to include page-specific context or structured instructions.\n\n**Diagram: Prompt Processing Flow**\n\n```\n```\n\n### Prompt Styles\n\n| Style    | Usage                            | Example                         |\n| -------- | -------------------------------- | ------------------------------- |\n| **CHAT** | Uses model's chat template       | \\`<                             |\n| **RAW**  | Direct prompt without formatting | `Convert this page to docling.` |\n| **NONE** | No text prompt (image-only)      | `\"\"`                            |\n\n### Dynamic Prompt Construction\n\nThe `build_prompt()` method can access page metadata for context-aware prompts:\n\n```\n```\n\nSources: [docling/models/base\\_model.py85-126](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L85-L126) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py20-24](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L20-L24)\n\n## Response Formats and Processing\n\nVLM models support multiple output formats optimized for different document understanding tasks and downstream processing requirements.\n\n### Response Format Types\n\n```\n```\n\n### Custom Response Processing\n\nVLM options support custom response processing through the `decode_response()` method, enabling specialized handling for specific model outputs:\n\n```\n```\n\nThis pattern allows integration with models that return structured responses requiring post-processing before integration into the document representation.\n\nSources: [docling/datamodel/pipeline\\_options\\_vlm\\_model.py18-22](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L18-L22) [docs/examples/vlm\\_pipeline\\_api\\_model.py78-85](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/vlm_pipeline_api_model.py#L78-L85)\n\n## VLM Integration Examples\n\nThe codebase includes comprehensive examples demonstrating VLM integration patterns for different deployment scenarios and model types.\n\n### Multi-Model Comparison Framework\n\nThe `compare_vlm_models.py` example provides a systematic approach for evaluating different VLM models and frameworks:\n\n```\n```\n\nThis framework enables systematic evaluation of model performance, output quality, and resource utilization across different VLM implementations.",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 964,
      "character_count": 4622,
      "created_at": "2025-10-16T17:42:17.012278",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 5,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "Sources: [docs/examples/compare\\_vlm\\_models.py33-101](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/compare_vlm_models.py#L33-L101) [docs/examples/compare\\_vlm\\_models.py146-198](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/compare_vlm_models.py#L146-L198)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.\n\n### On this page\n\n- [Vision Language Models](#vision-language-models.md)\n- [VLM Integration Architecture](#vlm-integration-architecture.md)\n- [Available VLM Model Variants](#available-vlm-model-variants.md)\n- [GraniteDocling Models](#granitedocling-models.md)\n- [SmolDocling Models](#smoldocling-models.md)\n- [General-Purpose VLM Models](#general-purpose-vlm-models.md)\n- [Custom Model Configuration](#custom-model-configuration.md)\n- [Response Formats](#response-formats.md)\n- [DOCTAGS Format](#doctags-format.md)\n- [Markdown Format](#markdown-format.md)\n- [HTML Format](#html-format.md)\n- [Custom Response Processing](#custom-response-processing.md)\n- [VLM Configuration Options](#vlm-configuration-options.md)\n- [Core Configuration Parameters](#core-configuration-parameters.md)\n- [Inline Model Parameters](#inline-model-parameters.md)\n- [API Model Parameters](#api-model-parameters.md)\n- [Generation Control](#generation-control.md)\n- [Prompt Construction and Formatting](#prompt-construction-and-formatting.md)\n- [Prompt Styles](#prompt-styles.md)\n- [Dynamic Prompt Construction](#dynamic-prompt-construction.md)\n- [Response Formats and Processing](#response-formats-and-processing.md)\n- [Response Format Types](#response-format-types.md)\n- [Custom Response Processing](#custom-response-processing-1.md)\n- [VLM Integration Examples](#vlm-integration-examples.md)\n- [Multi-Model Comparison Framework](#multi-model-comparison-framework.md)",
    "metadata": {
      "chunk_id": 6,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 465,
      "character_count": 1852,
      "created_at": "2025-10-16T17:42:17.012844",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 6,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3-vision-language-models.md",
      "collection_context": "Docling"
    }
  }
]