[
  {
    "text": "API-Based VLM Models | docling-project/docling | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[docling-project/docling](https://github.com/docling-project/docling \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 12 October 2025 ([f7244a](https://github.com/docling-project/docling/commits/f7244a43))",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 117,
      "character_count": 421,
      "created_at": "2025-10-16T17:42:17.061547",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 0,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [Overview](docling-project/docling/1-overview.md)\n- [Installation](docling-project/docling/1.1-installation.md)\n- [Quick Start](docling-project/docling/1.2-quick-start.md)\n- [Core Architecture](docling-project/docling/2-core-architecture.md)\n- [Document Conversion Flow](docling-project/docling/2.1-document-conversion-flow.md)\n- [DoclingDocument Data Model](docling-project/docling/2.2-doclingdocument-data-model.md)\n- [Configuration and Pipeline Options](docling-project/docling/2.3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.4-format-detection-and-routing.md)\n- [Document Backends](docling-project/docling/3-document-backends.md)\n- [PDF Processing Backends](docling-project/docling/3.1-pdf-processing-backends.md)\n- [Office Document Backends](docling-project/docling/3.2-office-document-backends.md)\n- [Web and Markup Backends](docling-project/docling/3.3-web-and-markup-backends.md)\n- [AI/ML Models](docling-project/docling/4-aiml-models.md)\n- [OCR Models](docling-project/docling/4.1-ocr-models.md)\n- [Layout and Table Structure Models](docling-project/docling/4.2-layout-and-table-structure-models.md)\n- [Vision Language Models](docling-project/docling/4.3-vision-language-models.md)\n- [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md)\n- [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)\n- [Processing Pipelines](docling-project/docling/5-processing-pipelines.md)\n- [Standard PDF Pipeline](docling-project/docling/5.1-standard-pdf-pipeline.md)\n- [Threaded PDF Pipeline](docling-project/docling/5.2-threaded-pdf-pipeline.md)\n- [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md)\n- [Extraction Pipeline](docling-project/docling/5.4-extraction-pipeline.md)\n- [ASR Pipeline](docling-project/docling/5.5-asr-pipeline.md)\n- [Base Pipeline Architecture](docling-project/docling/5.6-base-pipeline-architecture.md)\n- [Command Line Interface](docling-project/docling/6-command-line-interface.md)\n- [Document Conversion CLI](docling-project/docling/6.1-document-conversion-cli.md)\n- [Model Management CLI](docling-project/docling/6.2-model-management-cli.md)\n- [Python SDK](docling-project/docling/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)\n- [DocumentExtractor API](docling-project/docling/7.2-documentextractor-api.md)\n- [Usage Examples](docling-project/docling/7.3-usage-examples.md)\n- [Output and Integration](docling-project/docling/8-output-and-integration.md)\n- [Export Formats](docling-project/docling/8.1-export-formats.md)\n- [Document Chunking](docling-project/docling/8.2-document-chunking.md)\n- [Framework Integrations](docling-project/docling/8.3-framework-integrations.md)\n- [Development and Testing](docling-project/docling/9-development-and-testing.md)\n- [Testing Framework](docling-project/docling/9.1-testing-framework.md)\n- [Ground Truth Data](docling-project/docling/9.2-ground-truth-data.md)\n- [CI/CD and Development Workflow](docling-project/docling/9.3-cicd-and-development-workflow.md)\n- [Deployment](docling-project/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)\n- [Model Artifacts Management](docling-project/docling/10.2-model-artifacts-management.md)\n\nMenu\n\n# API-Based VLM Models\n\nRelevant source files",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 929,
      "character_count": 3419,
      "created_at": "2025-10-16T17:42:17.063790",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 1,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [docling/datamodel/extraction.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/extraction.py)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/document\\_extractor.py](https://github.com/docling-project/docling/blob/f7244a43/docling/document_extractor.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/nuextract\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/nuextract_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docling/pipeline/asr\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/asr_pipeline.py)\n- [docling/pipeline/base\\_extraction\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_extraction_pipeline.py)\n- [docling/pipeline/base\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_pipeline.py)\n- [docling/pipeline/extraction\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/extraction_vlm_pipeline.py)\n- [docling/pipeline/simple\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/simple_pipeline.py)\n- [docling/pipeline/threaded\\_standard\\_pdf\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/threaded_standard_pdf_pipeline.py)\n- [docling/pipeline/vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py)\n\n## Purpose and Scope\n\nThis page documents the API-based Vision Language Model (VLM) integration in Docling, which enables document processing using external VLM services via OpenAI-compatible HTTP APIs. API-based models connect to remote inference servers (e.g., Ollama, vLLM server, OpenAI) rather than loading models locally.\n\nFor locally-executed VLM models using Transformers, MLX, or vLLM frameworks, see [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md). For the broader VLM system architecture and pipeline integration, see [Vision Language Models](docling-project/docling/4.3-vision-language-models.md).\n\nAPI-based models are configured through `ApiVlmOptions` and executed by the `ApiVlmModel` class, which provides threaded request handling, streaming support, and early-abort capabilities through custom stopping criteria.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py1-102](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L1-L102) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py96-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L96-L112)\n\n---\n\n## System Architecture\n\n```\n```\n\n**Diagram: API-Based VLM Model Architecture**",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 1012,
      "character_count": 3904,
      "created_at": "2025-10-16T17:42:17.067683",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 2,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "The architecture separates configuration (`ApiVlmOptions`), execution (`ApiVlmModel`), and HTTP communication (`api_image_request` functions). The `ThreadPoolExecutor` enables concurrent processing of page batches, while the streaming path supports early termination via `GenerationStopper` instances.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py19-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L19-L101) [docling/pipeline/vlm\\_pipeline.py66-73](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py#L66-L73)\n\n---\n\n## Configuration: ApiVlmOptions\n\nThe `ApiVlmOptions` class defines all parameters for connecting to and configuring an external VLM API:\n\n| Field                      | Type                           | Default                                      | Description                                      |\n| -------------------------- | ------------------------------ | -------------------------------------------- | ------------------------------------------------ |\n| `kind`                     | `Literal[\"api_model_options\"]` | `\"api_model_options\"`                        | Discriminator for option type                    |\n| `url`                      | `AnyUrl`                       | `http://localhost:11434/v1/chat/completions` | API endpoint URL (OpenAI-compatible)             |\n| `headers`                  | `Dict[str, str]`               | `{}`                                         | HTTP headers (e.g., authorization)               |\n| `params`                   | `Dict[str, Any]`               | `{}`                                         | Model-specific parameters (e.g., `model` name)   |\n| `timeout`                  | `float`                        | `60`                                         | Request timeout in seconds                       |\n| `concurrency`              | `int`                          | `1`                                          | Number of concurrent page requests               |\n| `response_format`          | `ResponseFormat`               | —                                            | Expected response format (DOCTAGS/Markdown/HTML) |\n| `prompt`                   | `str`                          | —                                            | User prompt template                             |\n| `scale`                    | `float`                        | `2.0`                                        | Image scaling factor                             |\n| `max_size`                 | `Optional[int]`                | `None`                                       | Maximum image dimension                          |\n| `temperature`              | `float`                        | `0.0`                                        | Generation temperature                           |\n| `stop_strings`             | `List[str]`                    | `[]`                                         | Stop string tokens                               |\n| `custom_stopping_criteria` | `List[GenerationStopper]`      | `[]`                                         | Early-abort logic instances                      |\n\n**Key Configuration Patterns:**\n\n```\n```\n\nThe `url` must point to an OpenAI-compatible `/v1/chat/completions` endpoint. The `params` dict is merged with runtime temperature settings and passed as the request body. The `concurrency` parameter controls the `ThreadPoolExecutor` worker count.\n\n**Sources:** [docling/datamodel/pipeline\\_options\\_vlm\\_model.py96-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L96-L112) [docling/datamodel/vlm\\_model\\_specs.py171-179](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L171-L179)\n\n---\n\n## Request Flow Sequence\n\n```\n```\n\n**Diagram: API VLM Request Flow**\n\nThe `ApiVlmModel.__call__` method uses `ThreadPoolExecutor.map` to process pages concurrently. Each worker thread executes `_vlm_request`, which retrieves the page image, formats the prompt, and makes an HTTP request. If custom stopping criteria are configured, the streaming path (`api_image_request_streaming`) is used to enable early termination.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py43-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L43-L101)\n\n---\n\n## ApiVlmModel Implementation\n\n### Class Structure\n\nThe `ApiVlmModel` class implements `BasePageModel` and orchestrates API-based inference:\n\n```\n```\n\n**Initialization Validation:**\n\nThe constructor enforces the `enable_remote_services` flag to prevent accidental external connections:\n\n```\n```\n\nThis safety check requires explicit opt-in at the pipeline level before API requests are allowed.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py20-41](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L20-L41)\n\n### Request Execution Pattern\n\nThe `_vlm_request` helper function processes a single page:",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 994,
      "character_count": 5028,
      "created_at": "2025-10-16T17:42:17.078923",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 3,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "1. **Validation:** Check `page._backend.is_valid()`\n2. **Image Extraction:** Call `page.get_image(scale, max_size)` and convert to RGB\n3. **Prompt Construction:** Use `vlm_options.build_prompt(page.parsed_page)`\n4. **Stopping Criteria Processing:** Instantiate any `GenerationStopper` classes\n5. **API Call:** Route to streaming or non-streaming based on `custom_stopping_criteria`\n6. **Response Decoding:** Apply `vlm_options.decode_response()`\n7. **Result Attachment:** Set `page.predictions.vlm_response`\n\n**Concurrency Control:**\n\n```\n```\n\nThe executor processes up to `concurrency` pages in parallel, with each thread making independent HTTP requests. This is essential for throughput when processing large documents.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py43-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L43-L101)\n\n---\n\n## Streaming and Early Abort\n\n### Streaming Request Flow\n\nWhen `custom_stopping_criteria` is non-empty, the model uses the streaming API path:\n\n```\n```\n\n### GenerationStopper Interface\n\nThe `GenerationStopper` protocol enables custom early-abort logic:\n\n```\n```\n\nStreaming requests check `should_stop()` after each token chunk arrives. This allows stopping generation when:\n\n- A specific pattern is detected (e.g., closing XML tag)\n- A confidence threshold is crossed\n- A maximum content length is reached\n\n**Sources:** [docling/models/api\\_vlm\\_model.py63-97](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L63-L97)\n\n---\n\n## Pipeline Integration\n\n### VlmPipeline Instantiation\n\nThe `VlmPipeline` detects `ApiVlmOptions` and instantiates `ApiVlmModel`:\n\n```\n```\n\nThis is the sole model in the `build_pipe` list, as API-based inference is end-to-end (no separate OCR, layout, or table models).\n\n**Sources:** [docling/pipeline/vlm\\_pipeline.py66-73](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/vlm_pipeline.py#L66-L73)\n\n### Page Processing\n\nThe pipeline's `initialize_page` method loads page backends, then `_apply_on_pages` iterates the `build_pipe`:\n\n```\n```\n\nFor `ApiVlmModel`, the `__call__` method internally uses the thread pool, so the outer iteration is straightforward. The model modifies `page.predictions.vlm_response` in-place and yields the updated pages.\n\n**Sources:** [docling/pipeline/base\\_pipeline.py189-195](https://github.com/docling-project/docling/blob/f7244a43/docling/pipeline/base_pipeline.py#L189-L195)\n\n---\n\n## Predefined API Configurations\n\nThe `docling/datamodel/vlm_model_specs.py` module provides ready-to-use configurations:\n\n### GRANITE\\_VISION\\_OLLAMA\n\n```\n```\n\nThis configuration targets a local Ollama server running the Granite Vision model. The `scale=1.0` uses original image resolution, and `timeout=120` allows longer processing for complex pages.\n\n**Usage Pattern:**\n\n```\n```\n\n**Sources:** [docling/datamodel/vlm\\_model\\_specs.py171-179](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L171-L179)\n\n---\n\n## API Request Format\n\n### OpenAI Chat Completions Schema\n\nThe API client sends requests to the `/v1/chat/completions` endpoint using the OpenAI-compatible schema:\n\n```\n```\n\nThe image is base64-encoded and included as a data URL. Additional parameters from `ApiVlmOptions.params` are merged into the request body.\n\n### Response Parsing\n\n**Non-Streaming Response:**\n\n```\n```\n\nThe `api_image_request` function extracts `choices[0].message.content`.\n\n**Streaming Response:**\n\nServer-Sent Events (SSE) format:\n\n```\ndata: {\"choices\": [{\"delta\": {\"content\": \"# \"}}]}\n\ndata: {\"choices\": [{\"delta\": {\"content\": \"Document\"}}]}\n\ndata: [DONE]\n```\n\nThe `api_image_request_streaming` function accumulates chunks until a stopper triggers or the stream completes.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py76-97](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L76-L97)\n\n---\n\n## Error Handling and Timeout\n\n### Request-Level Timeouts\n\nEach API request respects the `timeout` parameter:\n\n```\n```",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 998,
      "character_count": 4071,
      "created_at": "2025-10-16T17:42:17.096352",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 4,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "If the server doesn't respond within `timeout` seconds, the request raises a timeout exception, which is caught by the pipeline's error handling.\n\n### Backend Validation\n\nBefore making API requests, the model validates the page backend:\n\n```\n```\n\nInvalid pages (e.g., corrupted PDFs) are returned unchanged, preventing unnecessary API calls.\n\n### Remote Services Flag\n\nThe `enable_remote_services` flag provides a safety gate:\n\n```\n```\n\nThis prevents accidental API calls in environments where external connections are forbidden or should be audited.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py28-49](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L28-L49)\n\n---\n\n## Comparison with Inline VLM Models\n\n| Aspect                | API-Based Models                        | Inline Models                                                            |\n| --------------------- | --------------------------------------- | ------------------------------------------------------------------------ |\n| **Execution**         | Remote HTTP API                         | Local model loading (Transformers/MLX/vLLM)                              |\n| **Configuration**     | `ApiVlmOptions`                         | `InlineVlmOptions`                                                       |\n| **Model Class**       | `ApiVlmModel`                           | `HuggingFaceTransformersVlmModel`, `HuggingFaceMlxModel`, `VllmVlmModel` |\n| **Dependencies**      | HTTP client only                        | `transformers`, `torch`, `mlx`, `vllm`                                   |\n| **Concurrency**       | `ThreadPoolExecutor` (I/O bound)        | Model batching (compute bound)                                           |\n| **Device**            | N/A (server-side)                       | CPU/CUDA/MPS                                                             |\n| **Artifacts**         | None (server manages)                   | Downloaded to `artifacts_path`                                           |\n| **Stopping Criteria** | `GenerationStopper` (streaming only)    | `StoppingCriteria` + `GenerationStopper`                                 |\n| **Use Case**          | Distributed inference, limited hardware | Local control, offline operation                                         |\n\n**Sources:** [docling/models/api\\_vlm\\_model.py19-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L19-L101) [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L376)\n\n---\n\n## Example: Custom Stopping Criteria\n\n### Implementing a GenerationStopper\n\n```\n```\n\n### Configuration with Stopper\n\n```\n```\n\nWhen configured, the streaming API path is automatically selected, and generation terminates as soon as `</doctag>` appears in the output, saving tokens and reducing latency.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py63-74](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L63-L74) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py110-112](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L110-L112)\n\n---\n\n## Performance Considerations\n\n### Concurrency Tuning\n\nThe `concurrency` parameter controls parallel requests:\n\n- **Low concurrency (1-2):** Sequential processing, minimal server load\n- **Medium concurrency (4-8):** Balanced throughput for typical documents\n- **High concurrency (16+):** Maximum speed for large batches, requires server capacity\n\nOptimal settings depend on:\n\n1. Server capacity (GPU count, batch size)\n2. Network latency and bandwidth\n3. Document complexity (larger images = longer inference)\n\n### Timeout Configuration\n\nAppropriate timeout values vary by model and document type:\n\n- **Simple text extraction:** 30-60 seconds\n- **Complex documents (tables, figures):** 120-300 seconds\n- **Large images (high resolution):** 300+ seconds\n\nInsufficient timeouts cause false failures; excessive timeouts delay error detection.\n\n**Sources:** [docling/models/api\\_vlm\\_model.py36-101](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py#L36-L101)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.\n\n### On this page",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 942,
      "character_count": 4425,
      "created_at": "2025-10-16T17:42:17.108985",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 5,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [API-Based VLM Models](#api-based-vlm-models.md)\n- [Purpose and Scope](#purpose-and-scope.md)\n- [System Architecture](#system-architecture.md)\n- [Configuration: ApiVlmOptions](#configuration-apivlmoptions.md)\n- [Request Flow Sequence](#request-flow-sequence.md)\n- [ApiVlmModel Implementation](#apivlmmodel-implementation.md)\n- [Class Structure](#class-structure.md)\n- [Request Execution Pattern](#request-execution-pattern.md)\n- [Streaming and Early Abort](#streaming-and-early-abort.md)\n- [Streaming Request Flow](#streaming-request-flow.md)\n- [GenerationStopper Interface](#generationstopper-interface.md)\n- [Pipeline Integration](#pipeline-integration.md)\n- [VlmPipeline Instantiation](#vlmpipeline-instantiation.md)\n- [Page Processing](#page-processing.md)\n- [Predefined API Configurations](#predefined-api-configurations.md)\n- [GRANITE\\_VISION\\_OLLAMA](#granite_vision_ollama.md)\n- [API Request Format](#api-request-format.md)\n- [OpenAI Chat Completions Schema](#openai-chat-completions-schema.md)\n- [Response Parsing](#response-parsing.md)\n- [Error Handling and Timeout](#error-handling-and-timeout.md)\n- [Request-Level Timeouts](#request-level-timeouts.md)\n- [Backend Validation](#backend-validation.md)\n- [Remote Services Flag](#remote-services-flag.md)\n- [Comparison with Inline VLM Models](#comparison-with-inline-vlm-models.md)\n- [Example: Custom Stopping Criteria](#example-custom-stopping-criteria.md)\n- [Implementing a GenerationStopper](#implementing-a-generationstopper.md)\n- [Configuration with Stopper](#configuration-with-stopper.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Concurrency Tuning](#concurrency-tuning.md)\n- [Timeout Configuration](#timeout-configuration.md)",
    "metadata": {
      "chunk_id": 6,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 432,
      "character_count": 1717,
      "created_at": "2025-10-16T17:42:17.109212",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 6,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.2-api-based-vlm-models.md",
      "collection_context": "Docling"
    }
  }
]