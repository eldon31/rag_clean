[
  {
    "text": "Inline VLM Models | docling-project/docling | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[docling-project/docling](https://github.com/docling-project/docling \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 12 October 2025 ([f7244a](https://github.com/docling-project/docling/commits/f7244a43))",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 116,
      "character_count": 418,
      "created_at": "2025-10-16T17:42:17.016027",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 0,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [Overview](docling-project/docling/1-overview.md)\n- [Installation](docling-project/docling/1.1-installation.md)\n- [Quick Start](docling-project/docling/1.2-quick-start.md)\n- [Core Architecture](docling-project/docling/2-core-architecture.md)\n- [Document Conversion Flow](docling-project/docling/2.1-document-conversion-flow.md)\n- [DoclingDocument Data Model](docling-project/docling/2.2-doclingdocument-data-model.md)\n- [Configuration and Pipeline Options](docling-project/docling/2.3-configuration-and-pipeline-options.md)\n- [Format Detection and Routing](docling-project/docling/2.4-format-detection-and-routing.md)\n- [Document Backends](docling-project/docling/3-document-backends.md)\n- [PDF Processing Backends](docling-project/docling/3.1-pdf-processing-backends.md)\n- [Office Document Backends](docling-project/docling/3.2-office-document-backends.md)\n- [Web and Markup Backends](docling-project/docling/3.3-web-and-markup-backends.md)\n- [AI/ML Models](docling-project/docling/4-aiml-models.md)\n- [OCR Models](docling-project/docling/4.1-ocr-models.md)\n- [Layout and Table Structure Models](docling-project/docling/4.2-layout-and-table-structure-models.md)\n- [Vision Language Models](docling-project/docling/4.3-vision-language-models.md)\n- [Inline VLM Models](docling-project/docling/4.3.1-inline-vlm-models.md)\n- [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md)\n- [Enrichment Models](docling-project/docling/4.4-enrichment-models.md)\n- [Processing Pipelines](docling-project/docling/5-processing-pipelines.md)\n- [Standard PDF Pipeline](docling-project/docling/5.1-standard-pdf-pipeline.md)\n- [Threaded PDF Pipeline](docling-project/docling/5.2-threaded-pdf-pipeline.md)\n- [VLM Pipeline](docling-project/docling/5.3-vlm-pipeline.md)\n- [Extraction Pipeline](docling-project/docling/5.4-extraction-pipeline.md)\n- [ASR Pipeline](docling-project/docling/5.5-asr-pipeline.md)\n- [Base Pipeline Architecture](docling-project/docling/5.6-base-pipeline-architecture.md)\n- [Command Line Interface](docling-project/docling/6-command-line-interface.md)\n- [Document Conversion CLI](docling-project/docling/6.1-document-conversion-cli.md)\n- [Model Management CLI](docling-project/docling/6.2-model-management-cli.md)\n- [Python SDK](docling-project/docling/7-python-sdk.md)\n- [DocumentConverter API](docling-project/docling/7.1-documentconverter-api.md)\n- [DocumentExtractor API](docling-project/docling/7.2-documentextractor-api.md)\n- [Usage Examples](docling-project/docling/7.3-usage-examples.md)\n- [Output and Integration](docling-project/docling/8-output-and-integration.md)\n- [Export Formats](docling-project/docling/8.1-export-formats.md)\n- [Document Chunking](docling-project/docling/8.2-document-chunking.md)\n- [Framework Integrations](docling-project/docling/8.3-framework-integrations.md)\n- [Development and Testing](docling-project/docling/9-development-and-testing.md)\n- [Testing Framework](docling-project/docling/9.1-testing-framework.md)\n- [Ground Truth Data](docling-project/docling/9.2-ground-truth-data.md)\n- [CI/CD and Development Workflow](docling-project/docling/9.3-cicd-and-development-workflow.md)\n- [Deployment](docling-project/docling/10-deployment.md)\n- [Docker Deployment](docling-project/docling/10.1-docker-deployment.md)\n- [Model Artifacts Management](docling-project/docling/10.2-model-artifacts-management.md)\n\nMenu\n\n# Inline VLM Models\n\nRelevant source files",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 928,
      "character_count": 3416,
      "created_at": "2025-10-16T17:42:17.017918",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 1,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [README.md](https://github.com/docling-project/docling/blob/f7244a43/README.md)\n- [docling/datamodel/pipeline\\_options\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py)\n- [docling/datamodel/vlm\\_model\\_specs.py](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py)\n- [docling/models/api\\_vlm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/api_vlm_model.py)\n- [docling/models/base\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py)\n- [docling/models/utils/hf\\_model\\_download.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py)\n- [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py)\n- [docling/models/vlm\\_models\\_inline/mlx\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py)\n- [docling/models/vlm\\_models\\_inline/vllm\\_model.py](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py)\n- [docs/examples/minimal\\_vlm\\_pipeline.py](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py)\n- [docs/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/index.md)\n- [docs/usage/index.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/index.md)\n- [docs/usage/mcp.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/mcp.md)\n- [docs/usage/vision\\_models.md](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md)\n- [mkdocs.yml](https://github.com/docling-project/docling/blob/f7244a43/mkdocs.yml)\n\nThis page documents the inline Vision Language Model (VLM) implementations in Docling. Inline VLM models run locally on the host machine, in contrast to API-based VLM models that connect to remote services. Three inference frameworks are supported: Hugging Face Transformers, MLX (for Apple Silicon acceleration), and vLLM (for optimized GPU inference).\n\nFor information about API-based VLM models that connect to remote services like Ollama or vLLM servers, see [API-Based VLM Models](docling-project/docling/4.3.2-api-based-vlm-models.md). For general VLM integration concepts and configuration options, see [Vision Language Models](docling-project/docling/4.3-vision-language-models.md).\n\n## Architecture Overview\n\nThe inline VLM model system provides three specialized implementations sharing a common interface:\n\n```\n```\n\n**Sources:** [docling/models/base\\_model.py46-127](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L46-L127) [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L376) [docling/models/vlm\\_models\\_inline/mlx\\_model.py33-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L33-L318) [docling/models/vlm\\_models\\_inline/vllm\\_model.py25-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L25-L301)\n\n## Configuration via InlineVlmOptions\n\nAll inline VLM models are configured through `InlineVlmOptions`, which specifies the model repository, inference framework, and generation parameters:",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 949,
      "character_count": 3611,
      "created_at": "2025-10-16T17:42:17.021094",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 2,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "| Parameter                   | Type                                               | Description                                                                                                |\n| --------------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| `repo_id`                   | `str`                                              | Hugging Face repository identifier (e.g., `\"ibm-granite/granite-docling-258M\"`)                            |\n| `inference_framework`       | `InferenceFramework`                               | One of `TRANSFORMERS`, `MLX`, or `VLLM`                                                                    |\n| `transformers_model_type`   | `TransformersModelType`                            | Auto-loading class: `AUTOMODEL`, `AUTOMODEL_VISION2SEQ`, `AUTOMODEL_CAUSALLM`, `AUTOMODEL_IMAGETEXTTOTEXT` |\n| `transformers_prompt_style` | `TransformersPromptStyle`                          | Prompt formatting: `CHAT`, `RAW`, or `NONE`                                                                |\n| `response_format`           | `ResponseFormat`                                   | Expected output format: `DOCTAGS`, `MARKDOWN`, `HTML`, `OTSL`, or `PLAINTEXT`                              |\n| `torch_dtype`               | `Optional[str]`                                    | PyTorch dtype (e.g., `\"bfloat16\"`)                                                                         |\n| `max_new_tokens`            | `int`                                              | Maximum tokens to generate (default: `4096`)                                                               |\n| `temperature`               | `float`                                            | Sampling temperature (default: `0.0` for greedy)                                                           |\n| `scale`                     | `float`                                            | Image scaling factor (default: `2.0`)                                                                      |\n| `max_size`                  | `Optional[int]`                                    | Maximum image dimension                                                                                    |\n| `use_kv_cache`              | `bool`                                             | Enable key-value caching (default: `True`)                                                                 |\n| `stop_strings`              | `List[str]`                                        | Strings that trigger generation stop                                                                       |\n| `custom_stopping_criteria`  | `List[Union[StoppingCriteria, GenerationStopper]]` | Custom stopping logic                                                                                      |\n| `extra_generation_config`   | `Dict[str, Any]`                                   | Additional framework-specific generation parameters                                                        |\n| `extra_processor_kwargs`    | `Dict[str, Any]`                                   | Additional processor parameters                                                                            |\n| `quantized`                 | `bool`                                             | Enable quantization (default: `False`)                                                                     |\n| `load_in_8bit`              | `bool`                                             | Use 8-bit quantization (default: `True`)                                                                   |\n| `trust_remote_code`         | `bool`                                             | Allow remote code execution (default: `False`)                                                             |\n| `revision`                  | `str`                                              | Model revision/branch (default: `\"main\"`)                                                                  |\n\n**Sources:** [docling/datamodel/pipeline\\_options\\_vlm\\_model.py54-89](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L54-L89)\n\n## Hugging Face Transformers Implementation\n\n### Model Loading and Initialization\n\n`HuggingFaceTransformersVlmModel` loads models using Transformers' auto-loading classes:\n\n```\n```\n\nThe model class is selected based on `transformers_model_type`:\n\n- `AUTOMODEL` → `AutoModel`\n- `AUTOMODEL_CAUSALLM` → `AutoModelForCausalLM`\n- `AUTOMODEL_VISION2SEQ` → `AutoModelForVision2Seq`\n- `AUTOMODEL_IMAGETEXTTOTEXT` → `AutoModelForImageTextToText`\n\nThe processor's tokenizer padding is configured with `padding_side = \"left\"` for batch processing.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py36-138](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L36-L138)\n\n### Batch Inference Pipeline\n\nThe Transformers implementation processes images in batches:\n\n```\n```\n\n**Key Implementation Details:**",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 852,
      "character_count": 5140,
      "created_at": "2025-10-16T17:42:17.028337",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 3,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "1. **Image Normalization** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py209-224](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L209-L224): Converts numpy arrays to PIL RGB images\n2. **Prompt Handling** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py229-236](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L229-L236): Accepts single prompt string or list of prompts (one per image)\n3. **Processor Integration** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py240-256](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L240-L256): Handles both text and image preprocessing with automatic padding\n4. **Stopping Criteria** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L296): Supports `StopStringCriteria` and custom `GenerationStopper` instances\n5. **Token Trimming** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py343-344](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L343-L344): Removes input tokens from output sequences using attention mask\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py139-376](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L139-L376)\n\n### Stopping Criteria Handling\n\nThe Transformers implementation supports two types of stopping criteria:\n\n```\n```\n\nThe implementation distinguishes between:\n\n- **String-based stopping** via `StopStringCriteria` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py264-269](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L264-L269)\n- **GenerationStopper classes/instances** wrapped in `HFStoppingCriteriaWrapper` [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py276-283](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L276-L283)\n- **Native StoppingCriteria classes** instantiated with tokenizer [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py284-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L284-L287)\n- **StoppingCriteria instances** used directly [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py294-296](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L294-L296)\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py260-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L260-L302)\n\n## MLX Implementation (Apple Silicon)\n\n### Architecture and Thread Safety\n\n`HuggingFaceMlxModel` uses the MLX framework for Apple Silicon acceleration with important thread safety considerations:\n\n```\n```\n\n**Critical Constraint:** MLX models are **not thread-safe**. All MLX inference operations are serialized using a global lock [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-30](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L30):\n\n```\n```\n\nThis means only one MLX model instance can perform inference at a time across the entire process.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py28-90](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L28-L90)\n\n### Streaming Generation and Token Collection\n\nUnlike the Transformers implementation, MLX uses streaming generation:\n\n```\n```\n\n**Key Characteristics:**",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 1005,
      "character_count": 3980,
      "created_at": "2025-10-16T17:42:17.036110",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 4,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "1. **No Batching** [docling/models/vlm\\_models\\_inline/mlx\\_model.py186-188](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L186-L188): Images are processed sequentially within the global lock\n2. **Token-Level Collection** [doclog/models/vlm\\_models\\_inline/mlx\\_model.py232-254](https://github.com/docling-project/docling/blob/f7244a43/doclog/models/vlm_models_inline/mlx_model.py#L232-L254): Each token includes text, token ID, and log probability\n3. **Early Stopping** [docling/models/vlm\\_models\\_inline/mlx\\_model.py258-302](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L258-L302): Stop strings and `GenerationStopper` instances are checked during streaming\n4. **Lookback Window** [docling/models/vlm\\_models\\_inline/mlx\\_model.py279-287](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L279-L287): Custom stoppers can specify how many recent characters to examine\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py149-318](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L149-L318)\n\n### Stopping Criteria Validation\n\nMLX enforces strict stopping criteria types [docling/models/vlm\\_models\\_inline/mlx\\_model.py75-89](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L75-L89):\n\n| Allowed                       | Not Allowed                  |\n| ----------------------------- | ---------------------------- |\n| `GenerationStopper` instances | `StoppingCriteria` instances |\n| `GenerationStopper` classes   | `StoppingCriteria` classes   |\n| Stop strings                  | -                            |\n\nIf Hugging Face `StoppingCriteria` is detected, a `ValueError` is raised with a clear message explaining that only `GenerationStopper` is supported for MLX.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/mlx\\_model.py75-89](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/mlx_model.py#L75-L89)\n\n## vLLM Implementation\n\n### Configuration and Initialization\n\n`VllmVlmModel` provides GPU-optimized inference with strict separation of load-time and runtime parameters:\n\n```\n```\n\n**Parameter Allowlists:**\n\nThe implementation maintains two explicit allowlists [docling/models/vlm\\_models\\_inline/vllm\\_model.py32-80](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L32-L80):\n\n1. **`_VLLM_ENGINE_KEYS`** - Parameters passed to `LLM.__init__()` (load time)\n2. **`_VLLM_SAMPLING_KEYS`** - Parameters passed to `SamplingParams` (runtime)\n\nAny keys in `extra_generation_config` not in either allowlist trigger a warning and are ignored.\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py82-174](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L82-L174)\n\n### Batch Inference with Multi-Modal Data\n\nvLLM processes images as multi-modal data in batch mode:\n\n```\n```\n\n**Key Features:**",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 803,
      "character_count": 3142,
      "created_at": "2025-10-16T17:42:17.042595",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 5,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "1. **True Batching** [docling/models/vlm\\_models\\_inline/vllm\\_model.py233-300](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L233-L300): vLLM processes all images in a single `generate()` call\n2. **Multi-Modal Data Format** [docling/models/vlm\\_models\\_inline/vllm\\_model.py277-280](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L277-L280): Images are passed via `multi_modal_data` dictionary with `\"image\"` key\n3. **Memory Limit** [docling/models/vlm\\_models\\_inline/vllm\\_model.py140](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L140-L140): `limit_mm_per_prompt={\"image\": 1}` restricts one image per prompt\n4. **GPU Memory Management** [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-151](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L151): Defaults to 30% GPU memory utilization to share with other models\n\n**Sources:** [docling/models/vlm\\_models\\_inline/vllm\\_model.py175-301](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L175-L301)\n\n## Prompt Formatting\n\nAll inline VLM models share the `formulate_prompt()` method from `BaseVlmPageModel`:\n\n```\n```\n\n**Prompt Style Options:**\n\n| Style          | Behavior                               | Use Case                                            |\n| -------------- | -------------------------------------- | --------------------------------------------------- |\n| `RAW`          | Returns user prompt unchanged          | Models that handle formatting internally            |\n| `NONE`         | Returns empty string                   | Models that don't need text prompts (e.g., GOT-OCR) |\n| `CHAT`         | Applies processor's chat template      | Standard instruction-following models               |\n| Custom (Phi-4) | Special formatting for specific models | Model-specific requirements                         |\n\n**Sources:** [docling/models/base\\_model.py85-126](https://github.com/docling-project/docling/blob/f7244a43/docling/models/base_model.py#L85-L126)\n\n## Model Download and Caching\n\nAll inline VLM implementations inherit from `HuggingFaceModelDownloadMixin`:\n\n```\n```\n\nThe `repo_cache_folder` property converts slashes in `repo_id` to dashes (e.g., `\"ibm-granite/granite-docling-258M\"` → `\"ibm-granite--granite-docling-258M\"`).\n\n**Sources:** [docling/models/utils/hf\\_model\\_download.py8-45](https://github.com/docling-project/docling/blob/f7244a43/docling/models/utils/hf_model_download.py#L8-L45) [docling/datamodel/pipeline\\_options\\_vlm\\_model.py86-88](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/pipeline_options_vlm_model.py#L86-L88)\n\n## Available Model Specifications\n\nDocling provides pre-configured model specifications in `vlm_model_specs`:\n\n### DocTags Output Models",
    "metadata": {
      "chunk_id": 6,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 746,
      "character_count": 2979,
      "created_at": "2025-10-16T17:42:17.047633",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 6,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "| Model Spec                    | Repository                                | Framework    | Devices   | Response Format |\n| ----------------------------- | ----------------------------------------- | ------------ | --------- | --------------- |\n| `GRANITEDOCLING_TRANSFORMERS` | `ibm-granite/granite-docling-258M`        | Transformers | CPU, CUDA | DOCTAGS         |\n| `GRANITEDOCLING_VLLM`         | `ibm-granite/granite-docling-258M`        | vLLM         | CUDA      | DOCTAGS         |\n| `GRANITEDOCLING_MLX`          | `ibm-granite/granite-docling-258M-mlx`    | MLX          | MPS       | DOCTAGS         |\n| `SMOLDOCLING_TRANSFORMERS`    | `ds4sd/SmolDocling-256M-preview`          | Transformers | CPU, CUDA | DOCTAGS         |\n| `SMOLDOCLING_VLLM`            | `ds4sd/SmolDocling-256M-preview`          | vLLM         | CUDA      | DOCTAGS         |\n| `SMOLDOCLING_MLX`             | `ds4sd/SmolDocling-256M-preview-mlx-bf16` | MLX          | MPS       | DOCTAGS         |\n\n### Markdown Output Models\n\n| Model Spec                    | Repository                                  | Framework    | Devices        | Response Format |\n| ----------------------------- | ------------------------------------------- | ------------ | -------------- | --------------- |\n| `GRANITE_VISION_TRANSFORMERS` | `ibm-granite/granite-vision-3.2-2b`         | Transformers | CPU, CUDA, MPS | MARKDOWN        |\n| `GRANITE_VISION_VLLM`         | `ibm-granite/granite-vision-3.2-2b`         | vLLM         | CUDA           | MARKDOWN        |\n| `PIXTRAL_12B_TRANSFORMERS`    | `mistral-community/pixtral-12b`             | Transformers | CPU, CUDA      | MARKDOWN        |\n| `PIXTRAL_12B_MLX`             | `mlx-community/pixtral-12b-bf16`            | MLX          | MPS            | MARKDOWN        |\n| `PHI4_TRANSFORMERS`           | `microsoft/Phi-4-multimodal-instruct`       | Transformers | CPU, CUDA      | MARKDOWN        |\n| `QWEN25_VL_3B_MLX`            | `mlx-community/Qwen2.5-VL-3B-Instruct-bf16` | MLX          | MPS            | MARKDOWN        |\n| `GOT2_TRANSFORMERS`           | `stepfun-ai/GOT-OCR-2.0-hf`                 | Transformers | CPU, CUDA      | MARKDOWN        |\n| `GEMMA3_12B_MLX`              | `mlx-community/gemma-3-12b-it-bf16`         | MLX          | MPS            | MARKDOWN        |\n| `GEMMA3_27B_MLX`              | `mlx-community/gemma-3-27b-it-bf16`         | MLX          | MPS            | MARKDOWN        |\n| `DOLPHIN_TRANSFORMERS`        | `ByteDance/Dolphin`                         | Transformers | CPU, CUDA, MPS | MARKDOWN        |\n\n### Plaintext Output Models\n\n| Model Spec                | Repository                            | Framework    | Devices   | Response Format |\n| ------------------------- | ------------------------------------- | ------------ | --------- | --------------- |\n| `SMOLVLM256_TRANSFORMERS` | `HuggingFaceTB/SmolVLM-256M-Instruct` | Transformers | CPU, CUDA | PLAINTEXT       |\n| `SMOLVLM256_MLX`          | `moot20/SmolVLM-256M-Instruct-MLX`    | MLX          | MPS       | PLAINTEXT       |\n| `SMOLVLM256_VLLM`         | `HuggingFaceTB/SmolVLM-256M-Instruct` | vLLM         | CUDA      | PLAINTEXT       |\n\n### Extraction Models\n\n| Model Spec                   | Repository                | Framework    | Devices        | Response Format |\n| ---------------------------- | ------------------------- | ------------ | -------------- | --------------- |\n| `NU_EXTRACT_2B_TRANSFORMERS` | `numind/NuExtract-2.0-2B` | Transformers | CPU, CUDA, MPS | PLAINTEXT       |\n\n**Special Configuration Notes:**",
    "metadata": {
      "chunk_id": 7,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 985,
      "character_count": 3568,
      "created_at": "2025-10-16T17:42:17.052417",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 7,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "1. **GOT-OCR-2.0** uses `TransformersPromptStyle.NONE` and includes `extra_processor_kwargs={\"format\": True}`\n2. **Phi-4** requires `transformers<4.52.0` and uses `extra_generation_config={\"num_logits_to_keep\": 0}`\n3. **Dolphin** uses `TransformersPromptStyle.RAW` with a custom prompt format\n4. **GraniteDocling VLLM** uses `revision=\"untied\"` for compatibility with vLLM ≤0.10.2\n\n**Sources:** [docling/datamodel/vlm\\_model\\_specs.py1-303](https://github.com/docling-project/docling/blob/f7244a43/docling/datamodel/vlm_model_specs.py#L1-L303)\n\n## Usage Examples\n\n### Basic Usage with Default Model\n\n```\n```\n\n### Selecting a Specific Model\n\n```\n```\n\n### Custom Model Configuration\n\n```\n```\n\n### Direct Image Processing\n\nAll inline VLM models support direct image processing via the `process_images()` method:\n\n```\n```\n\n**Sources:** [docs/examples/minimal\\_vlm\\_pipeline.py1-71](https://github.com/docling-project/docling/blob/f7244a43/docs/examples/minimal_vlm_pipeline.py#L1-L71) [docs/usage/vision\\_models.md1-124](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L1-L124)\n\n## Performance Considerations\n\n### Framework Comparison\n\n| Framework        | Batching             | Thread Safety          | Best For                            |\n| ---------------- | -------------------- | ---------------------- | ----------------------------------- |\n| **Transformers** | ✅ Full batch support | ✅ Thread-safe          | General purpose, CPU/CUDA/MPS       |\n| **MLX**          | ❌ Sequential only    | ❌ Global lock required | Apple Silicon (fastest on M-series) |\n| **vLLM**         | ✅ Optimized batching | ✅ Thread-safe          | High-throughput GPU inference       |\n\n### Memory Management\n\n1. **Transformers**: Uses PyTorch's default memory management; consider `torch_dtype=\"bfloat16\"` for memory savings\n2. **MLX**: Automatically manages unified memory on Apple Silicon\n3. **vLLM**: Set `gpu_memory_utilization` (default 0.3) to reserve GPU memory for other models\n\n### Acceleration Options\n\n- **Flash Attention 2**: Automatically enabled on CUDA devices when `accelerator_options.cuda_use_flash_attention2=True`\n- **Quantization**: Enable with `quantized=True` and `load_in_8bit=True` (Transformers and vLLM only)\n- **KV Cache**: Enabled by default with `use_kv_cache=True`; disable only if memory is constrained\n\n**Sources:** [docling/models/vlm\\_models\\_inline/hf\\_transformers\\_model.py123-128](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/hf_transformers_model.py#L123-L128) [docling/models/vlm\\_models\\_inline/vllm\\_model.py146-155](https://github.com/docling-project/docling/blob/f7244a43/docling/models/vlm_models_inline/vllm_model.py#L146-L155) [docs/usage/vision\\_models.md46-58](https://github.com/docling-project/docling/blob/f7244a43/docs/usage/vision_models.md#L46-L58)\n\nDismiss\n\nRefresh this wiki\n\nThis wiki was recently refreshed. Please wait 4 days to refresh again.\n\n### On this page",
    "metadata": {
      "chunk_id": 8,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 769,
      "character_count": 2986,
      "created_at": "2025-10-16T17:42:17.057651",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 8,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  },
  {
    "text": "- [Inline VLM Models](#inline-vlm-models.md)\n- [Architecture Overview](#architecture-overview.md)\n- [Configuration via InlineVlmOptions](#configuration-via-inlinevlmoptions.md)\n- [Hugging Face Transformers Implementation](#hugging-face-transformers-implementation.md)\n- [Model Loading and Initialization](#model-loading-and-initialization.md)\n- [Batch Inference Pipeline](#batch-inference-pipeline.md)\n- [Stopping Criteria Handling](#stopping-criteria-handling.md)\n- [MLX Implementation (Apple Silicon)](#mlx-implementation-apple-silicon.md)\n- [Architecture and Thread Safety](#architecture-and-thread-safety.md)\n- [Streaming Generation and Token Collection](#streaming-generation-and-token-collection.md)\n- [Stopping Criteria Validation](#stopping-criteria-validation.md)\n- [vLLM Implementation](#vllm-implementation.md)\n- [Configuration and Initialization](#configuration-and-initialization.md)\n- [Batch Inference with Multi-Modal Data](#batch-inference-with-multi-modal-data.md)\n- [Prompt Formatting](#prompt-formatting.md)\n- [Model Download and Caching](#model-download-and-caching.md)\n- [Available Model Specifications](#available-model-specifications.md)\n- [DocTags Output Models](#doctags-output-models.md)\n- [Markdown Output Models](#markdown-output-models.md)\n- [Plaintext Output Models](#plaintext-output-models.md)\n- [Extraction Models](#extraction-models.md)\n- [Usage Examples](#usage-examples.md)\n- [Basic Usage with Default Model](#basic-usage-with-default-model.md)\n- [Selecting a Specific Model](#selecting-a-specific-model.md)\n- [Custom Model Configuration](#custom-model-configuration.md)\n- [Direct Image Processing](#direct-image-processing.md)\n- [Performance Considerations](#performance-considerations.md)\n- [Framework Comparison](#framework-comparison.md)\n- [Memory Management](#memory-management.md)\n- [Acceleration Options](#acceleration-options.md)",
    "metadata": {
      "chunk_id": 9,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "input_type": "docling",
      "chunking_strategy": "hybrid_adaptive",
      "token_count": 434,
      "character_count": 1873,
      "created_at": "2025-10-16T17:42:17.057850",
      "parent_context": null,
      "semantic_type": "docling",
      "collection_name": "Docling",
      "subfolder_name": null,
      "collection_strategy": "hybrid_adaptive",
      "chunk_index_in_file": 9,
      "file_relative_path": "Docs\\Docling\\_docling-project_docling_4.3.1-inline-vlm-models.md",
      "collection_context": "Docling"
    }
  }
]