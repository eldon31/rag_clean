[
  {
    "text": "This page covers the evaluation and measurement of semantic textual similarity using sentence-transformers. It focuses on the various evaluators, similarity functions, and metrics available for assessing how semantically similar two pieces of text are to each other.\n\nFor information about using similarity for information retrieval tasks, see [Information Retrieval](#6.1). For details on training models with similarity-based objectives, see [Loss Functions for SentenceTransformer](#3.4).\n\n## Core Similarity Evaluators\n\nThe sentence-transformers library provides several specialized evaluators for measuring semantic textual similarity, each designed for different evaluation scenarios and data formats.\n\n### EmbeddingSimilarityEvaluator\n\nThe `EmbeddingSimilarityEvaluator` is the primary evaluator for semantic similarity tasks. It computes Spearman and Pearson rank correlations between predicted similarities and gold standard similarity scores.\n\n```mermaid\ngraph TD\n    ESE[\"EmbeddingSimilarityEvaluator\"]\n    \n    subgraph \"Input Data\"\n        S1[\"sentences1: List[str]\"]\n        S2[\"sentences2: List[str]\"] \n        SC[\"scores: List[float]\"]\n    end\n    \n    subgraph \"Similarity Functions\"\n        COS[\"cosine\"]\n        DOT[\"dot\"]\n        EUC[\"euclidean\"] \n        MAN[\"manhattan\"]\n    end\n    \n    subgraph \"Metrics\"\n        PEAR[\"pearson\"]\n        SPEAR[\"spearman\"]\n    end\n    \n    subgraph \"Embedding Process\"\n        EMB1[\"model.encode(sentences1)\"]\n        EMB2[\"model.encode(sentences2)\"]\n        SIMCALC[\"pairwise_similarity_functions\"]\n    end\n    \n    S1 --> EMB1\n    S2 --> EMB2\n    EMB1 --> SIMCALC\n    EMB2 --> SIMCALC\n    \n    SIMCALC --> COS\n    SIMCALC --> DOT\n    SIMCALC --> EUC\n    SIMCALC --> MAN\n    \n    COS --> PEAR\n    COS --> SPEAR\n    DOT --> PEAR\n    DOT --> SPEAR\n    EUC --> PEAR\n    EUC --> SPEAR\n    MAN --> PEAR\n    MAN --> SPEAR\n    \n    SC --> PEAR\n    SC --> SPEAR\n    \n    ESE --> S1\n    ESE --> S2\n    ESE --> SC\n```\n\n**Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:27-272]()\n\n### BinaryClassificationEvaluator\n\nThe `BinaryClassificationEvaluator` treats semantic similarity as a binary classification problem, determining whether pairs of sentences are similar (1) or dissimilar (0).\n\n```mermaid\ngraph TD\n    BCE[\"BinaryClassificationEvaluator\"]\n    \n    subgraph \"Input Processing\"\n        PAIRS[\"sentence pairs + binary labels\"]\n        EMBED[\"model.encode()\"]\n        HASHOPT[\"hashable optimization\"]\n    end\n    \n    subgraph \"Similarity Computation\"\n        COSF[\"pairwise_cos_sim\"]\n        DOTF[\"pairwise_dot_score\"]\n        EUCF[\"pairwise_euclidean_sim\"]\n        MANF[\"pairwise_manhattan_sim\"]\n    end\n    \n    subgraph \"Threshold Optimization\"\n        ACCTHRESH[\"find_best_acc_and_threshold\"]\n        F1THRESH[\"find_best_f1_and_threshold\"]\n    end\n    \n    subgraph \"Metrics\"\n        ACC[\"accuracy\"]\n        F1[\"f1\"]\n        PREC[\"precision\"]\n        REC[\"recall\"]\n        AP[\"average_precision\"]\n        MCC[\"matthews_corrcoef\"]\n    end\n    \n    BCE --> PAIRS\n    PAIRS --> EMBED\n    EMBED --> HASHOPT\n    HASHOPT --> COSF\n    HASHOPT --> DOTF\n    HASHOPT --> EUCF\n    HASHOPT --> MANF\n    \n    COSF --> ACCTHRESH\n    COSF --> F1THRESH\n    DOTF --> ACCTHRESH\n    DOTF --> F1THRESH\n    EUCF --> ACCTHRESH\n    EUCF --> F1THRESH\n    MANF --> ACCTHRESH\n    MANF --> F1THRESH\n    \n    ACCTHRESH --> ACC\n    F1THRESH --> F1\n    F1THRESH --> PREC\n    F1THRESH --> REC\n    COSF --> AP\n    F1THRESH --> MCC\n```\n\n**Sources:** [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379]()\n\n### TripletEvaluator\n\nThe `TripletEvaluator` evaluates models using triplets of (anchor, positive, negative) sentences, ensuring that the anchor is more similar to the positive than to the negative example.",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Semantic_Textual_Similarity.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 955,
      "character_count": 3789,
      "created_at": "2025-10-16T17:42:33.165412",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Semantic_Textual_Similarity.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    TE[\"TripletEvaluator\"]\n    \n    subgraph \"Triplet Input\"\n        ANC[\"anchors: List[str]\"]\n        POS[\"positives: List[str]\"]\n        NEG[\"negatives: List[str]\"]\n        MAR[\"margin: float | Dict[str, float]\"]\n    end\n    \n    subgraph \"Embedding Generation\"\n        EAANC[\"embed_inputs(model, anchors)\"]\n        EPOS[\"embed_inputs(model, positives)\"]\n        ENEG[\"embed_inputs(model, negatives)\"]\n    end\n    \n    subgraph \"Similarity Calculation\"\n        SIMPOS[\"similarity(anchor, positive)\"]\n        SIMNEG[\"similarity(anchor, negative)\"]\n    end\n    \n    subgraph \"Evaluation Logic\"\n        COMP[\"positive_scores > negative_scores + margin\"]\n        ACCUR[\"accuracy = mean(comparisons)\"]\n    end\n    \n    TE --> ANC\n    TE --> POS  \n    TE --> NEG\n    TE --> MAR\n    \n    ANC --> EAANC\n    POS --> EPOS\n    NEG --> ENEG\n    \n    EAANC --> SIMPOS\n    EPOS --> SIMPOS\n    EAANC --> SIMNEG\n    ENEG --> SIMNEG\n    \n    SIMPOS --> COMP\n    SIMNEG --> COMP\n    MAR --> COMP\n    COMP --> ACCUR\n```\n\n**Sources:** [sentence_transformers/evaluation/TripletEvaluator.py:26-271]()\n\n## Similarity Functions and Metrics\n\nThe evaluators support multiple similarity functions, each with different mathematical properties and use cases.\n\n| Similarity Function | Implementation | Use Case | Greater is Better |\n|-------------------|----------------|----------|------------------|\n| `cosine` | `pairwise_cos_sim` | General semantic similarity | ✓ |\n| `dot` | `pairwise_dot_score` | When magnitude matters | ✓ |\n| `euclidean` | `pairwise_euclidean_sim` | Distance-based similarity | ✗ |\n| `manhattan` | `pairwise_manhattan_sim` | L1 distance similarity | ✗ |\n\n```mermaid\ngraph LR\n    subgraph \"SimilarityFunction Enum\"\n        COSINE_ENUM[\"SimilarityFunction.COSINE\"]\n        DOT_ENUM[\"SimilarityFunction.DOT_PRODUCT\"] \n        EUC_ENUM[\"SimilarityFunction.EUCLIDEAN\"]\n        MAN_ENUM[\"SimilarityFunction.MANHATTAN\"]\n    end\n    \n    subgraph \"Implementation Functions\"\n        COS_FUNC[\"pairwise_cos_sim\"]\n        DOT_FUNC[\"pairwise_dot_score\"]\n        EUC_FUNC[\"pairwise_euclidean_sim\"]\n        MAN_FUNC[\"pairwise_manhattan_sim\"]\n    end\n    \n    subgraph \"Evaluator Integration\"\n        SIM_DICT[\"similarity_functions dict\"]\n        SCORE_COMP[\"score computation\"]\n        METRIC_CALC[\"metric calculation\"]\n    end\n    \n    COSINE_ENUM --> COS_FUNC\n    DOT_ENUM --> DOT_FUNC\n    EUC_ENUM --> EUC_FUNC\n    MAN_ENUM --> MAN_FUNC\n    \n    COS_FUNC --> SIM_DICT\n    DOT_FUNC --> SIM_DICT\n    EUC_FUNC --> SIM_DICT\n    MAN_FUNC --> SIM_DICT\n    \n    SIM_DICT --> SCORE_COMP\n    SCORE_COMP --> METRIC_CALC\n```\n\n**Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:184-189](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:238-259](), [sentence_transformers/evaluation/TripletEvaluator.py:187-204]()\n\n## STS Benchmark Integration\n\nThe library includes extensive testing and evaluation capabilities for the STS (Semantic Textual Similarity) benchmark, a standard dataset for evaluating semantic similarity models.\n\n### STS Benchmark Testing Framework\n\n```mermaid\ngraph TD\n    subgraph \"STS Dataset Loading\"\n        STSDATA[\"stsbenchmark.tsv.gz\"]\n        DOWNLOAD[\"util.http_get()\"]\n        PARSE[\"csv.DictReader parsing\"]\n    end\n    \n    subgraph \"Data Processing\"\n        NORM[\"score normalization (0-5 → 0-1)\"]\n        SPLIT[\"train/test split\"]\n        INPUTEX[\"InputExample creation\"]\n    end\n    \n    subgraph \"Evaluation Pipeline\"\n        EVALCREATE[\"EmbeddingSimilarityEvaluator.from_input_examples()\"]\n        MODELEVAL[\"model.evaluate()\"]\n        SCORERET[\"primary_metric score\"]\n    end\n    \n    subgraph \"Pretrained Model Testing\"\n        MODLIST[\"pretrained model list\"]\n        PERFTEST[\"pretrained_model_score()\"]\n        ASSERT[\"performance assertions\"]\n    end\n    \n    STSDATA --> DOWNLOAD\n    DOWNLOAD --> PARSE\n    PARSE --> NORM\n    NORM --> SPLIT\n    SPLIT --> INPUTEX\n    \n    INPUTEX --> EVALCREATE\n    EVALCREATE --> MODELEVAL\n    MODELEVAL --> SCORERET\n    \n    MODLIST --> PERFTEST\n    PERFTEST --> EVALCREATE\n    SCORERET --> ASSERT\n```",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Semantic_Textual_Similarity.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 999,
      "character_count": 4118,
      "created_at": "2025-10-16T17:42:33.169206",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Semantic_Textual_Similarity.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "**Sources:** [tests/test_pretrained_stsb.py:18-49](), [tests/test_train_stsb.py:33-51]()\n\n### Training with STS Data\n\nThe library supports training models specifically for semantic similarity using the STS benchmark:\n\n```mermaid\ngraph TD\n    subgraph \"Training Setup\"\n        STSLOAD[\"STS data loading\"]\n        DATASET[\"SentencesDataset creation\"]\n        DATALOADER[\"DataLoader setup\"]\n    end\n    \n    subgraph \"Loss Function\"\n        COSLOSS[\"CosineSimilarityLoss\"]\n        MODELREF[\"model reference\"]\n    end\n    \n    subgraph \"Training Process\"\n        FIT[\"model.fit()\"]\n        TRAINOBJ[\"train_objectives\"]\n        EPOCHS[\"epoch configuration\"]\n    end\n    \n    subgraph \"Evaluation\"\n        EVALFUNC[\"evaluate_stsb_test()\"]\n        THRESHOLD[\"expected score threshold\"]\n        ASSERTION[\"performance assertion\"]\n    end\n    \n    STSLOAD --> DATASET\n    DATASET --> DATALOADER\n    DATALOADER --> TRAINOBJ\n    \n    COSLOSS --> TRAINOBJ\n    MODELREF --> COSLOSS\n    \n    TRAINOBJ --> FIT\n    EPOCHS --> FIT\n    \n    FIT --> EVALFUNC\n    EVALFUNC --> THRESHOLD\n    THRESHOLD --> ASSERTION\n```\n\n**Sources:** [tests/test_train_stsb.py:74-103](), [tests/test_train_stsb.py:111-127]()\n\n## Advanced Evaluation Features\n\n### Precision and Quantization Support\n\nThe `EmbeddingSimilarityEvaluator` supports various embedding precisions for memory-efficient evaluation:\n\n| Precision | Description | Binary Unpacking |\n|-----------|-------------|------------------|\n| `float32` | Standard floating point | ✗ |\n| `int8` | 8-bit signed integer | ✗ |\n| `uint8` | 8-bit unsigned integer | ✗ |\n| `binary` | Binary with signed conversion | ✓ |\n| `ubinary` | Unsigned binary | ✓ |\n\n**Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:171-176]()\n\n### Multi-Metric Evaluation\n\nAll similarity evaluators support evaluation with multiple similarity functions simultaneously, computing max metrics across functions:\n\n```python",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Semantic_Textual_Similarity.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 459,
      "character_count": 1937,
      "created_at": "2025-10-16T17:42:33.171364",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\Semantic_Textual_Similarity.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]