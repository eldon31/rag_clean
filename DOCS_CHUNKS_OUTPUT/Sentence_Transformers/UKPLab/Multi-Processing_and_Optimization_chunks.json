[
  {
    "text": "This page covers performance optimization techniques available in sentence-transformers, including multi-processing for distributed encoding, backend optimization with ONNX and OpenVINO, quantization strategies, and hardware acceleration. These optimization methods can significantly improve inference speed and reduce memory usage when working with large datasets or resource-constrained environments.\n\nFor general model usage and basic inference, see the quickstart guide in [2.1](#2.1). For training optimization techniques, see [3.7](#3.7).\n\n## Multi-Processing Architecture\n\nThe sentence-transformers library provides built-in multi-processing capabilities that allow distributing encoding tasks across multiple devices or CPU processes. This is particularly useful when processing large datasets or when multiple GPUs are available.\n\n```mermaid\ngraph TD\n    MainProcess[\"Main Process<br/>SentenceTransformer\"] --> ProcessPool[\"Multi-Process Pool<br/>start_multi_process_pool()\"]\n    ProcessPool --> Worker1[\"Worker Process 1<br/>Device: cuda:0\"]\n    ProcessPool --> Worker2[\"Worker Process 2<br/>Device: cuda:1\"] \n    ProcessPool --> Worker3[\"Worker Process 3<br/>Device: cpu\"]\n    \n    MainProcess --> ChunkDistribution[\"Data Chunking<br/>chunk_size parameter\"]\n    ChunkDistribution --> Worker1\n    ChunkDistribution --> Worker2\n    ChunkDistribution --> Worker3\n    \n    Worker1 --> Results1[\"Partial Results 1\"]\n    Worker2 --> Results2[\"Partial Results 2\"]\n    Worker3 --> Results3[\"Partial Results 3\"]\n    \n    Results1 --> Aggregation[\"Result Aggregation<br/>_encode_multi_process()\"]\n    Results2 --> Aggregation\n    Results3 --> Aggregation\n    \n    Aggregation --> FinalOutput[\"Final Embeddings<br/>numpy.ndarray or torch.Tensor\"]\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:1046-1158](), [tests/test_multi_process.py:14-42]()\n\n### Pool Management\n\nMulti-processing pools are managed through dedicated methods that handle worker process lifecycle:\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `start_multi_process_pool(target_devices)` | Creates worker processes for specified devices | Pool dictionary |\n| `stop_multi_process_pool(pool)` | Terminates worker processes and cleanup | None |\n| `encode(..., pool=pool)` | Uses existing pool for encoding | Embeddings |\n| `encode(..., device=[\"cuda:0\", \"cuda:1\"])` | Auto-creates temporary pool | Embeddings |\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:1915-1970](), [tests/test_multi_process.py:61-81]()\n\n### Distributed Encoding Process\n\n```mermaid\nsequenceDiagram\n    participant Client as \"Client Code\"\n    participant ST as \"SentenceTransformer\"\n    participant Pool as \"Process Pool\"\n    participant W1 as \"Worker 1\"\n    participant W2 as \"Worker 2\"\n    \n    Client->>ST: encode(sentences, device=[\"cuda:0\", \"cuda:1\"])\n    ST->>ST: _encode_multi_process()\n    ST->>Pool: Create temporary pool\n    Pool->>W1: Initialize on cuda:0\n    Pool->>W2: Initialize on cuda:1\n    \n    ST->>ST: Split sentences into chunks\n    ST->>W1: Send chunk 1\n    ST->>W2: Send chunk 2\n    \n    W1->>W1: Process batch\n    W2->>W2: Process batch\n    \n    W1->>ST: Return embeddings 1\n    W2->>ST: Return embeddings 2\n    \n    ST->>ST: Concatenate results\n    ST->>Pool: Cleanup processes\n    ST->>Client: Return final embeddings\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:1077-1158](), [sentence_transformers/sparse_encoder/SparseEncoder.py:514-532]()\n\n## Backend Optimization\n\nSentence-transformers supports multiple inference backends beyond PyTorch, enabling significant performance improvements for production deployments.\n\n### Backend Architecture",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Multi-Processing_and_Optimization.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 840,
      "character_count": 3674,
      "created_at": "2025-10-16T17:42:33.007227",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Multi-Processing_and_Optimization.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph LR\n    Model[\"Model Loading\"] --> BackendChoice{\"Backend Selection\"}\n    \n    BackendChoice -->|\"backend='torch'\"| PyTorchBackend[\"PyTorch Backend<br/>AutoModel.from_pretrained()\"]\n    BackendChoice -->|\"backend='onnx'\"| ONNXBackend[\"ONNX Backend<br/>load_onnx_model()\"]\n    BackendChoice -->|\"backend='openvino'\"| OpenVINOBackend[\"OpenVINO Backend<br/>load_openvino_model()\"]\n    \n    PyTorchBackend --> PyTorchInference[\"PyTorch Inference<br/>model(**features)\"]\n    ONNXBackend --> ONNXInference[\"ONNX Runtime<br/>session.run()\"]\n    OpenVINOBackend --> OpenVINOInference[\"OpenVINO Runtime<br/>compiled_model()\"]\n    \n    PyTorchInference --> Output[\"Embeddings Output\"]\n    ONNXInference --> Output\n    OpenVINOInference --> Output\n```\n\n**Sources:** [sentence_transformers/models/Transformer.py:173-203](), [sentence_transformers/cross_encoder/CrossEncoder.py:236-257]()\n\n### Backend Configuration\n\nEach model type supports backend selection through the `backend` parameter:\n\n| Model Type | Backend Support | Configuration |\n|------------|----------------|---------------|\n| `SentenceTransformer` | torch, onnx, openvino | `SentenceTransformer(model_name, backend=\"onnx\")` |\n| `SparseEncoder` | torch, onnx, openvino | `SparseEncoder(model_name, backend=\"openvino\")` |\n| `CrossEncoder` | torch, onnx, openvino | `CrossEncoder(model_name, backend=\"torch\")` |\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:186](), [sentence_transformers/sparse_encoder/SparseEncoder.py:151](), [sentence_transformers/cross_encoder/CrossEncoder.py:135]()\n\n### Backend-Specific Parameters\n\nAdditional configuration options are available through `model_kwargs`:\n\n```python\n# ONNX provider selection\nmodel = SentenceTransformer(\n    \"model-name\", \n    backend=\"onnx\",\n    model_kwargs={\"provider\": \"CUDAExecutionProvider\"}\n)\n\n# Optimized model file selection\nmodel = SentenceTransformer(\n    \"model-name\",\n    backend=\"openvino\", \n    model_kwargs={\"file_name\": \"model_optimized.xml\"}\n)\n\n# Auto-export control\nmodel = SentenceTransformer(\n    \"model-name\",\n    backend=\"onnx\",\n    model_kwargs={\"export\": True}\n)\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:113-119](), [sentence_transformers/backend.py]()\n\n## Quantization and Precision\n\nThe library provides multiple quantization strategies to reduce memory usage and improve inference speed with minimal accuracy loss.\n\n### Quantization Pipeline\n\n```mermaid\ngraph TD\n    FloatEmbeddings[\"Float32 Embeddings<br/>model.encode()\"] --> QuantizationChoice{\"Precision Parameter\"}\n    \n    QuantizationChoice -->|\"precision='float32'\"| Float32[\"Float32 Output<br/>No quantization\"]\n    QuantizationChoice -->|\"precision='int8'\"| Int8Quant[\"INT8 Quantization<br/>quantize_embeddings()\"]\n    QuantizationChoice -->|\"precision='uint8'\"| UInt8Quant[\"UINT8 Quantization<br/>quantize_embeddings()\"]\n    QuantizationChoice -->|\"precision='binary'\"| BinaryQuant[\"Binary Quantization<br/>quantize_embeddings()\"]\n    QuantizationChoice -->|\"precision='ubinary'\"| UBinaryQuant[\"Unsigned Binary<br/>quantize_embeddings()\"]\n    \n    Int8Quant --> MemoryReduction[\"Memory Usage<br/>4x reduction\"]\n    UInt8Quant --> MemoryReduction\n    BinaryQuant --> MemoryReduction2[\"Memory Usage<br/>32x reduction\"]\n    UBinaryQuant --> MemoryReduction2\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:424](), [sentence_transformers/quantization.py]()\n\n### Precision Performance Characteristics\n\n| Precision | Memory Factor | Speed Factor | Use Case |\n|-----------|---------------|--------------|----------|\n| `float32` | 1x | 1x | Highest accuracy |\n| `int8` | 4x smaller | ~2x faster | Balanced accuracy/speed |\n| `uint8` | 4x smaller | ~2x faster | Positive-only embeddings |\n| `binary` | 32x smaller | ~10x faster | Similarity search |\n| `ubinary` | 32x smaller | ~10x faster | Unsigned binary encoding |\n\n**Sources:** [sentence_transformers/quantization.py:15-142](), [sentence_transformers/SentenceTransformer.py:469-473]()\n\n## Hardware Acceleration\n\n### Device Management\n\nThe library automatically detects and utilizes available hardware acceleration:",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Multi-Processing_and_Optimization.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 994,
      "character_count": 4125,
      "created_at": "2025-10-16T17:42:33.014917",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Multi-Processing_and_Optimization.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    DeviceDetection[\"Device Detection<br/>get_device_name()\"] --> AvailableCheck{\"Hardware Available?\"}\n    \n    AvailableCheck -->|\"torch.cuda.is_available()\"| CUDA[\"CUDA Devices<br/>cuda:0, cuda:1, ...\"]\n    AvailableCheck -->|\"torch.backends.mps.is_available()\"| MPS[\"Apple MPS<br/>mps device\"]\n    AvailableCheck -->|\"is_torch_npu_available()\"| NPU[\"Neural Processing Unit<br/>npu device\"]\n    AvailableCheck -->|\"HPU available\"| HPU[\"Habana HPU<br/>hpu device\"]\n    AvailableCheck -->|\"Default\"| CPU[\"CPU Fallback<br/>cpu device\"]\n    \n    CUDA --> OptimumHabana[\"Optimum Habana<br/>adapt_transformers_to_gaudi()\"]\n    NPU --> OptimumHabana\n    HPU --> OptimumHabana\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:217-224](), [sentence_transformers/util.py:47-77]()\n\n### Mixed Precision Support\n\nHardware acceleration includes mixed precision training and inference:\n\n| `torch_dtype` | Description | Memory Savings |\n|---------------|-------------|----------------|\n| `\"auto\"` | Use model's default dtype | Varies |\n| `torch.float16` | Half precision | 50% reduction |\n| `torch.bfloat16` | Brain floating point | 50% reduction |\n| `torch.float32` | Full precision | Baseline |\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:95-106](), [tests/test_sentence_transformer.py:96-106]()\n\n## Memory Optimization Strategies\n\n### Efficient Encoding Parameters\n\nSeveral parameters help optimize memory usage during encoding:\n\n```mermaid\ngraph LR\n    Input[\"Large Dataset\"] --> BatchSize[\"batch_size<br/>Control memory per batch\"]\n    BatchSize --> ChunkSize[\"chunk_size<br/>Multi-process distribution\"]\n    ChunkSize --> TruncateDim[\"truncate_dim<br/>Matryoshka model truncation\"]\n    TruncateDim --> Precision[\"precision<br/>Quantization strategy\"]\n    Precision --> Output[\"Optimized Embeddings\"]\n    \n    TruncateDim --> MatryoshkaNote[\"Matryoshka Models<br/>Maintain quality with<br/>reduced dimensions\"]\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:485-491](), [sentence_transformers/util.py:436-455]()\n\n### Sparse Encoding Optimization\n\nFor `SparseEncoder` models, additional memory optimizations are available:\n\n| Parameter | Effect | Usage |\n|-----------|--------|-------|\n| `max_active_dims` | Limits non-zero dimensions | Reduce memory and computation |\n| `convert_to_sparse_tensor` | Use sparse tensor format | Memory efficient storage |\n| `save_to_cpu` | Move results to CPU | Free GPU memory |\n\n**Sources:** [sentence_transformers/sparse_encoder/SparseEncoder.py:192](), [sentence_transformers/sparse_encoder/SparseEncoder.py:467-469]()\n\n### Pooling Configuration for Memory\n\nThe `Pooling` module provides memory-efficient pooling strategies:\n\n```mermaid\ngraph TD\n    TokenEmbeddings[\"Token Embeddings<br/>[batch_size, seq_len, hidden_dim]\"] --> PoolingChoice{\"Pooling Strategy\"}\n    \n    PoolingChoice -->|\"pooling_mode='mean'\"| MeanPool[\"Mean Pooling<br/>Average across sequence\"]\n    PoolingChoice -->|\"pooling_mode='max'\"| MaxPool[\"Max Pooling<br/>Maximum values\"]\n    PoolingChoice -->|\"pooling_mode='cls'\"| CLSPool[\"CLS Token<br/>First token only\"]\n    PoolingChoice -->|\"pooling_mode='lasttoken'\"| LastPool[\"Last Token<br/>Final valid token\"]\n    \n    MeanPool --> SentenceEmbedding[\"Sentence Embedding<br/>[batch_size, hidden_dim]\"]\n    MaxPool --> SentenceEmbedding\n    CLSPool --> SentenceEmbedding\n    LastPool --> SentenceEmbedding\n    \n    SentenceEmbedding --> IncludePrompt{\"include_prompt=False\"}\n    IncludePrompt --> PromptExclusion[\"Exclude prompt tokens<br/>from pooling calculation\"]\n```\n\n**Sources:** [sentence_transformers/models/Pooling.py:135-241](), [sentence_transformers/models/Pooling.py:142-152]()",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Multi-Processing_and_Optimization.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 883,
      "character_count": 3704,
      "created_at": "2025-10-16T17:42:33.020228",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\Multi-Processing_and_Optimization.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]