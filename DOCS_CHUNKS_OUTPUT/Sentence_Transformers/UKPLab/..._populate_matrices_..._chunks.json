[
  {
    "text": "loss = self.cross_entropy_loss(logits_matrix, labels_matrix.softmax(dim=1))\n```\n\nSources: [sentence_transformers/cross_encoder/losses/ListNetLoss.py:10-198]()\n\n### Position-Aware ListMLE\n\nThe `PListMLELoss` and `ListMLELoss` implement maximum likelihood estimation for permutations with optional position-aware weighting:\n\n```python\n# Core PListMLE computation from PListMLELoss.forward()\nscores = sorted_logits.exp()\ncumsum_scores = torch.flip(torch.cumsum(torch.flip(scores, [1]), 1), [1])\nlog_probs = sorted_logits - torch.log(cumsum_scores + self.eps)\n\nif self.lambda_weight is not None:\n    lambda_weight = self.lambda_weight(mask)\n    log_probs = log_probs * lambda_weight\n```\n\nSources: [sentence_transformers/cross_encoder/losses/PListMLELoss.py:45-295](), [sentence_transformers/cross_encoder/losses/ListMLELoss.py:9-127]()\n\n## Common Implementation Patterns\n\nAll learning-to-rank losses share several implementation patterns:\n\n### Mini-Batch Processing\n\nLarge document lists are processed in mini-batches to manage memory usage:\n\n```python\nmini_batch_size = self.mini_batch_size or batch_size\nif mini_batch_size <= 0:\n    mini_batch_size = len(pairs)\n\nfor i in range(0, len(pairs), mini_batch_size):\n    mini_batch_pairs = pairs[i : i + mini_batch_size]\n    # Process mini-batch...\n```\n\n### Padding Handling\n\nVariable document counts per query are handled using padding and masking:\n\n```python\n# Create padded matrices\nlogits_matrix = torch.full((batch_size, max_docs), -1e16, device=self.model.device)\nlabels_matrix = torch.full_like(logits_matrix, float(\"-inf\"))\n\n# Place valid logits and labels\ndoc_indices = torch.cat([torch.arange(len(docs)) for docs in docs_list], dim=0)\nbatch_indices = torch.repeat_interleave(torch.arange(batch_size), torch.tensor(docs_per_query))\nlogits_matrix[batch_indices, doc_indices] = logits\n```\n\nSources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:247-287](), [sentence_transformers/cross_encoder/losses/ListNetLoss.py:132-176]()\n\n## Configuration and Usage\n\n### Basic Usage Pattern\n\n```python\nfrom sentence_transformers.cross_encoder import CrossEncoder, CrossEncoderTrainer, losses\nfrom datasets import Dataset\n\nmodel = CrossEncoder(\"microsoft/mpnet-base\")\ntrain_dataset = Dataset.from_dict({\n    \"query\": [\"What are pandas?\", \"What is the capital of France?\"],\n    \"docs\": [\n        [\"Pandas are a kind of bear.\", \"Pandas are kind of like fish.\"],\n        [\"The capital of France is Paris.\", \"Paris is quite large.\"],\n    ],\n    \"labels\": [[1, 0], [1, 0]],\n})",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\..._populate_matrices_....md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 605,
      "character_count": 2520,
      "created_at": "2025-10-16T17:42:32.753582",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\..._populate_matrices_....md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]