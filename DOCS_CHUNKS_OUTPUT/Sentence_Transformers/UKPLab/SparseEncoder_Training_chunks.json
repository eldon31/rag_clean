[
  {
    "text": "This document covers the comprehensive training system for `SparseEncoder` models in the sentence-transformers library. It explains the specialized components, architectures, and workflows required to train sparse representation models such as SPLADE and CSR (Contrastive Sparse Representation) models.\n\nFor information about general training concepts that apply across all model types, see [Training](#3). For specific loss function details, see [Loss Functions for SparseEncoder](#3.5). For evaluation during training, see [SparseEncoder Evaluators](#4.2).\n\n## Training System Architecture\n\nThe `SparseEncoder` training system consists of specialized components that handle the unique requirements of sparse representation learning, including sparsity regularization and architecture-specific optimizations.\n\n```mermaid\ngraph TB\n    subgraph \"Training Components\"\n        SparseEncoderTrainer[\"SparseEncoderTrainer\"]\n        SparseEncoderTrainingArguments[\"SparseEncoderTrainingArguments\"]\n        DataCollator[\"DataCollator\"]\n    end\n    \n    subgraph \"Model Architectures\"\n        MLMTransformer[\"MLMTransformer\"]\n        SpladePooling[\"SpladePooling\"]\n        SparseAutoEncoder[\"SparseAutoEncoder\"]\n        SparseStaticEmbedding[\"SparseStaticEmbedding\"]\n        Router[\"Router\"]\n    end\n    \n    subgraph \"Loss Functions\"\n        SpladeLoss[\"SpladeLoss\"]\n        CSRLoss[\"CSRLoss\"]\n        FlopsLoss[\"FlopsLoss\"]\n        SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"]\n    end\n    \n    subgraph \"Evaluators\"\n        SparseNanoBEIREvaluator[\"SparseNanoBEIREvaluator\"]\n        SparseInformationRetrievalEvaluator[\"SparseInformationRetrievalEvaluator\"]\n        SparseEmbeddingSimilarityEvaluator[\"SparseEmbeddingSimilarityEvaluator\"]\n    end\n    \n    SparseEncoderTrainer --> MLMTransformer\n    SparseEncoderTrainer --> SpladePooling\n    SparseEncoderTrainer --> SparseAutoEncoder\n    SparseEncoderTrainer --> Router\n    \n    SparseEncoderTrainingArguments --> SparseEncoderTrainer\n    DataCollator --> SparseEncoderTrainer\n    \n    SpladeLoss --> SparseEncoderTrainer\n    CSRLoss --> SparseEncoderTrainer\n    FlopsLoss --> SpladeLoss\n    SparseMultipleNegativesRankingLoss --> SpladeLoss\n    SparseMultipleNegativesRankingLoss --> CSRLoss\n    \n    SparseNanoBEIREvaluator --> SparseEncoderTrainer\n    SparseInformationRetrievalEvaluator --> SparseEncoderTrainer\n    SparseEmbeddingSimilarityEvaluator --> SparseEncoderTrainer\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:17-46](), [sentence_transformers/sparse_encoder/__init__.py:1-14](), [sentence_transformers/sparse_encoder/losses/__init__.py:1-29]()\n\n## Sparse Encoder Architectures\n\nThe training system supports three primary sparse encoder architectures, each requiring different components and training strategies.\n\n### SPLADE Architecture\n\nSPLADE models use `MLMTransformer` followed by `SpladePooling` to create sparse lexical representations from masked language model logits.\n\n```mermaid\ngraph LR\n    Input[\"Text Input\"] --> MLMTransformer[\"MLMTransformer<br/>(BERT/RoBERTa/DistilBERT)\"]\n    MLMTransformer --> Logits[\"MLM Head Logits<br/>(vocab_size)\"]\n    Logits --> SpladePooling[\"SpladePooling<br/>(max/sum pooling)\"]\n    SpladePooling --> Activation[\"ReLU + log1p\"]\n    Activation --> SparseEmbedding[\"Sparse Embedding<br/>(vocab_size dimensions)\"]\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:59-98](), [sentence_transformers/sparse_encoder/models/MLMTransformer.py:26-55](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:13-40]()\n\n### Inference-Free SPLADE Architecture\n\nThis architecture uses `Router` to process queries and documents differently, with lightweight `SparseStaticEmbedding` for queries and full SPLADE processing for documents.\n\n```mermaid\ngraph TB\n    subgraph \"Router Module\"\n        QueryRoute[\"Query Route\"]\n        DocumentRoute[\"Document Route\"]\n    end\n    \n    QueryInput[\"Query Text\"] --> QueryRoute\n    DocumentInput[\"Document Text\"] --> DocumentRoute\n    \n    QueryRoute --> SparseStaticEmbedding[\"SparseStaticEmbedding<br/>(Static Weights)\"]\n    DocumentRoute --> MLMTransformer[\"MLMTransformer\"]\n    MLMTransformer --> SpladePooling[\"SpladePooling\"]\n    \n    SparseStaticEmbedding --> QueryEmbedding[\"Query Sparse Embedding\"]\n    SpladePooling --> DocumentEmbedding[\"Document Sparse Embedding\"]\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:99-168](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py:24-62]()\n\n### CSR (Contrastive Sparse Representation) Architecture\n\nCSR models apply `SparseAutoEncoder` on top of dense sentence transformer embeddings to create sparse representations.",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1018,
      "character_count": 4694,
      "created_at": "2025-10-16T17:42:33.225960",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph LR\n    Input[\"Text Input\"] --> Transformer[\"Transformer<br/>(BERT/etc)\"]\n    Transformer --> Pooling[\"Pooling<br/>(mean/cls)\"]\n    Pooling --> DenseEmbedding[\"Dense Embedding\"]\n    DenseEmbedding --> SparseAutoEncoder[\"SparseAutoEncoder<br/>(k=256, k_aux=512)\"]\n    SparseAutoEncoder --> SparseEmbedding[\"Sparse Embedding<br/>(k dimensions)\"]\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:169-228](), [sentence_transformers/sparse_encoder/models/__init__.py:1-9]()\n\n## Training Components\n\n### Model Initialization\n\nModels are initialized differently based on the target architecture:\n\n| Architecture | Initialization Method | Key Components |\n|-------------|----------------------|----------------|\n| SPLADE | `SparseEncoder(\"bert-base-uncased\")` | `MLMTransformer` + `SpladePooling` |\n| Inference-Free SPLADE | `Router.for_query_document()` | `SparseStaticEmbedding` + `MLMTransformer` + `SpladePooling` |\n| CSR | `SparseEncoder(\"sentence-transformer-model\")` | `Transformer` + `Pooling` + `SparseAutoEncoder` |\n\n**Sources:** [docs/sparse_encoder/training_overview.md:48-229]()\n\n### Loss Function Requirements\n\nSparse encoder training requires specialized loss functions that incorporate sparsity regularization:\n\n```mermaid\ngraph TB\n    subgraph \"Wrapper Losses\"\n        SpladeLoss[\"SpladeLoss<br/>(for SPLADE models)\"]\n        CSRLoss[\"CSRLoss<br/>(for CSR models)\"]\n    end\n    \n    subgraph \"Main Loss Functions\"\n        SparseMultipleNegativesRankingLoss[\"SparseMultipleNegativesRankingLoss\"]\n        SparseMarginMSELoss[\"SparseMarginMSELoss\"]\n        SparseDistillKLDivLoss[\"SparseDistillKLDivLoss\"]\n    end\n    \n    subgraph \"Regularization\"\n        FlopsLoss[\"FlopsLoss<br/>(default regularizer)\"]\n        CustomRegularizer[\"Custom Regularizer\"]\n    end\n    \n    subgraph \"Specialized Losses\"\n        CSRReconstructionLoss[\"CSRReconstructionLoss\"]\n        SparseMSELoss[\"SparseMSELoss<br/>(standalone distillation)\"]\n    end\n    \n    SpladeLoss --> SparseMultipleNegativesRankingLoss\n    SpladeLoss --> SparseMarginMSELoss\n    SpladeLoss --> SparseDistillKLDivLoss\n    SpladeLoss --> FlopsLoss\n    SpladeLoss --> CustomRegularizer\n    \n    CSRLoss --> SparseMultipleNegativesRankingLoss\n    CSRLoss --> CSRReconstructionLoss\n```\n\n**Sources:** [docs/sparse_encoder/training_overview.md:346-393](), [docs/sparse_encoder/loss_overview.md:4-28](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-187]()\n\n### Training Arguments\n\n`SparseEncoderTrainingArguments` extends standard training arguments with sparse-specific parameters:\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `router_mapping` | Maps dataset columns to Router tasks | `{\"question\": \"query\", \"answer\": \"document\"}` |\n| `learning_rate_mapping` | Sets different learning rates per component | `{\"SparseStaticEmbedding.*\": 1e-3}` |\n| `batch_sampler` | Controls batch composition | `BatchSamplers.NO_DUPLICATES` |\n| `prompts` | Task-specific prompts | `{\"query\": \"Represent this query:\"}` |\n\n**Sources:** [docs/sparse_encoder/training_overview.md:394-473](), [docs/sparse_encoder/training_overview.md:149-168]()\n\n## Training Workflow\n\nThe complete training workflow integrates all components through the `SparseEncoderTrainer`:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant SparseEncoderTrainer\n    participant Model as \"SparseEncoder\"\n    participant Loss as \"SpladeLoss/CSRLoss\"\n    participant Evaluator\n    participant TrainingArgs as \"SparseEncoderTrainingArguments\"\n    \n    User->>Model: Initialize architecture\n    User->>Loss: Configure loss function\n    User->>TrainingArgs: Set training parameters\n    User->>Evaluator: Setup evaluation\n    User->>SparseEncoderTrainer: Create trainer\n    \n    SparseEncoderTrainer->>Model: Forward pass\n    Model->>Loss: Compute loss + regularization\n    Loss->>SparseEncoderTrainer: Return loss dict\n    SparseEncoderTrainer->>Model: Backward pass\n    \n    alt Evaluation Step\n        SparseEncoderTrainer->>Evaluator: Run evaluation\n        Evaluator->>Model: Generate embeddings\n        Evaluator->>SparseEncoderTrainer: Return metrics\n    end\n    \n    SparseEncoderTrainer->>User: Training complete\n```",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1016,
      "character_count": 4205,
      "created_at": "2025-10-16T17:42:33.231124",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "**Sources:** [docs/sparse_encoder/training_overview.md:475-552]()\n\n### Dataset Format Requirements\n\nTraining datasets must match the loss function requirements:\n\n| Loss Function | Input Columns | Label Column | Example |\n|---------------|---------------|--------------|---------|\n| `SparseMultipleNegativesRankingLoss` | `(anchor, positive)` or `(anchor, positive, negative)` | None | `[\"query\", \"answer\"]` |\n| `SparseCoSENTLoss` | `(sentence_A, sentence_B)` | `score` (0-1) | `[\"text1\", \"text2\", \"score\"]` |\n| `SparseMarginMSELoss` | `(query, positive, negative)` | `margin_scores` | `[\"query\", \"pos\", \"neg\", \"margins\"]` |\n\n**Sources:** [docs/sparse_encoder/training_overview.md:328-344](), [docs/sparse_encoder/loss_overview.md:29-62]()\n\n## Advanced Training Features\n\n### Router-Based Training\n\nWhen using `Router` modules, special configuration is required to map dataset columns to routing paths:\n\n```python",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 233,
      "character_count": 912,
      "created_at": "2025-10-16T17:42:33.231894",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Training.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]