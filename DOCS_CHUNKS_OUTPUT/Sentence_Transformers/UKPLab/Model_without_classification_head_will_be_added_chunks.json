[
  {
    "text": "model = CrossEncoder(\"google-bert/bert-base-uncased\")\n```\n\nSources: [docs/cross_encoder/training_overview.md:42-72]()\n\n### Dataset Format Requirements\n\nCrossEncoder training datasets must match the chosen loss function requirements. The validation involves three steps:\n\n1. **Input Columns**: All columns except \"label\", \"labels\", \"score\", or \"scores\" are treated as inputs\n2. **Label Columns**: If the loss function requires labels, the dataset must have a column named \"label\", \"labels\", \"score\", or \"scores\"\n3. **Model Output Compatibility**: The number of model output labels must match the loss function requirements\n\n**Dataset Format Validation Flow**\n```mermaid\ngraph TD\n    InputDataset[Input Dataset]\n    \n    InputDataset --> CheckColumns{Check Column Names}\n    CheckColumns --> LabelCols[Extract Label Columns]\n    CheckColumns --> InputCols[Extract Input Columns]\n    \n    LabelCols --> ValidateLabels{Validate Labels}\n    InputCols --> ValidateInputs{Validate Input Count}\n    \n    ValidateLabels --> CheckModelOutput{Check Model Output}\n    ValidateInputs --> CheckModelOutput\n    \n    CheckModelOutput --> Compatible[Compatible Format]\n    CheckModelOutput --> Error[Format Error]\n    \n    Compatible --> Training[Begin Training]\n    Error --> FixDataset[Fix Dataset Format]\n    FixDataset --> CheckColumns\n```\n\nSources: [docs/cross_encoder/training_overview.md:171-189]()\n\n### Loss Functions\n\nCrossEncoder loss functions are designed for ranking and classification tasks. The choice depends on your data format and task type:\n\n| Input Format | Labels | Model Outputs | Common Loss Functions |\n|--------------|--------|---------------|----------------------|\n| `(sentence_A, sentence_B)` pairs | `class` | `num_classes` | `CrossEntropyLoss` |\n| `(anchor, positive)` pairs | `none` | `1` | `MultipleNegativesRankingLoss` |\n| `(anchor, positive/negative)` pairs | `1/0` | `1` | `BinaryCrossEntropyLoss` |\n| `(query, [doc1, ..., docN])` | `[score1, ..., scoreN]` | `1` | `LambdaLoss`, `ListNetLoss` |\n\nSources: [docs/cross_encoder/loss_overview.md:20-28]()\n\n### Hard Negatives Mining\n\nCrossEncoder performance often depends on the quality of negative examples. The `mine_hard_negatives` function helps generate challenging negatives:\n\n```python\nfrom sentence_transformers.util import mine_hard_negatives\n\nhard_train_dataset = mine_hard_negatives(\n    train_dataset,\n    embedding_model,\n    num_negatives=5,\n    range_min=10,\n    range_max=100,\n    max_score=0.8,\n    output_format=\"labeled-pair\"\n)\n```\n\nSources: [docs/cross_encoder/training_overview.md:204-242]()\n\n## Training Process Integration\n\nCrossEncoder training integrates with the broader sentence-transformers training infrastructure while maintaining its specialized functionality.\n\n**CrossEncoder Training Infrastructure**\n```mermaid\ngraph TB\n    subgraph \"CrossEncoder Specific\"\n        CEModel[CrossEncoder]\n        CETrainer[CrossEncoderTrainer]\n        CEArgs[CrossEncoderTrainingArguments]\n        CELoss[CrossEncoder Losses]\n        CEEval[CrossEncoder Evaluators]\n    end\n    \n    subgraph \"Shared Infrastructure\"\n        HFTrainer[transformers.Trainer]\n        DataCollator[SentenceTransformerDataCollator]\n        ModelCard[Model Card Callbacks]\n        Optimizers[Optimizers & Schedulers]\n    end\n    \n    subgraph \"External Integration\"\n        HFHub[Hugging Face Hub]\n        WandB[Weights & Biases]\n        TensorBoard[TensorBoard]\n    end\n    \n    CETrainer --> HFTrainer\n    CEArgs --> HFTrainer\n    DataCollator --> CETrainer\n    ModelCard --> CETrainer\n    Optimizers --> CETrainer\n    \n    CEModel --> CELoss\n    CELoss --> CETrainer\n    CEEval --> CETrainer\n    \n    CETrainer --> HFHub\n    CETrainer --> WandB\n    CETrainer --> TensorBoard\n```\n\nSources: [sentence_transformers/trainer.py:59-128](), [docs/cross_encoder/training_overview.md:314-400]()\n\n### Training Arguments\n\n`CrossEncoderTrainingArguments` extends the standard transformers training arguments with CrossEncoder-specific parameters:",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 925,
      "character_count": 3995,
      "created_at": "2025-10-16T17:42:32.931519",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "**Key Training Arguments:**\n- **Performance**: `learning_rate`, `per_device_train_batch_size`, `num_train_epochs`, `gradient_accumulation_steps`\n- **Optimization**: `fp16`, `bf16`, `optim`, `lr_scheduler_type`, `warmup_ratio`\n- **Evaluation**: `eval_strategy`, `eval_steps`, `load_best_model_at_end`, `metric_for_best_model`\n- **Tracking**: `report_to`, `run_name`, `logging_steps`, `push_to_hub`\n\nSources: [docs/cross_encoder/training_overview.md:320-344]()\n\n### Evaluation System\n\nCrossEncoder evaluators assess model performance during training with task-specific metrics:\n\n- `BinaryClassificationEvaluator`: For binary classification tasks\n- `CrossEncoderReranking`: For ranking performance evaluation  \n- `EmbeddingSimilarityEvaluator`: For similarity scoring tasks\n- `InformationRetrievalEvaluator`: For retrieval performance\n\nSources: [docs/cross_encoder/training_overview.md:365-400]()\n\n## Relationship to Other Training Systems\n\nCrossEncoder training shares infrastructure with other sentence-transformers training systems while maintaining its distinct characteristics.\n\n**Training System Relationships**\n```mermaid\ngraph TB\n    subgraph \"Base Training Infrastructure\"\n        BaseTrainer[SentenceTransformerTrainer]\n        BaseArgs[SentenceTransformerTrainingArguments]\n        BaseCollator[SentenceTransformerDataCollator]\n        BaseCard[SentenceTransformerModelCardCallback]\n    end\n    \n    subgraph \"CrossEncoder Training\"\n        CETrainer[CrossEncoderTrainer]\n        CEArgs[CrossEncoderTrainingArguments]\n        CEModel[CrossEncoder]\n        CELosses[CrossEncoder Losses]\n        CEEvals[CrossEncoder Evaluators]\n    end\n    \n    subgraph \"SparseEncoder Training\"\n        SETrainer[SparseEncoderTrainer]\n        SEArgs[SparseEncoderTrainingArguments]\n        SEModel[SparseEncoder]\n        SELosses[SparseEncoder Losses]\n    end\n    \n    subgraph \"SentenceTransformer Training\"  \n        STModel[SentenceTransformer]\n        STLosses[SentenceTransformer Losses]\n        STEvals[SentenceTransformer Evaluators]\n    end\n    \n    BaseTrainer --> CETrainer\n    BaseTrainer --> SETrainer\n    BaseArgs --> CEArgs\n    BaseArgs --> SEArgs\n    BaseCollator --> CETrainer\n    BaseCollator --> SETrainer\n    BaseCard --> CETrainer\n    \n    STModel --> BaseTrainer\n    CEModel --> CETrainer\n    SEModel --> SETrainer\n    \n    STLosses --> BaseTrainer\n    CELosses --> CETrainer\n    SELosses --> SETrainer\n    \n    STEvals --> BaseTrainer\n    CEEvals --> CETrainer\n```\n\nSources: [sentence_transformers/trainer.py:59-128](), [sentence_transformers/sparse_encoder/trainer.py:31-98]()\n\nThe CrossEncoder training system leverages the shared infrastructure while providing specialized components for joint text encoding and ranking tasks, making it suitable for reranking applications and text pair classification scenarios.\n\n# Loss Functions for SentenceTransformer\n\n## Introduction\n\nThis page documents the loss functions available for training SentenceTransformer models. Loss functions are a critical component that defines the training objective and directly influences the quality and properties of the resulting sentence embeddings.\n\nFor information about how to use these loss functions in training, see [SentenceTransformer Training](#3.1) and [CrossEncoder Training](#3.2).\n\nThe SentenceTransformer library offers a diverse set of loss functions, each designed for specific use cases and data formats:\n\n```mermaid\ngraph TD\n    subgraph \"Overview of Loss Functions\"\n        ST[SentenceTransformer]\n        Loss[Loss Functions]\n        Training[Training Process]\n        \n        ST --> Loss\n        Loss --> Training\n        Training --> ST\n    end\n```\n\nSources: \n- [sentence_transformers/losses/__init__.py:1-67]()\n\n## Loss Function Taxonomy\n\nSentenceTransformer provides several categories of loss functions, each with different approaches to learning sentence embeddings:\n\nLoss Function Taxonomy",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 838,
      "character_count": 3911,
      "created_at": "2025-10-16T17:42:32.938634",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    subgraph \"Complete Loss Function Taxonomy\"\n        LF[\"Loss Functions\"]\n        \n        Ranking[\"Ranking-based Losses\"]\n        Contrastive[\"Contrastive Losses\"]\n        Triplet[\"Triplet Losses\"]\n        MSE[\"MSE-based Losses\"]\n        Special[\"Specialized Losses\"]\n        \n        LF --> Ranking\n        LF --> Contrastive\n        LF --> Triplet\n        LF --> MSE\n        LF --> Special\n        \n        Ranking --> MNRL[\"MultipleNegativesRankingLoss\"]\n        Ranking --> CMNRL[\"CachedMultipleNegativesRankingLoss\"]\n        Ranking --> MNRSL[\"MultipleNegativesSymmetricRankingLoss\"]\n        Ranking --> CMNRSL[\"CachedMultipleNegativesSymmetricRankingLoss\"]\n        Ranking --> GISTL[\"GISTEmbedLoss\"]\n        Ranking --> CGISTL[\"CachedGISTEmbedLoss\"]\n        Ranking --> MBML[\"MegaBatchMarginLoss\"]\n        \n        Contrastive --> CL[\"ContrastiveLoss\"]\n        Contrastive --> OCL[\"OnlineContrastiveLoss\"]\n        Contrastive --> CTL[\"ContrastiveTensionLoss\"]\n        Contrastive --> CTLBN[\"ContrastiveTensionLossInBatchNegatives\"]\n        Contrastive --> COSENT[\"CoSENTLoss\"]\n        Contrastive --> AnglE[\"AnglELoss\"]\n        \n        Triplet --> TL[\"TripletLoss\"]\n        Triplet --> BHTL[\"BatchHardTripletLoss\"]\n        Triplet --> BHSM[\"BatchHardSoftMarginTripletLoss\"]\n        Triplet --> BSHT[\"BatchSemiHardTripletLoss\"]\n        Triplet --> BATL[\"BatchAllTripletLoss\"]\n        \n        MSE --> MSEL[\"MSELoss\"]\n        MSE --> MMSE[\"MarginMSELoss\"]\n        MSE --> CSL[\"CosineSimilarityLoss\"]\n        MSE --> DKL[\"DistillKLDivLoss\"]\n        \n        Special --> ML[\"MatryoshkaLoss\"]\n        Special --> M2D[\"Matryoshka2dLoss\"]\n        Special --> ALL[\"AdaptiveLayerLoss\"]\n        Special --> DAEL[\"DenoisingAutoEncoderLoss\"]\n        Special --> SL[\"SoftmaxLoss\"]\n    end\n```\n\nSources:\n- [sentence_transformers/losses/__init__.py:1-67]()\n\n## Ranking-Based Loss Functions\n\nRanking-based loss functions are commonly used for training retrieval models. They focus on learning representations where relevant pairs have higher similarity than irrelevant pairs.\n\n### MultipleNegativesRankingLoss\n\nThis is one of the most widely used loss functions for training sentence embeddings. It treats other samples in the batch as negatives, creating an effective training signal that improves with larger batch sizes.\n\n```mermaid\nflowchart TD\n    subgraph \"MultipleNegativesRankingLoss Flow\"\n        A[\"Anchor Embeddings\"] \n        P[\"Positive Embeddings\"]\n        S[\"Similarity Matrix\"]\n        L[\"Loss Computation\"]\n        \n        A --> S\n        P --> S\n        S --> L\n        \n        L -->|\"Cross Entropy\"| RL[\"Ranking Loss\"]\n    end\n```\n\n**Key Properties**:\n- Also known as InfoNCE loss, SimCSE loss, or in-batch negatives loss\n- Performance generally improves with increasing batch size\n- Requires (anchor, positive) pairs or (anchor, positive, negative) triplets\n- Each anchor should be most similar to its corresponding positive from all candidates in the batch\n\nSources:\n- [sentence_transformers/losses/MultipleNegativesRankingLoss.py:13-132]()\n\n### CachedMultipleNegativesRankingLoss\n\nA memory-efficient version of MultipleNegativesRankingLoss based on the GradCache technique, allowing for extremely large batch sizes with constant memory usage.\n\n**Key Properties**:\n- Divides computation into embedding and loss calculation stages\n- Allows training with much larger batch sizes on limited hardware\n- Approximately 20-30% slower than non-cached version\n- Technically superior for large batch sizes\n\n```mermaid\nflowchart TD\n    subgraph \"CachedMultipleNegativesRankingLoss Stages\"\n        Stage1[\"Stage 1: Embed without gradients\"]\n        Stage2[\"Stage 2: Calculate loss and cache gradients\"]\n        Stage3[\"Stage 3: Embed with gradients and apply cached gradients\"]\n        \n        Stage1 --> Stage2\n        Stage2 --> Stage3\n    end\n```\n\nSources:\n- [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:63-300]()\n\n### GISTEmbedLoss\n\nAn improved ranking loss that uses a guide model to guide the in-batch negative sample selection, providing a stronger training signal.\n\n**Key Properties**:\n- Uses a teacher/guide model to identify and suppress false negatives\n- Supports different margin strategies for negative filtering\n- Better training signal than MultipleNegativesRankingLoss",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1000,
      "character_count": 4333,
      "created_at": "2025-10-16T17:42:32.946509",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "Sources:\n- [sentence_transformers/losses/GISTEmbedLoss.py:13-221]()\n- [sentence_transformers/losses/CachedGISTEmbedLoss.py:63-382]()\n\n## Contrastive Loss Functions\n\nContrastive losses optimize embeddings so that similar items are closer together and dissimilar items are farther apart in the embedding space.\n\n### ContrastiveLoss\n\nThe standard contrastive loss function that minimizes distance between positive pairs and maximizes distance between negative pairs.\n\n**Key Properties**:\n- Expects pairs of texts with binary labels (1 for similar, 0 for dissimilar)\n- Uses a specified distance metric (cosine, euclidean, manhattan)\n- Includes a margin hyperparameter\n\n```mermaid\nflowchart TD\n    subgraph \"ContrastiveLoss Flow\"\n        Input1[\"Text A\"] --> Embedding1[\"Embedding A\"]\n        Input2[\"Text B\"] --> Embedding2[\"Embedding B\"]\n        \n        Embedding1 --> Distance[\"Distance Calculation\"]\n        Embedding2 --> Distance\n        \n        Distance --> Contrastive[\"Contrastive Loss\"]\n        Label[\"Label (0 or 1)\"] --> Contrastive\n    end\n```\n\nSources:\n- [sentence_transformers/losses/ContrastiveLoss.py:13-120]()\n\n### CoSENTLoss\n\nAn improved contrastive loss that provides a stronger training signal than standard CosineSimilarityLoss.\n\n**Key Properties**:\n- Uses a logsum formulation comparing multiple pairs in the batch\n- Faster convergence and better performance than CosineSimilarityLoss\n- Requires sentence pairs with similarity scores\n\nSources:\n- [sentence_transformers/losses/CoSENTLoss.py:13-115]()\n\n### ContrastiveTensionLoss\n\nDesigned for unsupervised learning, this loss creates positive and negative pairs automatically.\n\n**Key Properties**:\n- Works without explicit labels\n- Creates a copy of the encoder model to produce embeddings for the first sentence in each pair\n- Requires using `ContrastiveTensionDataLoader` for proper pair generation\n\nSources:\n- [sentence_transformers/losses/ContrastiveTensionLoss.py:17-204]()\n\n## Triplet Loss Functions\n\nTriplet losses use triplets of (anchor, positive, negative) to learn embeddings where the anchor is closer to the positive than to the negative by a certain margin.\n\n### TripletLoss\n\nThe basic triplet loss function minimizes the distance between anchor and positive while maximizing the distance between anchor and negative.\n\n**Key Properties**:\n- Requires (anchor, positive, negative) triplets\n- Uses a specified distance metric and margin\n- Optimizes: max(||anchor - positive|| - ||anchor - negative|| + margin, 0)\n\n```mermaid\nflowchart TD\n    subgraph \"TripletLoss Structure\"\n        A[\"Anchor\"] --> E_A[\"Anchor Embedding\"]\n        P[\"Positive\"] --> E_P[\"Positive Embedding\"]\n        N[\"Negative\"] --> E_N[\"Negative Embedding\"]\n        \n        E_A --> D_AP[\"Distance(Anchor, Positive)\"]\n        E_P --> D_AP\n        \n        E_A --> D_AN[\"Distance(Anchor, Negative)\"]\n        E_N --> D_AN\n        \n        D_AP --> TL[\"Triplet Loss\"]\n        D_AN --> TL\n        \n        M[\"Margin\"] --> TL\n    end\n```\n\nSources:\n- [sentence_transformers/losses/TripletLoss.py:13-112]()\n\n### Batch Triplet Losses\n\nThese are more advanced variants of triplet loss that use different strategies for mining triplets within a batch:\n\n1. **BatchHardTripletLoss**: Selects hardest positive and negative samples for each anchor.\n2. **BatchSemiHardTripletLoss**: Focuses on semi-hard triplets (not too easy, not too hard).\n3. **BatchAllTripletLoss**: Uses all valid triplets in the batch.\n4. **BatchHardSoftMarginTripletLoss**: Similar to BatchHardTripletLoss but with a soft margin.\n\n**Key Properties**:\n- Require single sentences with class labels\n- Create triplets on-the-fly from the batch\n- Recommend using batches with multiple examples per class\n- Different mining strategies for different training dynamics\n\nSources:\n- [sentence_transformers/losses/BatchHardTripletLoss.py:12-267]()\n- [sentence_transformers/losses/BatchSemiHardTripletLoss.py:13-188]()\n- [sentence_transformers/losses/BatchAllTripletLoss.py:13-151]()\n- [sentence_transformers/losses/BatchHardSoftMarginTripletLoss.py:13-153]()\n\n## MSE-Based Loss Functions\n\nThese loss functions use Mean Squared Error (MSE) to optimize embeddings against a target.\n\n### MSELoss\n\nMSE Loss computes the squared error between computed sentence embeddings and target embeddings.",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1000,
      "character_count": 4286,
      "created_at": "2025-10-16T17:42:32.955965",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 3,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "**Key Properties**:\n- Often used for knowledge distillation and multilingual model extension\n- Requires sentences with corresponding target embeddings\n- Simple and effective for teacher-student learning\n\n```mermaid\nflowchart TD\n    subgraph \"MSELoss Flow\"\n        Input[\"Input Text\"] --> StudentModel[\"Student Model\"]\n        StudentModel --> Embedding[\"Student Embedding\"]\n        \n        TargetEmb[\"Target Embedding\"] --> MSE[\"MSE Loss\"]\n        Embedding --> MSE\n    end\n```\n\nSources:\n- [sentence_transformers/losses/MSELoss.py:11-98]()\n\n### MarginMSELoss\n\nAn extension of MSE loss that focuses on the margin between pairs of passages for a query.\n\n**Key Properties**:\n- Computes MSE between predicted margins and gold margins\n- More suitable for ranking tasks\n- Does not require strict positive/negative distinction\n- Often used with a teacher model in knowledge distillation\n\nSources:\n- [sentence_transformers/losses/MarginMSELoss.py:10-143]()\n\n### CosineSimilarityLoss\n\nComputes cosine similarity between sentence pairs and optimizes against a similarity score.\n\n**Key Properties**:\n- Expects text pairs with a similarity score\n- Minimizes the difference between predicted and target similarity\n- Used for Semantic Textual Similarity (STS) tasks\n\nSources:\n- [sentence_transformers/losses/CosineSimilarityLoss.py:13-85]()\n\n## Specialized Loss Functions\n\n### MatryoshkaLoss\n\nA loss function modifier that enables training models to produce effective embeddings at multiple dimensions. This allows users to reduce the embedding dimension at inference time without retraining.\n\n**Key Properties**:\n- Trains on multiple embedding dimensions simultaneously\n- Allows flexible trade-off between quality and dimensionality at inference time\n- Compatible with other base losses (wraps another loss function)\n\n```mermaid\nflowchart TD\n    subgraph \"MatryoshkaLoss Architecture\"\n        BaseL[\"Base Loss Function\"]\n        ML[\"MatryoshkaLoss\"]\n        \n        Dims[\"Multiple Dimensions\"] --> ML\n        Weights[\"Dimension Weights\"] --> ML\n        BaseL --> ML\n        \n        ML --> MDim1[\"Train at Dim 768\"]\n        ML --> MDim2[\"Train at Dim 384\"]\n        ML --> MDim3[\"Train at Dim 128\"]\n        \n        MDim1 --> CombLoss[\"Combined Loss\"]\n        MDim2 --> CombLoss\n        MDim3 --> CombLoss\n    end\n```\n\nSources:\n- [sentence_transformers/losses/MatryoshkaLoss.py:113-253]()\n\n### AdaptiveLayerLoss\n\nTrains model to produce good embeddings with fewer transformer layers, enabling faster inference.\n\n**Key Properties**:\n- Applies loss to intermediate transformer layers\n- Allows layer reduction at inference time\n- KL divergence regularization between layer outputs\n\nSources:\n- [sentence_transformers/losses/AdaptiveLayerLoss.py:106-274]()\n\n### Matryoshka2dLoss\n\nCombines MatryoshkaLoss and AdaptiveLayerLoss to enable both dimension and layer reduction.\n\n**Key Properties**:\n- 2D flexibility in both dimensions and layers\n- Allows for different performance vs. efficiency trade-offs\n\nSources:\n- [sentence_transformers/losses/Matryoshka2dLoss.py:13-152]()\n\n### DenoisingAutoEncoderLoss\n\nTrains a model to reconstruct original sentences from damaged versions, useful for unsupervised learning.\n\n**Key Properties**:\n- Requires pairs of damaged and original sentences\n- Creates a decoder component to reconstruct from embeddings\n- Used in TSDAE (Transformer-based Sequential Denoising Auto-Encoder)\n\nSources:\n- [sentence_transformers/losses/DenoisingAutoEncoderLoss.py:15-203]()\n\n### SoftmaxLoss\n\nUsed for classification with text pairs, adds a softmax classifier on top of the embedding outputs.\n\n**Key Properties**:\n- Original loss from the SBERT paper\n- Uses sentence pairs with class labels\n- Multiple ways to combine embeddings (concatenation, difference, multiplication)\n\nSources:\n- [sentence_transformers/losses/SoftmaxLoss.py:17-156]()\n\n## Loss Function Selection Guide\n\nThe table below provides guidance on which loss function to use based on your data and task:",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 891,
      "character_count": 3968,
      "created_at": "2025-10-16T17:42:32.966550",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 4,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "| Loss Function | Best For | Input Format | Special Requirements |\n|---------------|----------|--------------|---------------------|\n| MultipleNegativesRankingLoss | Retrieval, general purpose | (anchor, positive) pairs | Large batch size beneficial |\n| CoSENTLoss | Semantic similarity | Text pairs with scores | - |\n| TripletLoss | Clustering, similarity | (anchor, positive, negative) triplets | - |\n| BatchHardTripletLoss | Classification | Single texts with labels | Multiple examples per class |\n| MSELoss | Distillation, transfer | Texts with target embeddings | Teacher model |\n| MatryoshkaLoss | Size-efficient models | Depends on base loss | - |\n| AdaptiveLayerLoss | Speed-efficient models | Depends on base loss | - |\n| ContrastiveTensionLoss | Unsupervised learning | Single sentences | Special dataloader |\n| GISTEmbedLoss | Better negative sampling | Same as MNRL | Guide model |\n\n## Memory-Efficient Variants\n\nFor training with large batch sizes on limited hardware, consider the cached variants:\n\n```mermaid\ngraph TD\n    subgraph \"Memory-Efficient Variants\"\n        MNRL[\"MultipleNegativesRankingLoss\"] --> CMNRL[\"CachedMultipleNegativesRankingLoss\"]\n        MNRSL[\"MultipleNegativesSymmetricRankingLoss\"] --> CMNRSL[\"CachedMultipleNegativesSymmetricRankingLoss\"]\n        GISTL[\"GISTEmbedLoss\"] --> CGISTL[\"CachedGISTEmbedLoss\"]\n        \n        CMNRL -->|\"Uses\"| GC[\"GradCache Technique\"]\n        CMNRSL -->|\"Uses\"| GC\n        CGISTL -->|\"Uses\"| GC\n        \n        GC --> S1[\"1. Embed without gradients\"]\n        GC --> S2[\"2. Calculate loss & cache gradients\"]\n        GC --> S3[\"3. Embed with gradients & apply cached gradients\"]\n    end\n```\n\n**Key Properties**:\n- Allow for much larger batch sizes with constant memory usage\n- Approximately 20-30% slower than non-cached versions\n- Recommended when batch size is a limiting factor\n\nSources:\n- [sentence_transformers/losses/CachedMultipleNegativesRankingLoss.py:63-300]()\n- [sentence_transformers/losses/CachedGISTEmbedLoss.py:63-382]()\n- [sentence_transformers/losses/CachedMultipleNegativesSymmetricRankingLoss.py:40-256]()\n\n## Implementation Details\n\nAll loss functions in SentenceTransformer follow a common pattern:\n\n1. They are subclasses of `torch.nn.Module`\n2. They implement a `forward` method that:\n   - Takes sentence features and optional labels as input\n   - Computes sentence embeddings using the model\n   - Calculates and returns the loss value\n\nMany loss functions also provide:\n- `get_config_dict` method for configuration serialization\n- `citation` property for academic references\n- Documentation about input requirements and recommendations\n\nThe loss function is typically passed to a `SentenceTransformerTrainer` along with the model and dataset, as shown in this example pattern:\n\n```python\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\nfrom datasets import Dataset\n\nmodel = SentenceTransformer(\"model_name\")\ntrain_dataset = Dataset.from_dict({\n    \"anchor\": [\"Text A\", \"Text B\"],  \n    \"positive\": [\"Similar to A\", \"Similar to B\"],\n})\nloss = losses.MultipleNegativesRankingLoss(model)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\nSources:\n- [sentence_transformers/losses/MultipleNegativesRankingLoss.py:13-132]()\n- [sentence_transformers/losses/__init__.py:1-67]()\n\n# Loss Functions for SparseEncoder\n\nThis document covers the specialized loss functions designed for training `SparseEncoder` models. These losses are specifically tailored for sparse neural information retrieval models like SPLADE and CSR architectures that require both effectiveness and efficiency through sparsity regularization.\n\nFor dense embedding loss functions, see [Loss Functions for SentenceTransformer](#3.4). For reranking model loss functions, see [Loss Functions for CrossEncoder](#3.6).\n\n## Overview\n\nSparseEncoder loss functions follow a hierarchical architecture where wrapper losses combine base contrastive/similarity losses with regularization terms to control sparsity. The two main wrapper losses are:\n\n- **`SpladeLoss`**: For SPLADE-style models that use MLM heads with pooling\n- **`CSRLoss`**: For CSR (Contrastive Sparse Representation) models with autoencoder components\n\nThese wrapper losses are required for training `SparseEncoder` models, as they provide the necessary sparsity regularization on top of standard contrastive learning objectives.\n\n## Loss Function Architecture",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 982,
      "character_count": 4493,
      "created_at": "2025-10-16T17:42:32.975559",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 5,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    subgraph \"Wrapper Losses\"\n        SpladeLoss[\"SpladeLoss<br/>SPLADE Models\"]\n        CSRLoss[\"CSRLoss<br/>CSR Models\"]\n    end\n    \n    subgraph \"Base Losses\"\n        SMNRL[\"SparseMultipleNegativesRankingLoss\"]\n        SMarginMSE[\"SparseMarginMSELoss\"] \n        SDistillKL[\"SparseDistillKLDivLoss\"]\n        STriplet[\"SparseTripletLoss\"]\n        SCosine[\"SparseCosineSimilarityLoss\"]\n        SCoSENT[\"SparseCoSENTLoss\"]\n        SAnglE[\"SparseAnglELoss\"]\n        SparseMSE[\"SparseMSELoss\"]\n    end\n    \n    subgraph \"Regularization\"\n        FlopsLoss[\"FlopsLoss<br/>Sparsity Regularization\"]\n        CSRReconstruction[\"CSRReconstructionLoss<br/>Autoencoder Reconstruction\"]\n    end\n    \n    SpladeLoss --> SMNRL\n    SpladeLoss --> SMarginMSE\n    SpladeLoss --> SDistillKL\n    SpladeLoss --> STriplet\n    SpladeLoss --> SCosine\n    SpladeLoss --> SCoSENT\n    SpladeLoss --> SAnglE\n    \n    CSRLoss --> SMNRL\n    CSRLoss --> SMarginMSE\n    CSRLoss --> SDistillKL\n    CSRLoss --> STriplet\n    CSRLoss --> SCosine\n    CSRLoss --> SCoSENT\n    CSRLoss --> SAnglE\n    \n    SpladeLoss --> FlopsLoss\n    CSRLoss --> CSRReconstruction\n    \n    SparseMSE -.-> |\"Standalone Only\"| SparseMSE\n```\n\nSources: [sentence_transformers/sparse_encoder/losses/__init__.py:1-29](), [sentence_transformers/sparse_encoder/losses/SpladeLoss.py:15-136](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-188]()\n\n## Wrapper Losses\n\n### SpladeLoss\n\n`SpladeLoss` is the primary wrapper loss for SPLADE-style models. It combines a base contrastive loss with regularization terms to encourage sparsity in both query and document representations.\n\n**Key Components:**\n- **Base Loss**: Any SparseEncoder loss except CSR-related and FLOPS losses\n- **Document Regularizer**: Typically `FlopsLoss` applied to positive documents and negatives\n- **Query Regularizer**: Typically `FlopsLoss` applied to query embeddings\n\n**Configuration Parameters:**\n- `document_regularizer_weight`: Weight for document sparsity regularization (λ_d)\n- `query_regularizer_weight`: Weight for query sparsity regularization (λ_q)\n- `document_regularizer_threshold`: Optional threshold for document regularization\n- `query_regularizer_threshold`: Optional threshold for query regularization\n- `use_document_regularizer_only`: Treat all inputs as documents (for symmetric training)\n\n```mermaid\ngraph LR\n    subgraph \"SpladeLoss Forward Pass\"\n        Input[\"sentence_features<br/>List[Dict[str, Tensor]]\"]\n        Model[\"SparseEncoder.forward()\"]\n        Embeddings[\"embeddings<br/>List[Tensor]\"]\n        \n        BaseLoss[\"base_loss.compute_loss_from_embeddings()\"]\n        DocReg[\"document_regularizer.compute_loss_from_embeddings()\"]\n        QueryReg[\"query_regularizer.compute_loss_from_embeddings()\"]\n        \n        Input --> Model\n        Model --> Embeddings\n        Embeddings --> BaseLoss\n        Embeddings --> DocReg\n        Embeddings --> QueryReg\n        \n        BaseLoss --> TotalLoss[\"Combined Loss Dict\"]\n        DocReg --> TotalLoss\n        QueryReg --> TotalLoss\n    end\n```\n\nSources: [sentence_transformers/sparse_encoder/losses/SpladeLoss.py:15-163]()\n\n### CSRLoss\n\n`CSRLoss` is designed for CSR (Contrastive Sparse Representation) models that use autoencoder components for reconstruction-based training.\n\n**Key Components:**\n- **Base Loss**: Typically `SparseMultipleNegativesRankingLoss` for contrastive learning\n- **Reconstruction Loss**: `CSRReconstructionLoss` with three components:\n  - L_k: Reconstruction loss using top-k sparse components\n  - L_4k: Reconstruction loss using top-4k sparse components  \n  - L_aux: Auxiliary loss for residual information\n\n**Configuration Parameters:**\n- `beta`: Weight for L_aux component in reconstruction loss\n- `gamma`: Weight for the main contrastive loss component",
    "metadata": {
      "chunk_id": 6,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 927,
      "character_count": 3806,
      "created_at": "2025-10-16T17:42:32.980925",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 6,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    subgraph \"CSRLoss Components\"\n        CSRMain[\"CSRLoss\"]\n        MainLoss[\"base_loss<br/>(e.g., SparseMultipleNegativesRankingLoss)\"]\n        ReconLoss[\"CSRReconstructionLoss\"]\n        \n        subgraph \"Reconstruction Components\"\n            LossK[\"L_k: MSE(x, recons_k)\"]\n            Loss4K[\"L_4k: MSE(x, recons_4k)\"]\n            LossAux[\"L_aux: NMSE(recons_aux, residual)\"]\n        end\n        \n        CSRMain --> MainLoss\n        CSRMain --> ReconLoss\n        ReconLoss --> LossK\n        ReconLoss --> Loss4K  \n        ReconLoss --> LossAux\n    end\n```\n\nSources: [sentence_transformers/sparse_encoder/losses/CSRLoss.py:129-215](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:28-127]()\n\n## Regularization Losses\n\n### FlopsLoss\n\n`FlopsLoss` implements sparsity regularization by calculating the squared L2 norm of the mean embedding vector. This encourages more zero values in embeddings, reducing floating-point operations during inference.\n\n**Formula**: `torch.sum(torch.mean(embeddings, dim=0) ** 2)`\n\n**Key Features:**\n- Optional threshold parameter to ignore overly sparse embeddings\n- Used as a component within `SpladeLoss` rather than standalone\n- Supports L0 masking when threshold is specified\n\n```mermaid\ngraph LR\n    subgraph \"FlopsLoss Computation\"\n        Embeddings[\"embeddings<br/>Tensor[batch, vocab_size]\"]\n        Threshold{\"threshold<br/>!= None?\"}\n        L0Mask[\"L0 Masking<br/>(embeddings != 0).sum()\"]\n        MeanEmb[\"torch.mean(embeddings, dim=0)\"]\n        SquaredNorm[\"torch.sum(mean_emb ** 2)\"]\n        \n        Embeddings --> Threshold\n        Threshold -->|Yes| L0Mask\n        Threshold -->|No| MeanEmb\n        L0Mask --> MeanEmb\n        MeanEmb --> SquaredNorm\n    end\n```\n\nSources: [sentence_transformers/sparse_encoder/losses/FlopsLoss.py:11-54]()\n\n## Base Losses\n\nThe following base losses can be used within the wrapper losses to provide the main learning signal:\n\n| Loss Function | Purpose | Key Features |\n|---------------|---------|--------------|\n| `SparseMultipleNegativesRankingLoss` | Contrastive learning with in-batch negatives | Most common base loss, InfoNCE-style |\n| `SparseMarginMSELoss` | Knowledge distillation with margin | Used with teacher models |\n| `SparseDistillKLDivLoss` | KL divergence distillation | Probability distribution matching |\n| `SparseTripletLoss` | Triplet-based contrastive learning | Anchor-positive-negative relationships |\n| `SparseCosineSimilarityLoss` | Cosine similarity regression | Direct similarity score prediction |\n| `SparseCoSENTLoss` | Cosine sentence embeddings | Variant of cosine similarity |\n| `SparseAnglELoss` | Angle-based similarity | Complex number representation |\n| `SparseMSELoss` | Direct embedding distillation | **Standalone only** - no wrapper needed |\n\n**Important Note**: `SparseMSELoss` is the only loss that can be used independently without a wrapper, as it performs direct embedding-level distillation from a teacher model.\n\nSources: [sentence_transformers/sparse_encoder/losses/__init__.py:15-28](), [docs/package_reference/sparse_encoder/losses.md:8-10]()\n\n## Usage Patterns\n\n### Training with SpladeLoss\n\n```python\n# Typical SPLADE training setup\nstudent_model = SparseEncoder(\"distilbert/distilbert-base-uncased\")\nteacher_model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")\n\nloss = SpladeLoss(\n    model=student_model,\n    loss=SparseMarginMSELoss(student_model),\n    document_regularizer_weight=3e-5,\n    query_regularizer_weight=5e-5,\n)\n```\n\n### Training with CSRLoss\n\n```python",
    "metadata": {
      "chunk_id": 7,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 867,
      "character_count": 3545,
      "created_at": "2025-10-16T17:42:32.987014",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 7,
      "file_relative_path": "UKPLab\\sentence-transformers\\Model_without_classification_head_will_be_added.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]