[
  {
    "text": "This document covers the training system for CrossEncoder models in sentence-transformers. CrossEncoders are designed for reranking and classification tasks where two texts are jointly encoded to produce similarity scores or class predictions.\n\nFor information about training SentenceTransformer models (bi-encoders), see [3.1](#3.1). For SparseEncoder training, see [3.2](#3.2). For loss function details specific to CrossEncoders, see [3.6](#3.6).\n\n## CrossEncoder Training Architecture\n\nCrossEncoder training follows a similar pattern to other model types in sentence-transformers but with specific adaptations for joint text encoding and ranking/classification tasks.\n\n**CrossEncoder Training System Overview**\n```mermaid\ngraph TB\n    subgraph \"Core Components\"\n        CE[CrossEncoder]\n        CETrainer[CrossEncoderTrainer]\n        CEArgs[CrossEncoderTrainingArguments]\n        CELosses[CrossEncoder Losses]\n        CEEvals[CrossEncoder Evaluators]\n    end\n    \n    subgraph \"Data Processing\"\n        Dataset[datasets.Dataset]\n        DataCollator[Data Collator]\n        HardNegMining[Hard Negatives Mining]\n    end\n    \n    subgraph \"Loss Functions\"\n        BCE[BinaryCrossEntropyLoss]\n        MNR[MultipleNegativesRankingLoss]\n        Lambda[LambdaLoss]\n        ListNet[ListNetLoss]\n        CrossEntropy[CrossEntropyLoss]\n    end\n    \n    subgraph \"Training Infrastructure\"\n        HFTrainer[Transformers Trainer]\n        ModelCard[Model Card Generation]\n        HFHub[Hugging Face Hub]\n    end\n    \n    CE --> CETrainer\n    Dataset --> DataCollator\n    CEArgs --> CETrainer\n    CELosses --> CETrainer\n    CEEvals --> CETrainer\n    \n    BCE --> CELosses\n    MNR --> CELosses\n    Lambda --> CELosses\n    ListNet --> CELosses\n    CrossEntropy --> CELosses\n    \n    CETrainer --> HFTrainer\n    CETrainer --> ModelCard\n    ModelCard --> HFHub\n    \n    HardNegMining --> Dataset\n```\n\nSources: [docs/cross_encoder/training_overview.md:1-500](), [docs/cross_encoder/loss_overview.md:1-100]()\n\n## Training Components\n\nCrossEncoder training involves six main components that work together to fine-tune models for ranking and classification tasks.\n\n**CrossEncoder Training Data Flow**\n```mermaid\ngraph LR\n    subgraph \"Input Data\"\n        TextPairs[\"(text_A, text_B) pairs\"]\n        Triplets[\"(query, positive, negative)\"]\n        Rankings[\"(query, [doc1, doc2, ...])\"]\n        Labels[Class Labels / Scores]\n    end\n    \n    subgraph \"Data Processing\"\n        DataCollator[\"Data Collator\"]\n        Tokenization[Tokenization]\n        BatchFormat[Batch Formatting]\n    end\n    \n    subgraph \"Model & Loss\"\n        CrossEncoder[CrossEncoder Model]\n        LossFunction[Loss Function]\n        ForwardPass[Forward Pass]\n    end\n    \n    subgraph \"Training Loop\"\n        Optimizer[Optimizer]\n        BackwardPass[Backward Pass]\n        WeightUpdate[Weight Update]\n    end\n    \n    subgraph \"Evaluation\"\n        Evaluator[CrossEncoder Evaluator]\n        Metrics[Metrics Calculation]\n    end\n    \n    TextPairs --> DataCollator\n    Triplets --> DataCollator\n    Rankings --> DataCollator\n    Labels --> DataCollator\n    \n    DataCollator --> Tokenization\n    Tokenization --> BatchFormat\n    \n    BatchFormat --> CrossEncoder\n    CrossEncoder --> ForwardPass\n    ForwardPass --> LossFunction\n    \n    LossFunction --> BackwardPass\n    BackwardPass --> Optimizer\n    Optimizer --> WeightUpdate\n    \n    CrossEncoder --> Evaluator\n    Evaluator --> Metrics\n```\n\nSources: [docs/cross_encoder/training_overview.md:170-190](), [sentence_transformers/data_collator.py:35-120]()\n\n### Model Initialization\n\nCrossEncoder models are initialized by loading a pretrained transformers model with a sequence classification head. If the model doesn't have such a head, it's added automatically.\n\n```python\nfrom sentence_transformers import CrossEncoder",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 872,
      "character_count": 3827,
      "created_at": "2025-10-16T17:42:32.818560",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\CrossEncoder_Training.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]