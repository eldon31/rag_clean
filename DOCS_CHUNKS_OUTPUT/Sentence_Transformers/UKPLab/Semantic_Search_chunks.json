[
  {
    "text": "This document covers implementing semantic search using dense embeddings generated by `SentenceTransformer` models. Semantic search enables finding relevant documents based on meaning rather than exact keyword matching, using vector similarity in high-dimensional embedding spaces.\n\nFor sparse retrieval approaches using search engines, see [Sparse Search Integration](#6.2). For two-stage systems combining retrieval and reranking, see [Retrieve & Rerank Architecture](#6.3). For measuring text similarity in general, see [Semantic Textual Similarity](#6.4).\n\n## Overview\n\nSemantic search with sentence-transformers uses dense vector embeddings to represent both queries and documents in a shared semantic space. Unlike traditional keyword-based search, this approach captures semantic meaning and can find relevant results even when exact terms don't match.\n\n### Semantic Search Flow\n\n```mermaid\nflowchart TD\n    Query[\"Query Text\"] --> QueryEncode[\"model.encode()\"]\n    Docs[\"Document Collection\"] --> DocEncode[\"model.encode()\"]\n    \n    QueryEncode --> QueryEmb[\"Query Embedding<br/>(dense vector)\"]\n    DocEncode --> DocEmb[\"Document Embeddings<br/>(dense vectors)\"]\n    \n    QueryEmb --> Similarity[\"util.pytorch_cos_sim()\"]\n    DocEmb --> Similarity\n    \n    Similarity --> Results[\"Ranked Results\"]\n    \n    subgraph VectorDB [\"Vector Database Storage\"]\n        DocEmb --> Store[\"Store embeddings\"]\n        Store --> Retrieve[\"Similarity search\"]\n        QueryEmb --> Retrieve\n        Retrieve --> Results\n    end\n```\n\nSources: [docs/pretrained-models/msmarco-v2.md:8-15]()\n\n## Basic Implementation Pattern\n\nThe fundamental pattern for semantic search involves three steps: encoding the query, encoding the document collection, and computing similarity scores.\n\n### Core Components\n\n```mermaid\ngraph LR\n    subgraph Input [\"Input Layer\"]\n        QueryText[\"Query Text\"]\n        DocText[\"Document Text\"]\n    end\n    \n    subgraph Model [\"SentenceTransformer\"]\n        Encode[\"encode() method\"]\n    end\n    \n    subgraph Similarity [\"Similarity Computation\"]\n        CosSim[\"util.pytorch_cos_sim()\"]\n        SemanticSim[\"util.semantic_search()\"]\n    end\n    \n    subgraph Output [\"Output Layer\"]\n        Scores[\"Similarity Scores\"]\n        RankedResults[\"Ranked Results\"]\n    end\n    \n    QueryText --> Encode\n    DocText --> Encode\n    Encode --> CosSim\n    Encode --> SemanticSim\n    CosSim --> Scores\n    SemanticSim --> RankedResults\n```\n\nSources: [docs/pretrained-models/msmarco-v2.md:8-15]()\n\n### Model Selection\n\nThe choice of `SentenceTransformer` model significantly impacts search quality. Models trained on information retrieval datasets like MS MARCO provide better performance for search tasks compared to general-purpose models.\n\n| Model Type | Use Case | Example |\n|------------|----------|---------|\n| MSMARCO-trained | Information retrieval | `msmarco-distilroberta-base-v2` |\n| General-purpose | Broad semantic similarity | `all-MiniLM-L6-v2` |\n| Domain-specific | Specialized fields | BioBERT variants |\n\nSources: [docs/pretrained-models/msmarco-v2.md:10](), [docs/pretrained-models/msmarco-v2.md:27-32]()\n\n## Vector Database Integration\n\nFor large-scale semantic search, vector databases provide efficient storage and retrieval of dense embeddings. The integration pattern involves pre-computing document embeddings and storing them for fast similarity search.\n\n### Vector Database Architecture\n\n```mermaid\ngraph TB\n    subgraph Preprocessing [\"Offline Preprocessing\"]\n        Corpus[\"Document Corpus\"]\n        STModel[\"SentenceTransformer\"]\n        Corpus --> STModel\n        STModel --> Embeddings[\"Dense Embeddings\"]\n    end\n    \n    subgraph VectorDB [\"Vector Database\"]\n        Embeddings --> Store[\"store() / index()\"]\n        Store --> Index[\"Vector Index<br/>(HNSW, IVF, etc.)\"]\n    end\n    \n    subgraph Runtime [\"Query Runtime\"]\n        QueryText[\"Query Text\"]\n        QueryEmbed[\"Query Encoding\"]\n        QueryText --> QueryEmbed\n        QueryEmbed --> Search[\"similarity_search()\"]\n        Index --> Search\n        Search --> Results[\"Top-K Results\"]\n    end\n    \n    subgraph Databases [\"Supported Databases\"]\n        Pinecone[\"Pinecone\"]\n        Weaviate[\"Weaviate\"] \n        Qdrant[\"Qdrant\"]\n        ChromaDB[\"ChromaDB\"]\n    end\n    \n    Store -.-> Pinecone\n    Store -.-> Weaviate\n    Store -.-> Qdrant\n    Store -.-> ChromaDB\n```\n\nSources: Based on integration patterns mentioned in the repository overview\n\n## Performance Considerations\n\nSemantic search performance depends on both model quality and computational efficiency. The choice of model affects both retrieval quality and inference speed.\n\n### Model Performance Comparison\n\nBased on evaluation against traditional keyword search (BM25), dense embedding models show significant improvements in retrieval quality:",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Semantic_Search.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1017,
      "character_count": 4815,
      "created_at": "2025-10-16T17:42:33.157735",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Semantic_Search.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "| Approach | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco) |\n|----------|---------------------|-------------------|\n| BM25 (Elasticsearch) | 45.46 | 17.29 |\n| `msmarco-distilroberta-base-v2` | 65.65 | 28.55 |\n| `msmarco-roberta-base-v2` | 67.18 | 29.17 |\n| `msmarco-distilbert-base-v2` | 68.35 | 30.77 |\n\nThe substantial improvement over BM25 demonstrates the effectiveness of semantic search for information retrieval tasks.\n\nSources: [docs/pretrained-models/msmarco-v2.md:25-32]()\n\n### Optimization Strategies\n\nFor production semantic search systems, several optimization techniques apply:\n\n1. **Model Selection**: Choose appropriately sized models balancing quality and speed\n2. **Batch Processing**: Encode multiple documents simultaneously for better throughput  \n3. **Caching**: Cache frequently accessed embeddings to reduce computation\n4. **Quantization**: Use reduced precision embeddings to decrease memory usage\n5. **Approximate Search**: Leverage vector database indexing for sub-linear search time\n\n## Implementation Patterns\n\n### Single-Query Search\n\nThe basic pattern for single-query semantic search involves encoding the query and computing similarity against a collection of pre-computed document embeddings.\n\n```python\n# Referenced pattern from file\nmodel = SentenceTransformer(\"msmarco-distilroberta-base-v2\")\nquery_embedding = model.encode(\"How big is London\")\npassage_embedding = model.encode(\"London has 9,787,426 inhabitants at the 2011 census\")\nsimilarity = util.pytorch_cos_sim(query_embedding, passage_embedding)\n```\n\nSources: [docs/pretrained-models/msmarco-v2.md:10-15]()\n\n### Batch Processing\n\nFor processing multiple queries or documents, batch encoding provides better performance through parallelization within the model.\n\n### Cross-Lingual Search\n\n`SentenceTransformer` models trained on multilingual data enable semantic search across language boundaries, where queries in one language can retrieve relevant documents in another language.\n\n## Integration Examples\n\nThe MS MARCO models demonstrate effective semantic search for information retrieval tasks. These models are specifically trained on search query-passage pairs, making them well-suited for question-answering and document retrieval applications.\n\nTraining data characteristics:\n- Over 500,000 query-passage examples  \n- Complete corpus of 8.8 million passages\n- Real user search queries from Bing search engine\n\nSources: [docs/pretrained-models/msmarco-v2.md:2-4]()",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Semantic_Search.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 545,
      "character_count": 2458,
      "created_at": "2025-10-16T17:42:33.160186",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Semantic_Search.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]