[
  {
    "text": "```\n\n**Sources:** [sentence_transformers/cross_encoder/CrossEncoder.py:392-486](), [sentence_transformers/cross_encoder/CrossEncoder.py:488-590](), [README.md:93-132]()\n\n## Data Flow and Processing Pipeline\n\nThe following diagram shows how text data flows through the different model architectures and their underlying components:\n\n```mermaid\ngraph LR\n    subgraph \"Input Processing\"\n        Text[\"Text Input\"]\n        Text --> Tokenizer[\"Tokenizer<br/>transformers.AutoTokenizer\"]\n        Tokenizer --> Features[\"Token Features<br/>{input_ids, attention_mask}\"]\n    end\n    \n    subgraph \"Model Components\"\n        Features --> Transformer[\"Transformer<br/>sentence_transformers/models/Transformer.py\"]\n        Transformer --> TokenEmb[\"Token Embeddings<br/>[batch, seq_len, hidden_dim]\"]\n        \n        TokenEmb --> Pooling[\"Pooling<br/>sentence_transformers/models/Pooling.py\"]\n        TokenEmb --> SpladePooling[\"SpladePooling<br/>sparse_encoder/models/SpladePooling.py\"]\n        TokenEmb --> CrossScore[\"Classification Head<br/>transformers Model\"]\n    end\n    \n    subgraph \"Output Processing\"\n        Pooling --> DenseOut[\"Dense Embeddings<br/>model.encode()\"]\n        SpladePooling --> SparseOut[\"Sparse Embeddings<br/>sparse_model.encode()\"]\n        CrossScore --> ScoreOut[\"Similarity Scores<br/>cross_model.predict()\"]\n    end\n```\n\n**Sources:** [sentence_transformers/models/Transformer.py:226-257](), [sentence_transformers/models/Pooling.py:135-241](), [sentence_transformers/sparse_encoder/models/SpladePooling.py](), [sentence_transformers/cross_encoder/CrossEncoder.py:341-342]()\n\n## Common Usage Patterns\n\n### Typical Model Selection\n\n| Task | Model Type | Example Model | Key Method |\n|------|------------|---------------|------------|\n| Semantic similarity | `SentenceTransformer` | `all-mpnet-base-v2` | `encode()` |\n| Vector database search | `SentenceTransformer` | `all-MiniLM-L6-v2` | `encode()` |\n| Lexical + semantic search | `SparseEncoder` | `naver/splade-cocondenser-ensembledistil` | `encode()` |\n| Reranking search results | `CrossEncoder` | `cross-encoder/ms-marco-MiniLM-L6-v2` | `rank()` |\n| Text pair classification | `CrossEncoder` | `cross-encoder/nli-MiniLM2-L6-H768` | `predict()` |\n\n### Hybrid Retrieval Pipeline\n\n```mermaid\ngraph TB\n    Query[\"User Query\"]\n    \n    subgraph \"Stage 1: Initial Retrieval\"\n        Query --> DenseRetrieval[\"Dense Retrieval<br/>SentenceTransformer.encode_query()\"]\n        Query --> SparseRetrieval[\"Sparse Retrieval<br/>SparseEncoder.encode_query()\"]\n        \n        DenseRetrieval --> DenseCandidates[\"Dense Candidates<br/>(semantic similarity)\"]\n        SparseRetrieval --> SparseCandidates[\"Sparse Candidates<br/>(lexical matching)\"]\n    end\n    \n    subgraph \"Stage 2: Fusion & Reranking\"\n        DenseCandidates --> Fusion[\"Candidate Fusion\"]\n        SparseCandidates --> Fusion\n        Fusion --> TopK[\"Top-K Candidates\"]\n        TopK --> Reranker[\"CrossEncoder.rank()\"]\n    end\n    \n    Reranker --> FinalResults[\"Final Ranked Results\"]\n```\n\n**Sources:** [README.md:213-216](), [sentence_transformers/SentenceTransformer.py:416-675](), [sentence_transformers/sparse_encoder/SparseEncoder.py:181-410](), [sentence_transformers/cross_encoder/CrossEncoder.py:488-590]()\n\n## Next Steps\n\n- **Model Selection**: Browse available models in [Pretrained Models](#5) or on the [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers)\n- **Training**: Learn to fine-tune models for your domain in [Training](#3)\n- **Applications**: Explore real-world use cases in [Applications](#6)\n- **Performance**: Optimize inference speed using techniques in [Advanced Topics](#7)\n\n**Sources:** [index.rst:133-154](), [docs/sentence_transformer/pretrained_models.md:1-49]()\n\n# Training\n\nThis page covers the training system architecture and common concepts shared across all model types in sentence-transformers. The training system provides a unified interface for training `SentenceTransformer`, `SparseEncoder`, and `CrossEncoder` models using PyTorch and ðŸ¤— Transformers.",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 956,
      "character_count": 4055,
      "created_at": "2025-10-16T17:42:32.741124",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "For specific training guides, see [SentenceTransformer Training](#3.1), [SparseEncoder Training](#3.2), and [CrossEncoder Training](#3.3). For loss function details, see [Loss Functions for SentenceTransformer](#3.4), [Loss Functions for SparseEncoder](#3.5), and [Loss Functions for CrossEncoder](#3.6).\n\n## Training Architecture Overview\n\nThe sentence-transformers training system is built on top of ðŸ¤— Transformers `Trainer` with specialized components for each model type:\n\n```mermaid\ngraph TB\n    subgraph \"Model Types\"\n        ST[\"SentenceTransformer\"]\n        SE[\"SparseEncoder\"] \n        CE[\"CrossEncoder\"]\n    end\n    \n    subgraph \"Trainer Classes\"\n        STTrainer[\"SentenceTransformerTrainer\"]\n        SETrainer[\"SparseEncoderTrainer\"]\n        CETrainer[\"CrossEncoderTrainer\"]\n    end\n    \n    subgraph \"ðŸ¤— Transformers Base\"\n        BaseTrainer[\"transformers.Trainer\"]\n    end\n    \n    subgraph \"Common Components\"\n        Args[\"TrainingArguments\"]\n        DataCollator[\"DataCollator\"]\n        Loss[\"Loss Functions\"]\n        Evaluator[\"Evaluators\"]\n        Router[\"Router Module\"]\n    end\n    \n    ST --> STTrainer\n    SE --> SETrainer\n    CE --> CETrainer\n    \n    STTrainer --> BaseTrainer\n    SETrainer --> STTrainer\n    CETrainer --> BaseTrainer\n    \n    STTrainer --> Args\n    SETrainer --> Args  \n    CETrainer --> Args\n    \n    STTrainer --> DataCollator\n    SETrainer --> DataCollator\n    CETrainer --> DataCollator\n    \n    STTrainer --> Loss\n    SETrainer --> Loss\n    CETrainer --> Loss\n    \n    STTrainer --> Evaluator\n    SETrainer --> Evaluator\n    CETrainer --> Evaluator\n    \n    STTrainer --> Router\n    SETrainer --> Router\n```\n\n**Training Architecture Overview**\n\nSources: [sentence_transformers/trainer.py:59-127](), [sentence_transformers/sparse_encoder/trainer.py:31-98](), [docs/sentence_transformer/training_overview.md:17-46]()\n\n## Trainer Class Hierarchy\n\nThe training system uses specialized trainer classes that inherit from the ðŸ¤— Transformers `Trainer`:\n\n```mermaid\ngraph TB\n    BaseTrainer[\"transformers.Trainer\"]\n    \n    STTrainer[\"SentenceTransformerTrainer<br/>sentence_transformers/trainer.py\"]\n    SETrainer[\"SparseEncoderTrainer<br/>sparse_encoder/trainer.py\"] \n    CETrainer[\"CrossEncoderTrainer<br/>cross_encoder/trainer.py\"]\n    \n    STArgs[\"SentenceTransformerTrainingArguments\"]\n    SEArgs[\"SparseEncoderTrainingArguments\"]\n    CEArgs[\"CrossEncoderTrainingArguments\"]\n    \n    STDataCollator[\"SentenceTransformerDataCollator\"]\n    SEDataCollator[\"SparseEncoderDataCollator\"]\n    CEDataCollator[\"CrossEncoderDataCollator\"]\n    \n    BaseTrainer --> STTrainer\n    BaseTrainer --> CETrainer\n    STTrainer --> SETrainer\n    \n    STTrainer --> STArgs\n    SETrainer --> SEArgs\n    CETrainer --> CEArgs\n    \n    STTrainer --> STDataCollator\n    SETrainer --> SEDataCollator\n    CETrainer --> CEDataCollator\n```\n\n**Trainer Class Hierarchy and Components**\n\nSources: [sentence_transformers/trainer.py:59](), [sentence_transformers/sparse_encoder/trainer.py:31](), [sentence_transformers/data_collator.py:13]()\n\n## Data Flow During Training\n\nThe training process follows a consistent data flow across all model types:",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 780,
      "character_count": 3160,
      "created_at": "2025-10-16T17:42:32.745170",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph LR\n    subgraph \"Input Data\"\n        Dataset[\"datasets.Dataset<br/>or DatasetDict\"]\n        Features[\"Raw Text Features\"]\n    end\n    \n    subgraph \"Data Processing\"\n        DataCollator[\"DataCollator.call()\"]\n        Tokenize[\"tokenize_fn()\"]\n        Prompts[\"Prompt Addition\"]\n        RouterMapping[\"router_mapping\"]\n    end\n    \n    subgraph \"Model Forward\"\n        ModelForward[\"model.forward()\"]\n        LossCompute[\"compute_loss()\"]\n        Features_[\"Tokenized Features\"]\n        Labels[\"Labels\"]\n    end\n    \n    subgraph \"Training Step\"\n        Optimizer[\"Optimizer.step()\"]\n        Scheduler[\"LR Scheduler\"]\n        Backward[\"loss.backward()\"]\n    end\n    \n    subgraph \"Evaluation\"\n        EvalDataset[\"eval_dataset\"]\n        Evaluator[\"evaluator()\"]\n        Metrics[\"Evaluation Metrics\"]\n    end\n    \n    Dataset --> DataCollator\n    Features --> DataCollator\n    \n    DataCollator --> Tokenize\n    DataCollator --> Prompts\n    DataCollator --> RouterMapping\n    \n    Tokenize --> Features_\n    Prompts --> Features_\n    RouterMapping --> Features_\n    DataCollator --> Labels\n    \n    Features_ --> ModelForward\n    Labels --> LossCompute\n    ModelForward --> LossCompute\n    \n    LossCompute --> Backward\n    Backward --> Optimizer\n    Optimizer --> Scheduler\n    \n    EvalDataset --> Evaluator\n    Evaluator --> Metrics\n```\n\n**Training Data Flow**\n\nSources: [sentence_transformers/trainer.py:391-441](), [sentence_transformers/data_collator.py:35-119](), [sentence_transformers/trainer.py:545-592]()\n\n## Common Training Components\n\n### Data Collator\n\nThe `SentenceTransformerDataCollator` handles tokenization and batch preparation:\n\n| Component | Purpose |\n|-----------|---------|\n| `tokenize_fn` | Tokenizes text inputs using the model's tokenizer |\n| `router_mapping` | Maps dataset columns to router tasks (e.g., \"query\", \"document\") |\n| `prompts` | Adds prompts to input texts before tokenization |\n| `valid_label_columns` | Identifies label columns (\"label\", \"score\") |\n\nSources: [sentence_transformers/data_collator.py:13-31]()\n\n### Training Arguments\n\nEach model type has specialized training arguments:\n\n- `SentenceTransformerTrainingArguments` - Basic sentence transformer training\n- `SparseEncoderTrainingArguments` - Adds sparse-specific parameters  \n- `CrossEncoderTrainingArguments` - Cross encoder specific settings\n\nKey parameters include `batch_sampler`, `multi_dataset_batch_sampler`, `router_mapping`, and `learning_rate_mapping`.\n\nSources: [sentence_transformers/trainer.py:36-40]()\n\n### Loss Functions\n\nTraining supports both single loss functions and multi-dataset loss dictionaries:\n\n```python",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 577,
      "character_count": 2648,
      "created_at": "2025-10-16T17:42:32.748973",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]