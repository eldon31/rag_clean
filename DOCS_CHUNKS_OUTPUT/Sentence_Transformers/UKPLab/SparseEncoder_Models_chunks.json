[
  {
    "text": "This page documents the pretrained SparseEncoder models available in the sentence-transformers library, including their characteristics, performance metrics, and usage patterns. SparseEncoder models generate sparse vector representations that enable efficient neural search while maintaining interpretability through token-level activation patterns.\n\nFor information about training SparseEncoder models, see [SparseEncoder Training](#3.2). For evaluation strategies, see [SparseEncoder Evaluators](#4.2).\n\n## Model Architecture Types\n\nSparseEncoder models in the sentence-transformers ecosystem fall into two primary architectural categories, each optimized for different use cases and performance requirements.\n\n```mermaid\ngraph TB\n    subgraph \"SparseEncoder Model Types\"\n        CoreSPLADE[\"Core SPLADE Models<br/>Full Neural Inference\"]\n        InferenceFree[\"Inference-Free SPLADE Models<br/>Hybrid Architecture\"]\n    end\n    \n    subgraph \"Core SPLADE Architecture\"\n        CoreQuery[\"Query: MLMTransformer + SpladePooling\"]\n        CoreDoc[\"Document: MLMTransformer + SpladePooling\"]\n        CoreSPLADE --> CoreQuery\n        CoreSPLADE --> CoreDoc\n    end\n    \n    subgraph \"Inference-Free Architecture\"\n        IFQuery[\"Query: SparseStaticEmbedding<br/>(Pre-computed scores)\"]\n        IFDoc[\"Document: MLMTransformer + SpladePooling\"]\n        InferenceFree --> IFQuery\n        InferenceFree --> IFDoc\n    end\n    \n    subgraph \"Common Interface\"\n        EncodeQuery[\"encode_query()\"]\n        EncodeDoc[\"encode_document()\"]\n        Similarity[\"similarity()\"]\n    end\n    \n    CoreQuery --> EncodeQuery\n    CoreDoc --> EncodeDoc\n    IFQuery --> EncodeQuery\n    IFDoc --> EncodeDoc\n    EncodeQuery --> Similarity\n    EncodeDoc --> Similarity\n    \n    subgraph \"Output Characteristics\"\n        CoreOutput[\"Sparse Vectors<br/>Query Expansion: Yes<br/>Latency: Higher\"]\n        IFOutput[\"Sparse Vectors<br/>Query Expansion: No<br/>Latency: Near-instant\"]\n    end\n    \n    CoreSPLADE --> CoreOutput\n    InferenceFree --> IFOutput\n```\n\n**SparseEncoder Model Architecture Types**\n\nSources: [docs/sparse_encoder/pretrained_models.md:62-76]()\n\n## Core SPLADE Models\n\nCore SPLADE models use neural inference for both queries and documents, providing query expansion capabilities and optimal retrieval performance. These models are trained on datasets like MS MARCO Passage Retrieval and evaluated on BEIR benchmarks.",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 503,
      "character_count": 2412,
      "created_at": "2025-10-16T17:42:33.193869",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "| Model Name | MS MARCO MRR@10 | BEIR-13 avg nDCG@10 | Parameters | Architecture |\n|------------|:---------------:|:-------------------:|-----------:|-------------|\n| [opensearch-project/opensearch-neural-sparse-encoding-v2-distill](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v2-distill) | NA | **52.8** | 67M | DistilBERT |\n| [opensearch-project/opensearch-neural-sparse-encoding-v1](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-v1) | NA | 52.4 | 133M | BERT |\n| [naver/splade-v3](https://huggingface.co/naver/splade-v3) | **40.2** | 51.7 | 109M | BERT |\n| [ibm-granite/granite-embedding-30m-sparse](https://huggingface.co/ibm-granite/granite-embedding-30m-sparse) | NA | 50.8 | 30M | Custom |\n| [naver/splade-cocondenser-selfdistil](https://huggingface.co/naver/splade-cocondenser-selfdistil) | 37.6 | 50.7 | 109M | BERT |\n| [naver/splade_v2_distil](https://huggingface.co/naver/splade_v2_distil) | 36.8 | 50.6 | 67M | DistilBERT |\n| [naver/splade-v3-distilbert](https://huggingface.co/naver/splade-v3-distilbert) | 38.7 | 50.0 | 67M | DistilBERT |\n| [naver/splade-v3-lexical](https://huggingface.co/naver/splade-v3-lexical) | 40.0 | 49.1 | 109M | BERT |\n| [rasyosef/splade-mini](https://huggingface.co/rasyosef/splade-mini) | 33.2 | 42.5 | 11M | Mini |\n| [rasyosef/splade-tiny](https://huggingface.co/rasyosef/splade-tiny) | 30.9 | 40.6 | 4M | Tiny |\n\n**Note:** BM25 baseline achieves 18.4 MS MARCO MRR@10 and 45.6 BEIR-13 avg nDCG@10.\n\nSources: [docs/sparse_encoder/pretrained_models.md:36-60]()\n\n## Inference-Free SPLADE Models\n\nInference-free SPLADE models use `SparseStaticEmbedding` for queries (pre-computed token scores) and traditional SPLADE architecture for documents. This design sacrifices query expansion for near-instant query processing speed.\n\n| Model Name | BEIR-13 avg nDCG@10 | Parameters | Document Architecture |\n|------------|:-------------------:|-----------:|---------------------|\n| [opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte) | **54.6** | 137M | BERT-Large |\n| [opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill) | 51.7 | 67M | DistilBERT |\n| [opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill) | 50.4 | 67M | DistilBERT |\n| [opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini) | 49.7 | 23M | Mini |\n| [opensearch-project/opensearch-neural-sparse-encoding-doc-v1](https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v1) | 49.0 | 133M | BERT |\n| [naver/splade-v3-doc](https://huggingface.co/naver/splade-v3-doc) | 47.0 | 109M | BERT |\n\nSources: [docs/sparse_encoder/pretrained_models.md:62-76]()",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1022,
      "character_count": 3048,
      "created_at": "2025-10-16T17:42:33.196757",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "## Basic Usage Pattern\n\nAll SparseEncoder models follow a consistent interface for encoding queries and documents:\n\n```python\nfrom sentence_transformers import SparseEncoder\n\n# Load any SparseEncoder model\nmodel = SparseEncoder(\"naver/splade-v3\")\n\n# Encode queries and documents\nqueries = [\"what causes aging fast\"]\ndocuments = [\"UV-A light causes skin aging...\", \"Alzheimer's disease...\"]\n\nquery_embeddings = model.encode_query(queries)\ndocument_embeddings = model.encode_document(documents)\n\n# Compute similarities\nsimilarities = model.similarity(query_embeddings, document_embeddings)\n```\n\nThe `encode_query()` and `encode_document()` methods return sparse tensors with shape `[batch_size, vocab_size]`, where non-zero values indicate token activations.\n\nSources: [docs/sparse_encoder/pretrained_models.md:12-33]()\n\n## Integration with Search Systems\n\nSparseEncoder models integrate seamlessly with various search infrastructures, leveraging their sparse vector representations for efficient retrieval.\n\n```mermaid\ngraph TD\n    subgraph \"SparseEncoder Integration Architecture\"\n        QueryInput[\"Query Input<br/>encode_query()\"]\n        DocInput[\"Document Corpus<br/>encode_document()\"]\n        \n        SparseModel[\"SparseEncoder<br/>(naver/splade-v3)\"]\n        \n        QueryInput --> SparseModel\n        DocInput --> SparseModel\n    end\n    \n    subgraph \"Sparse Vector Processing\"\n        QuerySparse[\"Query Sparse Vector<br/>[1, 30522]\"]\n        DocSparse[\"Document Sparse Vectors<br/>[N, 30522]\"]\n        \n        SparseModel --> QuerySparse\n        SparseModel --> DocSparse\n    end\n    \n    subgraph \"Search Engine Integration\"\n        Elasticsearch[\"Elasticsearch<br/>sparse_vector field\"]\n        OpenSearch[\"OpenSearch<br/>neural-sparse plugin\"]\n        Qdrant[\"Qdrant<br/>sparse vectors\"]\n        SpladeIndex[\"splade-index<br/>specialized library\"]\n    end\n    \n    subgraph \"Application Layer\"\n        SemanticSearch[\"Semantic Search<br/>semantic_search_splade_index.py\"]\n        HybridRetrieval[\"Hybrid Retrieval<br/>Dense + Sparse\"]\n        NeuralLexical[\"Neural Lexical Search<br/>Token-level matching\"]\n    end\n    \n    QuerySparse --> Elasticsearch\n    DocSparse --> Elasticsearch\n    QuerySparse --> OpenSearch\n    DocSparse --> OpenSearch\n    QuerySparse --> Qdrant\n    DocSparse --> Qdrant\n    QuerySparse --> SpladeIndex\n    DocSparse --> SpladeIndex\n    \n    Elasticsearch --> SemanticSearch\n    OpenSearch --> SemanticSearch\n    SpladeIndex --> SemanticSearch\n    \n    SemanticSearch --> HybridRetrieval\n    SemanticSearch --> NeuralLexical\n```\n\n**SparseEncoder Integration with Search Systems**\n\nSources: [examples/sparse_encoder/applications/semantic_search/semantic_search_splade_index.py:1-52](), [docs/sparse_encoder/pretrained_models.md:1-83]()\n\n## Model Selection Guidelines\n\n### Performance Considerations\n\n- **Highest BEIR Performance**: `opensearch-project/opensearch-neural-sparse-encoding-v2-distill` (52.8 nDCG@10)\n- **Highest MS MARCO Performance**: `naver/splade-v3` (40.2 MRR@10)\n- **Best Efficiency Trade-off**: `rasyosef/splade-tiny` (4M parameters, 40.6 BEIR nDCG@10)\n- **Fastest Query Processing**: Inference-free models with `SparseStaticEmbedding`\n\n### Use Case Recommendations\n\n| Use Case | Recommended Model | Rationale |\n|----------|-------------------|-----------|\n| Production Search | `naver/splade-v3` | Best MS MARCO performance, proven reliability |\n| Resource-Constrained | `rasyosef/splade-tiny` | Minimal parameters, decent performance |\n| High-Throughput Queries | `opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte` | Inference-free query processing |\n| Research/Experimentation | `opensearch-project/opensearch-neural-sparse-encoding-v2-distill` | Highest BEIR performance |\n\nSources: [docs/sparse_encoder/pretrained_models.md:36-76]()\n\n## Model Collections\n\nPre-organized collections of SparseEncoder models are available on the Hugging Face Hub:\n\n- **[SPLADE Models](https://huggingface.co/collections/sparse-encoder/splade-models-6862be100374b320d826eeaa)**: Complete collection of core SPLADE models\n- **[Inference-Free SPLADE Models](https://huggingface.co/collections/sparse-encoder/inference-free-splade-models-6862be3a1d72eab38920bc6a)**: Models optimized for query speed\n\nThese collections provide curated access to models with consistent naming conventions and documented performance characteristics.",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1014,
      "character_count": 4390,
      "created_at": "2025-10-16T17:42:33.203685",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "Sources: [docs/sparse_encoder/pretrained_models.md:77-83]()\n\n# CrossEncoder Models\n\nThis document covers the pretrained CrossEncoder models available in the sentence-transformers library, their characteristics, performance metrics, and usage patterns. CrossEncoder models are designed for pairwise text scoring and classification tasks, making them particularly effective for reranking, semantic similarity measurement, and natural language inference.\n\nFor information about training CrossEncoder models, see [CrossEncoder Training](#3.3). For details about CrossEncoder evaluation methods, see [CrossEncoder Evaluators](#4.3).\n\n## Model Architecture and Purpose\n\nCrossEncoder models process pairs of texts jointly through a single transformer model, producing similarity scores or classification outputs. Unlike bi-encoder architectures that encode texts independently, CrossEncoders perform cross-attention between the input texts, enabling more precise but computationally expensive comparisons.\n\n```mermaid\ngraph TD\n    subgraph \"CrossEncoder Architecture\"\n        TextPair[\"Text Pair Input<br/>(query, passage)\"] --> Tokenizer[\"Tokenizer\"]\n        Tokenizer --> CrossAttention[\"Cross-Attention<br/>Transformer\"]\n        CrossAttention --> Classifier[\"Classification Head\"]\n        Classifier --> ActivationFn[\"Activation Function<br/>(Sigmoid/Identity)\"]\n        ActivationFn --> Score[\"Relevance Score<br/>(0-1 or logit)\"]\n    end\n    \n    subgraph \"Use Cases\"\n        Score --> Reranking[\"Reranking Pipeline\"]\n        Score --> STS[\"Semantic Similarity\"]\n        Score --> NLI[\"Natural Language Inference\"]\n        Score --> QA[\"Question Answering\"]\n    end\n```\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:1-33](), [docs/pretrained-models/ce-msmarco.md:1-63]()\n\n## Available Model Categories\n\nThe sentence-transformers library provides several categories of pretrained CrossEncoder models, each optimized for specific tasks and domains.\n\n| Category | Purpose | Key Models | Performance Metric |\n|----------|---------|------------|-------------------|\n| MS MARCO | Information Retrieval | `cross-encoder/ms-marco-MiniLM-L6-v2` | NDCG@10, MRR@10 |\n| STSbenchmark | Semantic Similarity | `cross-encoder/stsb-roberta-base` | Pearson Correlation |\n| NLI | Natural Language Inference | `cross-encoder/nli-deberta-v3-base` | Accuracy |\n| QNLI | Question-Passage Matching | `cross-encoder/qnli-electra-base` | Accuracy |\n| Quora | Duplicate Detection | `cross-encoder/quora-roberta-base` | Average Precision |\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:27-112]()\n\n### MS MARCO Models\n\nMS MARCO CrossEncoder models are specifically trained for information retrieval and reranking tasks using real user queries from Bing search engine.\n\n```mermaid\ngraph LR\n    subgraph \"MS MARCO Model Hierarchy\"\n        TinyBERT[\"cross-encoder/ms-marco-TinyBERT-L2-v2<br/>NDCG@10: 69.84<br/>9000 docs/sec\"]\n        MiniLM2[\"cross-encoder/ms-marco-MiniLM-L2-v2<br/>NDCG@10: 71.01<br/>4100 docs/sec\"]\n        MiniLM4[\"cross-encoder/ms-marco-MiniLM-L4-v2<br/>NDCG@10: 73.04<br/>2500 docs/sec\"]\n        MiniLM6[\"cross-encoder/ms-marco-MiniLM-L6-v2<br/>NDCG@10: 74.30<br/>1800 docs/sec\"]\n        MiniLM12[\"cross-encoder/ms-marco-MiniLM-L12-v2<br/>NDCG@10: 74.31<br/>960 docs/sec\"]\n    end\n    \n    TinyBERT --> |\"Higher Performance\"| MiniLM2\n    MiniLM2 --> MiniLM4\n    MiniLM4 --> MiniLM6\n    MiniLM6 --> MiniLM12\n    \n    TinyBERT --> |\"Higher Speed\"| MiniLM2\n```\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:35-42](), [docs/pretrained-models/ce-msmarco.md:41-48]()\n\n### Community Models\n\nThe ecosystem includes high-quality community-contributed models for specialized domains:",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 905,
      "character_count": 3690,
      "created_at": "2025-10-16T17:42:33.209005",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 3,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "- **BAAI BGE Rerankers**: `BAAI/bge-reranker-base`, `BAAI/bge-reranker-large`, `BAAI/bge-reranker-v2-m3`\n- **Jina AI Models**: `jinaai/jina-reranker-v1-tiny-en`, `jinaai/jina-reranker-v1-turbo-en`\n- **Mixedbread AI**: `mixedbread-ai/mxbai-rerank-base-v1`, `mixedbread-ai/mxbai-rerank-large-v1`\n- **Alibaba GTE**: `Alibaba-NLP/gte-reranker-modernbert-base`, `Alibaba-NLP/gte-multilingual-reranker-base`\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:114-130]()\n\n## Usage Patterns\n\n### Basic Usage with SentenceTransformers\n\n```python\nfrom sentence_transformers import CrossEncoder\nimport torch\n\n# Load with sigmoid activation for 0-1 scores\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\", activation_fn=torch.nn.Sigmoid())\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\n# => array([0.9998173 , 0.01312432], dtype=float32)\n```\n\n### Integration with Transformers Library\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\ntokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\nfeatures = tokenizer([\"Query\", \"Query\"], [\"Paragraph1\", \"Paragraph2\"], \n                    padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    scores = model(**features).logits\n```\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:12-33](), [docs/pretrained-models/ce-msmarco.md:19-33]()\n\n## Performance Characteristics\n\n### Speed vs Accuracy Trade-offs\n\n```mermaid\ngraph TD\n    subgraph \"Performance Matrix\"\n        FastLow[\"Fast & Lower Accuracy<br/>TinyBERT-L2: 9000 docs/sec<br/>NDCG@10: 69.84\"]\n        MediumMed[\"Medium Speed & Accuracy<br/>MiniLM-L6: 1800 docs/sec<br/>NDCG@10: 74.30\"]\n        SlowHigh[\"Slower & Higher Accuracy<br/>MiniLM-L12: 960 docs/sec<br/>NDCG@10: 74.31\"]\n    end\n    \n    FastLow --> |\"Diminishing Returns\"| MediumMed\n    MediumMed --> SlowHigh\n    \n    subgraph \"Use Case Mapping\"\n        RealTime[\"Real-time Applications\"] --> FastLow\n        BatchProcessing[\"Batch Processing\"] --> MediumMed\n        HighPrecision[\"High Precision Tasks\"] --> SlowHigh\n    end\n```\n\n### Activation Function Impact\n\n| Activation Function | Output Range | Use Case |\n|-------------------|--------------|----------|\n| `torch.nn.Sigmoid()` | 0.0 - 1.0 | Probability-like scores |\n| `None` (Identity) | -∞ to +∞ | Raw logits for ranking |\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:31-42](), [docs/pretrained-models/ce-msmarco.md:41-62]()\n\n## Integration with Retrieval Systems\n\nCrossEncoder models are typically used in the reranking stage of two-stage retrieval systems, where they refine results from faster bi-encoder models.\n\n```mermaid\ngraph LR\n    subgraph \"Retrieve & Rerank Pipeline\"\n        Query[\"User Query\"] --> BiEncoder[\"Bi-Encoder<br/>SentenceTransformer\"]\n        Corpus[\"Document Corpus<br/>8.8M passages\"] --> BiEncoder\n        BiEncoder --> CandidateSet[\"Top-k Candidates<br/>(e.g., 100-1000)\"]\n        CandidateSet --> CrossEncoder[\"CrossEncoder<br/>cross-encoder/ms-marco-MiniLM-L6-v2\"]\n        CrossEncoder --> RankedResults[\"Final Ranked Results<br/>(e.g., top 10)\"]\n    end\n    \n    subgraph \"Performance Benefits\"\n        BiEncoder --> |\"Fast Retrieval<br/>Dense Search\"| Speed[\"Speed: ~10k docs/sec\"]\n        CrossEncoder --> |\"Precise Reranking<br/>Cross-Attention\"| Accuracy[\"Accuracy: NDCG@10 74.30\"]\n    end\n```\n\n### Hard Negatives Mining\n\nCrossEncoder models are used to generate hard negatives for training bi-encoder models, creating a feedback loop for model improvement.",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 1022,
      "character_count": 3797,
      "created_at": "2025-10-16T17:42:33.216045",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 4,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    subgraph \"Hard Negatives Pipeline\"\n        BiEncoderV2[\"Bi-Encoder v2<br/>msmarco-distilbert-base-v2\"] --> SimilarPassages[\"Retrieved Similar Passages\"]\n        SimilarPassages --> CrossEncoderElectra[\"CrossEncoder<br/>electra-base-msmarco\"]\n        CrossEncoderElectra --> LowScores[\"Low Cross-Encoder Scores<br/>(Hard Negatives)\"]\n        LowScores --> TrainingData[\"Enhanced Training Data\"]\n        TrainingData --> BiEncoderV3[\"Bi-Encoder v3<br/>msmarco-distilbert-base-v3\"]\n    end\n```\n\n**Sources:** [docs/pretrained-models/msmarco-v3.md:53-58](), [docs/cross_encoder/pretrained_models.md:44]()\n\n## Model Selection Guidelines\n\n| Task Type | Recommended Model | Key Considerations |\n|-----------|------------------|-------------------|\n| Information Retrieval | `cross-encoder/ms-marco-MiniLM-L6-v2` | Best balance of speed/accuracy |\n| Semantic Similarity | `cross-encoder/stsb-roberta-large` | Optimized for similarity scoring |\n| Question Answering | `cross-encoder/qnli-electra-base` | Passage-question relevance |\n| Duplicate Detection | `cross-encoder/quora-roberta-base` | Text pair classification |\n| Multilingual Tasks | `Alibaba-NLP/gte-multilingual-reranker-base` | Cross-lingual support |\n\n**Sources:** [docs/cross_encoder/pretrained_models.md:1-130](), [docs/pretrained-models/ce-msmarco.md:1-63]()",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 348,
      "character_count": 1339,
      "created_at": "2025-10-16T17:42:33.216760",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 5,
      "file_relative_path": "UKPLab\\sentence-transformers\\SparseEncoder_Models.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]