[
  {
    "text": "inputs = [\n    Image.open('image1.jpg'),\n    \"Text description 1\", \n    Image.open('image2.jpg'),\n    \"Text description 2\"\n]\n\nembeddings = model.encode(inputs)\n```\n\nSources: [tests/test_image_embeddings.py:14-31]()\n\n## Integration with SentenceTransformer Ecosystem\n\nThe `CLIPModel` integrates with the sentence-transformers ecosystem through the modular architecture and supports the same operations as text-only models.\n\n**Integration Architecture**\n```mermaid\ngraph TB\n    subgraph SentenceTransformer[\"SentenceTransformer Class\"]\n        ENCODE[\"encode() method\"]\n        SIMILARITY[\"similarity() method\"]\n        MODULES[\"_modules list\"]\n    end\n    \n    subgraph CLIPModule[\"CLIPModel Module\"]\n        CLIPMOD[\"CLIPModel(InputModule)\"]\n        TOKENIZE[\"tokenize()\"]\n        FORWARD[\"forward()\"]\n    end\n    \n    subgraph Processing[\"Shared Processing\"]\n        MULTIPROC[\"Multi-processing support\"]\n        NORMALIZE[\"normalize_embeddings\"]\n        TENSOR[\"convert_to_tensor\"]\n    end\n    \n    subgraph Applications[\"Application Support\"]\n        SEARCH[\"Semantic search\"]\n        SIMILARITY_COMP[\"Cosine similarity\"]\n        RETRIEVAL[\"Cross-modal retrieval\"]\n    end\n    \n    SentenceTransformer --> CLIPModule\n    CLIPModule --> Processing\n    Processing --> Applications\n```\n\n**Module System Integration**\n\n| Component | Role | Implementation |\n|-----------|------|----------------|\n| `CLIPModel` | Input processing | Inherits from `InputModule` |\n| `Pooling` | Optional embedding processing | Can be added after `CLIPModel` |\n| `Normalize` | L2 normalization | Applied to final embeddings |\n\nThe `CLIPModel` appears in the module registry and supports the same configuration patterns as other sentence-transformers modules.\n\nSources: [sentence_transformers/models/CLIPModel.py:15](), [sentence_transformers/models/__init__.py:6,40]()\n\n## Model Loading and Configuration\n\nCLIP models are loaded through the standard `SentenceTransformer` interface or by constructing `CLIPModel` instances directly.\n\n**Configuration Options**\n\n| Parameter | Purpose | Default | Example |\n|-----------|---------|---------|---------|\n| `model_name` | Base CLIP model | Required | `'openai/clip-vit-base-patch32'` |\n| `processor_name` | Processor configuration | `model_name` | Custom processor path |\n| `max_seq_length` | Text sequence limit | From tokenizer | 77 for CLIP models |\n\n**Loading Patterns**\n\n```python",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Mixed_input_batch.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 526,
      "character_count": 2406,
      "created_at": "2025-10-16T17:42:32.924208",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Mixed_input_batch.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]