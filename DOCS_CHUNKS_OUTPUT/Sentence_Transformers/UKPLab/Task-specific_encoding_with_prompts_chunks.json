[
  {
    "text": "model.prompts = {\"query\": \"query: \", \"document\": \"passage: \"}\nquery_emb = model.encode_query(\"What is AI?\")\ndoc_emb = model.encode_document(\"AI is artificial intelligence\")\n```\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:61-407](), [sentence_transformers/SentenceTransformer.py:416-675](), [tests/test_sentence_transformer.py:309-346]()\n\n## SparseEncoder\n\nThe `SparseEncoder` class extends `SentenceTransformer` to produce sparse vector representations where most dimensions are zero. This architecture is particularly effective for lexical matching and hybrid retrieval scenarios.\n\n### Sparse Architecture Components\n\n```mermaid\ngraph TB\n    subgraph \"SparseEncoder Components\"\n        MLMTransformer[\"MLMTransformer<br/>Token-level predictions\"]\n        SpladePooling[\"SpladePooling<br/>Sparsification\"]\n        SparseAutoEncoder[\"SparseAutoEncoder<br/>k-sparse activation\"]\n        Router[\"Router<br/>Query/Document paths\"]\n    end\n    \n    subgraph \"Output Processing\"\n        ActiveDims[\"max_active_dims<br/>Sparsity control\"]\n        SparseOutput[\"Sparse COO Tensor<br/>[batch_size, vocab_size]\"]\n    end\n    \n    Input[\"Text\"] --> Router\n    Router --> MLMTransformer\n    MLMTransformer --> SpladePooling\n    SpladePooling --> ActiveDims\n    ActiveDims --> SparseOutput\n```\n\n### Key Differences from SentenceTransformer\n\n- **Vocabulary-Sized Output**: Embeddings have dimensions equal to tokenizer vocabulary size\n- **Sparsity Control**: `max_active_dims` parameter limits non-zero dimensions\n- **Sparse Tensor Format**: Outputs can be sparse COO tensors for memory efficiency\n- **Term Importance**: Non-zero values represent importance of vocabulary terms\n\n### Decoding and Interpretation\n\nThe `SparseEncoder` provides a `decode()` method to interpret sparse embeddings as weighted vocabulary terms:\n\n```python\nmodel = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")\nembeddings = model.encode(\"machine learning\")\ntokens_weights = model.decode(embeddings, top_k=10)",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 428,
      "character_count": 1997,
      "created_at": "2025-10-16T17:42:33.281341",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Task-specific_encoding_with_prompts.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]