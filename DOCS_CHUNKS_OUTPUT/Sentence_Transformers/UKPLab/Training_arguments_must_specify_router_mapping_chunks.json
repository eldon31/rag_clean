[
  {
    "text": "args = SentenceTransformerTrainingArguments(\n    router_mapping={\n        \"question\": \"query\",\n        \"positive\": \"document\", \n        \"negative\": \"document\"\n    }\n)\n```\n\nThe `router_mapping` tells the data collator which dataset columns correspond to which router tasks.\n\nSources: [sentence_transformers/trainer.py:198-204](), [sentence_transformers/sparse_encoder/trainer.py:180-186]()\n\n## Advanced Module Types\n\n### Sparse Encoder Modules\n\nFor sparse embeddings, specialized modules are used that produce high-dimensional sparse vectors:\n\n#### Sparse Module Pipeline\n\n```mermaid\ngraph LR\n    Text[\"Input Text\"] --> MLMTrans[\"MLMTransformer<br/>auto_model + MLM head\"]\n    MLMTrans --> TokenLogits[\"Token Logits<br/>[batch, seq_len, vocab_size]\"]\n    TokenLogits --> SpladePool[\"SpladePooling<br/>pooling_strategy + activation\"]\n    SpladePool --> SparseEmb[\"Sparse Embedding<br/>[batch, vocab_size]\"]\n    \n    subgraph SparseModules[\"Sparse Module Types\"]\n        MLMTrans\n        SpladePool\n    end\n```\n\n#### Sparse Module Types\n\n| Module | Purpose | Output Shape |\n|--------|---------|--------------|\n| `MLMTransformer` | Contextual token predictions | `[batch, seq_len, vocab_size]` |\n| `SpladePooling` | Aggregate tokens to sparse vector | `[batch, vocab_size]` |\n| `SparseStaticEmbedding` | Pre-computed static weights | `[batch, vocab_size]` |\n| `SparseAutoEncoder` | Learned sparse representations | `[batch, latent_dim]` |\n\nKey configuration:\n- `pooling_strategy`: `\"max\"`, `\"mean\"`, `\"sum\"`  \n- `activation_function`: `\"relu\"`, `\"log1p_relu\"`, `\"identity\"`\n- `k`: Number of top-k active dimensions (for `SparseAutoEncoder`)\n\nSources: [sentence_transformers/sparse_encoder/models/MLMTransformer.py](), [sentence_transformers/sparse_encoder/models/SpladePooling.py](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py](), [sentence_transformers/sparse_encoder/models/SparseAutoEncoder.py]()\n\n### Backend Support\n\nThe `Transformer` module supports multiple inference backends:\n\n| Backend | Description | Requirements |\n|---------|-------------|--------------|\n| `torch` | Standard PyTorch | Default |\n| `onnx` | ONNX Runtime optimization | `optimum[onnxruntime]` |\n| `openvino` | Intel hardware acceleration | `optimum[openvino]` |\n\nBackend selection affects model loading and inference performance but not the module interface.\n\nSources: [sentence_transformers/models/Transformer.py:174-195](), [sentence_transformers/models/Transformer.py:196-248]()\n\n## Module Forward Pass Interface\n\n### Feature Dictionary Flow\n\nAll modules operate on a shared features dictionary that flows through the pipeline:\n\n```mermaid\ngraph TD\n    Input[\"features = {}\"] --> InputMod[\"InputModule.tokenize()\"]\n    InputMod --> TokenFeats[\"{input_ids, attention_mask, ...}\"]\n    \n    TokenFeats --> Mod1[\"Module 1.forward(features)\"]\n    Mod1 --> F1[\"{input_ids, attention_mask,<br/>token_embeddings, ...}\"]\n    \n    F1 --> Mod2[\"Module 2.forward(features)\"]\n    Mod2 --> F2[\"{..., sentence_embedding}\"]\n    \n    F2 --> ModN[\"Module N.forward(features)\"]\n    ModN --> Final[\"{final features}\"]\n```\n\nCommon feature keys:\n- `input_ids`, `attention_mask`: From tokenization\n- `token_embeddings`: Per-token representations\n- `sentence_embedding`: Final sentence representation\n- `task`: Task type for Router modules\n\n### Module Interface Requirements\n\nEach module must implement:\n\n```python\ndef forward(self, features: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n    # Transform features and return updated dictionary\n    pass\n\ndef save(self, output_path: str, **kwargs) -> None:\n    # Save module configuration and weights\n    pass\n```\n\nInputModules additionally require:\n\n```python\ndef tokenize(self, texts: list[str], **kwargs) -> dict[str, torch.Tensor]:\n    # Convert text to model features\n    pass\n```\n\nSources: [sentence_transformers/models/Module.py:33-89](), [sentence_transformers/models/InputModule.py:60-73]()\n\n## Use Cases and Applications\n\nThe CrossEncoder is commonly used in several scenarios:\n\n1. **Reranking**: As the second stage in a retrieve-and-rerank pipeline\n   ```\n   Query → SentenceTransformer (retrieval) → Top-k documents → CrossEncoder (reranking) → Reranked results\n   ```",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Training_arguments_must_specify_router_mapping.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 999,
      "character_count": 4217,
      "created_at": "2025-10-16T17:42:33.304294",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Training_arguments_must_specify_router_mapping.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "2. **Text Pair Classification**: Directly scoring or classifying pairs of texts\n   - Natural language inference (entailment/contradiction)\n   - Semantic textual similarity\n   - Question-answer relevance\n\n3. **High-Precision Scoring**: When maximum accuracy is needed for a limited number of text pairs\n\nCrossEncoder models typically achieve higher accuracy than SentenceTransformer models for direct text pair comparison tasks, but they are computationally more expensive for large-scale comparisons.\n\nSources: [README.md:93-131]()\n\n## Selection Guide: CrossEncoder vs SentenceTransformer\n\n| Aspect | CrossEncoder | SentenceTransformer |\n|--------|-------------|---------------------|\n| **Output** | Scores/labels for text pairs | Embeddings for individual texts |\n| **Comparison** | Direct scoring of pairs | Similarity between embeddings |\n| **Scaling** | O(n) comparisons need O(n) inference calls | O(n) comparisons need O(1) inference calls |\n| **Accuracy** | Generally higher for direct comparisons | Good for retrieval, less precise for direct comparison |\n| **Use case** | Reranking, high-precision scoring of few pairs | Retrieval, semantic search, clustering |\n\nTypically, the optimal approach is to combine both: use SentenceTransformer for efficient retrieval of candidate documents, then use CrossEncoder for precise reranking of the top results.\n\nSources: [README.md:93-131](), [index.rst:65-98]()\n\n# Installation & Setup\n\nThis document covers installation procedures and dependency requirements for the sentence-transformers library. It explains the different installation options available for various use cases including basic inference, training, backend optimization, and development. For basic usage examples after installation, see [Quickstart Guide](#2.1). For training-specific setup details, see [Training](#3).\n\n## System Requirements\n\nThe sentence-transformers library requires **Python 3.9+**, **PyTorch 1.11.0+**, and **transformers v4.41.0+**. The library supports multiple backends and deployment scenarios through optional dependencies.\n\n### Installation Options Overview\n\nThe library provides five main installation configurations that correspond to different usage patterns:\n\n```mermaid\ngraph TB\n    subgraph \"Installation Options\"\n        Default[\"Default<br/>Basic inference\"]\n        ONNX[\"ONNX<br/>Optimized inference\"]\n        OpenVINO[\"OpenVINO<br/>Intel optimization\"]\n        Training[\"Default + Training<br/>Model training\"]\n        Development[\"Development<br/>Contributing\"]\n    end\n    \n    subgraph \"Core Capabilities\"\n        Default --> LoadSave[\"Model loading/saving\"]\n        Default --> Inference[\"Embedding generation\"]\n        \n        ONNX --> ONNXOpt[\"ONNX optimization\"]\n        ONNX --> Quantization[\"Model quantization\"]\n        \n        OpenVINO --> IntelOpt[\"Intel hardware optimization\"]\n        \n        Training --> TrainLoop[\"Training loops\"]\n        Training --> Evaluation[\"Model evaluation\"]\n        \n        Development --> Testing[\"Unit testing\"]\n        Development --> Linting[\"Code formatting\"]\n    end\n    \n    subgraph \"Backend Support\"\n        LoadSave --> PyTorchBackend[\"PyTorch backend\"]\n        ONNXOpt --> ONNXBackend[\"ONNX Runtime backend\"]\n        IntelOpt --> OpenVINOBackend[\"OpenVINO backend\"]\n    end\n    \n    subgraph \"Model Types\"\n        PyTorchBackend --> SentenceTransformer[\"SentenceTransformer\"]\n        PyTorchBackend --> SparseEncoder[\"SparseEncoder\"]\n        PyTorchBackend --> CrossEncoder[\"CrossEncoder\"]\n        \n        ONNXBackend --> OptimizedModels[\"Optimized models\"]\n        OpenVINOBackend --> IntelModels[\"Intel-optimized models\"]\n    end\n```\n\n**Sources:** [docs/installation.md:3-8]()\n\n## Package Manager Installation\n\n### pip Installation\n\nThe recommended installation method uses pip with specific extras for different use cases:\n\n| Installation Type | Command | Use Case |\n|------------------|---------|----------|\n| Default | `pip install -U sentence-transformers` | Basic inference only |\n| ONNX (GPU+CPU) | `pip install -U \"sentence-transformers[onnx-gpu]\"` | Optimized inference with GPU support |\n| ONNX (CPU only) | `pip install -U \"sentence-transformers[onnx]\"` | CPU-only optimized inference |\n| OpenVINO | `pip install -U \"sentence-transformers[openvino]\"` | Intel hardware optimization |\n| Training | `pip install -U \"sentence-transformers[train]\"` | Model training capabilities |\n| Development | `pip install -U \"sentence-transformers[dev]\"` | Full development environment |\n\n### conda Installation\n\nFor conda users, the base package is available through conda-forge:\n\n```bash",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Training_arguments_must_specify_router_mapping.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 971,
      "character_count": 4598,
      "created_at": "2025-10-16T17:42:33.310518",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Training_arguments_must_specify_router_mapping.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]