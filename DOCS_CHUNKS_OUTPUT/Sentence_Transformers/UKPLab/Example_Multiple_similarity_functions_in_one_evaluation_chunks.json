[
  {
    "text": "similarity_fn_names = [\"cosine\", \"dot\", \"euclidean\", \"manhattan\"]\n```\n\nThis generates metrics for each function, with optional `max_*` aggregated metrics for overall performance assessment.\n\n### Prediction Output\n\nThe `InformationRetrievalEvaluator` supports `write_predictions=True` to output retrieval results in JSONL format, enabling downstream analysis and fusion with other retrieval systems.\n\n### Embedding Optimization\n\nSeveral evaluators support advanced embedding configurations:\n- **Precision Control**: `precision` parameter for quantized embeddings (`\"int8\"`, `\"binary\"`, etc.)\n- **Dimension Truncation**: `truncate_dim` for reduced-dimension evaluation\n- **Normalization**: Automatic normalization for certain precision modes\n\nSources: [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:171-176](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:374-396]()\n\n# SentenceTransformer Evaluators\n\nThis document covers the evaluation system for `SentenceTransformer` models, including the base evaluator architecture and all available evaluator implementations. These evaluators are used during training to assess model performance on various downstream tasks, enabling automatic model selection and performance monitoring.\n\nFor evaluation of sparse encoder models, see [SparseEncoder Evaluators](#4.2). For cross-encoder evaluation, see [CrossEncoder Evaluators](#4.3). For comprehensive benchmark evaluation, see [NanoBEIR Evaluation](#4.4).\n\n## Evaluator Architecture\n\nAll SentenceTransformer evaluators inherit from the `SentenceEvaluator` base class, which provides a standardized interface for evaluation during training and inference.\n\n### Base Evaluator Structure\n\n```mermaid\nclassDiagram\n    class SentenceEvaluator {\n        +bool greater_is_better\n        +str primary_metric\n        +__call__(model, output_path, epoch, steps)\n        +prefix_name_to_metrics(metrics, name)\n        +store_metrics_in_model_card_data(model, metrics, epoch, steps)\n        +embed_inputs(model, sentences)\n        +get_config_dict()\n        +description : str\n    }\n    \n    class InformationRetrievalEvaluator {\n        +dict queries\n        +dict corpus\n        +dict relevant_docs\n        +compute_metrices(model)\n        +compute_metrics(queries_result_list)\n    }\n    \n    class EmbeddingSimilarityEvaluator {\n        +list sentences1\n        +list sentences2\n        +list scores\n        +compute_metrices(model)\n    }\n    \n    class BinaryClassificationEvaluator {\n        +list sentences1\n        +list sentences2\n        +list labels\n        +find_best_acc_and_threshold()\n        +find_best_f1_and_threshold()\n    }\n    \n    class RerankingEvaluator {\n        +list samples\n        +int at_k\n        +compute_metrices_batched(model)\n        +compute_metrices_individual(model)\n    }\n    \n    class TripletEvaluator {\n        +list anchors\n        +list positives\n        +list negatives\n        +dict margin\n    }\n    \n    class ParaphraseMiningEvaluator {\n        +dict sentences_map\n        +dict duplicates\n        +add_transitive_closure()\n    }\n    \n    SentenceEvaluator <|-- InformationRetrievalEvaluator\n    SentenceEvaluator <|-- EmbeddingSimilarityEvaluator\n    SentenceEvaluator <|-- BinaryClassificationEvaluator\n    SentenceEvaluator <|-- RerankingEvaluator\n    SentenceEvaluator <|-- TripletEvaluator\n    SentenceEvaluator <|-- ParaphraseMiningEvaluator\n```\n\n**Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:13-121]()\n\n### Key Base Class Features\n\nThe `SentenceEvaluator` base class provides several critical features:\n\n| Feature | Purpose | Key Methods |\n|---------|---------|-------------|\n| **Primary Metric** | Identifies the main metric for model selection | `primary_metric` attribute |\n| **Metric Direction** | Indicates if higher scores are better | `greater_is_better` attribute |\n| **Metric Prefixing** | Adds evaluator names to metric keys | `prefix_name_to_metrics()` |\n| **Model Card Integration** | Stores evaluation results in model metadata | `store_metrics_in_model_card_data()` |\n| **Embedding Interface** | Standardized text encoding | `embed_inputs()` |\n\n**Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:26-121]()\n\n## Core Evaluator Types\n\n### Information Retrieval Evaluator\n\nThe `InformationRetrievalEvaluator` is designed for search and retrieval tasks, computing standard IR metrics across large corpora.",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 916,
      "character_count": 4425,
      "created_at": "2025-10-16T17:42:32.843335",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TB\n    subgraph \"InformationRetrievalEvaluator\"\n        Queries[\"queries: Dict[str, str]\"]\n        Corpus[\"corpus: Dict[str, str]\"]\n        RelevantDocs[\"relevant_docs: Dict[str, Set[str]]\"]\n        \n        subgraph \"Metrics Computed\"\n            MRR[\"MRR@k (Mean Reciprocal Rank)\"]\n            NDCG[\"NDCG@k (Normalized DCG)\"]\n            MAP[\"MAP@k (Mean Average Precision)\"]\n            Accuracy[\"Accuracy@k\"]\n            PrecisionRecall[\"Precision@k / Recall@k\"]\n        end\n        \n        subgraph \"Configuration\"\n            ChunkSize[\"corpus_chunk_size: 50000\"]\n            BatchSize[\"batch_size: 32\"]\n            ScoreFunctions[\"score_functions: Dict\"]\n            Prompts[\"query_prompt / corpus_prompt\"]\n        end\n    end\n    \n    Queries --> MRR\n    Corpus --> MRR\n    RelevantDocs --> MRR\n    ChunkSize --> MRR\n```\n\nKey features:\n- **Chunked Processing**: Handles large corpora via `corpus_chunk_size` parameter [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:130]()\n- **Multiple Score Functions**: Supports different similarity functions via `score_functions` [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:141]()\n- **Asymmetric Encoding**: Different prompts for queries vs documents [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:143-146]()\n- **Prediction Export**: Optional JSONL output for downstream analysis [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:147]()\n\n**Sources:** [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-568]()\n\n### Embedding Similarity Evaluator\n\nThe `EmbeddingSimilarityEvaluator` measures correlation between predicted and ground-truth similarity scores.\n\n```mermaid\ngraph LR\n    subgraph \"Input Data\"\n        S1[\"sentences1: List[str]\"]\n        S2[\"sentences2: List[str]\"]\n        GoldScores[\"scores: List[float]\"]\n    end\n    \n    subgraph \"Similarity Functions\"\n        Cosine[\"cosine: pairwise_cos_sim\"]\n        Dot[\"dot: pairwise_dot_score\"]\n        Euclidean[\"euclidean: pairwise_euclidean_sim\"]\n        Manhattan[\"manhattan: pairwise_manhattan_sim\"]\n    end\n    \n    subgraph \"Correlation Metrics\"\n        Pearson[\"Pearson Correlation\"]\n        Spearman[\"Spearman Correlation\"]\n    end\n    \n    S1 --> Cosine\n    S2 --> Cosine\n    Cosine --> Pearson\n    Cosine --> Spearman\n    GoldScores --> Pearson\n    GoldScores --> Spearman\n```\n\nKey features:\n- **Multiple Similarity Functions**: Supports cosine, dot product, Euclidean, and Manhattan distance [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:184-189]()\n- **Precision Support**: Handles quantized embeddings (int8, uint8, binary, ubinary) [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:93]()\n- **Automatic Deduplication**: Avoids re-encoding identical sentences [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:225-237]()\n\n**Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:27-272]()\n\n### Binary Classification Evaluator\n\nThe `BinaryClassificationEvaluator` treats similarity as a binary classification problem.\n\n```mermaid\ngraph TD\n    subgraph \"Input Processing\"\n        Pairs[\"sentence pairs + binary labels\"]\n        Embeddings[\"encode sentence pairs\"]\n        Similarities[\"compute pairwise similarities\"]\n    end\n    \n    subgraph \"Threshold Optimization\"\n        AccThreshold[\"find_best_acc_and_threshold()\"]\n        F1Threshold[\"find_best_f1_and_threshold()\"]\n    end\n    \n    subgraph \"Metrics Output\"\n        Accuracy[\"Accuracy + optimal threshold\"]\n        F1Score[\"F1, Precision, Recall + threshold\"]\n        AP[\"Average Precision\"]\n        MCC[\"Matthews Correlation Coefficient\"]\n    end\n    \n    Pairs --> Embeddings\n    Embeddings --> Similarities\n    Similarities --> AccThreshold\n    Similarities --> F1Threshold\n    AccThreshold --> Accuracy\n    F1Threshold --> F1Score\n    Similarities --> AP\n    F1Threshold --> MCC\n```\n\n**Sources:** [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379]()\n\n### Reranking Evaluator\n\nThe `RerankingEvaluator` evaluates models on reranking tasks with query-document relevance.",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 946,
      "character_count": 4153,
      "created_at": "2025-10-16T17:42:32.848567",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TB\n    subgraph \"Sample Structure\"\n        Query[\"query: str\"]\n        Positive[\"positive: List[str]\"]\n        Negative[\"negative: List[str]\"]\n    end\n    \n    subgraph \"Processing Modes\"\n        Batched[\"compute_metrices_batched()\"]\n        Individual[\"compute_metrices_individual()\"]\n    end\n    \n    subgraph \"Ranking Metrics\"\n        MAP[\"Mean Average Precision\"]\n        MRR[\"MRR@k\"]\n        NDCG[\"NDCG@k\"]\n    end\n    \n    Query --> Batched\n    Positive --> Batched\n    Negative --> Batched\n    \n    Batched --> MAP\n    Batched --> MRR\n    Batched --> NDCG\n```\n\nKey features:\n- **Flexible Processing**: Choice between batched and individual encoding [sentence_transformers/evaluation/RerankingEvaluator.py:98]()\n- **Memory Optimization**: Batched mode for speed, individual mode for memory efficiency [sentence_transformers/evaluation/RerankingEvaluator.py:210-214]()\n\n**Sources:** [sentence_transformers/evaluation/RerankingEvaluator.py:25-372]()\n\n## Specialized Evaluators\n\n### Triplet Evaluator\n\nEvaluates triplet ranking performance where positive examples should be closer than negative examples.\n\n```mermaid\ngraph LR\n    subgraph \"Triplet Components\"\n        Anchors[\"anchors: List[str]\"]\n        Positives[\"positives: List[str]\"]\n        Negatives[\"negatives: List[str]\"]\n    end\n    \n    subgraph \"Margin Configuration\"\n        MarginDict[\"margin: Dict[str, float]\"]\n        DefaultMargin[\"default: 0.0 for all metrics\"]\n    end\n    \n    subgraph \"Evaluation Logic\"\n        Condition[\"similarity(anchor, positive) > similarity(anchor, negative) + margin\"]\n        Accuracy[\"accuracy per similarity function\"]\n    end\n    \n    Anchors --> Condition\n    Positives --> Condition\n    Negatives --> Condition\n    MarginDict --> Condition\n    Condition --> Accuracy\n```\n\n**Sources:** [sentence_transformers/evaluation/TripletEvaluator.py:26-271]()\n\n### Paraphrase Mining Evaluator\n\nEvaluates paraphrase detection performance by mining similar sentences from a corpus.\n\nKey features:\n- **Transitive Closure**: Optional transitive relationship enforcement [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:97]()\n- **Scalable Mining**: Uses `paraphrase_mining` utility for efficient processing [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:172-182]()\n- **F1 Optimization**: Finds optimal similarity threshold for paraphrase detection [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:187-212]()\n\n**Sources:** [sentence_transformers/evaluation/ParaphraseMiningEvaluator.py:18-279]()\n\n### Translation Evaluator\n\nMeasures cross-lingual alignment by checking if translations have highest mutual similarity.\n\n**Sources:** [sentence_transformers/evaluation/TranslationEvaluator.py:22-192]()\n\n### Knowledge Distillation Evaluators\n\nTwo evaluators support knowledge distillation scenarios:\n\n- **MSEEvaluator**: Computes MSE between teacher and student embeddings [sentence_transformers/evaluation/MSEEvaluator.py:18-158]()\n- **MSEEvaluatorFromDataFrame**: Structured multilingual distillation evaluation [sentence_transformers/evaluation/MSEEvaluatorFromDataFrame.py:20-139]()\n\n## Common Usage Patterns\n\n### Evaluation During Training\n\nAll evaluators follow the same calling convention for integration with training loops:\n\n```python\n# Standard evaluator call signature\nresults = evaluator(\n    model=sentence_transformer_model,\n    output_path=\"./evaluation_results\",\n    epoch=current_epoch,\n    steps=current_step\n)\n```\n\n### Metric Organization\n\n```mermaid\ngraph TD\n    subgraph \"Metric Processing Pipeline\"\n        RawMetrics[\"evaluator.compute_metrices()\"]\n        PrefixedMetrics[\"evaluator.prefix_name_to_metrics()\"]\n        ModelCard[\"evaluator.store_metrics_in_model_card_data()\"]\n        CSVOutput[\"CSV file output (optional)\"]\n    end\n    \n    subgraph \"Metric Structure\"\n        PrimaryMetric[\"evaluator.primary_metric\"]\n        AllMetrics[\"Dict[str, float] return value\"]\n        GreaterIsBetter[\"evaluator.greater_is_better\"]\n    end\n    \n    RawMetrics --> PrefixedMetrics\n    PrefixedMetrics --> ModelCard\n    PrefixedMetrics --> CSVOutput\n    PrefixedMetrics --> AllMetrics\n    PrimaryMetric --> AllMetrics\n```\n\n**Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:57-75]()\n\n### Configuration Management\n\nEach evaluator provides configuration introspection via `get_config_dict()`:",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 956,
      "character_count": 4370,
      "created_at": "2025-10-16T17:42:32.857283",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "| Evaluator | Key Configuration Parameters |\n|-----------|------------------------------|\n| `InformationRetrievalEvaluator` | `truncate_dim`, `query_prompt`, `corpus_prompt` |\n| `EmbeddingSimilarityEvaluator` | `truncate_dim`, `precision` |\n| `BinaryClassificationEvaluator` | `truncate_dim` |\n| `RerankingEvaluator` | `at_k`, `truncate_dim` |\n| `TripletEvaluator` | `margin`, `truncate_dim` |\n\n**Sources:** Multiple evaluator `get_config_dict()` methods across evaluation files\n\n## Integration with Training\n\nEvaluators integrate seamlessly with the SentenceTransformer training system:\n\n```mermaid\nsequenceDiagram\n    participant Trainer as \"SentenceTransformerTrainer\"\n    participant Evaluator as \"SentenceEvaluator\"\n    participant Model as \"SentenceTransformer\"\n    participant ModelCard as \"ModelCardData\"\n    \n    Trainer->>Evaluator: __call__(model, output_path, epoch, steps)\n    Evaluator->>Model: encode(sentences, **kwargs)\n    Model-->>Evaluator: embeddings\n    Evaluator->>Evaluator: compute_metrices()\n    Evaluator->>Evaluator: prefix_name_to_metrics()\n    Evaluator->>ModelCard: store_metrics_in_model_card_data()\n    Evaluator-->>Trainer: metrics dict with primary_metric\n```\n\nThe training system uses `evaluator.primary_metric` and `evaluator.greater_is_better` for:\n- **Model Selection**: Choosing best checkpoint when `load_best_model_at_end=True`\n- **Early Stopping**: Monitoring convergence based on evaluation metrics\n- **Logging**: Tracking primary metrics across training runs\n\n**Sources:** [sentence_transformers/evaluation/SentenceEvaluator.py:26-75]()",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 350,
      "character_count": 1581,
      "created_at": "2025-10-16T17:42:32.858165",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 3,
      "file_relative_path": "UKPLab\\sentence-transformers\\Example_Multiple_similarity_functions_in_one_evaluation.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]