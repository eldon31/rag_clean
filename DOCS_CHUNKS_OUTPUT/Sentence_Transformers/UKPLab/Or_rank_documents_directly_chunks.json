[
  {
    "text": "ranked_results = cross_encoder.rank(query, candidates, return_documents=True)\n```\n\n**Sources:** [sentence_transformers/cross_encoder/CrossEncoder.py:48-226](), [sentence_transformers/cross_encoder/CrossEncoder.py:394-486](), [sentence_transformers/cross_encoder/CrossEncoder.py:488-586](), [tests/cross_encoder/test_cross_encoder.py:71-96]()\n\n## Module Architecture\n\nAll model types in sentence-transformers use a modular architecture where functionality is composed of discrete, reusable modules.\n\n### Core Module Hierarchy\n\n```mermaid\ngraph TB\n    subgraph \"Base Module System\"\n        Module[\"Module<br/>Base class\"]\n        InputModule[\"InputModule<br/>Input processing\"]\n        Transformer[\"Transformer<br/>Token embeddings\"]\n        Pooling[\"Pooling<br/>Sentence embeddings\"]\n    end\n    \n    subgraph \"Processing Modules\"\n        Dense[\"Dense<br/>Linear transformation\"]\n        Normalize[\"Normalize<br/>L2 normalization\"]\n        LayerNorm[\"LayerNorm<br/>Layer normalization\"]\n        Router[\"Router<br/>Task routing\"]\n    end\n    \n    subgraph \"Specialized Modules\"\n        MLMTransformer[\"MLMTransformer<br/>Masked LM head\"]\n        SpladePooling[\"SpladePooling<br/>SPLADE activation\"]\n        CLIPModel[\"CLIPModel<br/>Multimodal encoding\"]\n    end\n    \n    Module --> InputModule\n    Module --> Dense\n    Module --> Normalize\n    InputModule --> Transformer\n    InputModule --> Pooling\n    Module --> Router\n    Module --> MLMTransformer\n    Module --> SpladePooling\n    Module --> CLIPModel\n```\n\n### Router Module for Asymmetric Architectures\n\nThe `Router` module enables asymmetric architectures where queries and documents follow different processing paths:\n\n```mermaid\ngraph TB\n    subgraph \"Router Architecture\"\n        Input[\"Input + Task\"]\n        Router[\"Router Module\"]\n        QueryPath[\"Query Submodules\"]\n        DocumentPath[\"Document Submodules\"]\n        Output[\"Task-specific Output\"]\n    end\n    \n    Input --> Router\n    Router -->|task=\"query\"| QueryPath\n    Router -->|task=\"document\"| DocumentPath\n    QueryPath --> Output\n    DocumentPath --> Output\n```\n\nThis enables models like the inference-free SPLADE variants where queries use only necessary tokens while documents use expanded representations.\n\n**Sources:** [sentence_transformers/models/Transformer.py:36-58](), [sentence_transformers/models/Pooling.py:9-41](), [sentence_transformers/models/Router.py](), [tests/sparse_encoder/test_sparse_encoder.py:171-196]()\n\n## Model Comparison and Selection\n\n| Model Type | Input Format | Output Format | Use Case | Computational Cost |\n|------------|--------------|---------------|----------|-------------------|\n| `SentenceTransformer` | Single texts | Dense vectors | Semantic similarity, clustering | Moderate |\n| `SparseEncoder` | Single texts | Sparse vectors | Lexical search, hybrid retrieval | Moderate |\n| `CrossEncoder` | Text pairs | Similarity scores | Reranking, pairwise classification | High |\n\n### When to Use Each Type\n\n- **SentenceTransformer**: General-purpose semantic tasks, when you need individual text embeddings\n- **SparseEncoder**: When interpretability matters, hybrid search scenarios, or working with search engines\n- **CrossEncoder**: When highest accuracy is needed for pairwise decisions, reranking scenarios\n\n### Backend Support\n\nAll three model types support multiple inference backends:\n- **PyTorch**: Full functionality, training support\n- **ONNX**: Optimized inference, 2-3x speedup\n- **OpenVINO**: Intel hardware optimization\n\n**Sources:** [sentence_transformers/SentenceTransformer.py:408-414](), [sentence_transformers/cross_encoder/CrossEncoder.py:259-265](), [README.md:89-167](), [index.rst:12-13]()\n\n# Module Architecture\n\nThis page details the modular building blocks that compose sentence transformer models. The sentence-transformers library uses a sequential pipeline architecture where modules are chained together to transform input text into final embeddings or representations.\n\n## Overview of Modular Design\n\nThe sentence-transformers library implements a modular architecture where models are composed of sequential modules. Each module performs a specific transformation on input features, passing the result to the next module in the pipeline.\n\n#### Core Model Architecture\n\n```mermaid\ngraph LR\n    Input[\"Input Text\"] --> M1[\"Module 1<br/>(InputModule)\"]\n    M1 --> M2[\"Module 2<br/>(Module)\"]\n    M2 --> M3[\"Module 3<br/>(Module)\"]\n    M3 --> Output[\"Final Output\"]\n    \n    subgraph \"Sequential Pipeline\"\n        M1\n        M2\n        M3\n    end\n```\n\nThe core models inherit from `nn.Sequential` and store modules in an ordered collection:",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Or_rank_documents_directly.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 996,
      "character_count": 4629,
      "created_at": "2025-10-16T17:42:33.055532",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\Or_rank_documents_directly.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "- **`SentenceTransformer`**: Inherits from `nn.Sequential`, produces dense embeddings\n- **`SparseEncoder`**: Inherits from `SentenceTransformer`, produces sparse embeddings  \n- **`CrossEncoder`**: Separate architecture (`nn.Module`), scores text pairs directly\n\nSources: [sentence_transformers/SentenceTransformer.py:61](), [sentence_transformers/sparse_encoder/SparseEncoder.py:27](), [sentence_transformers/cross_encoder/CrossEncoder.py:48]()\n\n## Module Type Hierarchy\n\nThe library defines two main types of modules that form the building blocks of all models:\n\n#### Module Class Hierarchy\n\n```mermaid\nclassDiagram\n    class Module {\n        <<abstract>>\n        +forward(features)\n        +save(output_path)\n        +load(model_name_or_path)\n        +get_sentence_embedding_dimension()\n        #config_keys: list[str]\n        #config_file_name: str\n    }\n    \n    class InputModule {\n        <<abstract>>\n        +tokenize(texts)\n        +save_tokenizer(output_path)\n        +tokenizer\n        +save_in_root: bool\n    }\n    \n    Module <|-- InputModule\n    \n    class Transformer {\n        +auto_model: PreTrainedModel\n        +tokenizer: PreTrainedTokenizer\n        +max_seq_length: int\n        +backend: str\n        +do_lower_case: bool\n    }\n    \n    class Pooling {\n        +pooling_mode_cls_token: bool\n        +pooling_mode_mean_tokens: bool\n        +pooling_mode_max_tokens: bool\n        +word_embedding_dimension: int\n        +include_prompt: bool\n    }\n    \n    class Router {\n        +sub_modules: ModuleDict\n        +default_route: str\n        +allow_empty_key: bool\n        +forward_kwargs: list[str]\n    }\n    \n    InputModule <|-- Transformer\n    InputModule <|-- Router\n    Module <|-- Pooling\n```\n\n#### Module Types\n\n- **`Module`**: Base class defining `forward()`, `save()`, `load()` interface\n- **`InputModule`**: Subclass adding `tokenize()` capability (must be first in pipeline)\n- **Dense modules**: `Transformer`, `Pooling`, `Normalize`, `Dense` \n- **Sparse modules**: `MLMTransformer`, `SpladePooling`, `SparseStaticEmbedding`\n- **Routing modules**: `Router` for asymmetric architectures\n\nSources: [sentence_transformers/models/Module.py:33-89](), [sentence_transformers/models/InputModule.py:13-93](), [sentence_transformers/models/Transformer.py:36](), [sentence_transformers/models/Pooling.py:9](), [sentence_transformers/models/Router.py:22]()\n\n## Core Module Types\n\n### Transformer Module\n\nThe `Transformer` module is the most common input module, wrapping Hugging Face transformer models:\n\n```mermaid\ngraph TD\n    subgraph \"Transformer Module\"\n        Input[\"Input Texts\"] --> Tokenizer[\"AutoTokenizer\"]\n        Tokenizer --> Features[\"TokenizedFeatures<br/>{input_ids, attention_mask}\"]\n        Features --> AutoModel[\"AutoModel<br/>(BERT, RoBERTa, etc.)\"]\n        AutoModel --> TokenEmb[\"token_embeddings\"]\n        TokenEmb --> Output[\"{token_embeddings,<br/>attention_mask,<br/>all_layer_embeddings?}\"]\n    end\n```\n\nKey attributes:\n- `auto_model`: The wrapped Hugging Face model\n- `tokenizer`: The associated tokenizer\n- `max_seq_length`: Maximum sequence length\n- `backend`: Inference backend (`torch`, `onnx`, `openvino`)\n\nSources: [sentence_transformers/models/Transformer.py:37-646]()\n\n### Pooling Module\n\nThe `Pooling` module aggregates token embeddings into sentence embeddings:\n\n```mermaid\ngraph TD\n    Input[\"{token_embeddings,<br/>attention_mask}\"] --> Strategy[\"Pooling Strategy\"]\n    \n    Strategy --> CLS[\"CLS Token<br/>pooling_mode_cls_token\"]\n    Strategy --> Mean[\"Mean Pooling<br/>pooling_mode_mean_tokens\"] \n    Strategy --> Max[\"Max Pooling<br/>pooling_mode_max_tokens\"]\n    Strategy --> Last[\"Last Token<br/>pooling_mode_lasttoken\"]\n    Strategy --> Weighted[\"Weighted Mean<br/>pooling_mode_weightedmean_tokens\"]\n    \n    CLS --> Concat[\"Concatenate Outputs\"]\n    Mean --> Concat\n    Max --> Concat\n    Last --> Concat\n    Weighted --> Concat\n    \n    Concat --> SentEmb[\"{sentence_embedding}\"]\n```\n\nPooling strategies can be combined by setting multiple `pooling_mode_*` flags to `True`.\n\nSources: [sentence_transformers/models/Pooling.py:9-248]()\n\n### Router Module\n\nThe `Router` module enables asymmetric architectures with different processing paths based on task type:\n\n#### Router Architecture",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Or_rank_documents_directly.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 993,
      "character_count": 4253,
      "created_at": "2025-10-16T17:42:33.064761",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\Or_rank_documents_directly.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "```mermaid\ngraph TD\n    Input[\"{features, task='query'}\"] --> RouterForward[\"Router.forward()\"]\n    \n    RouterForward --> TaskCheck{task in sub_modules?}\n    TaskCheck -->|\"query\"| QuerySeq[\"self.sub_modules['query']<br/>nn.Sequential\"]\n    TaskCheck -->|\"document\"| DocSeq[\"self.sub_modules['document']<br/>nn.Sequential\"]  \n    TaskCheck -->|None| DefaultRoute[\"self.default_route<br/>or raise ValueError\"]\n    \n    QuerySeq --> QueryMods[\"Query-specific modules<br/>(e.g. SparseStaticEmbedding)\"]\n    DocSeq --> DocMods[\"Document-specific modules<br/>(e.g. MLMTransformer + SpladePooling)\"]\n    \n    QueryMods --> QueryEmb[\"{sentence_embedding}\"]\n    DocMods --> DocEmb[\"{sentence_embedding}\"]\n```\n\n#### Router Configuration\n\n```python",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\Or_rank_documents_directly.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 184,
      "character_count": 739,
      "created_at": "2025-10-16T17:42:33.065101",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "UKPLab\\sentence-transformers\\Or_rank_documents_directly.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]