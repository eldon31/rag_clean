[
  {
    "text": "CrossEncoder evaluators assess CrossEncoder models that take pairs of texts as input and output classification or ranking scores. While SentenceTransformer evaluators focus on embedding quality, CrossEncoder evaluators measure pairwise scoring accuracy for classification, ranking, and retrieval tasks.\n\n## Overview\n\nCrossEncoder models are evaluated using specialized evaluator classes that measure their ability to correctly score text pairs. The main evaluators applicable to CrossEncoder models include `BinaryClassificationEvaluator` for classification tasks, `RerankingEvaluator` for ranking tasks, and `InformationRetrievalEvaluator` for retrieval tasks.\n\n**CrossEncoder Evaluation Architecture**\n```mermaid\nflowchart TD\n    subgraph models[\"Model Types\"]\n        CE[\"CrossEncoder\"]\n    end\n    \n    subgraph evaluators[\"Evaluator Classes\"]\n        BCE[\"BinaryClassificationEvaluator\"]\n        RE[\"RerankingEvaluator\"] \n        IRE[\"InformationRetrievalEvaluator\"]\n    end\n    \n    subgraph metrics[\"Evaluation Metrics\"]\n        ACC[\"Accuracy\"]\n        F1[\"F1 Score\"]\n        NDCG[\"NDCG@k\"]\n        MAP[\"MAP\"]\n        MRR[\"MRR@k\"]\n    end\n    \n    CE --> BCE\n    CE --> RE\n    CE --> IRE\n    \n    BCE --> ACC\n    BCE --> F1\n    RE --> NDCG\n    RE --> MAP\n    RE --> MRR\n    IRE --> NDCG\n    IRE --> MAP\n    IRE --> MRR\n```\n\nSources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27](), [sentence_transformers/evaluation/RerankingEvaluator.py:25](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23]()\n\n## Core Evaluator Classes\n\n### BinaryClassificationEvaluator\n\nThe `BinaryClassificationEvaluator` class evaluates models on binary classification tasks where pairs of texts are classified as similar (1) or dissimilar (0). It computes accuracy, F1 score, precision, and recall metrics.\n\n**Key Methods:**\n- `__call__(model, output_path, epoch, steps)` - Main evaluation method\n- `compute_metrices(model)` - Computes classification metrics  \n- `find_best_acc_and_threshold()` - Finds optimal classification threshold\n- `find_best_f1_and_threshold()` - Finds optimal F1 threshold\n\n**Supported Similarity Functions:**\n- Cosine similarity (`SimilarityFunction.COSINE`)\n- Dot product (`SimilarityFunction.DOT_PRODUCT`) \n- Manhattan distance (`SimilarityFunction.MANHATTAN`)\n- Euclidean distance (`SimilarityFunction.EUCLIDEAN`)\n\nSources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-83](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:223-294]()\n\n### RerankingEvaluator\n\nThe `RerankingEvaluator` class evaluates models on reranking tasks by computing similarity scores between queries and documents, then measuring ranking quality.\n\n**Key Methods:**\n- `__call__(model, output_path, epoch, steps)` - Main evaluation method\n- `compute_metrices(model)` - Computes ranking metrics\n- `compute_metrices_batched(model)` - Batched computation for efficiency\n- `compute_metrices_individual(model)` - Individual computation for memory efficiency\n\n**Metrics Computed:**\n- Mean Average Precision (MAP)\n- Mean Reciprocal Rank (MRR@k)  \n- Normalized Discounted Cumulative Gain (NDCG@k)\n\nSources: [sentence_transformers/evaluation/RerankingEvaluator.py:25-87](), [sentence_transformers/evaluation/RerankingEvaluator.py:200-342]()\n\n### InformationRetrievalEvaluator\n\nThe `InformationRetrievalEvaluator` class evaluates models on information retrieval tasks using query-document pairs and relevance judgments.\n\n**Key Methods:**\n- `__call__(model, output_path, epoch, steps)` - Main evaluation method\n- `compute_metrices(model, corpus_model, corpus_embeddings)` - Computes IR metrics\n- `embed_inputs(model, sentences, encode_fn_name)` - Handles encoding for queries/documents\n\n**Metrics Computed:**\n- Accuracy@k, Precision@k, Recall@k\n- Mean Reciprocal Rank (MRR@k)\n- Normalized Discounted Cumulative Gain (NDCG@k) \n- Mean Average Precision (MAP@k)\n\nSources: [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-123](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:292-408]()",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\CrossEncoder_Evaluators.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 932,
      "character_count": 4087,
      "created_at": "2025-10-16T17:42:32.807828",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "UKPLab\\sentence-transformers\\CrossEncoder_Evaluators.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  },
  {
    "text": "**Evaluator Class Hierarchy**\n```mermaid\nclassDiagram\n    class SentenceEvaluator {\n        <<abstract>>\n        +__call__(model, output_path, epoch, steps) dict\n        +primary_metric str\n        +prefix_name_to_metrics(metrics, name) dict\n        +store_metrics_in_model_card_data(model, metrics, epoch, steps)\n    }\n    \n    class BinaryClassificationEvaluator {\n        +sentences1 list\n        +sentences2 list  \n        +labels list\n        +similarity_fn_names list\n        +compute_metrices(model) dict\n        +find_best_acc_and_threshold(scores, labels, high_score_more_similar) tuple\n        +find_best_f1_and_threshold(scores, labels, high_score_more_similar) tuple\n    }\n    \n    class RerankingEvaluator {\n        +samples list\n        +at_k int\n        +similarity_fct Callable\n        +compute_metrices_batched(model) dict\n        +compute_metrices_individual(model) dict\n    }\n    \n    class InformationRetrievalEvaluator {\n        +queries dict\n        +corpus dict\n        +relevant_docs dict\n        +compute_metrices(model, corpus_model, corpus_embeddings) dict\n        +embed_inputs(model, sentences, encode_fn_name) ndarray\n    }\n    \n    SentenceEvaluator <|-- BinaryClassificationEvaluator\n    SentenceEvaluator <|-- RerankingEvaluator  \n    SentenceEvaluator <|-- InformationRetrievalEvaluator\n```\n\nSources: [sentence_transformers/evaluation/SentenceEvaluator.py](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27](), [sentence_transformers/evaluation/RerankingEvaluator.py:25](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23]()\n\n## Evaluation Workflow\n\nCrossEncoder evaluators follow a standard evaluation process that involves data preparation, model scoring, metric computation, and result reporting.\n\n**Evaluation Process Flow**\n```mermaid\nsequenceDiagram\n    participant Data as \"Test Data\"\n    participant Eval as \"Evaluator\"\n    participant Model as \"CrossEncoder\"\n    participant Metrics as \"Metrics Computer\"\n    participant Output as \"CSV/Results\"\n    \n    Data ->> Eval: \"Load test pairs & labels\"\n    Eval ->> Model: \"encode(sentences1, sentences2)\"\n    Model ->> Eval: \"Return embeddings/scores\"\n    Eval ->> Metrics: \"compute_metrices(scores, labels)\"\n    Metrics ->> Eval: \"Return metric values\"\n    Eval ->> Output: \"Write CSV results\"\n    Eval ->> Model: \"Store in model_card_data\"\n```\n\n### Standard Evaluation Steps\n\n1. **Data Loading**: Load sentence pairs and ground truth labels\n2. **Model Encoding**: Generate embeddings or scores for text pairs\n3. **Score Computation**: Apply similarity functions or classification\n4. **Metric Calculation**: Compute task-specific evaluation metrics\n5. **Result Storage**: Write results to CSV files and model metadata\n\n### Primary Metric Selection\n\nEach evaluator defines a `primary_metric` property that identifies the main performance measure:\n\n| Evaluator | Primary Metric | Description |\n|-----------|----------------|-------------|\n| `BinaryClassificationEvaluator` | `{name}_cosine_ap` or `{name}_max_ap` | Average Precision |\n| `RerankingEvaluator` | `{name}_ndcg@{k}` | NDCG at rank k |\n| `InformationRetrievalEvaluator` | `{name}_{score_fn}_ndcg@{k}` | NDCG with score function |\n\nSources: [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:215-218](), [sentence_transformers/evaluation/RerankingEvaluator.py:135](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:272-280]()\n\n## Usage Examples\n\n### Binary Classification Evaluation\n\n```python\nfrom sentence_transformers.evaluation import BinaryClassificationEvaluator\n\n# Initialize with sentence pairs and binary labels\nevaluator = BinaryClassificationEvaluator(\n    sentences1=[\"The cat sat on the mat\", \"Hello world\"],\n    sentences2=[\"A cat was sitting on a rug\", \"Goodbye earth\"], \n    labels=[1, 0],  # 1 for similar, 0 for dissimilar\n    name=\"similarity_test\"\n)\n\n# Evaluate model and get metrics\nresults = evaluator(model)\nprint(f\"Accuracy: {results['similarity_test_cosine_accuracy']}\")\nprint(f\"F1 Score: {results['similarity_test_cosine_f1']}\")\n```\n\n### Reranking Evaluation\n\n```python\nfrom sentence_transformers.evaluation import RerankingEvaluator",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Sentence_Transformers\\UKPLab\\sentence-transformers\\CrossEncoder_Evaluators.md",
      "input_type": "sentence_transformers",
      "chunking_strategy": "programming_language_documentation",
      "token_count": 947,
      "character_count": 4186,
      "created_at": "2025-10-16T17:42:32.813779",
      "parent_context": null,
      "semantic_type": "sentence_transformers",
      "collection_name": "Sentence_Transformers",
      "subfolder_name": "UKPLab",
      "collection_strategy": "programming_language_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "UKPLab\\sentence-transformers\\CrossEncoder_Evaluators.md",
      "collection_context": "Sentence_Transformers/UKPLab"
    }
  }
]