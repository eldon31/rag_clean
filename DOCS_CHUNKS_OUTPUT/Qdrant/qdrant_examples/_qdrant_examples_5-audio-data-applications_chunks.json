[
  {
    "text": "Audio Data Applications | qdrant/examples | DeepWiki\n\n[Index your code with Devin](private-repo.md)\n\n[DeepWiki](https://deepwiki.com)\n\n[DeepWiki](.md)\n\n[qdrant/examples](https://github.com/qdrant/examples \"Open repository\")\n\n[Index your code with](private-repo.md)\n\n[Devin](private-repo.md)\n\nShare\n\nLast indexed: 26 June 2025 ([b3c4b2](https://github.com/qdrant/examples/commits/b3c4b28f))\n\n- [Overview](qdrant/examples/1-overview.md)\n- [Getting Started with Qdrant](qdrant/examples/2-getting-started-with-qdrant.md)\n- [Text Data Applications](qdrant/examples/3-text-data-applications.md)\n- [Code Search with Dual Embeddings](qdrant/examples/3.1-code-search-with-dual-embeddings.md)\n- [Extractive Question Answering](qdrant/examples/3.2-extractive-question-answering.md)\n- [Movie Recommendations with Sparse Vectors](qdrant/examples/3.3-movie-recommendations-with-sparse-vectors.md)\n- [Image Data Applications](qdrant/examples/4-image-data-applications.md)\n- [E-commerce Reverse Image Search](qdrant/examples/4.1-e-commerce-reverse-image-search.md)\n- [Medical Image Search with Vision Transformers](qdrant/examples/4.2-medical-image-search-with-vision-transformers.md)\n- [Audio Data Applications](qdrant/examples/5-audio-data-applications.md)\n- [Music Recommendation Engine](qdrant/examples/5.1-music-recommendation-engine.md)\n- [Advanced RAG Systems](qdrant/examples/6-advanced-rag-systems.md)\n- [Multivector RAG with DSPy](qdrant/examples/6.1-multivector-rag-with-dspy.md)\n- [Graph-Enhanced RAG with Neo4j](qdrant/examples/6.2-graph-enhanced-rag-with-neo4j.md)\n- [PDF Retrieval at Scale](qdrant/examples/6.3-pdf-retrieval-at-scale.md)\n- [Agentic Systems with CrewAI](qdrant/examples/7-agentic-systems-with-crewai.md)\n- [Meeting Analysis with Agentic RAG](qdrant/examples/7.1-meeting-analysis-with-agentic-rag.md)\n- [Additional Use Cases](qdrant/examples/8-additional-use-cases.md)\n- [Self-Query Systems with LangChain](qdrant/examples/8.1-self-query-systems-with-langchain.md)\n- [Development Environment Setup](qdrant/examples/8.2-development-environment-setup.md)\n\nMenu\n\n# Audio Data Applications\n\nRelevant source files\n\n- [qdrant\\_101\\_audio\\_data/03\\_qdrant\\_101\\_audio.ipynb](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/03_qdrant_101_audio.ipynb)\n- [qdrant\\_101\\_audio\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md)\n- [qdrant\\_101\\_text\\_data/README.md](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/README.md)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_25\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_25_0.png)\n- [qdrant\\_101\\_text\\_data/qdrant\\_and\\_text\\_data\\_files/qdrant\\_and\\_text\\_data\\_28\\_0.png](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_text_data/qdrant_and_text_data_files/qdrant_and_text_data_28_0.png)\n\nThis document covers audio data processing and music recommendation systems using Qdrant vector database. The implementation demonstrates how to extract embeddings from audio files and build semantic search and recommendation engines for music discovery.\n\nFor text-based search and recommendations, see [Text Data Applications](qdrant/examples/3-text-data-applications.md). For image-based similarity search, see [Image Data Applications](qdrant/examples/4-image-data-applications.md).\n\n## Overview",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 992,
      "character_count": 3463,
      "created_at": "2025-10-16T17:42:29.520732",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_examples",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "Qdrant\\qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
      "collection_context": "Qdrant/qdrant_examples"
    }
  },
  {
    "text": "The audio data applications showcase a complete pipeline for processing music files and building recommendation systems. The implementation uses the Ludwig Music Dataset containing over 10,000 songs across different genres and subgenres, demonstrating three different approaches to audio embedding generation and their integration with Qdrant's vector search capabilities.\n\n**Audio Processing Pipeline**\n\n```\n```\n\nSources: [qdrant\\_101\\_audio\\_data/README.md1-52](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1-L52) [qdrant\\_101\\_audio\\_data/README.md620-642](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L620-L642)\n\n## Dataset and Data Preparation\n\nThe system processes the Ludwig Music Dataset, which provides a comprehensive collection for music information retrieval (MIR). The dataset structure includes multiple data modalities and metadata for each track.\n\n**Dataset Structure**\n\n| Component     | Description                         | Purpose                                   |\n| ------------- | ----------------------------------- | ----------------------------------------- |\n| `mp3/`        | Audio files by genre                | Raw audio data for embedding generation   |\n| `labels.json` | Track metadata                      | Artist, genre, subgenre, name information |\n| `spectogram/` | Visual frequency representations    | Alternative data representation           |\n| `mfccs/`      | Mel-frequency cepstral coefficients | Audio feature representations             |\n\n**Data Processing Components**\n\n```\n```\n\nThe data preparation pipeline extracts unique identifiers from audio file paths and processes the complex nested metadata structure from `labels.json`. The `get_metadata()` function normalizes artist, genre, name, and subgenre information, while `get_vals()` flattens the subgenre lists for easier processing.\n\nSources: [qdrant\\_101\\_audio\\_data/README.md131-617](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L131-L617) [qdrant\\_101\\_audio\\_data/README.md196-235](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L196-L235)\n\n## Audio Embedding Generation\n\nThe system implements three distinct approaches for generating audio embeddings, each with different characteristics and use cases. The embeddings capture important audio features such as pitch, timbre, and spatial characteristics of sound.\n\n**Embedding Architecture Comparison**\n\n```\n```\n\n### OpenL3 Implementation\n\nOpenL3 provides pre-trained models specifically designed for audio processing tasks. The implementation uses the music-specific model with mel128 input representation.\n\n**Key Functions:**\n\n- `get_open_embs()`: Batch processing function for embedding extraction\n- `openl3.models.load_audio_embedding_model()`: Model initialization with specific parameters\n- Mean pooling over timestamp dimension for fixed-size embeddings\n\n### PANNS Inference Implementation\n\nThe PANNS (PANNs: Large-Scale Pretrained Audio Neural Networks) approach offers the best performance for music classification tasks and is used as the primary method in the tutorial.\n\n**Core Components:**\n\n- `AudioTagging` class initialization with automatic checkpoint download\n- `at.inference()` method for batch processing\n- Direct 2048-dimensional embedding output without additional pooling\n\n### Transformers Wav2Vec2 Implementation\n\nThe Wav2Vec2 approach demonstrates transformer architecture adaptation for audio, though optimized for speech rather than music.\n\n**Processing Pipeline:**\n\n- Audio resampling to 16kHz for model compatibility\n- `AutoFeatureExtractor` for input preprocessing\n- `mean_pooling()` function for sequence-to-vector conversion\n\nSources: [qdrant\\_101\\_audio\\_data/README.md644-897](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L644-L897) [qdrant\\_101\\_audio\\_data/README.md746-800](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L746-L800)\n\n## Qdrant Vector Database Integration\n\nThe audio embeddings are stored and managed using Qdrant vector database, configured specifically for audio similarity search and recommendation tasks.\n\n**Database Configuration**\n\n```\n```\n\n### Collection Configuration\n\nThe `music_collection` is configured with:\n\n- **Vector dimension**: 2048 (matching PANNS output)\n- **Distance metric**: COSINE similarity\n- **Payload structure**: Artist, genre, name, subgenres, file URLs\n\n### Payload Structure\n\nThe metadata payload contains structured information for each track:\n\n```\n```",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 978,
      "character_count": 4607,
      "created_at": "2025-10-16T17:42:29.537924",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_examples",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "Qdrant\\qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
      "collection_context": "Qdrant/qdrant_examples"
    }
  },
  {
    "text": "Sources: [qdrant\\_101\\_audio\\_data/README.md898-919](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L898-L919) [qdrant\\_101\\_audio\\_data/README.md115-126](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L115-L126)\n\n## Music Recommendation System\n\nThe recommendation system leverages Qdrant's search and recommendation APIs to provide semantic music discovery based on audio content similarity.\n\n**Recommendation Architecture**\n\n```\n```\n\n### Search Functionality\n\nThe system implements multiple search patterns:\n\n1. **Vector Similarity Search**: Direct comparison using pre-computed embeddings\n2. **Recommendation API**: Using positive and negative examples for refined results\n3. **Filtered Search**: Combining semantic similarity with metadata constraints\n\n### Example Usage Patterns\n\n**Basic Similarity Search**:\n\n- Query with embedding vector from existing track\n- Returns top-k most similar tracks with similarity scores\n- Includes full metadata payload for each result\n\n**Recommendation with Preferences**:\n\n- Specify positive examples (liked tracks)\n- Optional negative examples (disliked tracks)\n- Qdrant computes optimized recommendation vector\n\n**Genre-Filtered Search**:\n\n- Combine vector similarity with metadata filtering\n- Use `models.Filter` and `models.FieldCondition` for constraints\n- Example: Find similar tracks within \"Business\" or \"Latin\" genres\n\nSources: [qdrant\\_101\\_audio\\_data/README.md920-1216](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L920-L1216) [qdrant\\_101\\_audio\\_data/README.md1053-1091](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1053-L1091)\n\n## System Integration and Architecture\n\nThe complete audio processing system integrates multiple components into a cohesive music recommendation platform.\n\n**End-to-End System Flow**\n\n```\n```\n\n### Key Performance Characteristics\n\n| Embedding Method | Dimensions | Quality | Speed  | Use Case            |\n| ---------------- | ---------- | ------- | ------ | ------------------- |\n| OpenL3           | 512        | High    | Slow   | Research/Accuracy   |\n| PANNS Inference  | 2048       | Highest | Fast   | Production          |\n| Wav2Vec2         | 768        | Lower   | Medium | Speech/Experimental |\n\n### Scalability Considerations\n\nThe system architecture supports scaling through:\n\n- **Batch Processing**: Efficient embedding generation for large datasets\n- **Vector Database**: Qdrant's optimized storage and retrieval\n- **Flexible Embedding**: Support for multiple embedding approaches\n- **Metadata Integration**: Rich payload structure for complex queries\n\nSources: [qdrant\\_101\\_audio\\_data/README.md1-52](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L1-L52) [qdrant\\_101\\_audio\\_data/README.md898-1216](https://github.com/qdrant/examples/blob/b3c4b28f/qdrant_101_audio_data/README.md#L898-L1216)\n\nDismiss\n\nRefresh this wiki\n\nEnter email to refresh\n\n### On this page\n\n- [Audio Data Applications](#audio-data-applications.md)\n- [Overview](#overview.md)\n- [Dataset and Data Preparation](#dataset-and-data-preparation.md)\n- [Audio Embedding Generation](#audio-embedding-generation.md)\n- [OpenL3 Implementation](#openl3-implementation.md)\n- [PANNS Inference Implementation](#panns-inference-implementation.md)\n- [Transformers Wav2Vec2 Implementation](#transformers-wav2vec2-implementation.md)\n- [Qdrant Vector Database Integration](#qdrant-vector-database-integration.md)\n- [Collection Configuration](#collection-configuration.md)\n- [Payload Structure](#payload-structure.md)\n- [Music Recommendation System](#music-recommendation-system.md)\n- [Search Functionality](#search-functionality.md)\n- [Example Usage Patterns](#example-usage-patterns.md)\n- [System Integration and Architecture](#system-integration-and-architecture.md)\n- [Key Performance Characteristics](#key-performance-characteristics.md)\n- [Scalability Considerations](#scalability-considerations.md)",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 965,
      "character_count": 4029,
      "created_at": "2025-10-16T17:42:29.546366",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_examples",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "Qdrant\\qdrant_examples\\_qdrant_examples_5-audio-data-applications.md",
      "collection_context": "Qdrant/qdrant_examples"
    }
  }
]