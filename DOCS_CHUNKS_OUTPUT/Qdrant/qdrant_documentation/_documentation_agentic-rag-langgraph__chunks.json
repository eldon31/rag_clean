[
  {
    "text": "Agentic RAG With LangGraph - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 933,
      "character_count": 3501,
      "created_at": "2025-10-16T17:42:21.409743",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 968,
      "character_count": 3625,
      "created_at": "2025-10-16T17:42:21.412792",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 965,
      "character_count": 3819,
      "created_at": "2025-10-16T17:42:21.417997",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 978,
      "character_count": 3651,
      "created_at": "2025-10-16T17:42:21.420012",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 3,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- Agentic RAG With LangGraph\n\n# Agentic RAG With LangGraph and Qdrant\n\nTraditional Retrieval-Augmented Generation (RAG) systems follow a straightforward path: query → retrieve → generate. Sure, this works well for many scenarios. But let’s face it—this linear approach often struggles when you’re dealing with complex queries that demand multiple steps or pulling together diverse types of information.",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 969,
      "character_count": 3905,
      "created_at": "2025-10-16T17:42:21.424183",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 4,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "[Agentic RAG](https://qdrant.tech/articles/agentic-rag/) takes things up a notch by introducing AI agents that can orchestrate multiple retrieval steps and smartly decide how to gather and use the information you need. Think of it this way: in an Agentic RAG workflow, RAG becomes just one powerful tool in a much bigger and more versatile toolkit.\n\nBy combining LangGraph’s robust state management with Qdrant’s cutting-edge vector search, we’ll build a system that doesn’t just answer questions—it tackles complex, multi-step information retrieval tasks with finesse.\n\n## What We’ll Build\n\nWe’re building an AI agent to answer questions about Hugging Face and Transformers documentation using LangGraph. At the heart of our AI agent lies LangGraph, which acts like a conductor in an orchestra. It directs the flow between various components—deciding when to retrieve information, when to perform a web search, and when to generate responses.\n\nThe components are: two Qdrant vector stores and the Brave web search engine. However, our agent doesn’t just blindly follow one path. Instead, it evaluates each query and decides whether to tap into the first vector store, the second one, or search the web.\n\nThis selective approach gives your system the flexibility to choose the best data source for the job, rather than being locked into the same retrieval process every time, like traditional RAG. While we won’t dive into query refinement in this tutorial, the concepts you’ll learn here are a solid foundation for adding that functionality down the line.\n\n## Workflow\n\n| **Step**                            | **Description**                                                                                                                                                                                                                                                                                         |\n| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **1. User Input**                   | You start by entering a query or request through an interface, like a chatbot or a web form. This query is sent straight to the AI Agent, the brain of the operation.                                                                                                                                   |\n| **2. AI Agent Processes the Query** | The AI Agent analyzes your query, figuring out what you’re asking and which tools or data sources will best answer your question.                                                                                                                                                                       |\n| **3. Tool Selection**               | Based on its analysis, the AI Agent picks the right tool for the job. Your data is spread across two vector databases, and depending on the query, it chooses the appropriate one. For queries needing real-time or external web data, the agent taps into a web search tool powered by BraveSearchAPI. |\n| **4. Query Execution**              | The AI Agent then puts its chosen tool to work: - **RAG Tool 1** queries Vector Database 1. - **RAG Tool 2** queries Vector Database 2. - **Web Search Tool** dives into the internet using the search API.                                                                                             |\n| **5. Data Retrieval**               | The results roll in: - Vector Database 1 and 2 return the most relevant documents for your query. - The Web Search Tool provides up-to-date or external information.                                                                                                                                    |\n| **6. Response Generation**          | Using a text generation model (like GPT), the AI Agent crafts a detailed and accurate response tailored to your query.                                                                                                                                                                                  |\n| **7. User Response**                | The polished response is sent back to you through the interface, ready to use.                                                                                                                                                                                                                          |\n\n## The Stack\n\nThe architecture taps into cutting-edge tools to power efficient Agentic RAG workflows. Here’s a quick overview of its components and the technologies you’ll need:\n\n- **AI Agent:** The mastermind of the system, this agent parses your queries, picks the right tools, and integrates the responses. We’ll use OpenAI’s *gpt-4o* as the reasoning engine, managed seamlessly by LangGraph.\n- **Embedding:** Queries are transformed into vector embeddings using OpenAI’s *text-embedding-3-small* model.\n- **Vector Database:** Embeddings are stored and used for similarity searches, with Qdrant stepping in as our database of choice.\n- **LLM:** Responses are generated using OpenAI’s *gpt-4o*, ensuring answers are accurate and contextually grounded.\n- **Search Tools:** To extend RAG’s capabilities, we’ve added a web search component powered by BraveSearchAPI, perfect for real-time and external data retrieval.\n- **Workflow Management:** The entire orchestration and decision-making flow is built with LangGraph, providing the flexibility and intelligence needed to handle complex workflows.\n\nReady to start building this system from the ground up? Let’s get to it!\n\n## Implementation\n\nBefore we dive into building our agent, let’s get everything set up.\n\n### Imports\n\nHere’s a list of key imports required:",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 956,
      "character_count": 5927,
      "created_at": "2025-10-16T17:42:21.432803",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 5,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "```python\nimport os\nimport json\nfrom typing import Annotated, TypedDict\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langgraph import StateGraph, tool, ToolNode, ToolMessage\nfrom langchain.document_loaders import HuggingFaceDatasetLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.llms import ChatOpenAI\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import VectorParams\nfrom brave_search import BraveSearch\n```\n\n### Qdrant Vector Database Setup\n\nWe’ll use **Qdrant Cloud** as our vector store for document embeddings. Here’s how to set it up:\n\n| **Step**                   | **Description**                                                                                                                                                                                  |\n| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **1. Create an Account**   | If you don’t already have one, head to Qdrant Cloud and sign up.                                                                                                                                 |\n| **2. Set Up a Cluster**    | Log in to your account and find the **Create New Cluster** button on the dashboard. Follow the prompts to configure: - Select your **preferred region**. - Choose the **free tier** for testing. |\n| **3. Secure Your Details** | Once your cluster is ready, note these details: - **Cluster URL** (e.g., <https://xxx-xxx-xxx.aws.cloud.qdrant.io>) - **API Key**                                                                |\n\nSave these securely for future use!\n\n### OpenAI API Configuration\n\nYour OpenAI API key will power both embedding generation and language model interactions. Visit [OpenAI’s platform](https://platform.openai.com/) and sign up for an account. In the API section of your dashboard, create a new API key. We’ll use the text-embedding-3-small model for embeddings and GPT-4 as the language model.\n\n### Brave Search\n\nTo enhance search capabilities, we’ll integrate Brave Search. Visit the [Brave API](https://api.search.brave.com/) and complete their API access request process to obtain an API key. This key will enable web search functionality for our agent.\n\nFor added security, store all API keys in a .env file.\n\n```json\nOPENAI_API_KEY = <your-openai-api-key>\nQDRANT_KEY = <your-qdrant-api-key>\nQDRANT_URL = <your-qdrant-url>\nBRAVE_API_KEY = <your-brave-api-key>\n```\n\n---\n\nThen load the environment variables:\n\n```python\nload_dotenv()\nqdrant_key = os.getenv(\"QDRANT_KEY\")\nqdrant_url = os.getenv(\"QDRANT_URL\")\nbrave_key = os.getenv(\"BRAVE_API_KEY\")\n```\n\n---\n\n### Document Processing\n\nBefore we can create our agent, we need to process and store the documentation. We’ll be working with two datasets from Hugging Face: their general documentation and Transformers-specific documentation.\n\nHere’s our document preprocessing function:\n\n```python\ndef preprocess_dataset(docs_list):\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=700,\n        chunk_overlap=50,\n        disallowed_special=()\n    )\n    doc_splits = text_splitter.split_documents(docs_list)\n    return doc_splits\n```\n\n---\n\nThis function processes our documents by splitting them into manageable chunks, ensuring important context is preserved at the chunk boundaries through overlap. We’ll use the HuggingFaceDatasetLoader to load the datasets into Hugging Face documents.\n\n```python\nhugging_face_doc = HuggingFaceDatasetLoader(\"m-ric/huggingface_doc\",\"text\")\ntransformers_doc = HuggingFaceDatasetLoader(\"m-ric/transformers_documentation_en\",\"text\")\n```\n\n---\n\nIn this demo, we are selecting the first 50 documents from the dataset and passing them to the processing function.\n\n```python\nhf_splits = preprocess_dataset(hugging_face_doc.load()[:number_of_docs])\ntransformer_splits = preprocess_dataset(transformers_doc.load()[:number_of_docs])\n```\n\n---\n\nOur splits are ready. Let’s create a collection in Qdrant to store them.\n\n### Defining the State\n\nIn LangGraph, a **state** refers to the data or information stored and maintained at a specific point during the execution of a process or a series of operations. States capture the intermediate or final results that the system needs to keep track of to manage and control the flow of tasks,\n\nLangGraph works with a state-based system. We define our state like this:\n\n```python\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n```\n\n---\n\nLet’s build our tools.\n\n### Building the Tools\n\nOur agent is equipped with three powerful tools:\n\n1. **Hugging Face Documentation Retriever**\n2. **Transformers Documentation Retriever**\n3. **Web Search Tool**",
    "metadata": {
      "chunk_id": 6,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 1002,
      "character_count": 4896,
      "created_at": "2025-10-16T17:42:21.447852",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 6,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "Let’s start by defining a retriever that takes documents and a collection name, then returns a retriever. The query is transformed into vectors using **OpenAIEmbeddings**.\n\n```python\ndef create_retriever(collection_name, doc_splits):\n    vectorstore = QdrantVectorStore.from_documents(\n        doc_splits,\n        OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n        url=qdrant_url,\n        api_key=qdrant_key,\n        collection_name=collection_name,\n    )\n    return vectorstore.as_retriever()\n```\n\n---\n\nBoth the Hugging Face documentation retriever and the Transformers documentation retriever use this same function. With this setup, it’s incredibly simple to create separate tools for each.\n\n```python\nhf_retriever_tool = create_retriever_tool(\n    hf_retriever,\n    \"retriever_hugging_face_documentation\",\n    \"Search and return information about hugging face documentation, it includes the guide and Python code.\",\n)\n\ntransformer_retriever_tool = create_retriever_tool(\n    transformer_retriever,\n    \"retriever_transformer\",\n    \"Search and return information specifically about transformers library\",\n)\n```\n\n---\n\nFor web search, we create a simple yet effective tool using Brave Search:\n\n```python\n@tool(\"web_search_tool\")\ndef search_tool(query):\n    search = BraveSearch.from_api_key(api_key=brave_key, search_kwargs={\"count\": 3})\n    return search.run(query)\n```\n\n---\n\nThe search\\_tool function leverages the BraveSearch API to perform a search. It takes a query, retrieves the top 3 search results using the API key, and returns the results.\n\nNext, we’ll set up and integrate our tools with a language model:\n\n```python\ntools = [hf_retriever_tool, transformer_retriever_tool, search_tool]\n\ntool_node = ToolNode(tools=tools)\n\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\nllm_with_tools = llm.bind_tools(tools)\n```\n\n---\n\nHere, the ToolNode class handles and orchestrates our tools:\n\n```python\nclass ToolNode:\n    def __init__(self, tools: list) -> None:\n        self.tools_by_name = {tool.name: tool for tool in tools}\n\ndef __call__(self, inputs: dict):\n        if messages := inputs.get(\"messages\", []):\n            message = messages[-1]\n        else:\n            raise ValueError(\"No message found in input\")\n\noutputs = []\n        for tool_call in message.tool_calls:\n            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n                tool_call[\"args\"]\n            )\n            outputs.append(\n                ToolMessage(\n                    content=json.dumps(tool_result),\n                    name=tool_call[\"name\"],\n                    tool_call_id=tool_call[\"id\"],\n                )\n            )\n\nreturn {\"messages\": outputs}\n```\n\n---\n\nThe ToolNode class handles tool execution by initializing a list of tools and mapping tool names to their corresponding functions. It processes input dictionaries, extracts the last message, and checks for tool\\_calls from LLM tool-calling capability providers such as Anthropic, OpenAI, and others.\n\n### Routing and Decision Making\n\nOur agent needs to determine when to use tools and when to end the cycle. This decision is managed by the routing function:\n\n```python\ndef route(state: State):\n    if isinstance(state, list):\n        ai_message = state[-1]\n    elif messages := state.get(\"messages\", []):\n        ai_message = messages[-1]\n    else:\n        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n\nif hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n        return \"tools\"\n\nreturn END\n```\n\n---\n\n## Putting It All Together: The Graph\n\nFinally, we’ll construct the graph that ties everything together:\n\n```python\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"agent\", agent)\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"agent\",\n    route,\n    {\"tools\": \"tools\", END: END},\n)\n\ngraph_builder.add_edge(\"tools\", \"agent\")\ngraph_builder.add_edge(START, \"agent\")\n```\n\n---\n\nThis is what the graph looks like:\n\nFig. 3: Agentic RAG with LangGraph\n\n### Running the Agent\n\nWith everything set up, we can run our agent using a simple function:\n\n```python\ndef run_agent(user_input: str):\n    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n```\n\n---\n\nNow, you’re ready to ask questions about Hugging Face and Transformers! Our agent will intelligently combine information from the documentation with web search results when needed.\n\nFor example, you can ask:",
    "metadata": {
      "chunk_id": 7,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 1011,
      "character_count": 4554,
      "created_at": "2025-10-16T17:42:21.463596",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 7,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "```txt\nIn the Transformers library, are there any multilingual models?\n```\n\nThe agent will dive into the Transformers documentation, extract relevant details about multilingual models, and deliver a clear, comprehensive answer.\n\nHere’s what the response might look like:\n\n```txt\nYes, the Transformers library includes several multilingual models. Here are some examples:\n\nBERT Multilingual: \nModels like `bert-base-multilingual-uncased` can be used just like monolingual models.\n\nXLM (Cross-lingual Language Model): \nModels like `xlm-mlm-ende-1024` (English-German), `xlm-mlm-enfr-1024` (English-French), and others use language embeddings to specify the language used at inference.\n\nM2M100: \nModels like `facebook/m2m100_418M` and `facebook/m2m100_1.2B` are used for multilingual translation.\n\nMBart: \nModels like `facebook/mbart-large-50-one-to-many-mmt` and `facebook/mbart-large-50-many-to-many-mmt` are used for multilingual machine translation across 50 languages.\n\nThese models are designed to handle multiple languages and can be used for tasks like translation, classification, and more.\n```\n\n---\n\n## Conclusion\n\nWe’ve successfully implemented Agentic RAG. But this is just the beginning—there’s plenty more you can explore to take your system to the next level.\n\nAgentic RAG is transforming how businesses connect data sources with AI, enabling smarter and more dynamic interactions. In this tutorial, you’ve learned how to build an Agentic RAG system that combines the power of LangGraph, Qdrant, and web search into one seamless workflow.\n\nThis system doesn’t just stop at retrieving relevant information from Hugging Face and Transformers documentation. It also smartly falls back to web search when needed, ensuring no query goes unanswered. With Qdrant as the vector database backbone, you get fast, scalable semantic search that excels at retrieving precise information—even from massive datasets.\n\nTo truly grasp the potential of this approach, why not apply these concepts to your own projects? Customize the template we’ve shared to fit your unique use case, and unlock the full potential of Agentic RAG for your business needs. The possibilities are endless.\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! 🙏\n\nWe are sorry to hear that. 😔 You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/agentic-rag-langgraph.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Agentic RAG With LangGraph and Qdrant](#agentic-rag-with-langgraph-and-qdrant.md)\n\n- [What We’ll Build](#what-well-build.md)\n\n- [Workflow](#workflow.md)\n\n- [The Stack](#the-stack.md)\n\n- [Implementation](#implementation.md)\n\n- [Imports](#imports.md)\n    - [Qdrant Vector Database Setup](#qdrant-vector-database-setup.md)\n    - [OpenAI API Configuration](#openai-api-configuration.md)\n    - [Brave Search](#brave-search.md)\n    - [Document Processing](#document-processing.md)\n    - [Defining the State](#defining-the-state.md)\n    - [Building the Tools](#building-the-tools.md)\n    - [Routing and Decision Making](#routing-and-decision-making.md)\n\n- [Putting It All Together: The Graph](#putting-it-all-together-the-graph.md)\n    - [Running the Agent](#running-the-agent.md)\n\n- [Conclusion](#conclusion.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/agentic-rag-langgraph.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
    "metadata": {
      "chunk_id": 8,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 921,
      "character_count": 3813,
      "created_at": "2025-10-16T17:42:21.471651",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 8,
      "file_relative_path": "qdrant_documentation\\documentation_agentic-rag-langgraph\\_documentation_agentic-rag-langgraph_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  }
]