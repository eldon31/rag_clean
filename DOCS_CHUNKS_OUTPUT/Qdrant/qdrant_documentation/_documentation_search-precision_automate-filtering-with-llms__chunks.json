[
  {
    "text": "Automate filtering with LLMs - Qdrant\n\n[](https://qdrant.tech/)\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\nSearch\n\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n\nSearch\n\n- [Qdrant](https://qdrant.tech/documentation/)\n- [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n- [Build](https://qdrant.tech/documentation/build/)\n- [Learn](https://qdrant.tech/articles/)\n- [API Reference](https://api.qdrant.tech/api-reference)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)\n\n- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)",
    "metadata": {
      "chunk_id": 0,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 933,
      "character_count": 3503,
      "created_at": "2025-10-16T17:42:28.878410",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 0,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)\n\n- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)",
    "metadata": {
      "chunk_id": 1,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 968,
      "character_count": 3625,
      "created_at": "2025-10-16T17:42:28.880637",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 1,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n### Essentials\n\n[Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/)\n\n[Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/)\n\n[Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/)\n\n[Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/)\n\n[Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/)\n\n[5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/)\n\n[Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/)\n\n### Integrations\n\n[Data Management](https://qdrant.tech/documentation/data-management/)\n\n- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/)\n- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/)\n- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/)\n- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/)\n- [cognee](https://qdrant.tech/documentation/data-management/cognee/)\n- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/)\n- [DLT](https://qdrant.tech/documentation/data-management/dlt/)\n- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/)\n- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/)\n- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/)\n\n[Embeddings](https://qdrant.tech/documentation/embeddings/)",
    "metadata": {
      "chunk_id": 2,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 965,
      "character_count": 3819,
      "created_at": "2025-10-16T17:42:28.884844",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 2,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/)\n- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/)\n- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/)\n- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/)\n- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/)\n- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/)\n- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/)\n- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/)\n- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/)\n- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/)\n- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/)\n- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/)\n- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/)\n- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/)\n- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/)\n- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/)\n- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/)\n\n[Frameworks](https://qdrant.tech/documentation/frameworks/)\n\n- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/)\n- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/)\n- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/)\n- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/)\n- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/)\n- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/)\n- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/)\n- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/)\n- [Feast](https://qdrant.tech/documentation/frameworks/feast/)\n- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/)\n- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/)\n- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/)\n- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/)\n- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/)\n- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/)\n- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/)\n- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/)\n- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/)\n- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/)\n- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/)\n- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/)\n- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/)\n- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/)\n- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/)\n- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/)\n- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/)\n- [Swiftide](https://qdrant.tech/documentation/frameworks/swiftide/)\n- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/)\n- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/)\n- [txtai](https://qdrant.tech/documentation/frameworks/txtai/)\n- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/)\n- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/)\n- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/)\n\n[Observability](https://qdrant.tech/documentation/observability/)",
    "metadata": {
      "chunk_id": 3,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 978,
      "character_count": 3651,
      "created_at": "2025-10-16T17:42:28.887070",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 3,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/)\n- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/)\n- [Datadog](https://qdrant.tech/documentation/observability/datadog/)\n\n[Platforms](https://qdrant.tech/documentation/platforms/)\n\n- [Apify](https://qdrant.tech/documentation/platforms/apify/)\n- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/)\n- [Keboola](https://qdrant.tech/documentation/platforms/keboola/)\n- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/)\n- [Make.com](https://qdrant.tech/documentation/platforms/make/)\n- [N8N](https://qdrant.tech/documentation/platforms/n8n/)\n- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/)\n- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/)\n- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/)\n- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/)\n- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/)\n- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/)\n\n### Examples\n\n[Search Enhancement](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n\n- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/)\n- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/)\n\n[Send Data to Qdrant](https://qdrant.tech/documentation/send-data/)\n\n- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/)\n- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/)\n- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/)\n\n[Build Prototypes](https://qdrant.tech/documentation/examples/)\n\n- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/)\n- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/)\n- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/)\n- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/)\n- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/)\n- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/)\n- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/)\n- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/)\n- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/)\n- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/)\n- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/)\n- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/)\n\n[Practice Datasets](https://qdrant.tech/documentation/datasets/)\n\n- [Documentation](https://qdrant.tech/documentation/)\n-\n- [Search precision](https://qdrant.tech/documentation/search-precision/)\n-\n- Automate filtering with LLMs\n\n# Automate filtering with LLMs",
    "metadata": {
      "chunk_id": 4,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 920,
      "character_count": 3641,
      "created_at": "2025-10-16T17:42:28.890644",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 4,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "Our [complete guide to filtering in vector search](https://qdrant.tech/articles/vector-search-filtering/) describes why filtering is important, and how to implement it with Qdrant. However, applying filters is easier when you build an application with a traditional interface. Your UI may contain a form with checkboxes, sliders, and other elements that users can use to set their criteria. But what if you want to build a RAG-powered application with just the conversational interface, or even voice commands? In this case, you need to automate the filtering process!\n\nLLMs seem to be particularly good at this task. They can understand natural language and generate structured output based on it. In this tutorial, we‚Äôll show you how to use LLMs to automate filtering in your vector search application.\n\n## Few notes on Qdrant filters\n\nQdrant Python SDK defines the models using [Pydantic](https://docs.pydantic.dev/latest/). This library is de facto standard for data validation and serialization in Python. It allows you to define the structure of your data using Python type hints. For example, our `Filter` model is defined as follows:\n\n```python\nclass Filter(BaseModel, extra=\"forbid\"):\n    should: Optional[Union[List[\"Condition\"], \"Condition\"]] = Field(\n        default=None, description=\"At least one of those conditions should match\"\n    )\n    min_should: Optional[\"MinShould\"] = Field(\n        default=None, description=\"At least minimum amount of given conditions should match\"\n    )\n    must: Optional[Union[List[\"Condition\"], \"Condition\"]] = Field(default=None, description=\"All conditions must match\")\n    must_not: Optional[Union[List[\"Condition\"], \"Condition\"]] = Field(\n        default=None, description=\"All conditions must NOT match\"\n    )\n```\n\nQdrant filters may be nested, and you can express even the most complex conditions using the `must`, `should`, and `must_not` notation.\n\n## Structured output from LLMs\n\nIt isn‚Äôt an uncommon practice to use LLMs to generate structured output. It is primarily useful if their output is intended for further processing by a different application. For example, you can use LLMs to generate SQL queries, JSON objects, and most importantly, Qdrant filters. Pydantic got adopted by the LLM ecosystem quite well, so there is plenty of libraries which uses Pydantic models to define the structure of the output for the Language Models.\n\nOne of the interesting projects in this area is [Instructor](https://python.useinstructor.com/) that allows you to play with different LLM providers and restrict their output to a specific structure. Let‚Äôs install the library and already choose a provider we‚Äôll use in this tutorial:\n\n```shell\npip install \"instructor[anthropic]\"\n```\n\nAnthropic is not the only option out there, as Instructor supports many other providers including OpenAI, Ollama, Llama, Gemini, Vertex AI, Groq, Litellm and others. You can choose the one that fits your needs the best, or the one you already use in your RAG.\n\n## Using Instructor to generate Qdrant filters\n\nInstructor has some helper methods to decorate the LLM APIs, so you can interact with them as if you were using their normal SDKs. In case of Anthropic, you just pass an instance of `Anthropic` class to the `from_anthropic` function:\n\n```python\nimport instructor\nfrom anthropic import Anthropic\n\nanthropic_client = instructor.from_anthropic(\n    client=Anthropic(\n        api_key=\"YOUR_API_KEY\",\n    )\n)\n```\n\nA decorated client slightly modifies the original API, so you can pass the `response_model` parameter to the `.messages.create` method. This parameter should be a Pydantic model that defines the structure of the output. In case of Qdrant filters, it should be a `Filter` model:\n\n```python\nfrom qdrant_client import models\n\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"red T-shirt\"\n        }\n    ],\n)\n```\n\nThe output of this code will be a Pydantic model that represents a Qdrant filter. Surprisingly, there is no need to pass additional instructions to already figure out that the user wants to filter by the color and the type of the product. Here is how the output looks like:",
    "metadata": {
      "chunk_id": 5,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 947,
      "character_count": 4300,
      "created_at": "2025-10-16T17:42:28.896518",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 5,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "```python\nFilter(\n    should=None, \n    min_should=None, \n    must=[\n        FieldCondition(\n            key=\"color\", \n            match=MatchValue(value=\"red\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"type\", \n            match=MatchValue(value=\"t-shirt\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        )\n    ], \n    must_not=None\n)\n```\n\nObviously, giving the model complete freedom to generate the filter may lead to unexpected results, or no results at all. Your collection probably has payloads with a specific structure, so it doesn‚Äôt make sense to use anything else. Moreover, **it‚Äôs considered a good practice to filter by the fields that have been indexed**. That‚Äôs why it makes sense to automatically determine the indexed fields and restrict the output to them.\n\n### Restricting the available fields\n\nQdrant collection info contains a list of the indexes created on a particular collection. You can use this information to automatically determine the fields that can be used for filtering. Here is how you can do it:\n\n```python\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\"http://localhost:6333\")\ncollection_info = client.get_collection(collection_name=\"test_filter\")\nindexes = collection_info.payload_schema\nprint(indexes)\n```\n\nOutput:\n\n```python\n{\n    \"city.location\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.GEO,\n        ...\n    ),\n    \"city.name\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.KEYWORD,\n        ...\n    ),\n    \"color\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.KEYWORD,\n        ...\n    ),\n    \"fabric\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.KEYWORD,\n        ...\n    ),\n    \"price\": PayloadIndexInfo(\n        data_type=PayloadSchemaType.FLOAT,\n        ...\n    ),\n}\n```\n\nOur LLM should know the names of the fields it can use, but also their type, as e.g., range filtering only makes sense for numerical fields, and geo filtering on non-geo fields won‚Äôt yield anything meaningful. You can pass this information as a part of the prompt to the LLM, so let‚Äôs encode it as a string:\n\n```python\nformatted_indexes = \"\\n\".join([\n    f\"- {index_name} - {index.data_type.name}\"\n    for index_name, index in indexes.items()\n])\nprint(formatted_indexes)\n```\n\nOutput:\n\n```text\n- fabric - KEYWORD\n- city.name - KEYWORD\n- color - KEYWORD\n- price - FLOAT\n- city.location - GEO\n```\n\n**It‚Äôs a good idea to cache the list of the available fields and their types**, as they are not supposed to change often. Our interactions with the LLM should be slightly different now:\n\n```python\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>color is red</query>\"\n                f\"<indexes>\\n{formatted_indexes}\\n</indexes>\"\n            )\n        }\n    ],\n)\n```\n\nOutput:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=FieldCondition(\n        key=\"color\", \n        match=MatchValue(value=\"red\"), \n        range=None, \n        geo_bounding_box=None, \n        geo_radius=None, \n        geo_polygon=None, \n        values_count=None\n    ), \n    must_not=None\n)\n```\n\nThe same query, restricted to the available fields, now generates better criteria, as it doesn‚Äôt try to filter by the fields that don‚Äôt exist in the collection.\n\n### Testing the LLM output\n\nAlthough the LLMs are quite powerful, they are not perfect. If you plan to automate filtering, it makes sense to run some tests to see how well they perform. Especially edge cases, like queries that cannot be expressed as filters. Let‚Äôs see how the LLM will handle the following query:\n\n```python\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>fruit salad with no more than 100 calories</query>\"\n                f\"<indexes>\\n{formatted_indexes}\\n</indexes>\"\n            )\n        }\n    ],\n)\n```\n\nOutput:",
    "metadata": {
      "chunk_id": 6,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 1006,
      "character_count": 4416,
      "created_at": "2025-10-16T17:42:28.905007",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 6,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "```python\nFilter(\n    should=None, \n    min_should=None, \n    must=FieldCondition(\n        key=\"price\", \n        match=None, \n        range=Range(lt=None, gt=None, gte=None, lte=100.0), \n        geo_bounding_box=None, \n        geo_radius=None, \n        geo_polygon=None, \n        values_count=None\n    ), \n    must_not=None\n)\n```\n\nSurprisingly, the LLM extracted the calorie information from the query and generated a filter based on the price field. It somehow extracts any numerical information from the query and tries to match it with the available fields.\n\nGenerally, giving model some more guidance on how to interpret the query may lead to better results. Adding a system prompt that defines the rules for the query interpretation may help the model to do a better job. Here is how you can do it:\n\n```python\nSYSTEM_PROMPT = \"\"\"\nYou are extracting filters from a text query. Please follow the following rules:\n1. Query is provided in the form of a text enclosed in <query> tags.\n2. Available indexes are put at the end of the text in the form of a list enclosed in <indexes> tags.\n3. You cannot use any field that is not available in the indexes.\n4. Generate a filter only if you are certain that user's intent matches the field name.\n5. Prices are always in USD.\n6. It's better not to generate a filter than to generate an incorrect one.\n\"\"\"\n\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": SYSTEM_PROMPT.strip(),\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Okay, I will follow all the rules.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>fruit salad with no more than 100 calories</query>\"\n                f\"<indexes>\\n{formatted_indexes}\\n</indexes>\"\n            )\n        }\n    ],\n)\n```\n\nCurrent output:\n\n```python\nFilter(\n    should=None, \n    min_should=None, \n    must=None, \n    must_not=None\n)\n```\n\n### Handling complex queries\n\nWe have a bunch of indexes created on the collection, and it is quite interesting to see how the LLM will handle more complex queries. For example, let‚Äôs see how it will handle the following query:\n\n```python\nqdrant_filter = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-latest\",\n    response_model=models.Filter,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": SYSTEM_PROMPT.strip(),\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Okay, I will follow all the rules.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"<query>\"\n                \"white T-shirt available no more than 30 miles from London, \"\n                \"but not in the city itself, below $15.70, not made from polyester\"\n                \"</query>\\n\"\n                \"<indexes>\\n\"\n                f\"{formatted_indexes}\\n\"\n                \"</indexes>\"\n            )\n        },\n    ],\n)\n```\n\nIt might be surprising, but Anthropic Claude is able to generate even such complex filters. Here is the output:",
    "metadata": {
      "chunk_id": 7,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 728,
      "character_count": 3208,
      "created_at": "2025-10-16T17:42:28.908639",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 7,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  },
  {
    "text": "```python\nFilter(\n    should=None, \n    min_should=None, \n    must=[\n        FieldCondition(\n            key=\"color\", \n            match=MatchValue(value=\"white\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"city.location\", \n            match=None, \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=GeoRadius(\n                center=GeoPoint(lon=-0.1276, lat=51.5074), \n                radius=48280.0\n            ), \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"price\", \n            match=None, \n            range=Range(lt=15.7, gt=None, gte=None, lte=None), \n            geo_bounding_box=None,\n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        )\n    ], must_not=[\n        FieldCondition(\n            key=\"city.name\", \n            match=MatchValue(value=\"London\"), \n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None, \n            geo_polygon=None, \n            values_count=None\n        ), \n        FieldCondition(\n            key=\"fabric\", \n            match=MatchValue(value=\"polyester\"),\n            range=None, \n            geo_bounding_box=None, \n            geo_radius=None,\n            geo_polygon=None, \n            values_count=None\n        )\n    ]\n)\n```\n\nThe model even knows the coordinates of London and uses them to generate the geo filter. It isn‚Äôt the best idea to rely on the model to generate such complex filters, but it‚Äôs quite impressive that it can do it.\n\n## Further steps\n\nReal production systems would rather require more testing and validation of the LLM output. Building a ground truth dataset with the queries and the expected filters would be a good idea. You can use this dataset to evaluate the model performance and to see how it behaves in different scenarios.\n\n##### Was this page useful?\n\nYes No\n\nThank you for your feedback! üôè\n\nWe are sorry to hear that. üòî You can [edit](https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/search-precision/automate-filtering-with-llms.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\n\nOn this page:\n\n- [Automate filtering with LLMs](#automate-filtering-with-llms.md)\n\n- [Few notes on Qdrant filters](#few-notes-on-qdrant-filters.md)\n\n- [Structured output from LLMs](#structured-output-from-llms.md)\n\n- [Using Instructor to generate Qdrant filters](#using-instructor-to-generate-qdrant-filters.md)\n\n- [Restricting the available fields](#restricting-the-available-fields.md)\n    - [Testing the LLM output](#testing-the-llm-output.md)\n    - [Handling complex queries](#handling-complex-queries.md)\n\n- [Further steps](#further-steps.md)\n\n* [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/search-precision/automate-filtering-with-llms.md)\n* [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n#### Ready to get started with Qdrant?\n\n[Start Free](https://qdrant.to/cloud/)\n\n¬© 2025 Qdrant.\n\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)",
    "metadata": {
      "chunk_id": 8,
      "source_file": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\\Docs\\Qdrant\\qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "input_type": "qdrant",
      "chunking_strategy": "platform_documentation",
      "token_count": 817,
      "character_count": 3453,
      "created_at": "2025-10-16T17:42:28.916581",
      "parent_context": null,
      "semantic_type": "qdrant",
      "collection_name": "Qdrant",
      "subfolder_name": "qdrant_documentation",
      "collection_strategy": "platform_documentation",
      "chunk_index_in_file": 8,
      "file_relative_path": "qdrant_documentation\\documentation_search-precision_automate-filtering-with-llms\\_documentation_search-precision_automate-filtering-with-llms_.md",
      "collection_context": "Qdrant/qdrant_documentation"
    }
  }
]