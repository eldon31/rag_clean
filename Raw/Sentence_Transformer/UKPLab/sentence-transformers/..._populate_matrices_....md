loss = self.cross_entropy_loss(logits_matrix, labels_matrix.softmax(dim=1))
```

Sources: [sentence_transformers/cross_encoder/losses/ListNetLoss.py:10-198]()

### Position-Aware ListMLE

The `PListMLELoss` and `ListMLELoss` implement maximum likelihood estimation for permutations with optional position-aware weighting:

```python
# Core PListMLE computation from PListMLELoss.forward()
scores = sorted_logits.exp()
cumsum_scores = torch.flip(torch.cumsum(torch.flip(scores, [1]), 1), [1])
log_probs = sorted_logits - torch.log(cumsum_scores + self.eps)

if self.lambda_weight is not None:
    lambda_weight = self.lambda_weight(mask)
    log_probs = log_probs * lambda_weight
```

Sources: [sentence_transformers/cross_encoder/losses/PListMLELoss.py:45-295](), [sentence_transformers/cross_encoder/losses/ListMLELoss.py:9-127]()

## Common Implementation Patterns

All learning-to-rank losses share several implementation patterns:

### Mini-Batch Processing

Large document lists are processed in mini-batches to manage memory usage:

```python
mini_batch_size = self.mini_batch_size or batch_size
if mini_batch_size <= 0:
    mini_batch_size = len(pairs)

for i in range(0, len(pairs), mini_batch_size):
    mini_batch_pairs = pairs[i : i + mini_batch_size]
    # Process mini-batch...
```

### Padding Handling

Variable document counts per query are handled using padding and masking:

```python
# Create padded matrices
logits_matrix = torch.full((batch_size, max_docs), -1e16, device=self.model.device)
labels_matrix = torch.full_like(logits_matrix, float("-inf"))

# Place valid logits and labels
doc_indices = torch.cat([torch.arange(len(docs)) for docs in docs_list], dim=0)
batch_indices = torch.repeat_interleave(torch.arange(batch_size), torch.tensor(docs_per_query))
logits_matrix[batch_indices, doc_indices] = logits
```

Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:247-287](), [sentence_transformers/cross_encoder/losses/ListNetLoss.py:132-176]()

## Configuration and Usage

### Basic Usage Pattern

```python
from sentence_transformers.cross_encoder import CrossEncoder, CrossEncoderTrainer, losses
from datasets import Dataset

model = CrossEncoder("microsoft/mpnet-base")
train_dataset = Dataset.from_dict({
    "query": ["What are pandas?", "What is the capital of France?"],
    "docs": [
        ["Pandas are a kind of bear.", "Pandas are kind of like fish."],
        ["The capital of France is Paris.", "Paris is quite large."],
    ],
    "labels": [[1, 0], [1, 0]],
})