# Embed Collections V6 - Visual Flow Diagram

This document provides visual flowcharts for the complete embedding pipeline.

---

## High-Level System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    START PROGRAM                        â”‚
â”‚                  python embed_v6.py                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  parse_arguments()     â”‚
            â”‚  Detect Environment    â”‚
            â”‚  (Kaggle or Local)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Setup Logging        â”‚
            â”‚   - File Handler       â”‚
            â”‚   - Console Handler    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DISCOVERY PHASE                          â”‚
â”‚                                                        â”‚
â”‚  discover_collections(chunked_dir, max_depth=5)       â”‚
â”‚                                                        â”‚
â”‚  Recursively scan directories:                         â”‚
â”‚  â€¢ Find directories with *_chunks.json files           â”‚
â”‚  â€¢ Handle name collisions                              â”‚
â”‚  â€¢ Build collections map                               â”‚
â”‚                                                        â”‚
â”‚  Output: {"collection_name": Path(...), ...}          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Collections â”‚  YES
                  â”‚   Found?    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                         â”‚ NO            â”‚
                         â–¼               â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                  â”‚    ERROR    â”‚        â”‚
                  â”‚   & EXIT    â”‚        â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                                         â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ _filter_collections()  â”‚
            â”‚                        â”‚
            â”‚ If --collections:      â”‚
            â”‚   Filter to requested  â”‚
            â”‚ Else:                  â”‚
            â”‚   Use all discovered   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INITIALIZATION PHASE                      â”‚
â”‚                                                        â”‚
â”‚  _initialize_embedder(models, exclusive, weights)     â”‚
â”‚                                                        â”‚
â”‚  1. Create EnsembleConfig                              â”‚
â”‚     - model_names: [model1, model2, model3]            â”‚
â”‚     - weights: [0.33, 0.33, 0.34] or custom            â”‚
â”‚     - normalization: "l2"                              â”‚
â”‚                                                        â”‚
â”‚  2. Create KaggleGPUConfig                             â”‚
â”‚     - exclusive_ensemble_mode: True/False              â”‚
â”‚     - max_wait_time: 600s                              â”‚
â”‚     - check_interval: 30s                              â”‚
â”‚                                                        â”‚
â”‚  3. Create KaggleExportConfig                          â”‚
â”‚     - export_format: "qdrant"                          â”‚
â”‚     - output_dir: Path                                 â”‚
â”‚     - enable_validation: True                          â”‚
â”‚                                                        â”‚
â”‚  4. Instantiate UltimateKaggleEmbedderV4               â”‚
â”‚                                                        â”‚
â”‚  Output: embedder instance                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROCESSING PHASE                          â”‚
â”‚                                                        â”‚
â”‚  For each collection in filtered_collections:         â”‚
â”‚                                                        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚  process_collection(name, path, ...)    â”‚       â”‚
â”‚    â”‚                                          â”‚       â”‚
â”‚    â”‚  1. Find chunk files                     â”‚       â”‚
â”‚    â”‚  2. Validate chunks exist                â”‚       â”‚
â”‚    â”‚  3. Run embedding job                    â”‚       â”‚
â”‚    â”‚     â”œâ”€ EXCLUSIVE MODE (if enabled)       â”‚       â”‚
â”‚    â”‚     â”‚  For each model:                   â”‚       â”‚
â”‚    â”‚     â”‚    - Acquire GPU lease             â”‚       â”‚
â”‚    â”‚     â”‚    - Load model                    â”‚       â”‚
â”‚    â”‚     â”‚    - Generate embeddings           â”‚       â”‚
â”‚    â”‚     â”‚    - Release GPU                   â”‚       â”‚
â”‚    â”‚     â”‚  Combine with weighted average     â”‚       â”‚
â”‚    â”‚                                          â”‚       â”‚
â”‚    â”‚  4. Extract telemetry                    â”‚       â”‚
â”‚    â”‚  5. Log completion                       â”‚       â”‚
â”‚    â”‚  6. Return results dict                  â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚              â”‚                                         â”‚
â”‚              â–¼                                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚    â”‚   Success?      â”‚  YES â†’ Store in results        â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚              â”‚ NO                                      â”‚
â”‚              â–¼                                         â”‚
â”‚         Store in failed_collections                    â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FINALIZATION PHASE                        â”‚
â”‚                                                        â”‚
â”‚  1. _log_processing_summary(results, failed)          â”‚
â”‚     - Log total/success/failure counts                 â”‚
â”‚     - Log failed collection details                    â”‚
â”‚                                                        â”‚
â”‚  2. _export_summary_json(results, file, ...)          â”‚
â”‚     - Create JSON with timestamp                       â”‚
â”‚     - Include all results and failures                 â”‚
â”‚     - Write to output_dir/embedding_summary_v6.json    â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Log "Complete!"      â”‚
            â”‚   EXIT (code 0)        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Discovery Flow

```
discover_collections(chunked_dir, max_depth=5)
â”‚
â”œâ”€ Validate chunked_dir exists and is directory
â”‚  â””â”€ If not: raise ValueError
â”‚
â”œâ”€ Initialize: collections = {}, seen_names = set()
â”‚
â””â”€ Call _scan_directory_recursive(chunked_dir, max_depth, ...)
   â”‚
   â”‚  For each entry in directory:
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  â”‚                                        â”‚
   â”‚  â–¼                                        â”‚
   â”‚  Is entry a directory?                    â”‚
   â”‚  â”‚                                        â”‚
   â”‚  â”œâ”€ NO: Skip                              â”‚
   â”‚  â”‚                                        â”‚
   â”‚  â””â”€ YES:                                  â”‚
   â”‚     â”‚                                     â”‚
   â”‚     â–¼                                     â”‚
   â”‚     _is_collection_directory(entry)?      â”‚
   â”‚     â”‚                                     â”‚
   â”‚     â”œâ”€ YES: FOUND COLLECTION!            â”‚
   â”‚     â”‚  â”‚                                  â”‚
   â”‚     â”‚  â”œâ”€ _resolve_collection_name()     â”‚
   â”‚     â”‚  â”‚  â€¢ Check if name in seen_names   â”‚
   â”‚     â”‚  â”‚  â€¢ If collision:                 â”‚
   â”‚     â”‚  â”‚    Use relative path as name     â”‚
   â”‚     â”‚  â”‚  â€¢ Add to seen_names             â”‚
   â”‚     â”‚  â”‚                                  â”‚
   â”‚     â”‚  â”œâ”€ collections[name] = entry       â”‚
   â”‚     â”‚  â”‚                                  â”‚
   â”‚     â”‚  â””â”€ Log discovery with chunk count  â”‚
   â”‚     â”‚                                     â”‚
   â”‚     â””â”€ NO: NOT A COLLECTION              â”‚
   â”‚        â”‚                                  â”‚
   â”‚        â””â”€ Recurse deeper:                 â”‚
   â”‚           _scan_directory_recursive(      â”‚
   â”‚              entry,                       â”‚
   â”‚              max_depth,                   â”‚
   â”‚              ...,                         â”‚
   â”‚              current_depth + 1            â”‚
   â”‚           )                               â”‚
   â”‚           â”‚                               â”‚
   â”‚           â”œâ”€ Check: current_depth > max_depth?
   â”‚           â”‚  YES: Return (stop recursion) â”‚
   â”‚           â”‚  NO: Continue scanning        â”‚
   â”‚           â”‚                               â”‚
   â”‚           â””â”€ (Repeat for subdirectories)  â”‚
   â”‚                                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Return collections dictionary
```

---

## Collection Processing Detail

```
process_collection(name, path, embedder, args)
â”‚
â”œâ”€ Log header: "Processing collection: {name}"
â”‚  Log mode: EXCLUSIVE or PARALLEL
â”‚
â”œâ”€ find_chunk_files(path)
â”‚  â””â”€ Returns: List[Path] of *_chunks.json files
â”‚
â”œâ”€ Validate chunk files exist
â”‚  â””â”€ If empty: return {}
â”‚
â”œâ”€ Log: "Found N chunk files"
â”‚
â””â”€ embedder.generate_embeddings_kaggle_optimized(
      enable_monitoring=True,
      save_intermediate=True
   )
   â”‚
   â”œâ”€ batch_runner.run() detects exclusive_mode
   â”‚  â””â”€ Calls batch_runner.run_exclusive_ensemble()
   â”‚
   â”œâ”€ EXCLUSIVE MODE (ONLY MODE - args.exclusive_ensemble = True):
   â”‚  â”‚
   â”‚  â”‚  models = ["model1", "model2", "model3"]
   â”‚  â”‚  all_embeddings = []
   â”‚  â”‚
   â”‚  â”‚  For each model in models:
   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  â”‚  â”‚                                      â”‚
   â”‚  â”‚  â”‚  1. GPU_LEASE.acquire(model)        â”‚
   â”‚  â”‚  â”‚     â””â”€ Wait for T4 GPU availability  â”‚
   â”‚  â”‚  â”‚     â””â”€ Log lease event: "acquire"    â”‚
   â”‚  â”‚  â”‚                                      â”‚
   â”‚  â”‚  â”‚  2. Load model to GPU                â”‚
   â”‚  â”‚  â”‚     â€¢ Enable DataParallel mode       â”‚
   â”‚  â”‚  â”‚     â€¢ GPU memory limit: 12GB/GPU     â”‚
   â”‚  â”‚  â”‚     model = SentenceTransformer(     â”‚
   â”‚  â”‚  â”‚         model_name,                  â”‚
   â”‚  â”‚  â”‚         device="cuda"                â”‚
   â”‚  â”‚  â”‚     )                                â”‚
   â”‚  â”‚  â”‚     if torch.cuda.device_count() > 1:â”‚
   â”‚  â”‚  â”‚       model = DataParallel(model)    â”‚
   â”‚  â”‚  â”‚       torch.cuda.set_per_process_    â”‚
   â”‚  â”‚  â”‚         memory_fraction(0.75)  # 12GBâ”‚
   â”‚  â”‚  â”‚                                      â”‚
   â”‚  â”‚  â”‚  3. Generate embeddings              â”‚
   â”‚  â”‚  â”‚     For each chunk_file:             â”‚
   â”‚  â”‚  â”‚       â”‚                              â”‚
   â”‚  â”‚  â”‚       â”œâ”€ Log throughput START:       â”‚
   â”‚  â”‚  â”‚       â”‚  â€¢ Model: {model_name}       â”‚
   â”‚  â”‚  â”‚       â”‚  â€¢ File: {chunk_file_name}   â”‚
   â”‚  â”‚  â”‚       â”‚  â€¢ Chunks: {num_chunks}      â”‚
   â”‚  â”‚  â”‚       â”‚  â€¢ Timestamp: {start_time}   â”‚
   â”‚  â”‚  â”‚       â”‚  â€¢ GPUs: {gpu_count}         â”‚
   â”‚  â”‚  â”‚       â”‚  â€¢ Batch size/GPU: {batch}   â”‚
   â”‚  â”‚  â”‚       â”‚                              â”‚
   â”‚  â”‚  â”‚       â”œâ”€ For each chunk in file:     â”‚
   â”‚  â”‚  â”‚       â”‚    embedding = model.encode( â”‚
   â”‚  â”‚  â”‚       â”‚        chunk["text"],        â”‚
   â”‚  â”‚  â”‚       â”‚        batch_size=batch_size â”‚
   â”‚  â”‚  â”‚       â”‚    )  # Distributed across   â”‚
   â”‚  â”‚  â”‚       â”‚       # GPUs via DataParallelâ”‚
   â”‚  â”‚  â”‚       â”‚    all_embeddings.append(    â”‚
   â”‚  â”‚  â”‚       â”‚        embedding             â”‚
   â”‚  â”‚  â”‚       â”‚    )                         â”‚
   â”‚  â”‚  â”‚       â”‚                              â”‚
   â”‚  â”‚  â”‚       â””â”€ Log throughput END:         â”‚
   â”‚  â”‚  â”‚          â€¢ Chunks processed: {count} â”‚
   â”‚  â”‚  â”‚          â€¢ Duration: {elapsed}s      â”‚
   â”‚  â”‚  â”‚          â€¢ Rate: {chunks/sec}        â”‚
   â”‚  â”‚  â”‚          â€¢ GPU memory used: {mem}GB  â”‚
   â”‚  â”‚  â”‚          â€¢ Timestamp: {end_time}     â”‚
   â”‚  â”‚  â”‚                                      â”‚
   â”‚  â”‚  â”‚  4. Unload model from GPU            â”‚
   â”‚  â”‚  â”‚     del model                        â”‚
   â”‚  â”‚  â”‚     torch.cuda.empty_cache()         â”‚
   â”‚  â”‚  â”‚                                      â”‚
   â”‚  â”‚  â”‚  5. GPU_LEASE.release(model)         â”‚
   â”‚  â”‚  â”‚     â””â”€ Log lease event: "release"    â”‚
   â”‚  â”‚  â”‚                                      â”‚
   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚  â”‚
   â”‚  â”‚  6. Combine embeddings:
   â”‚  â”‚     ensemble_embedding = weighted_avg(
   â”‚  â”‚         all_embeddings,
   â”‚  â”‚         weights=[0.33, 0.33, 0.34]
   â”‚  â”‚     )
   â”‚  â”‚
   â”‚  â”‚  7. Normalize: L2 normalization
   â”‚  â”‚
   â”‚  â”‚  8. Store in Qdrant:
   â”‚  â”‚     For each chunk, embedding:
   â”‚  â”‚       qdrant.upsert(
   â”‚  â”‚           collection_name=name,
   â”‚  â”‚           points=[{
   â”‚  â”‚               "id": chunk_id,
   â”‚  â”‚               "vector": ensemble_embedding,
   â”‚  â”‚               "payload": {
   â”‚  â”‚                   "text": chunk_text,
   â”‚  â”‚                   "metadata": {...}
   â”‚  â”‚               }
   â”‚  â”‚           }]
   â”‚  â”‚       )
   â”‚  â”‚
   â”‚  â””â”€ Return: {
   â”‚       "models_executed": ["model1", "model2", "model3"],
   â”‚       "lease_events": [
   â”‚           {"event_type": "acquire", "model": "model1", ...},
   â”‚           {"event_type": "release", "model": "model1", ...},
   â”‚           ...
   â”‚       ],
   â”‚       "total_embeddings_generated": 1000,
   â”‚       "qdrant_collection": "Docling"
   â”‚     }
```

---

## Function Call Tree

```
main()
â”‚
â”œâ”€â”€â”€ parse_arguments()
â”‚    â””â”€â”€â”€ Returns: Namespace with all args
â”‚
â”œâ”€â”€â”€ logging.basicConfig(...)
â”‚
â”œâ”€â”€â”€ discover_collections(chunked_dir, max_depth)
â”‚    â”‚
â”‚    â””â”€â”€â”€ _scan_directory_recursive(directory, max_depth, ...)
â”‚         â”‚
â”‚         â”œâ”€â”€â”€ _is_collection_directory(entry)
â”‚         â”‚    â””â”€â”€â”€ Returns: bool
â”‚         â”‚
â”‚         â””â”€â”€â”€ _resolve_collection_name(entry, chunked_dir, seen_names)
â”‚              â””â”€â”€â”€ Returns: str (unique name)
â”‚
â”œâ”€â”€â”€ _filter_collections(all_collections, args.collections)
â”‚    â””â”€â”€â”€ Returns: dict[str, Path] (filtered)
â”‚
â”œâ”€â”€â”€ _initialize_embedder(args.ensemble_models, args.exclusive_ensemble, args.ensemble_weights)
â”‚    â”‚
â”‚    â”œâ”€â”€â”€ EnsembleConfig(...)
â”‚    â”œâ”€â”€â”€ KaggleGPUConfig(...)
â”‚    â”œâ”€â”€â”€ KaggleExportConfig(...)
â”‚    â””â”€â”€â”€ UltimateKaggleEmbedderV4(...)
â”‚         â””â”€â”€â”€ Returns: embedder instance
â”‚
â”œâ”€â”€â”€ For each collection:
â”‚    â”‚
â”‚    â””â”€â”€â”€ process_collection(name, path, embedder, args)
â”‚         â”‚
â”‚         â”œâ”€â”€â”€ find_chunk_files(path)
â”‚         â”‚    â””â”€â”€â”€ Returns: List[Path]
â”‚         â”‚
â”‚         â”œâ”€â”€â”€ embedder.load_chunks_from_processing(chunks_dir)
â”‚         â”‚    â””â”€â”€â”€ Returns: load_summary
â”‚         â”‚
â”‚         â”œâ”€â”€â”€ embedder.generate_embeddings_kaggle_optimized(...)
â”‚         â”‚    â”‚
â”‚         â”‚    â””â”€â”€â”€ batch_runner.run(enable_monitoring, save_intermediate)
â”‚         â”‚         â”‚
â”‚         â”‚         â””â”€â”€â”€ batch_runner.run_exclusive_ensemble()
â”‚         â”‚              â””â”€â”€â”€ Returns: dict (results with telemetry)
â”‚         â”‚
â”‚         â”œâ”€â”€â”€ _extract_telemetry(results)
â”‚         â”‚    â””â”€â”€â”€ Returns: (models, events, count)
â”‚         â”‚
â”‚         â””â”€â”€â”€ _log_collection_completion(name, models, events, count, exclusive)
â”‚
â”œâ”€â”€â”€ _log_processing_summary(results, failed_collections)
â”‚
â””â”€â”€â”€ _export_summary_json(results, output_file, models, exclusive)
```

---

## Data Structure Flow

### Input Data Structure

```python
# Directory structure on disk:
Chunked/
â”œâ”€â”€ Docling/
â”‚   â”œâ”€â”€ intro_chunks.json          # {"chunks": [...]}
â”‚   â””â”€â”€ tutorial_chunks.json       # {"chunks": [...]}
â””â”€â”€ FAST_DOCS/
    â””â”€â”€ fastapi_fastapi/
        â””â”€â”€ docs_chunks.json       # {"chunks": [...]}

# Each *_chunks.json file contains:
{
    "doc_id": "intro",
    "chunks": [
        {
            "chunk_id": "intro_chunk_0",
            "text": "This is the introduction...",
            "metadata": {
                "source": "intro.md",
                "section": "Overview"
            }
        },
        {
            "chunk_id": "intro_chunk_1",
            "text": "Next section content...",
            "metadata": {...}
        }
    ]
}
```

### Intermediate Data Structures

```python
# After discover_collections():
collections = {
    "Docling": Path("Chunked/Docling"),
    "FAST_DOCS_fastapi_fastapi": Path("Chunked/FAST_DOCS/fastapi_fastapi")
}

# After _filter_collections():
filtered = {
    "Docling": Path("Chunked/Docling")  # If --collections Docling specified
}

# After _initialize_embedder():
embedder = UltimateKaggleEmbedderV4(
    ensemble_config=EnsembleConfig(...),
    gpu_config=KaggleGPUConfig(...),
    export_config=KaggleExportConfig(...)
)

# During process_collection():
chunk_files = [
    Path("Chunked/Docling/intro_chunks.json"),
    Path("Chunked/Docling/tutorial_chunks.json")
]

# After embedder.generate_embeddings_kaggle_optimized():
# (called via batch_runner.run() -> run_exclusive_ensemble())
results = {
    "models_executed": [
        "sentence-transformers/all-MiniLM-L6-v2",
        "BAAI/bge-small-en-v1.5",
        "nomic-ai/nomic-embed-text-v1.5"
    ],
    "lease_events": [
        {
            "event_type": "acquire",
            "model": "sentence-transformers/all-MiniLM-L6-v2",
            "timestamp": "2025-10-22T14:30:10.123456"
        },
        {
            "event_type": "release",
            "model": "sentence-transformers/all-MiniLM-L6-v2",
            "timestamp": "2025-10-22T14:31:25.789012"
        }
        # ... more events
    ],
    "total_embeddings_generated": 1000,
    "qdrant_collection": "Docling",
    "processing_time": 235.4
}
```

### Output Data Structures

```python
# In Qdrant (vector database):
{
    "collection_name": "Docling",
    "vectors_count": 1000,
    "points": [
        {
            "id": "intro_chunk_0",
            "vector": [0.123, -0.456, 0.789, ...],  # 384 dimensions
            "payload": {
                "text": "This is the introduction...",
                "chunk_id": "intro_chunk_0",
                "doc_id": "intro",
                "metadata": {
                    "source": "intro.md",
                    "section": "Overview"
                }
            }
        }
        # ... 999 more points
    ]
}

# In embedding_summary_v6.json:
{
    "timestamp": "2025-10-22T14:35:22.123456",
    "environment": "Kaggle",
    "ensemble_models": [...],
    "exclusive_ensemble_mode": true,
    "results": {
        "Docling": {
            "models_executed": [...],
            "lease_events": [...],
            "total_embeddings_generated": 1000,
            "qdrant_collection": "Docling"
        }
    },
    "failed_collections": {}
}
```

---

## Timeline: Exclusive Mode (Only Mode)

### Exclusive Mode (Sequential) - Model-at-a-Time

```
Time â†’
0s     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Start: Process Collection "Docling"              â”‚
       â”‚ (Exclusive mode - ONLY execution path)           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
10s    â”‚
       â”œâ”€ [GPU LEASE ACQUIRE] model1
       â”‚
30s    â”œâ”€ Load model1 to GPU
       â”‚
45s    â”œâ”€ Generate embeddings with model1
       â”‚  (process all chunks)
       â”‚
75s    â”œâ”€ Unload model1
       â”‚
80s    â”œâ”€ [GPU LEASE RELEASE] model1
       â”‚
       â”œâ”€ [GPU LEASE ACQUIRE] model2
       â”‚
100s   â”œâ”€ Load model2 to GPU
       â”‚
115s   â”œâ”€ Generate embeddings with model2
       â”‚
145s   â”œâ”€ Unload model2
       â”‚
150s   â”œâ”€ [GPU LEASE RELEASE] model2
       â”‚
       â”œâ”€ [GPU LEASE ACQUIRE] model3
       â”‚
170s   â”œâ”€ Load model3 to GPU
       â”‚
185s   â”œâ”€ Generate embeddings with model3
       â”‚
215s   â”œâ”€ Unload model3
       â”‚
220s   â”œâ”€ [GPU LEASE RELEASE] model3
       â”‚
225s   â”œâ”€ Combine ensemble (weighted average)
       â”‚
230s   â”œâ”€ Store in Qdrant
       â”‚
235s   â””â”€ âœ… Complete

Total: 235 seconds
GPU Usage: One model at a time (low memory, safer)
Benefits: âœ… Works on all GPU types, optimal VRAM management
Status: âœ… PRODUCTION READY - Parallel mode removed in V6.1
```

### ğŸ“ Architecture Simplification Note

**Previous Version (V6.0):** Supported both parallel and exclusive modes
**Current Version (V6.1):** Exclusive mode only (445 lines of parallel code removed)

**Rationale:**
- Parallel mode had zero test coverage
- High OOM risk on Kaggle T4x2 GPUs
- Exclusive mode provides better VRAM management
- Single execution path improves maintainability

---

## Error Handling Flow

```
Try Block:
â”œâ”€ discover_collections()
â”‚  â”œâ”€ ValueError: chunked_dir doesn't exist
â”‚  â”‚  â””â”€ Log error â†’ Exit
â”‚  â”‚
â”‚  â””â”€ OSError/PermissionError in subdirectory
â”‚     â””â”€ Log warning â†’ Continue (skip directory)
â”‚
â”œâ”€ _filter_collections()
â”‚  â””â”€ ValueError: requested collections not found
â”‚     â””â”€ Log error â†’ Exit
â”‚
â”œâ”€ _initialize_embedder()
â”‚  â””â”€ Exception: embedder initialization failed
â”‚     â””â”€ Raise RuntimeError â†’ Log error â†’ Exit
â”‚
â””â”€ process_collection() for each collection:
   â”œâ”€ Exception during processing
   â”‚  â””â”€ Catch â†’ Store in failed_collections â†’ Continue
   â”‚
   â””â”€ RuntimeError: invalid results structure
      â””â”€ Catch â†’ Store in failed_collections â†’ Continue

Finalization:
â”œâ”€ _log_processing_summary()
â”‚  â””â”€ Logs all failures with exception types
â”‚
â””â”€ _export_summary_json()
   â”œâ”€ IOError: can't write JSON file
   â”‚  â””â”€ Log error â†’ Continue (non-fatal)
   â”‚
   â””â”€ Success â†’ Log file path â†’ Exit
```

---

## Memory Management Flow (Exclusive Mode)

```
Collection Processing Loop:
â”‚
â”œâ”€ Collection 1: "Docling"
â”‚  â”‚
â”‚  â”œâ”€ Model 1:
â”‚  â”‚  â”œâ”€ Load to GPU: +2GB VRAM
â”‚  â”‚  â”œâ”€ Generate embeddings: +1GB VRAM (batches)
â”‚  â”‚  â”œâ”€ Unload: torch.cuda.empty_cache()
â”‚  â”‚  â””â”€ GPU memory released: -3GB VRAM
â”‚  â”‚
â”‚  â”œâ”€ Model 2:
â”‚  â”‚  â”œâ”€ Load to GPU: +2.5GB VRAM
â”‚  â”‚  â”œâ”€ Generate embeddings: +1GB VRAM
â”‚  â”‚  â”œâ”€ Unload: torch.cuda.empty_cache()
â”‚  â”‚  â””â”€ GPU memory released: -3.5GB VRAM
â”‚  â”‚
â”‚  â””â”€ Model 3:
â”‚     â”œâ”€ Load to GPU: +2.8GB VRAM
â”‚     â”œâ”€ Generate embeddings: +1GB VRAM
â”‚     â”œâ”€ Unload: torch.cuda.empty_cache()
â”‚     â””â”€ GPU memory released: -3.8GB VRAM
â”‚
â”‚  Peak GPU Usage: ~4GB (single model + batches)
â”‚  âœ… Fits in Kaggle T4 (16GB)
â”‚  âœ… DEFAULT BEHAVIOR (safest option)
â”‚
â””â”€ Collection 2: "FAST_DOCS"
   â””â”€ (Repeat same pattern)

Key Advantages:
â€¢ GPU memory reused for each model sequentially
â€¢ Prevents OOM errors on all GPU types
â€¢ Only execution mode in V6.1 (parallel mode removed)
â€¢ Optimal VRAM management for Kaggle T4x2 and other GPUs
```

---

## Summary Table: Function Responsibilities

| Function | Module | Input | Output | Side Effects |
|----------|--------|-------|--------|--------------|
| `_is_collection_directory` | Discovery | Path | bool | Logs warnings |
| `_resolve_collection_name` | Discovery | Path, Path, set | str | None |
| `_scan_directory_recursive` | Discovery | Path, int, dict, set | None | Modifies dict/set, logs |
| `discover_collections` | Discovery | Path, int | dict[str, Path] | Logs, raises |
| `_get_ensemble_mode_label` | Processing | bool | str | None |
| `_extract_telemetry` | Processing | dict | tuple | None |
| `_log_collection_completion` | Processing | str, list, list, int, bool | None | Logs |
| `process_collection` | Processing | str, Path, Embedder, args | dict | Logs, GPU ops, raises |
| `_filter_collections` | Orchestration | dict, list | dict | Raises |
| `_initialize_embedder` | Orchestration | list, bool, list | Embedder | Raises |
| `_log_processing_summary` | Orchestration | dict, dict | None | Logs |
| `_export_summary_json` | Orchestration | dict, Path, list, bool | None | Writes file, logs |
| `main` | Orchestration | None | None | All side effects |
| `parse_arguments` | Parsing | None | Namespace | None |

---

## Conclusion

These visual diagrams show the complete flow of `embed_collections_v6.py` from command-line invocation to final outputs. The modular architecture with 13 helper functions ensures:

1. **Clear separation of concerns**: Each function has a single, well-defined purpose
2. **Testability**: All helpers can be unit tested independently
3. **Maintainability**: Visual flow makes understanding and modifications easier
4. **Debugging**: Structured logging at each phase aids troubleshooting
5. **Performance**: Exclusive mode manages GPU memory efficiently for all environments

**V6.1 Update (October 23, 2025):**
- Parallel mode completely removed (445 lines)
- Exclusive mode is now the only execution path
- Simplified architecture improves maintainability
- ThroughputMonitor extracted to separate module

Ready for production deployment! ğŸš€
