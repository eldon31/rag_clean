[
  "### Code Quality Tools The project maintains code quality through: - **Type Hints**: Extensive use of type annotations with `from __future__ import annotations` - **Import Organization**: Consistent import ordering and structure - **Documentation Standards**: Comprehensive docstrings and examples",
  "# Score query-passage pairs scores = model.predict([ (\"How big is London\", \"London has 9,787,426 inhabitants at the 2011 census\"), (\"How big is London\", \"London is well known for its museums\") ])",
  "### Model Selection for Reranking CrossEncoder models provide different speed-quality trade-offs: | Model | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev) | Docs/Sec | |-------|---------------------|----------------------|----------| | `cross-encoder/ms-marco-TinyBERT-L2-v2` | 69.84 | 32.56 | 9000 | | `cross-encoder/ms-marco-MiniLM-L6-v2` | 74.30 | 39.01 | 1800 | | `cross-encoder/ms-marco-MiniLM-L12-v2` | 74.31 | 39.02 | 960 | | `cross-encoder/ms-marco-electra-base` | 71.99 | 36.41 | 340 | The `cross-encoder/ms-marco-MiniLM-L6-v2` model provides the best balance of quality and speed for most applications. Sources: [docs/cross_encoder/pretrained_models.md:35-43](), [docs/pretrained-models/ce-msmarco.md:41-53]()",
  "### Quality Improvements - **Broader Recall**: Bi-encoders capture semantic similarity across large corpora - **Precise Ranking**: Cross-encoders provide accurate relevance scoring for final results - **Hard Negative Training**: Improved training data quality through cross-encoder feedback",
  "### Model Card Integration Sparse evaluators store evaluation metrics in the model's card data for documentation: ```python def store_metrics_in_model_card_data(self, model, metrics, epoch=0, step=0): model.model_card_data.set_evaluation_metrics(self, metrics, epoch=epoch, step=step) ``` Sources: [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:271-274](), [sentence_transformers/sparse_encoder/evaluation/SparseEmbeddingSimilarityEvaluator.py:159-162]()",
  "## Serialization and Persistence The `CLIPModel` implements custom save/load methods to properly handle both the model and processor components: ```mermaid graph LR subgraph Save[\"save() Method\"] SAVEMODEL[\"model.save_pretrained()\"] SAVEPROC[\"processor.save_pretrained()\"] end subgraph Load[\"load() Method\"] LOADPATH[\"load_dir_path()\"] CONSTRUCT[\"CLIPModel(local_path)\"] end Save --> DISK[\"Disk Storage\"] DISK --> Load ``` This ensures that both the vision and text processing components are preserved during model serialization. Sources: [sentence_transformers/models/CLIPModel.py:98-121]()",
  "### Sparse Embedding Generation All sparse evaluators override the `embed_inputs` method to generate sparse embeddings instead of dense ones: | Component | Dense Evaluators | Sparse Evaluators | |-----------|------------------|-------------------| | Embedding Format | Dense tensors | Sparse tensors via `convert_to_sparse_tensor=True` | | Memory Management | Standard tensor operations | `save_to_cpu=True` for memory efficiency | | Dimension Control | `truncate_dim` for reducing dimensions | `max_active_dims` for sparsity control | | Similarity Functions | Cosine, dot product, euclidean, manhattan | Same functions but optimized for sparse tensors |",
  "### Dataset Metadata Extraction The system automatically analyzes training and evaluation datasets to generate comprehensive statistics and examples: | Metadata Type | Information Collected | Implementation | |---------------|----------------------|----------------| | **Size & Structure** | Dataset size, column names, data types | `compute_dataset_metrics` | | **Content Statistics** | Token/character counts, value distributions | Statistical analysis per column | | **Hub Integration** | Dataset ID, revision, download checksums | `extract_dataset_metadata` | | **Sample Examples** | Representative examples for documentation | First 3 samples with formatting | | **Loss Configuration** | Loss function details and parameters | `get_config_dict` introspection | The dataset analysis includes automatic tokenization to provide meaningful statistics: ```python",
  "### Core Module Hierarchy ```mermaid graph TB subgraph \"Base Module System\" Module[\"Module<br/>Base class\"] InputModule[\"InputModule<br/>Input processing\"] Transformer[\"Transformer<br/>Token embeddings\"] Pooling[\"Pooling<br/>Sentence embeddings\"] end subgraph \"Processing Modules\" Dense[\"Dense<br/>Linear transformation\"] Normalize[\"Normalize<br/>L2 normalization\"] LayerNorm[\"LayerNorm<br/>Layer normalization\"] Router[\"Router<br/>Task routing\"] end subgraph \"Specialized Modules\" MLMTransformer[\"MLMTransformer<br/>Masked LM head\"] SpladePooling[\"SpladePooling<br/>SPLADE activation\"] CLIPModel[\"CLIPModel<br/>Multimodal encoding\"] end Module --> InputModule Module --> Dense Module --> Normalize InputModule --> Transformer InputModule --> Pooling Module --> Router Module --> MLMTransformer Module --> SpladePooling Module --> CLIPModel ```",
  "## Model Loading and Configuration CLIP models are loaded through the standard `SentenceTransformer` interface or by constructing `CLIPModel` instances directly. **Configuration Options** | Parameter | Purpose | Default | Example | |-----------|---------|---------|---------| | `model_name` | Base CLIP model | Required | `'openai/clip-vit-base-patch32'` | | `processor_name` | Processor configuration | `model_name` | Custom processor path | | `max_seq_length` | Text sequence limit | From tokenizer | 77 for CLIP models | **Loading Patterns** ```python",
  "## Integration with SentenceTransformer Ecosystem The `CLIPModel` integrates with the sentence-transformers ecosystem through the modular architecture and supports the same operations as text-only models. **Integration Architecture** ```mermaid graph TB subgraph SentenceTransformer[\"SentenceTransformer Class\"] ENCODE[\"encode() method\"] SIMILARITY[\"similarity() method\"] MODULES[\"_modules list\"] end subgraph CLIPModule[\"CLIPModel Module\"] CLIPMOD[\"CLIPModel(InputModule)\"] TOKENIZE[\"tokenize()\"] FORWARD[\"forward()\"] end subgraph Processing[\"Shared Processing\"] MULTIPROC[\"Multi-processing support\"] NORMALIZE[\"normalize_embeddings\"] TENSOR[\"convert_to_tensor\"] end subgraph Applications[\"Application Support\"] SEARCH[\"Semantic search\"] SIMILARITY_COMP[\"Cosine similarity\"] RETRIEVAL[\"Cross-modal retrieval\"] end SentenceTransformer --> CLIPModule CLIPModule --> Processing Processing --> Applications ``` **Module System Integration** | Component | Role | Implementation | |-----------|------|----------------| | `CLIPModel` | Input processing | Inherits from `InputModule` | | `Pooling` | Optional embedding processing | Can be added after `CLIPModel` | | `Normalize` | L2 normalization | Applied to final embeddings | The `CLIPModel` appears in the module registry and supports the same configuration patterns as other sentence-transformers modules. Sources: [sentence_transformers/models/CLIPModel.py:15](), [sentence_transformers/models/__init__.py:6,40]()",
  "## Similarity Functions and Metrics The evaluators support multiple similarity functions, each with different mathematical properties and use cases. | Similarity Function | Implementation | Use Case | Greater is Better | |-------------------|----------------|----------|------------------| | `cosine` | `pairwise_cos_sim` | General semantic similarity | ✓ | | `dot` | `pairwise_dot_score` | When magnitude matters | ✓ | | `euclidean` | `pairwise_euclidean_sim` | Distance-based similarity | ✗ | | `manhattan` | `pairwise_manhattan_sim` | L1 distance similarity | ✗ | ```mermaid graph LR subgraph \"SimilarityFunction Enum\" COSINE_ENUM[\"SimilarityFunction.COSINE\"] DOT_ENUM[\"SimilarityFunction.DOT_PRODUCT\"] EUC_ENUM[\"SimilarityFunction.EUCLIDEAN\"] MAN_ENUM[\"SimilarityFunction.MANHATTAN\"] end subgraph \"Implementation Functions\" COS_FUNC[\"pairwise_cos_sim\"] DOT_FUNC[\"pairwise_dot_score\"] EUC_FUNC[\"pairwise_euclidean_sim\"] MAN_FUNC[\"pairwise_manhattan_sim\"] end subgraph \"Evaluator Integration\" SIM_DICT[\"similarity_functions dict\"] SCORE_COMP[\"score computation\"] METRIC_CALC[\"metric calculation\"] end COSINE_ENUM --> COS_FUNC DOT_ENUM --> DOT_FUNC EUC_ENUM --> EUC_FUNC MAN_ENUM --> MAN_FUNC COS_FUNC --> SIM_DICT DOT_FUNC --> SIM_DICT EUC_FUNC --> SIM_DICT MAN_FUNC --> SIM_DICT SIM_DICT --> SCORE_COMP SCORE_COMP --> METRIC_CALC ``` **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:184-189](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:238-259](), [sentence_transformers/evaluation/TripletEvaluator.py:187-204]()",
  "## CrossEncoder Training Architecture CrossEncoder training follows a similar pattern to other model types in sentence-transformers but with specific adaptations for joint text encoding and ranking/classification tasks. **CrossEncoder Training System Overview** ```mermaid graph TB subgraph \"Core Components\" CE[CrossEncoder] CETrainer[CrossEncoderTrainer] CEArgs[CrossEncoderTrainingArguments] CELosses[CrossEncoder Losses] CEEvals[CrossEncoder Evaluators] end subgraph \"Data Processing\" Dataset[datasets.Dataset] DataCollator[Data Collator] HardNegMining[Hard Negatives Mining] end subgraph \"Loss Functions\" BCE[BinaryCrossEntropyLoss] MNR[MultipleNegativesRankingLoss] Lambda[LambdaLoss] ListNet[ListNetLoss] CrossEntropy[CrossEntropyLoss] end subgraph \"Training Infrastructure\" HFTrainer[Transformers Trainer] ModelCard[Model Card Generation] HFHub[Hugging Face Hub] end CE --> CETrainer Dataset --> DataCollator CEArgs --> CETrainer CELosses --> CETrainer CEEvals --> CETrainer BCE --> CELosses MNR --> CELosses Lambda --> CELosses ListNet --> CELosses CrossEntropy --> CELosses CETrainer --> HFTrainer CETrainer --> ModelCard ModelCard --> HFHub HardNegMining --> Dataset ``` Sources: [docs/cross_encoder/training_overview.md:1-500](), [docs/cross_encoder/loss_overview.md:1-100]()",
  "## Training Components CrossEncoder training involves six main components that work together to fine-tune models for ranking and classification tasks. **CrossEncoder Training Data Flow** ```mermaid graph LR subgraph \"Input Data\" TextPairs[\"(text_A, text_B) pairs\"] Triplets[\"(query, positive, negative)\"] Rankings[\"(query, [doc1, doc2, ...])\"] Labels[Class Labels / Scores] end subgraph \"Data Processing\" DataCollator[\"Data Collator\"] Tokenization[Tokenization] BatchFormat[Batch Formatting] end subgraph \"Model & Loss\" CrossEncoder[CrossEncoder Model] LossFunction[Loss Function] ForwardPass[Forward Pass] end subgraph \"Training Loop\" Optimizer[Optimizer] BackwardPass[Backward Pass] WeightUpdate[Weight Update] end subgraph \"Evaluation\" Evaluator[CrossEncoder Evaluator] Metrics[Metrics Calculation] end TextPairs --> DataCollator Triplets --> DataCollator Rankings --> DataCollator Labels --> DataCollator DataCollator --> Tokenization Tokenization --> BatchFormat BatchFormat --> CrossEncoder CrossEncoder --> ForwardPass ForwardPass --> LossFunction LossFunction --> BackwardPass BackwardPass --> Optimizer Optimizer --> WeightUpdate CrossEncoder --> Evaluator Evaluator --> Metrics ``` Sources: [docs/cross_encoder/training_overview.md:170-190](), [sentence_transformers/data_collator.py:35-120]()",
  "### Aggregated Results Aggregated metrics follow the pattern: `NanoBEIR_{aggregate_key}_{score_function}_{metric}@{k}`: - `NanoBEIR_mean_cosine_ndcg@10` - `NanoBEIR_mean_dot_mrr@10`",
  "## Loss Function Hierarchy The following diagram shows the inheritance and relationship structure of CrossEncoder loss functions: ```mermaid graph TD Module[\"nn.Module\"] subgraph \"Learning-to-Rank Losses\" LambdaLoss[\"LambdaLoss\"] ListNetLoss[\"ListNetLoss\"] PListMLELoss[\"PListMLELoss\"] ListMLELoss[\"ListMLELoss\"] RankNetLoss[\"RankNetLoss\"] end subgraph \"Classification Losses\" BinaryCrossEntropyLoss[\"BinaryCrossEntropyLoss\"] CrossEntropyLoss[\"CrossEntropyLoss\"] MultipleNegativesRankingLoss[\"MultipleNegativesRankingLoss\"] CachedMultipleNegativesRankingLoss[\"CachedMultipleNegativesRankingLoss\"] end subgraph \"Regression Losses\" MSELoss[\"MSELoss\"] MarginMSELoss[\"MarginMSELoss\"] end subgraph \"Weighting Schemes\" BaseWeightingScheme[\"BaseWeightingScheme\"] NoWeightingScheme[\"NoWeightingScheme\"] NDCGLoss1Scheme[\"NDCGLoss1Scheme\"] NDCGLoss2Scheme[\"NDCGLoss2Scheme\"] LambdaRankScheme[\"LambdaRankScheme\"] NDCGLoss2PPScheme[\"NDCGLoss2PPScheme\"] PListMLELambdaWeight[\"PListMLELambdaWeight\"] end Module --> LambdaLoss Module --> ListNetLoss Module --> PListMLELoss Module --> BinaryCrossEntropyLoss Module --> CrossEntropyLoss Module --> MultipleNegativesRankingLoss Module --> CachedMultipleNegativesRankingLoss Module --> MSELoss Module --> MarginMSELoss Module --> BaseWeightingScheme ListMLELoss --> PListMLELoss LambdaLoss --> RankNetLoss BaseWeightingScheme --> NoWeightingScheme BaseWeightingScheme --> NDCGLoss1Scheme BaseWeightingScheme --> NDCGLoss2Scheme BaseWeightingScheme --> LambdaRankScheme BaseWeightingScheme --> NDCGLoss2PPScheme Module --> PListMLELambdaWeight LambdaLoss -.-> BaseWeightingScheme PListMLELoss -.-> PListMLELambdaWeight ``` Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:103-361](), [sentence_transformers/cross_encoder/losses/ListNetLoss.py:10-198](), [sentence_transformers/cross_encoder/losses/PListMLELoss.py:45-295](), [sentence_transformers/cross_encoder/losses/ListMLELoss.py:9-127](), [sentence_transformers/cross_encoder/losses/RankNetLoss.py:11-124](), [docs/package_reference/cross_encoder/losses.md:1-68]()",
  "## Overview CrossEncoder loss functions are designed to train models that process text pairs jointly through a single transformer encoder. These loss functions fall into three main categories: - **Learning-to-Rank Losses**: Optimize ranking metrics like NDCG for information retrieval tasks - **Classification Losses**: Handle binary or multi-class classification scenarios - **Regression Losses**: Predict continuous similarity scores between text pairs",
  "### Data Format Requirements All learning-to-rank losses expect the following input format: | Component | Format | Description | |-----------|--------|-------------| | Inputs | `(queries, documents_list)` | List of query strings and list of document lists | | Labels | `[score1, score2, ..., scoreN]` | List of relevance scores per query | | Model Output | 1 label | Single relevance score per query-document pair |",
  "### LambdaLoss Framework The `LambdaLoss` class implements a comprehensive framework for ranking metric optimization with multiple weighting schemes: ```mermaid graph LR subgraph \"Input Processing\" QueryDocs[\"queries + docs_list\"] --> Pairs[\"query-document pairs\"] Labels[\"labels list\"] --> LabelMatrix[\"labels_matrix\"] end subgraph \"Model Processing\" Pairs --> CrossEncoder[\"model.forward()\"] CrossEncoder --> Logits[\"logits\"] Logits --> ActivationFn[\"activation_fn\"] ActivationFn --> LogitsMatrix[\"logits_matrix\"] end subgraph \"LambdaLoss Computation\" LogitsMatrix --> Sorting[\"sort by logits\"] LabelMatrix --> Sorting Sorting --> TrueDiffs[\"true_diffs\"] Sorting --> Gains[\"gain calculation\"] Sorting --> Discounts[\"discount calculation\"] Gains --> WeightingScheme[\"weighting_scheme.forward()\"] Discounts --> WeightingScheme WeightingScheme --> Weights[\"weights\"] TrueDiffs --> ScoreDiffs[\"score differences\"] ScoreDiffs --> WeightedProbas[\"weighted probabilities\"] Weights --> WeightedProbas WeightedProbas --> Loss[\"final loss\"] end ``` The `LambdaLoss` supports five weighting schemes: | Scheme | Class | Purpose | |--------|-------|---------| | No Weighting | `NoWeightingScheme` | Uniform weights (RankNet equivalent) | | NDCG Loss1 | `NDCGLoss1Scheme` | Basic NDCG optimization | | NDCG Loss2 | `NDCGLoss2Scheme` | Improved NDCG with tighter bounds | | LambdaRank | `LambdaRankScheme` | Coarse upper bound optimization | | NDCG Loss2++ | `NDCGLoss2PPScheme` | Hybrid scheme (recommended) | Sources: [sentence_transformers/cross_encoder/losses/LambdaLoss.py:103-361](), [sentence_transformers/cross_encoder/losses/LambdaLoss.py:12-101]()",
  "## Multi-Dataset Training The training system supports training on multiple datasets simultaneously using `DatasetDict`: ```mermaid graph TB subgraph \"Multi-Dataset Input\" DD[\"DatasetDict\"] DS1[\"Dataset 'nli'\"] DS2[\"Dataset 'sts'\"] DS3[\"Dataset 'quora'\"] end subgraph \"Loss Mapping\" LossDict[\"Loss Dictionary\"] L1[\"nli: CoSENTLoss\"] L2[\"sts: CosineSimilarityLoss\"] L3[\"quora: MNRL\"] end subgraph \"Batch Sampling\" BatchSampler[\"MultiDatasetBatchSampler\"] RoundRobin[\"RoundRobinBatchSampler\"] Proportional[\"ProportionalBatchSampler\"] end subgraph \"Training Process\" DataCollator[\"add_dataset_name_column()\"] ComputeLoss[\"compute_loss()\"] LossSelect[\"Select loss by dataset_name\"] end DD --> DS1 DD --> DS2 DD --> DS3 LossDict --> L1 LossDict --> L2 LossDict --> L3 DS1 --> BatchSampler DS2 --> BatchSampler DS3 --> BatchSampler BatchSampler --> RoundRobin BatchSampler --> Proportional BatchSampler --> DataCollator DataCollator --> ComputeLoss LossDict --> LossSelect ComputeLoss --> LossSelect ``` **Multi-Dataset Training Architecture** Sources: [sentence_transformers/trainer.py:295-310](), [sentence_transformers/trainer.py:416-422](), [sentence_transformers/trainer.py:785-800]()",
  "## Dataset Overview MS MARCO (Microsoft Machine Reading Comprehension) is a large-scale information retrieval corpus created from real user search queries using the Bing search engine. The dataset consists of: - **Training data**: Over 500,000 query-passage examples - **Complete corpus**: Over 8.8 million passages - **Evaluation**: TREC Deep Learning 2019 and MS MARCO Passage Retrieval datasets - **Task type**: Asymmetric semantic search (short queries → longer passages) The dataset enables training models that can find semantically relevant passages given natural language queries, making it ideal for search and question-answering applications. **Sources:** [docs/pretrained-models/msmarco-v3.md:1-5](), [docs/pretrained-models/ce-msmarco.md:1-6]()",
  "### Encoding Methods ```mermaid graph LR subgraph \"Encoding Methods\" GenEnc[\"encode()\"] --> Text1[\"General text encoding\"] QueryEnc[\"encode_query()\"] --> Text2[\"Query-optimized encoding\"] DocEnc[\"encode_document()\"] --> Text3[\"Document-optimized encoding\"] end subgraph \"Internal Flow\" Input[\"Input Text\"] --> Prompt[\"Apply Prompts\"] Prompt --> Task[\"Task Routing\"] Task --> Modules[\"Sequential Modules\"] Modules --> Embeddings[\"Dense Embeddings\"] end subgraph \"Configuration\" Prompts[\"model.prompts\"] --> QueryEnc DefaultPrompt[\"model.default_prompt_name\"] --> GenEnc TaskType[\"task parameter\"] --> Router[\"Router Module\"] end ``` **Method Selection**: - `encode()`: General-purpose encoding for similarity tasks - `encode_query()`: Optimized for search queries, applies \"query\" prompt if available - `encode_document()`: Optimized for documents, applies \"document\"/\"passage\"/\"corpus\" prompts Sources: [sentence_transformers/SentenceTransformer.py:416-543](), [sentence_transformers/SentenceTransformer.py:545-675]()",
  "### Key Features - **Modular Design**: Composed of sequential modules like `Transformer`, `Pooling`, `Normalize` - **Prompt Support**: Configurable prompts for different tasks via `prompts` dictionary - **Task-Specific Encoding**: `encode_query()` and `encode_document()` methods for asymmetric retrieval - **Multiple Backends**: Supports PyTorch, ONNX, and OpenVINO backends - **Similarity Functions**: Built-in similarity computation with configurable functions",
  "### Core Architecture ```mermaid graph LR subgraph \"SentenceTransformer Pipeline\" Input[\"Text Input\"] Tokenizer[\"tokenize()\"] Transformer[\"Transformer Module\"] Pooling[\"Pooling Module\"] Optional[\"Optional Modules<br/>(Normalize, Dense, etc.)\"] Output[\"Dense Embedding\"] end Input --> Tokenizer Tokenizer --> Transformer Transformer --> Pooling Pooling --> Optional Optional --> Output subgraph \"Key Methods\" Encode[\"encode()\"] EncodeQuery[\"encode_query()\"] EncodeDoc[\"encode_document()\"] Similarity[\"similarity()\"] end ``` The `SentenceTransformer` class inherits from `nn.Sequential`, `FitMixin`, and `PeftAdapterMixin`, allowing it to function as a sequential pipeline of modules while supporting training and PEFT adapters.",
  "## Architecture Overview The sentence-transformers library provides three main model architectures that differ in their encoding approach and use cases: ```mermaid graph TB subgraph \"Input Processing\" Text[\"Text Input(s)\"] end subgraph \"Core Model Types\" ST[\"SentenceTransformer<br/>Dense Embeddings\"] SE[\"SparseEncoder<br/>Sparse Embeddings\"] CE[\"CrossEncoder<br/>Pairwise Scoring\"] end subgraph \"Output Types\" Dense[\"Dense Vectors<br/>[batch_size, embedding_dim]\"] Sparse[\"Sparse Vectors<br/>[batch_size, vocab_size]\"] Scores[\"Similarity Scores<br/>[batch_size] or [batch_size, num_labels]\"] end subgraph \"Use Cases\" SemanticSearch[\"Semantic Search\"] Clustering[\"Clustering\"] LexicalSearch[\"Neural Lexical Search\"] HybridRetrieval[\"Hybrid Retrieval\"] Reranking[\"Reranking\"] Classification[\"Text Classification\"] end Text --> ST Text --> SE Text --> CE ST --> Dense SE --> Sparse CE --> Scores Dense --> SemanticSearch Dense --> Clustering Sparse --> LexicalSearch Sparse --> HybridRetrieval Scores --> Reranking Scores --> Classification ``` **Sources:** [sentence_transformers/SentenceTransformer.py:61-163](), [sentence_transformers/sparse_encoder/SparseEncoder.py:27-129](), [sentence_transformers/cross_encoder/CrossEncoder.py:48-116](), [README.md:15-17]()",
  "## Loss Function Selection Guide The table below provides guidance on which loss function to use based on your data and task: | Loss Function | Best For | Input Format | Special Requirements | |---------------|----------|--------------|---------------------| | MultipleNegativesRankingLoss | Retrieval, general purpose | (anchor, positive) pairs | Large batch size beneficial | | CoSENTLoss | Semantic similarity | Text pairs with scores | - | | TripletLoss | Clustering, similarity | (anchor, positive, negative) triplets | - | | BatchHardTripletLoss | Classification | Single texts with labels | Multiple examples per class | | MSELoss | Distillation, transfer | Texts with target embeddings | Teacher model | | MatryoshkaLoss | Size-efficient models | Depends on base loss | - | | AdaptiveLayerLoss | Speed-efficient models | Depends on base loss | - | | ContrastiveTensionLoss | Unsupervised learning | Single sentences | Special dataloader | | GISTEmbedLoss | Better negative sampling | Same as MNRL | Guide model |",
  "### OpenSearch Integration The `semantic_search_opensearch()` function leverages OpenSearch's `neural_sparse` query capabilities. **Key Differences from Elasticsearch:** - Uses `neural_sparse` query type instead of `rank_feature` - Compatible with Amazon OpenSearch Service - Supports asymmetric sparse encoder architectures Sources: [sentence_transformers/sparse_encoder/search_engines.py:428-556](), [examples/sparse_encoder/applications/semantic_search/semantic_search_opensearch.py:1-87]()",
  "### Reusable Index Pattern All search engine integrations support index reuse through the `output_index` parameter: ```mermaid graph TD FirstCall[\"First Function Call\"] CreateIndex[\"Create Index<br/>(corpus_embeddings required)\"] SearchResults1[\"Search Results + Index\"] SecondCall[\"Subsequent Calls\"] ReuseIndex[\"Reuse Existing Index<br/>(corpus_index provided)\"] SearchResults2[\"Search Results Only\"] FirstCall --> CreateIndex CreateIndex --> SearchResults1 SecondCall --> ReuseIndex ReuseIndex --> SearchResults2 SearchResults1 --> IndexStorage[\"Store Index Reference\"] IndexStorage --> SecondCall ``` **Index Reuse Pattern for Production Workflows**",
  "### Sparsity Advantages Sparse embeddings provide several performance benefits for search: | Advantage | Description | Impact | |-----------|-------------|---------| | Storage Efficiency | Most dimensions are zero | Reduced memory footprint | | Search Speed | Skip zero-value computations | Faster similarity calculations | | Interpretability | Non-zero dimensions map to tokens | Explainable search results | | Exact Matching | Preserve lexical signals | Hybrid semantic-lexical search |",
  "### Dataset Format Requirements Training datasets must match the loss function requirements: | Loss Function | Input Columns | Label Column | Example | |---------------|---------------|--------------|---------| | `SparseMultipleNegativesRankingLoss` | `(anchor, positive)` or `(anchor, positive, negative)` | None | `[\"query\", \"answer\"]` | | `SparseCoSENTLoss` | `(sentence_A, sentence_B)` | `score` (0-1) | `[\"text1\", \"text2\", \"score\"]` | | `SparseMarginMSELoss` | `(query, positive, negative)` | `margin_scores` | `[\"query\", \"pos\", \"neg\", \"margins\"]` | **Sources:** [docs/sparse_encoder/training_overview.md:328-344](), [docs/sparse_encoder/loss_overview.md:29-62]()",
  "### Dense Embedding Model (SentenceTransformer) ```mermaid graph LR Text[\"Input Text\"] --> Transformer[\"Transformer<br/>(bert-base-uncased)\"] Transformer --> Pooling[\"Pooling<br/>(mean pooling)\"] Pooling --> Normalize[\"Normalize<br/>(L2 normalization)\"] Normalize --> Embedding[\"Dense Embedding<br/>(768-dim vector)\"] subgraph \"modules[0]\" Transformer end subgraph \"modules[1]\" Pooling end subgraph \"modules[2]\" Normalize end ```",
  "### Sparse Architecture Components ```mermaid graph TB subgraph \"SparseEncoder Components\" MLMTransformer[\"MLMTransformer<br/>Token-level predictions\"] SpladePooling[\"SpladePooling<br/>Sparsification\"] SparseAutoEncoder[\"SparseAutoEncoder<br/>k-sparse activation\"] Router[\"Router<br/>Query/Document paths\"] end subgraph \"Output Processing\" ActiveDims[\"max_active_dims<br/>Sparsity control\"] SparseOutput[\"Sparse COO Tensor<br/>[batch_size, vocab_size]\"] end Input[\"Text\"] --> Router Router --> MLMTransformer MLMTransformer --> SpladePooling SpladePooling --> ActiveDims ActiveDims --> SparseOutput ```",
  "### Key Differences from SentenceTransformer - **Vocabulary-Sized Output**: Embeddings have dimensions equal to tokenizer vocabulary size - **Sparsity Control**: `max_active_dims` parameter limits non-zero dimensions - **Sparse Tensor Format**: Outputs can be sparse COO tensors for memory efficiency - **Term Importance**: Non-zero values represent importance of vocabulary terms",
  "### Decoding and Interpretation The `SparseEncoder` provides a `decode()` method to interpret sparse embeddings as weighted vocabulary terms: ```python model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") embeddings = model.encode(\"machine learning\") tokens_weights = model.decode(embeddings, top_k=10)",
  "### Performance Optimization Techniques The library implements several optimization strategies for production deployment: | Optimization | Implementation | Use Case | |-------------|----------------|----------| | **Multi-Processing** | Process pools for batch encoding | Large-scale text processing | | **ONNX Conversion** | Model quantization and optimization | CPU inference optimization | | **Backend Selection** | Runtime backend switching | Hardware-specific optimization | | **Memory Management** | Gradient caching, efficient batching | Memory-constrained training | | **Sparse Operations** | Optimized sparse tensor operations | Sparse encoder efficiency |",
  "### Architecture Characteristics - **No Individual Embeddings**: Cannot encode single texts independently - **Joint Processing**: Both texts processed together through transformer layers - **Classification Head**: Uses sequence classification architecture - **Configurable Labels**: Supports regression (`num_labels=1`) or multi-class classification - **Higher Accuracy**: Generally more accurate than bi-encoder approaches for pairwise tasks",
  "### CrossEncoder Pipeline ```mermaid graph TB subgraph \"CrossEncoder Architecture\" Pairs[\"Text Pairs<br/>[(query, document), ...]\"] TokenizerCE[\"AutoTokenizer<br/>Joint encoding\"] ModelCE[\"AutoModelForSequenceClassification\"] Activation[\"Activation Function<br/>(Sigmoid/Identity)\"] ScoresOut[\"Similarity Scores\"] end subgraph \"Key Methods\" Predict[\"predict()<br/>Score pairs\"] Rank[\"rank()<br/>Rank documents\"] end Pairs --> TokenizerCE TokenizerCE --> ModelCE ModelCE --> Activation Activation --> ScoresOut ScoresOut --> Predict ScoresOut --> Rank ```",
  "# sentence-transformers Documentation Index This file contains links to all extracted documents. Please refer to the files below for detailed information. - [Introduction](sentence-transformers/Introduction.md) - [Overview](sentence-transformers/Overview.md) - [Core Model Types](sentence-transformers/Core_Model_Types.md) - [Basic encoding](sentence-transformers/Basic_encoding.md) - [Task-specific encoding with prompts](sentence-transformers/Task-specific_encoding_with_prompts.md) - [Returns machine 0.85 learning 0.72 algorithm 0.45 ...](sentence-transformers/Returns_machine_0.85_learning_0.72_algorithm_0.45_....md) - [Reranking pipeline](sentence-transformers/Reranking_pipeline.md) - [Score all pairs](sentence-transformers/Score_all_pairs.md) - [Or rank documents directly](sentence-transformers/Or_rank_documents_directly.md) - [Created via Router.for query document](sentence-transformers/Created_via_Router.for_query_document.md) - [Or manual configuration](sentence-transformers/Or_manual_configuration.md) - [Training arguments must specify router mapping](sentence-transformers/Training_arguments_must_specify_router_mapping.md) - [Default installation](sentence-transformers/Default_installation.md) - [Training setup](sentence-transformers/Training_setup.md) - [Development setup](sentence-transformers/Development_setup.md) - [Test basic model loading](sentence-transformers/Test_basic_model_loading.md) - [Test encoding](sentence-transformers/Test_encoding.md) - [Quickstart Guide](sentence-transformers/Quickstart_Guide.md) - [Query-optimized encoding](sentence-transformers/Query-optimized_encoding.md) - [Document-optimized encoding](sentence-transformers/Document-optimized_encoding.md) - [Specialized encoding for retrieval](sentence-transformers/Specialized_encoding_for_retrieval.md) - [Automatically rank documents by relevance](sentence-transformers/Automatically_rank_documents_by_relevance.md) - [Query How many people live in Berlin](sentence-transformers/Query_How_many_people_live_in_Berlin.md) - [- 0 8.61 Berlin had a population of 3520031 registered inhabitants...](sentence-transformers/-_0_8.61_Berlin_had_a_population_of_3520031_registered_inhabitants....md) - [- 2 6.35 In 2013 around 600000 Berliners were registered...](sentence-transformers/-_2_6.35_In_2013_around_600000_Berliners_were_registered....md) - [- 1 5.51 Berlin has a yearly total of about 135 million day visitors...](sentence-transformers/-_1_5.51_Berlin_has_a_yearly_total_of_about_135_million_day_visitors....md) - [Single loss](sentence-transformers/Single_loss.md) - [Multi-dataset losses](sentence-transformers/Multi-dataset_losses.md) - [Training arguments with router mapping](sentence-transformers/Training_arguments_with_router_mapping.md) - [SparseEncoder Training](sentence-transformers/SparseEncoder_Training.md) - [Training arguments for Router models](sentence-transformers/Training_arguments_for_Router_models.md) - [CrossEncoder Training](sentence-transformers/CrossEncoder_Training.md) - [Model with existing classification head](sentence-transformers/Model_with_existing_classification_head.md) - [Model without classification head will be added](sentence-transformers/Model_without_classification_head_will_be_added.md) - [CSR model training setup](sentence-transformers/CSR_model_training_setup.md) - [Loss Functions for CrossEncoder](sentence-transformers/Loss_Functions_for_CrossEncoder.md) - [Key computation from ListNetLoss.forward](sentence-transformers/Key_computation_from_ListNetLoss.forward.md) - [... populate matrices ...](sentence-transformers/..._populate_matrices_....md) - [Choose appropriate loss function](sentence-transformers/Choose_appropriate_loss_function.md) - [loss losses.ListNetLossmodel Alternative listwise approach](sentence-transformers/loss__losses.ListNetLossmodel___Alternative_listwise_approach.md) - [loss losses.PListMLELossmodel Position-aware MLE](sentence-transformers/loss__losses.PListMLELossmodel___Position-aware_MLE.md) - [Router module validation in trainer](sentence-transformers/Router_module_validation_in_trainer.md) - [Example Multiple similarity functions in one evaluation](sentence-transformers/Example_Multiple_similarity_functions_in_one_evaluation.md) - [SparseEncoder Evaluators](sentence-transformers/SparseEncoder_Evaluators.md) - [Example from SparseInformationRetrievalEvaluator](sentence-transformers/Example_from_SparseInformationRetrievalEvaluator.md) - [CrossEncoder Evaluators](sentence-transformers/CrossEncoder_Evaluators.md) - [Prepare samples with queries positives and negatives](sentence-transformers/Prepare_samples_with_queries_positives_and_negatives.md) - [NanoBEIR Evaluation](sentence-transformers/NanoBEIR_Evaluation.md) - [Basic usage with all datasets](sentence-transformers/Basic_usage_with_all_datasets.md) - [Subset of datasets with prompts](sentence-transformers/Subset_of_datasets_with_prompts.md) - [SparseEncoder - Sparse vector embeddings](sentence-transformers/SparseEncoder_-_Sparse_vector_embeddings.md) - [Check sparsity statistics](sentence-transformers/Check_sparsity_statistics.md) - [Load general purpose model](sentence-transformers/Load_general_purpose_model.md) - [Load with specific configuration](sentence-transformers/Load_with_specific_configuration.md) - [Model with prompt support](sentence-transformers/Model_with_prompt_support.md) - [Automatic prompt selection](sentence-transformers/Automatic_prompt_selection.md) - [Manual prompt specification](sentence-transformers/Manual_prompt_specification.md) - [SparseEncoder Models](sentence-transformers/SparseEncoder_Models.md) - [MSMARCO Models](sentence-transformers/MSMARCO_Models.md) - [Load cosine similarity optimized model](sentence-transformers/Load_cosine_similarity_optimized_model.md) - [Encode query and passage](sentence-transformers/Encode_query_and_passage.md) - [Calculate similarity choose based on model optimization](sentence-transformers/Calculate_similarity_choose_based_on_model_optimization.md) - [For large-scale retrieval](sentence-transformers/For_large-scale_retrieval.md) - [Efficient batch encoding](sentence-transformers/Efficient_batch_encoding.md) - [Compute similarity matrix](sentence-transformers/Compute_similarity_matrix.md) - [Semantic Search](sentence-transformers/Semantic_Search.md) - [Sparse Search Integration](sentence-transformers/Sparse_Search_Integration.md) - [Retrieve Rerank Architecture](sentence-transformers/Retrieve__Rerank_Architecture.md) - [Semantic Textual Similarity](sentence-transformers/Semantic_Textual_Similarity.md) - [Multiple similarity functions evaluated together](sentence-transformers/Multiple_similarity_functions_evaluated_together.md) - [Multimodal Applications](sentence-transformers/Multimodal_Applications.md) - [Mixed input batch](sentence-transformers/Mixed_input_batch.md) - [Through SentenceTransformer](sentence-transformers/Through_SentenceTransformer.md) - [Direct CLIPModel construction](sentence-transformers/Direct_CLIPModel_construction.md) - [Advanced Topics](sentence-transformers/Advanced_Topics.md) - [Example of automatic statistics generation](sentence-transformers/Example_of_automatic_statistics_generation.md) - [From SparseEncoderModelCardData.register model](sentence-transformers/From_SparseEncoderModelCardData.register_model.md) - [Multi-Processing and Optimization](sentence-transformers/Multi-Processing_and_Optimization.md) - [Testing and Development](sentence-transformers/Testing_and_Development.md) - [Example test structure from test pretrained stsb.py](sentence-transformers/Example_test_structure_from_test_pretrained_stsb.py.md) - [Run all tests excluding slow tests](sentence-transformers/Run_all_tests_excluding_slow_tests.md) - [Run specific test categories](sentence-transformers/Run_specific_test_categories.md) - [Run tests for specific model type](sentence-transformers/Run_tests_for_specific_model_type.md) - [Run with coverage](sentence-transformers/Run_with_coverage.md)",
  "### InformationRetrievalEvaluator The `InformationRetrievalEvaluator` class evaluates models on information retrieval tasks using query-document pairs and relevance judgments. **Key Methods:** - `__call__(model, output_path, epoch, steps)` - Main evaluation method - `compute_metrices(model, corpus_model, corpus_embeddings)` - Computes IR metrics - `embed_inputs(model, sentences, encode_fn_name)` - Handles encoding for queries/documents **Metrics Computed:** - Accuracy@k, Precision@k, Recall@k - Mean Reciprocal Rank (MRR@k) - Normalized Discounted Cumulative Gain (NDCG@k) - Mean Average Precision (MAP@k) Sources: [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-123](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:292-408]() **Evaluator Class Hierarchy** ```mermaid classDiagram class SentenceEvaluator { <<abstract>> +__call__(model, output_path, epoch, steps) dict +primary_metric str +prefix_name_to_metrics(metrics, name) dict +store_metrics_in_model_card_data(model, metrics, epoch, steps) } class BinaryClassificationEvaluator { +sentences1 list +sentences2 list +labels list +similarity_fn_names list +compute_metrices(model) dict +find_best_acc_and_threshold(scores, labels, high_score_more_similar) tuple +find_best_f1_and_threshold(scores, labels, high_score_more_similar) tuple } class RerankingEvaluator { +samples list +at_k int +similarity_fct Callable +compute_metrices_batched(model) dict +compute_metrices_individual(model) dict } class InformationRetrievalEvaluator { +queries dict +corpus dict +relevant_docs dict +compute_metrices(model, corpus_model, corpus_embeddings) dict +embed_inputs(model, sentences, encode_fn_name) ndarray } SentenceEvaluator <|-- BinaryClassificationEvaluator SentenceEvaluator <|-- RerankingEvaluator SentenceEvaluator <|-- InformationRetrievalEvaluator ``` Sources: [sentence_transformers/evaluation/SentenceEvaluator.py](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27](), [sentence_transformers/evaluation/RerankingEvaluator.py:25](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23]()",
  "# Initialize with sentence pairs and binary labels evaluator = BinaryClassificationEvaluator( sentences1=[\"The cat sat on the mat\", \"Hello world\"], sentences2=[\"A cat was sitting on a rug\", \"Goodbye earth\"], labels=[1, 0], # 1 for similar, 0 for dissimilar name=\"similarity_test\" )",
  "### Configuration Management Each evaluator provides configuration introspection via `get_config_dict()`: | Evaluator | Key Configuration Parameters | |-----------|------------------------------| | `InformationRetrievalEvaluator` | `truncate_dim`, `query_prompt`, `corpus_prompt` | | `EmbeddingSimilarityEvaluator` | `truncate_dim`, `precision` | | `BinaryClassificationEvaluator` | `truncate_dim` | | `RerankingEvaluator` | `at_k`, `truncate_dim` | | `TripletEvaluator` | `margin`, `truncate_dim` | **Sources:** Multiple evaluator `get_config_dict()` methods across evaluation files",
  "### Memory Optimization For large models, several memory optimization techniques are available: - **Gradient Checkpointing**: `gradient_checkpointing=True` - **Mixed Precision**: `fp16=True` or `bf16=True` - **Chunked Processing**: Configure `chunk_size` in `SpladePooling` - **Gradient Accumulation**: `gradient_accumulation_steps=N` **Sources:** [docs/sparse_encoder/training_overview.md:400-425](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:92-128]()",
  "### Multi-Dataset Training The system supports training on multiple datasets simultaneously with different batch sampling strategies: ```python args = SparseEncoderTrainingArguments( multi_dataset_batch_sampler=BatchSamplers.PROPORTIONAL, batch_sampler=BatchSamplers.NO_DUPLICATES, ) ``` **Sources:** [docs/sparse_encoder/training_overview.md:419-425]()",
  "### Architecture Requirements ```mermaid graph TD subgraph \"Model Architecture Requirements\" SPLADE[\"SPLADE Models\"] CSR[\"CSR Models\"] MLMTransformer[\"MLMTransformer<br/>MLM head access\"] SpladePooling[\"SpladePooling<br/>Sparse pooling\"] Autoencoder[\"Autoencoder Components<br/>encode/decode methods\"] BackboneEmb[\"sentence_embedding_backbone\"] DecodedEmb[\"decoded_embedding_k/4k/aux\"] SPLADE --> MLMTransformer SPLADE --> SpladePooling CSR --> Autoencoder CSR --> BackboneEmb CSR --> DecodedEmb end ``` Sources: [sentence_transformers/sparse_encoder/models/MLMTransformer.py:26-54](), [sentence_transformers/sparse_encoder/models/SpladePooling.py:13-39](), [sentence_transformers/sparse_encoder/losses/CSRLoss.py:68-98]()",
  "### SparseEncoder The `SparseEncoder` class generates sparse vector embeddings where most dimensions are zero, creating interpretable representations that combine neural and lexical matching signals. **Key characteristics:** - Output: Sparse vectors (vocabulary-size dimensions, ~99% zeros) - Use case: Neural lexical search, hybrid retrieval, interpretability - Similarity functions: Dot product on sparse representations - Example models: `naver/splade-cocondenser-ensembledistil` **Basic usage pattern:** ```python from sentence_transformers import SparseEncoder model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\") embeddings = model.encode(sentences) stats = SparseEncoder.sparsity(embeddings) ``` Sources: [README.md:133-167](), [sentence_transformers/__init__.py:29-34]()",
  "### SentenceTransformer The `SentenceTransformer` class produces dense vector embeddings where semantically similar texts have similar vector representations. These models use bi-encoder architectures that independently encode each input text. **Key characteristics:** - Output: Dense vectors (typically 384-1024 dimensions) - Use case: Semantic similarity, clustering, dense retrieval - Similarity functions: Cosine similarity, dot product, Euclidean distance - Example models: `all-MiniLM-L6-v2`, `all-mpnet-base-v2` **Basic usage pattern:** ```python from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = model.encode(sentences) similarities = model.similarity(embeddings, embeddings) ``` Sources: [README.md:56-87](), [sentence_transformers/__init__.py:27]()",
  "### CrossEncoder The `CrossEncoder` class performs joint encoding of text pairs to produce similarity scores, making it ideal for reranking and classification tasks where high precision is required. **Key characteristics:** - Output: Scalar similarity scores - Use case: Reranking, text pair classification, high-precision ranking - Architecture: Joint encoding (both texts processed together) - Example models: `cross-encoder/ms-marco-MiniLM-L6-v2` **Basic usage pattern:** ```python from sentence_transformers import CrossEncoder model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\") scores = model.predict([(query, passage) for passage in passages]) ranks = model.rank(query, passages, return_documents=True) ``` Sources: [README.md:89-132](), [sentence_transformers/__init__.py:15-20]()",
  "## Purpose and Scope The sentence-transformers library is a comprehensive Python framework for accessing, using, and training state-of-the-art embedding and reranker models. It provides three core model types that serve different purposes in natural language processing pipelines: `SentenceTransformer` for dense embeddings, `SparseEncoder` for sparse embeddings, and `CrossEncoder` for pairwise scoring and reranking. This document covers the high-level architecture and core concepts of the sentence-transformers library. For specific usage instructions, see [Quickstart Guide](#2.1). For detailed training procedures, see [Training](#3). For performance optimization, see [Advanced Topics](#7). Sources: [README.md:15-21](), [sentence_transformers/__init__.py:27-34]()",
  "### Training Dependencies For training workflows, additional dependencies provide enhanced functionality: ```mermaid graph LR subgraph \"Core Training\" SentenceTransformers[\"sentence-transformers[train]\"] Accelerate[\"accelerate\"] Datasets[\"datasets\"] end subgraph \"Optional Training Tools\" WandB[\"wandb<br/>(tracking)\"] CodeCarbon[\"codecarbon<br/>(emissions)\"] end subgraph \"Training Components\" SentenceTransformers --> Trainers[\"SentenceTransformerTrainer<br/>SparseEncoderTrainer<br/>CrossEncoderTrainer\"] Accelerate --> DistributedTraining[\"Distributed training\"] Datasets --> DataLoading[\"Dataset loading\"] WandB --> LogTracking[\"Training log tracking\"] CodeCarbon --> ModelCards[\"Automatic model card generation\"] end ``` Install recommended training tools: ```bash pip install wandb # For experiment tracking pip install codecarbon # For carbon emissions tracking ```",
  "### Key Evaluation Concepts | Concept | Description | Implementation | |---------|-------------|----------------| | **Primary Metric** | Main metric used for model selection | `evaluator.primary_metric` attribute | | **Greater is Better** | Whether higher scores indicate better performance | `evaluator.greater_is_better` boolean | | **CSV Logging** | Track metrics over training epochs/steps | `write_csv` parameter across evaluators | | **Model Card Integration** | Store evaluation results in model metadata | `store_metrics_in_model_card_data()` method | ```mermaid graph LR subgraph \"Base Class\" SentenceEvaluator[\"SentenceEvaluator\"] end subgraph \"Task-Specific Evaluators\" IRE[\"InformationRetrievalEvaluator\"] BCE[\"BinaryClassificationEvaluator\"] ESE[\"EmbeddingSimilarityEvaluator\"] RE[\"RerankingEvaluator\"] TE[\"TripletEvaluator\"] PME[\"ParaphraseMiningEvaluator\"] TRE[\"TranslationEvaluator\"] MSE[\"MSEEvaluator\"] LAE[\"LabelAccuracyEvaluator\"] end SentenceEvaluator --> IRE SentenceEvaluator --> BCE SentenceEvaluator --> ESE SentenceEvaluator --> RE SentenceEvaluator --> TE SentenceEvaluator --> PME SentenceEvaluator --> TRE SentenceEvaluator --> MSE SentenceEvaluator --> LAE ``` **Evaluator Class Hierarchy** Sources: [sentence_transformers/evaluation/SentenceEvaluator.py:13-121](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23-568](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:27-379]()",
  "## Model Selection Guide ```mermaid flowchart TD Start[\"Model Selection\"] --> Task{\"Task Type?\"} Task -->|\"General Purpose\"| Speed{\"Speed vs Quality?\"} Task -->|\"Semantic Search\"| Domain{\"Domain?\"} Task -->|\"Multilingual\"| Languages{\"Language Count?\"} Speed -->|\"Best Quality\"| MPNet[\"all-mpnet-base-v2\"] Speed -->|\"Balanced\"| MiniLM6[\"all-MiniLM-L6-v2\"] Speed -->|\"Fastest\"| MiniLM3[\"paraphrase-MiniLM-L3-v2\"] Domain -->|\"Web Search\"| MSMARCO[\"msmarco-distilbert-dot-v5\"] Domain -->|\"QA Diverse\"| MultiQA[\"multi-qa-mpnet-base-cos-v1\"] Domain -->|\"Scientific\"| Specter[\"allenai-specter\"] Languages -->|\"15 Major\"| DistilUSE1[\"distiluse-base-multilingual-cased-v1\"] Languages -->|\"50+\"| DistilUSE2[\"distiluse-base-multilingual-cased-v2\"] Languages -->|\"Translation\"| LaBSE[\"LaBSE\"] ```",
  "ranks = model.rank(query, passages, return_documents=True) print(\"Query:\", query) for rank in ranks: print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")",
  "### Mini-Batch Processing Large document lists are processed in mini-batches to manage memory usage: ```python mini_batch_size = self.mini_batch_size or batch_size if mini_batch_size <= 0: mini_batch_size = len(pairs) for i in range(0, len(pairs), mini_batch_size): mini_batch_pairs = pairs[i : i + mini_batch_size] # Process mini-batch... ```",
  "### Trainer Integration Model card callbacks are automatically added during trainer initialization: **Callback Registration**: The `SentenceTransformerModelCardCallback` is automatically integrated during trainer initialization and responds to training lifecycle events. **Hyperparameter Filtering** ([sentence_transformers/model_card.py:97-129]()): The system tracks only meaningful hyperparameters, filtering out logging and infrastructure settings like `output_dir`, `logging_dir`, `eval_steps`, etc.",
  "### Version and Citation Management The system automatically manages framework versions and academic citations: **Version Tracking** ([sentence_transformers/model_card.py:217-236]()): - Python, sentence-transformers, transformers, PyTorch versions - Optional accelerate, datasets, tokenizers versions **Citation Generation** ([sentence_transformers/model_card.py:411-440]()): - Loss function citations from `loss.citation` attributes - Automatic deduplication of identical citations - BibTeX formatting for academic references",
  "### Carbon Emissions Tracking Integration with CodeCarbon for environmental impact measurement: **Automatic Detection** ([sentence_transformers/model_card.py:63-68]()): ```python callbacks = [callback for callback in trainer.callback_handler.callbacks if isinstance(callback, CodeCarbonCallback)] if callbacks: model.model_card_data.code_carbon_callback = callbacks[0] ``` Sources: [sentence_transformers/trainer.py:315-333](), [sentence_transformers/model_card.py:47-192]()",
  "## 8. Performance Considerations When evaluating large datasets, consider: - Batch processing: Evaluate models in batches to avoid memory issues - Caching: Cache model outputs to avoid redundant computation - Metrics selection: Choose metrics appropriate for your task and dataset size Efficient evaluation is especially important when working with resource-intensive models or large test sets.",
  "## 9. Comparison with SentenceTransformer Evaluators While both types of evaluators assess model performance, they differ in key ways: | CrossEncoder Evaluators | SentenceTransformer Evaluators | |------------------------|--------------------------------| | Evaluate pair scoring | Evaluate embedding quality | | Focus on classification/ranking metrics | Focus on similarity and retrieval metrics | | Work with direct text pair inputs | Work with embeddings | | Suited for reranking tasks | Suited for retrieval and similarity tasks | Understanding these differences helps in selecting the appropriate evaluation method for your model type and task.",
  "### Typical Model Selection | Task | Model Type | Example Model | Key Method | |------|------------|---------------|------------| | Semantic similarity | `SentenceTransformer` | `all-mpnet-base-v2` | `encode()` | | Vector database search | `SentenceTransformer` | `all-MiniLM-L6-v2` | `encode()` | | Lexical + semantic search | `SparseEncoder` | `naver/splade-cocondenser-ensembledistil` | `encode()` | | Reranking search results | `CrossEncoder` | `cross-encoder/ms-marco-MiniLM-L6-v2` | `rank()` | | Text pair classification | `CrossEncoder` | `cross-encoder/nli-MiniLM2-L6-H768` | `predict()` |",
  "## Test Framework Overview The sentence-transformers library uses a comprehensive pytest-based testing framework that validates functionality across all three core model types: SentenceTransformer, SparseEncoder, and CrossEncoder. ```mermaid graph TB subgraph \"Test Structure\" TestRoot[\"tests/\"] MainTests[\"tests/*.py\"] CrossTests[\"tests/cross_encoder/\"] SparseTests[\"tests/sparse_encoder/\"] end subgraph \"Test Categories\" UnitTests[\"Unit Tests\"] IntegrationTests[\"Integration Tests\"] SlowTests[\"Slow Tests (@pytest.mark.slow)\"] ModelTests[\"Pretrained Model Tests\"] TrainingTests[\"Training Tests\"] end subgraph \"Test Configuration\" MainConf[\"tests/conftest.py\"] CrossConf[\"tests/cross_encoder/conftest.py\"] SparseConf[\"tests/sparse_encoder/conftest.py\"] end TestRoot --> MainTests TestRoot --> CrossTests TestRoot --> SparseTests MainConf --> UnitTests MainConf --> IntegrationTests CrossConf --> CrossTests SparseConf --> SparseTests SlowTests --> ModelTests SlowTests --> TrainingTests ``` Sources: [tests/conftest.py:1-115](), [tests/cross_encoder/conftest.py:1-23](), [tests/sparse_encoder/conftest.py:1-46]()",
  "### Model Loading Strategy ```mermaid graph LR subgraph \"Session Fixtures\" SessionModel[\"_stsb_bert_tiny_model<br/>(session scope)\"] SessionSparse[\"_splade_bert_tiny_model<br/>(session scope)\"] end subgraph \"Function Fixtures\" FuncModel[\"stsb_bert_tiny_model<br/>(function scope)\"] FuncSparse[\"splade_bert_tiny_model<br/>(function scope)\"] end SessionModel --> |\"deepcopy()\"| FuncModel SessionSparse --> |\"deepcopy()\"| FuncSparse FuncModel --> TestCase1[\"test_encode()\"] FuncModel --> TestCase2[\"test_similarity()\"] FuncSparse --> TestCase3[\"test_sparse_encode()\"] ``` The testing framework uses session-scoped fixtures to load models once per test session, then creates function-scoped copies using `deepcopy()` to ensure test isolation without the overhead of repeatedly loading models. Sources: [tests/conftest.py:19-40](), [tests/sparse_encoder/conftest.py:10-31](), [tests/cross_encoder/conftest.py:15-22]()",
  "### Configuration and Extensibility Each evaluator implements a `get_config_dict()` method for serializing evaluation configuration: ```mermaid graph TD subgraph \"Configuration Parameters\" TRUNC[\"truncate_dim\"] PREC[\"precision\"] MARGIN[\"margin (TripletEvaluator)\"] SIMFNS[\"similarity_fn_names\"] end subgraph \"Config Dict Generation\" GETCONFIG[\"get_config_dict()\"] CONFIGOUT[\"serializable configuration\"] end subgraph \"Model Card Integration\" MODELCARD[\"store_metrics_in_model_card_data()\"] METADATA[\"evaluation metadata\"] end TRUNC --> GETCONFIG PREC --> GETCONFIG MARGIN --> GETCONFIG SIMFNS --> GETCONFIG GETCONFIG --> CONFIGOUT CONFIGOUT --> MODELCARD MODELCARD --> METADATA ``` **Sources:** [sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:265-271](), [sentence_transformers/evaluation/BinaryClassificationEvaluator.py:374-378](), [sentence_transformers/evaluation/TripletEvaluator.py:264-270]()",
  "#### Sparse Module Pipeline ```mermaid graph LR Text[\"Input Text\"] --> MLMTrans[\"MLMTransformer<br/>auto_model + MLM head\"] MLMTrans --> TokenLogits[\"Token Logits<br/>[batch, seq_len, vocab_size]\"] TokenLogits --> SpladePool[\"SpladePooling<br/>pooling_strategy + activation\"] SpladePool --> SparseEmb[\"Sparse Embedding<br/>[batch, vocab_size]\"] subgraph SparseModules[\"Sparse Module Types\"] MLMTrans SpladePool end ```",
  "### Installation Options Overview The library provides five main installation configurations that correspond to different usage patterns: ```mermaid graph TB subgraph \"Installation Options\" Default[\"Default<br/>Basic inference\"] ONNX[\"ONNX<br/>Optimized inference\"] OpenVINO[\"OpenVINO<br/>Intel optimization\"] Training[\"Default + Training<br/>Model training\"] Development[\"Development<br/>Contributing\"] end subgraph \"Core Capabilities\" Default --> LoadSave[\"Model loading/saving\"] Default --> Inference[\"Embedding generation\"] ONNX --> ONNXOpt[\"ONNX optimization\"] ONNX --> Quantization[\"Model quantization\"] OpenVINO --> IntelOpt[\"Intel hardware optimization\"] Training --> TrainLoop[\"Training loops\"] Training --> Evaluation[\"Model evaluation\"] Development --> Testing[\"Unit testing\"] Development --> Linting[\"Code formatting\"] end subgraph \"Backend Support\" LoadSave --> PyTorchBackend[\"PyTorch backend\"] ONNXOpt --> ONNXBackend[\"ONNX Runtime backend\"] IntelOpt --> OpenVINOBackend[\"OpenVINO backend\"] end subgraph \"Model Types\" PyTorchBackend --> SentenceTransformer[\"SentenceTransformer\"] PyTorchBackend --> SparseEncoder[\"SparseEncoder\"] PyTorchBackend --> CrossEncoder[\"CrossEncoder\"] ONNXBackend --> OptimizedModels[\"Optimized models\"] OpenVINOBackend --> IntelModels[\"Intel-optimized models\"] end ``` **Sources:** [docs/installation.md:3-8]()",
  "### pip Installation The recommended installation method uses pip with specific extras for different use cases: | Installation Type | Command | Use Case | |------------------|---------|----------| | Default | `pip install -U sentence-transformers` | Basic inference only | | ONNX (GPU+CPU) | `pip install -U \"sentence-transformers[onnx-gpu]\"` | Optimized inference with GPU support | | ONNX (CPU only) | `pip install -U \"sentence-transformers[onnx]\"` | CPU-only optimized inference | | OpenVINO | `pip install -U \"sentence-transformers[openvino]\"` | Intel hardware optimization | | Training | `pip install -U \"sentence-transformers[train]\"` | Model training capabilities | | Development | `pip install -U \"sentence-transformers[dev]\"` | Full development environment |",
  "#### Sparse Module Types | Module | Purpose | Output Shape | |--------|---------|--------------| | `MLMTransformer` | Contextual token predictions | `[batch, seq_len, vocab_size]` | | `SpladePooling` | Aggregate tokens to sparse vector | `[batch, vocab_size]` | | `SparseStaticEmbedding` | Pre-computed static weights | `[batch, vocab_size]` | | `SparseAutoEncoder` | Learned sparse representations | `[batch, latent_dim]` | Key configuration: - `pooling_strategy`: `\"max\"`, `\"mean\"`, `\"sum\"` - `activation_function`: `\"relu\"`, `\"log1p_relu\"`, `\"identity\"` - `k`: Number of top-k active dimensions (for `SparseAutoEncoder`) Sources: [sentence_transformers/sparse_encoder/models/MLMTransformer.py](), [sentence_transformers/sparse_encoder/models/SpladePooling.py](), [sentence_transformers/sparse_encoder/models/SparseStaticEmbedding.py](), [sentence_transformers/sparse_encoder/models/SparseAutoEncoder.py]()",
  "## Overview The NanoBEIR evaluation system enables quick assessment of model performance across multiple information retrieval tasks using significantly smaller datasets compared to the full BEIR benchmark. The system supports both dense embedding models (`SentenceTransformer`) and sparse embedding models (`SparseEncoder`), providing the same metrics as standard IR evaluation but aggregated across multiple datasets. The core evaluators are `NanoBEIREvaluator` for dense models and `SparseNanoBEIREvaluator` for sparse models, both extending the functionality of `InformationRetrievalEvaluator` to handle multiple datasets efficiently. Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:72-79](), [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:26-35]()",
  "### Evaluator Class Hierarchy ```mermaid graph TD SentenceEvaluator[\"SentenceEvaluator<br/>__call__(), primary_metric\"] InformationRetrievalEvaluator[\"InformationRetrievalEvaluator<br/>compute_metrices(), embed_inputs()\"] SparseInformationRetrievalEvaluator[\"SparseInformationRetrievalEvaluator<br/>+ sparsity_stats, max_active_dims\"] NanoBEIREvaluator[\"NanoBEIREvaluator<br/>_load_dataset(), aggregate_fn\"] SparseNanoBEIREvaluator[\"SparseNanoBEIREvaluator<br/>information_retrieval_class\"] SentenceEvaluator --> InformationRetrievalEvaluator SentenceEvaluator --> NanoBEIREvaluator InformationRetrievalEvaluator --> SparseInformationRetrievalEvaluator NanoBEIREvaluator --> SparseNanoBEIREvaluator SparseNanoBEIREvaluator -.->|\"information_retrieval_class = <br/>SparseInformationRetrievalEvaluator\"| SparseInformationRetrievalEvaluator NanoBEIREvaluator -.->|\"information_retrieval_class = <br/>InformationRetrievalEvaluator\"| InformationRetrievalEvaluator ``` Sources: [sentence_transformers/evaluation/NanoBEIREvaluator.py:191](), [sentence_transformers/sparse_encoder/evaluation/SparseNanoBEIREvaluator.py:157](), [sentence_transformers/evaluation/InformationRetrievalEvaluator.py:23](), [sentence_transformers/sparse_encoder/evaluation/SparseInformationRetrievalEvaluator.py:23]()",
  "### Training Validation Tests The `test_train_stsb.py` module validates training functionality across different scenarios: ```mermaid graph TD subgraph \"Training Test Types\" STSBTrain[\"STSB Training Tests\"] NLITrain[\"NLI Training Tests\"] SlowTrain[\"Slow Training Tests\"] FastTrain[\"Fast Training Tests\"] end subgraph \"Test Conditions\" FullDataset[\"Full Dataset (slow)\"] SubsetDataset[\"Subset Dataset (fast)\"] CI_Skip[\"CI Environment Skip\"] end STSBTrain --> FullDataset STSBTrain --> SubsetDataset NLITrain --> FullDataset NLITrain --> SubsetDataset SlowTrain --> CI_Skip FastTrain --> SubsetDataset ``` Sources: [tests/test_train_stsb.py:82-187]()",
  "### Test Execution Control Tests use pytest markers to control execution: - `@pytest.mark.slow`: Comprehensive tests with full datasets - `@pytest.mark.skipif(\"CI\" in os.environ)`: Skip resource-intensive tests in CI - `@pytest.mark.parametrize`: Run same test with multiple parameter sets",
  "## Model Discovery and Metadata The interactive model browser provides comprehensive model information: ```mermaid graph TB subgraph \"Model Browser Interface\" HTML[\"models_en_sentence_embeddings.html\"] --> Filters[\"Performance Filters\"] HTML --> Sort[\"Sortable Columns\"] HTML --> Details[\"Expandable Details\"] end subgraph \"Model Metadata\" Name[\"Model Name\"] --> HFHub[\"Hugging Face Hub\"] Perf[\"Performance Metrics\"] --> Benchmarks[\"14 Sentence Tasks<br/>6 Search Tasks\"] Speed[\"Encoding Speed\"] --> Hardware[\"V100 GPU Benchmarks\"] Size[\"Model Size\"] --> Storage[\"MB Requirements\"] end subgraph \"Selection Criteria\" Task[\"Task Requirements\"] --> Filter1[\"Performance Threshold\"] Hardware[\"Hardware Constraints\"] --> Filter2[\"Speed/Size Limits\"] Quality[\"Quality Needs\"] --> Filter3[\"Metric Requirements\"] end ``` The browser enables filtering by performance, speed, and size to find optimal models for specific requirements. Sources: [docs/_static/html/models_en_sentence_embeddings.html:106-228](), [docs/sentence_transformer/pretrained_models.md:41-49]()"
]