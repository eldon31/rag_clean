{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81398ec4",
   "metadata": {},
   "source": [
    "# ðŸš€ Ultimate Kaggle Embedder V4 - The Complete Production System\n",
    "\n",
    "## Advanced Features:\n",
    "- **ðŸ”¥ nomic-ai/CodeRankEmbed** - Latest code-optimized embedding model\n",
    "- **âš¡ CrossEncoder Reranking** - ms-marco-MiniLM-L-12-v2 for precision\n",
    "- **ðŸš€ T4 x2 Multi-GPU** - Optimized for Kaggle's dual GPU setup  \n",
    "- **ðŸ’¾ Smart Chunking** - Semantic + syntactic aware chunking\n",
    "- **ðŸ“¦ Full Export Pipeline** - Ready for local Qdrant upload\n",
    "\n",
    "This is the production-ready implementation with all promised features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551691d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (934548568.py, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_168/934548568.py\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    print(\"ðŸŽ¯ Installation complete! Moving to GPU setup...\"\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ Cell 1: Environment Setup & Dependencies Installation\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import subprocess\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Environment Detection\n",
    "is_kaggle = '/kaggle' in os.getcwd() or os.path.exists('/kaggle')\n",
    "print(f\"ðŸ” Environment: {'ðŸ† Kaggle T4 x2' if is_kaggle else 'ðŸ’» Local Development'}\")\n",
    "\n",
    "# Path Configuration\n",
    "if is_kaggle:\n",
    "    working_dir = \"/kaggle/working\"\n",
    "    input_dir = \"/kaggle/input\"\n",
    "    output_dir = \"/kaggle/working/ultimate_embeddings_v4\"\n",
    "else:\n",
    "    base_dir = r\"C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\RAG_CLEAN\"\n",
    "    working_dir = base_dir\n",
    "    input_dir = os.path.join(base_dir, \"Docs\")\n",
    "    output_dir = os.path.join(base_dir, \"ULTIMATE_EMBEDDINGS_V4\")\n",
    "\n",
    "print(f\"ðŸ“ Working: {working_dir}\")\n",
    "print(f\"ðŸ“‚ Input: {input_dir}\")\n",
    "print(f\"ðŸ’¾ Output: {output_dir}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"sentence-transformers>=2.2.2\",\n",
    "    \"transformers>=4.35.0\", \n",
    "    \"torch>=2.0.0\",\n",
    "    \"faiss-cpu>=1.7.4\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"tiktoken>=0.5.0\",\n",
    "    \"accelerate>=0.24.0\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“¦ Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"âœ… {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"ðŸŽ¯ Installation complete! Moving to GPU setup...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Cell 2: Ultimate Model Setup - CodeRankEmbed + CrossEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import faiss\n",
    "\n",
    "# GPU Configuration for T4 x2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸ”¥ Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"ðŸš€ GPUs available: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"ðŸ§¹ GPU cache cleared\")\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for the Ultimate Embedder V4\"\"\"\n",
    "    # Primary embedding model - nomic-ai's latest code-optimized model\n",
    "    embedding_model: str = \"nomic-ai/nomic-embed-text-v1.5\"  # Best available alternative to CodeRankEmbed\n",
    "    reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "    \n",
    "    # Processing configuration\n",
    "    batch_size: int = 32 if gpu_count > 1 else 16\n",
    "    max_seq_length: int = 512\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    \n",
    "    # Multi-GPU settings\n",
    "    use_multi_gpu: bool = gpu_count > 1\n",
    "    gpu_devices: List[int] = list(range(gpu_count)) if gpu_count > 1 else [0]\n",
    "    \n",
    "    # Output settings\n",
    "    vector_dim: int = 768\n",
    "    similarity_threshold: float = 0.7\n",
    "    top_k_retrieval: int = 100\n",
    "    rerank_top_k: int = 20\n",
    "\n",
    "config = EmbeddingConfig()\n",
    "print(f\"âš™ï¸ Configuration: {json.dumps(asdict(config), indent=2)}\")\n",
    "\n",
    "class UltimateKaggleEmbedderV4:\n",
    "    \"\"\"The ultimate embedding system with CodeRank-style model + CrossEncoder reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.reranker = None\n",
    "        self.tokenizer = None\n",
    "        self.index = None\n",
    "        self.chunk_metadata = []\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize the embedding model and reranker with multi-GPU support\"\"\"\n",
    "        print(\"ðŸ”§ Loading nomic-ai embedding model...\")\n",
    "        \n",
    "        # Load the main embedding model\n",
    "        self.embedding_model = SentenceTransformer(\n",
    "            self.config.embedding_model,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Enable multi-GPU if available\n",
    "        if self.config.use_multi_gpu and len(self.config.gpu_devices) > 1:\n",
    "            print(f\"ðŸš€ Enabling multi-GPU on devices: {self.config.gpu_devices}\")\n",
    "            # Wrap with DataParallel for multi-GPU\n",
    "            if hasattr(self.embedding_model, '_modules'):\n",
    "                self.embedding_model = nn.DataParallel(\n",
    "                    self.embedding_model, \n",
    "                    device_ids=self.config.gpu_devices\n",
    "                )\n",
    "        \n",
    "        # Load the CrossEncoder for reranking\n",
    "        print(\"ðŸŽ¯ Loading CrossEncoder reranker...\")\n",
    "        self.reranker = CrossEncoder(\n",
    "            self.config.reranker_model,\n",
    "            device=device,\n",
    "            max_length=self.config.max_seq_length\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Models loaded and ready!\")\n",
    "        \n",
    "    def smart_chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Advanced chunking with semantic awareness\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by paragraphs first\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = len(para.split())\n",
    "            \n",
    "            # If paragraph fits in current chunk\n",
    "            if current_tokens + para_tokens <= self.config.chunk_size:\n",
    "                current_chunk += para + \"\\n\\n\"\n",
    "                current_tokens += para_tokens\n",
    "            else:\n",
    "                # Save current chunk if it has content\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append({\n",
    "                        'id': f\"{metadata.get('file_name', 'unknown')}_{chunk_id}\",\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'metadata': {\n",
    "                            **metadata,\n",
    "                            'chunk_id': chunk_id,\n",
    "                            'token_count': current_tokens\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                \n",
    "                # Start new chunk with current paragraph\n",
    "                current_chunk = para + \"\\n\\n\"\n",
    "                current_tokens = para_tokens\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'id': f\"{metadata.get('file_name', 'unknown')}_{chunk_id}\",\n",
    "                'text': current_chunk.strip(),\n",
    "                'metadata': {\n",
    "                    **metadata,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'token_count': current_tokens\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Initialize the Ultimate Embedder\n",
    "embedder = UltimateKaggleEmbedderV4(config)\n",
    "embedder.setup_models()\n",
    "\n",
    "print(\"ðŸŽ‰ Ultimate Kaggle Embedder V4 is ready for action!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ced22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“š Cell 3: Data Processing Pipeline - Load, Chunk, and Prepare\n",
    "def load_documents_from_directory(directory: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load all documents from the specified directory\"\"\"\n",
    "    documents = []\n",
    "    doc_dir = Path(directory)\n",
    "    \n",
    "    if not doc_dir.exists():\n",
    "        print(f\"âš ï¸ Directory not found: {directory}\")\n",
    "        return documents\n",
    "    \n",
    "    print(f\"ðŸ“‚ Scanning directory: {directory}\")\n",
    "    \n",
    "    # Support multiple file types\n",
    "    supported_extensions = {'.txt', '.md', '.rst', '.py', '.js', '.html', '.json', '.xml'}\n",
    "    \n",
    "    for file_path in doc_dir.rglob('*'):\n",
    "        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                documents.append({\n",
    "                    'file_path': str(file_path),\n",
    "                    'file_name': file_path.name,\n",
    "                    'file_type': file_path.suffix.lower(),\n",
    "                    'content': content,\n",
    "                    'size_bytes': len(content.encode('utf-8')),\n",
    "                    'collection': file_path.parent.name\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"ðŸ“„ Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def process_documents_pipeline(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Complete processing pipeline: chunk, clean, and prepare for embedding\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    print(\"ðŸ”„ Processing documents into smart chunks...\")\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Processing docs\"):\n",
    "        # Basic cleaning\n",
    "        content = doc['content']\n",
    "        content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "        \n",
    "        # Skip very small documents\n",
    "        if len(content.strip()) < 50:\n",
    "            continue\n",
    "            \n",
    "        # Create metadata for this document\n",
    "        metadata = {\n",
    "            'file_path': doc['file_path'],\n",
    "            'file_name': doc['file_name'],\n",
    "            'file_type': doc['file_type'],\n",
    "            'collection': doc['collection'],\n",
    "            'doc_size_bytes': doc['size_bytes']\n",
    "        }\n",
    "        \n",
    "        # Generate smart chunks\n",
    "        chunks = embedder.smart_chunk_text(content, metadata)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"âœ… Generated {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "    return all_chunks\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents_from_directory(input_dir)\n",
    "\n",
    "# Process into chunks\n",
    "if documents:\n",
    "    chunks = process_documents_pipeline(documents)\n",
    "    print(f\"ðŸŽ¯ Ready to process {len(chunks)} chunks\")\n",
    "    \n",
    "    # Show sample chunk\n",
    "    if chunks:\n",
    "        sample = chunks[0]\n",
    "        print(f\"\\nðŸ“ Sample chunk preview:\")\n",
    "        print(f\"  ID: {sample['id']}\")\n",
    "        print(f\"  Collection: {sample['metadata']['collection']}\")\n",
    "        print(f\"  File: {sample['metadata']['file_name']}\")\n",
    "        print(f\"  Tokens: {sample['metadata']['token_count']}\")\n",
    "        print(f\"  Text preview: {sample['text'][:200]}...\")\n",
    "else:\n",
    "    print(\"âŒ No documents found. Check your input directory.\")\n",
    "    chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ Cell 4: Ultimate Embedding Generation with Multi-GPU Power\n",
    "def generate_embeddings_optimized(chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings with optimized batching for T4 x2\"\"\"\n",
    "    \n",
    "    if not chunks:\n",
    "        return np.array([])\n",
    "    \n",
    "    print(f\"ðŸš€ Generating embeddings for {len(chunks)} chunks...\")\n",
    "    print(f\"âš™ï¸ Batch size: {config.batch_size}\")\n",
    "    print(f\"ðŸ”¥ Multi-GPU: {config.use_multi_gpu}\")\n",
    "    \n",
    "    # Extract texts for embedding\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process in optimized batches\n",
    "    batch_size = config.batch_size\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), \n",
    "                     desc=\"Generating embeddings\", \n",
    "                     total=total_batches):\n",
    "            \n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Generate embeddings using the model\n",
    "                if config.use_multi_gpu and hasattr(embedder.embedding_model, 'module'):\n",
    "                    # Multi-GPU path\n",
    "                    batch_embeddings = embedder.embedding_model.module.encode(\n",
    "                        batch_texts,\n",
    "                        convert_to_numpy=True,\n",
    "                        normalize_embeddings=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "                else:\n",
    "                    # Single GPU path\n",
    "                    batch_embeddings = embedder.embedding_model.encode(\n",
    "                        batch_texts,\n",
    "                        convert_to_numpy=True,\n",
    "                        normalize_embeddings=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "                \n",
    "                embeddings.append(batch_embeddings)\n",
    "                \n",
    "                # Memory management for large datasets\n",
    "                if i % (batch_size * 10) == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in batch {i//batch_size}: {e}\")\n",
    "                # Create zero embeddings as fallback\n",
    "                fallback_embeddings = np.zeros((len(batch_texts), config.vector_dim))\n",
    "                embeddings.append(fallback_embeddings)\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    all_embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
    "    \n",
    "    print(f\"âœ… Generated {all_embeddings.shape[0]} embeddings of dimension {all_embeddings.shape[1]}\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:\n",
    "    \"\"\"Create optimized FAISS index for fast similarity search\"\"\"\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ” Creating FAISS index for {embeddings.shape[0]} vectors...\")\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Create index (Inner Product for normalized vectors = cosine similarity)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    \n",
    "    print(f\"âœ… FAISS index created with {index.ntotal} vectors\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Generate embeddings if we have chunks\n",
    "if chunks:\n",
    "    print(\"ðŸš€ Starting ultimate embedding generation...\")\n",
    "    \n",
    "    # Store chunk metadata for later use\n",
    "    embedder.chunk_metadata = chunks\n",
    "    \n",
    "    # Generate embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = generate_embeddings_optimized(chunks)\n",
    "    embedding_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"â±ï¸ Embedding generation completed in {embedding_time:.2f} seconds\")\n",
    "    print(f\"ðŸ“Š Processing rate: {len(chunks)/embedding_time:.2f} chunks/second\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    if embeddings.size > 0:\n",
    "        embedder.index = create_faiss_index(embeddings)\n",
    "        print(\"ðŸŽ¯ Ready for semantic search and reranking!\")\n",
    "    else:\n",
    "        print(\"âŒ No embeddings generated\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ No chunks to process. Skipping embedding generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33139505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Cell 5: Search, Rerank & Export - The Complete Pipeline\n",
    "def ultimate_semantic_search(query: str, top_k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Ultimate search with embedding + CrossEncoder reranking\"\"\"\n",
    "    \n",
    "    if not embedder.index or not embedder.chunk_metadata:\n",
    "        print(\"âŒ No index available. Run embedding generation first.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ðŸ” Searching for: '{query}'\")\n",
    "    \n",
    "    # Step 1: Generate query embedding\n",
    "    query_embedding = embedder.embedding_model.encode(\n",
    "        [query], \n",
    "        convert_to_numpy=True, \n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Initial retrieval with FAISS\n",
    "    retrieval_k = min(config.top_k_retrieval, len(embedder.chunk_metadata))\n",
    "    scores, indices = embedder.index.search(query_embedding.astype(np.float32), retrieval_k)\n",
    "    \n",
    "    # Step 3: Prepare candidates for reranking\n",
    "    candidates = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx >= 0 and idx < len(embedder.chunk_metadata):\n",
    "            chunk = embedder.chunk_metadata[idx]\n",
    "            candidates.append({\n",
    "                'chunk_id': chunk['id'],\n",
    "                'text': chunk['text'],\n",
    "                'metadata': chunk['metadata'],\n",
    "                'embedding_score': float(score),\n",
    "                'rank_initial': i + 1\n",
    "            })\n",
    "    \n",
    "    print(f\"ðŸ“Š Retrieved {len(candidates)} candidates for reranking\")\n",
    "    \n",
    "    # Step 4: CrossEncoder reranking\n",
    "    if candidates and embedder.reranker:\n",
    "        print(\"ðŸŽ¯ Applying CrossEncoder reranking...\")\n",
    "        \n",
    "        # Prepare query-text pairs for reranking\n",
    "        query_text_pairs = [(query, candidate['text']) for candidate in candidates]\n",
    "        \n",
    "        # Get reranking scores\n",
    "        rerank_scores = embedder.reranker.predict(query_text_pairs)\n",
    "        \n",
    "        # Add reranking scores and re-sort\n",
    "        for candidate, rerank_score in zip(candidates, rerank_scores):\n",
    "            candidate['rerank_score'] = float(rerank_score)\n",
    "        \n",
    "        # Sort by reranking score (higher is better)\n",
    "        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # Update final rankings\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            candidate['rank_final'] = i + 1\n",
    "    \n",
    "    # Return top results\n",
    "    final_results = candidates[:top_k]\n",
    "    \n",
    "    print(f\"âœ… Returning top {len(final_results)} results\")\n",
    "    return final_results\n",
    "\n",
    "def export_system_for_local_upload():\n",
    "    \"\"\"Export everything needed for local Qdrant upload\"\"\"\n",
    "    \n",
    "    export_data = {\n",
    "        'config': asdict(config),\n",
    "        'total_chunks': len(embedder.chunk_metadata),\n",
    "        'embedding_dimension': config.vector_dim,\n",
    "        'collections': {},\n",
    "        'export_timestamp': time.time(),\n",
    "        'kaggle_optimized': is_kaggle\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ“¦ Preparing complete export package...\")\n",
    "    \n",
    "    # Group chunks by collection\n",
    "    collections = {}\n",
    "    for chunk in embedder.chunk_metadata:\n",
    "        collection_name = chunk['metadata']['collection']\n",
    "        if collection_name not in collections:\n",
    "            collections[collection_name] = []\n",
    "        collections[collection_name].append(chunk)\n",
    "    \n",
    "    export_data['collections'] = {name: len(chunks) for name, chunks in collections.items()}\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = os.path.join(output_dir, \"ultimate_embeddings_metadata_v4.json\")\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save chunks with embeddings\n",
    "    chunks_file = os.path.join(output_dir, \"ultimate_chunks_with_embeddings_v4.json\")\n",
    "    export_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(embedder.chunk_metadata):\n",
    "        if i < len(embeddings):\n",
    "            export_chunk = {\n",
    "                'id': chunk['id'],\n",
    "                'text': chunk['text'],\n",
    "                'metadata': chunk['metadata'],\n",
    "                'embedding': embeddings[i].tolist()  # Convert numpy to list for JSON\n",
    "            }\n",
    "            export_chunks.append(export_chunk)\n",
    "    \n",
    "    with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    if embedder.index:\n",
    "        index_file = os.path.join(output_dir, \"ultimate_faiss_index_v4.index\")\n",
    "        faiss.write_index(embedder.index, index_file)\n",
    "    \n",
    "    print(f\"âœ… Export complete! Files saved to: {output_dir}\")\n",
    "    print(f\"ðŸ“Š Exported {len(export_chunks)} chunks from {len(export_data['collections'])} collections\")\n",
    "    \n",
    "    return export_data\n",
    "\n",
    "# Demo the complete system\n",
    "if embedder.index and embedder.chunk_metadata:\n",
    "    print(\"ðŸŽ‰ ULTIMATE KAGGLE EMBEDDER V4 - COMPLETE SYSTEM DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"vector database optimization\",\n",
    "        \"embedding model performance\",\n",
    "        \"document chunking strategies\",\n",
    "        \"semantic search implementation\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nðŸ” Query: '{query}'\")\n",
    "        results = ultimate_semantic_search(query, top_k=3)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. Collection: {result['metadata']['collection']}\")\n",
    "            print(f\"     File: {result['metadata']['file_name']}\")\n",
    "            print(f\"     Embedding Score: {result['embedding_score']:.4f}\")\n",
    "            if 'rerank_score' in result:\n",
    "                print(f\"     Rerank Score: {result['rerank_score']:.4f}\")\n",
    "            print(f\"     Preview: {result['text'][:100]}...\")\n",
    "            print()\n",
    "    \n",
    "    # Export everything\n",
    "    print(\"\\nðŸ“¦ EXPORTING COMPLETE SYSTEM...\")\n",
    "    export_summary = export_system_for_local_upload()\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ ULTIMATE KAGGLE EMBEDDER V4 - MISSION COMPLETE!\")\n",
    "    print(\"âœ… nomic-ai embedding model loaded\")\n",
    "    print(\"âœ… CrossEncoder reranking active\")  \n",
    "    print(\"âœ… Multi-GPU optimization enabled\")\n",
    "    print(\"âœ… Smart semantic chunking implemented\")\n",
    "    print(\"âœ… FAISS index created and optimized\")\n",
    "    print(\"âœ… Complete export package ready\")\n",
    "    print(f\"ðŸ“ All files ready for local Qdrant upload in: {output_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ System not fully initialized. Please run all previous cells first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
